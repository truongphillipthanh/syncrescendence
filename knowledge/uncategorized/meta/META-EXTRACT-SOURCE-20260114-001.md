# Extraction: SOURCE-20260114-001

**Source**: `SOURCE-20260114-x-article-pat_grady_sonya_huang-2026_this_is_agi_gradypb.md`
**Atoms extracted**: 18
**Categories**: analogy, claim, concept, framework, prediction

---

## Analogy (1)

### ATOM-SOURCE-20260114-001-0007
**Lines**: 84-87
**Context**: anecdote / evidence
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.30, actionability=0.80, epistemic_stability=0.60

> The process of an agent navigating ambiguity to accomplish a goal, forming hypotheses, testing them, hitting dead ends, and pivoting, is analogous to how a great human recruiter operates, but done tirelessly and without explicit instruction.

## Claim (9)

### ATOM-SOURCE-20260114-001-0001
**Lines**: 18-18
**Context**: consensus / claim
**Tension**: novelty=0.80, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.70, actionability=0.30, epistemic_stability=0.20

> AGI is here, now.

### ATOM-SOURCE-20260114-001-0005
**Lines**: 48-51
**Context**: consensus / evidence
**Tension**: novelty=0.40, consensus_pressure=0.70, contradiction_load=0.10, speculation_risk=0.20, actionability=0.50, epistemic_stability=0.80

> The original ChatGPT moment in 2022 was fueled by knowledge/pre-training. Reasoning/inference-time compute came with the release of o1 in late 2024. Iteration/long-horizon agents crossed a capability threshold in recent weeks with Claude Code and other coding agents.

### ATOM-SOURCE-20260114-001-0006
**Lines**: 53-54
**Context**: consensus / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.60, actionability=0.70, epistemic_stability=0.40

> Generally intelligent agents can work autonomously for hours, making and fixing mistakes, and figuring out what to do next without explicit instructions.

### ATOM-SOURCE-20260114-001-0008
**Lines**: 89-90
**Context**: consensus / limitation
**Tension**: novelty=0.30, consensus_pressure=0.80, contradiction_load=0.10, speculation_risk=0.20, actionability=0.40, epistemic_stability=0.70

> Agents still fail by hallucinating, losing context, and pursuing incorrect paths, but their trajectory shows unmistakable progress and failures are increasingly fixable.

### ATOM-SOURCE-20260114-001-0009
**Lines**: 93-94
**Context**: consensus / claim
**Tension**: novelty=0.60, consensus_pressure=0.50, contradiction_load=0.10, speculation_risk=0.40, actionability=0.70, epistemic_stability=0.60

> Long-horizon agents advance the reasoning model paradigm by enabling models to take actions and iterate over time.

### ATOM-SOURCE-20260114-001-0011
**Lines**: 98-100
**Context**: consensus / evidence
**Tension**: novelty=0.10, consensus_pressure=0.80, contradiction_load=0.10, speculation_risk=0.10, actionability=0.20, epistemic_stability=0.80

> The AI applications of 2023 and 2024 were primarily "talkers" or sophisticated conversationalists with limited impact.

### ATOM-SOURCE-20260114-001-0012
**Lines**: 102-105
**Context**: consensus / claim
**Tension**: novelty=0.40, consensus_pressure=0.70, contradiction_load=0.10, speculation_risk=0.20, actionability=0.60, epistemic_stability=0.80

> Scaling reinforcement learning is the domain of research labs, while designing agent harnesses is the domain of the application layer, with products like Manus, Claude Code, and Factory's Droids known for their engineered harnesses.

### ATOM-SOURCE-20260114-001-0016
**Lines**: 109-110
**Context**: consensus / claim
**Tension**: novelty=0.40, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.30, actionability=0.30, epistemic_stability=0.70

> The capabilities of long-horizon agents are significantly different from a single forward pass of a model.

### ATOM-SOURCE-20260114-001-0018
**Lines**: 127-128
**Context**: speculation / claim
**Tension**: novelty=0.70, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.80, actionability=0.50, epistemic_stability=0.20

> The ambitious version of a roadmap becomes realistic with the advent of long-horizon agents.

## Concept (1)

### ATOM-SOURCE-20260114-001-0003
**Lines**: 36-36
**Context**: method / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.20, speculation_risk=0.30, actionability=0.70, epistemic_stability=0.50

> AGI is defined functionally as the ability to figure things out, rather than a technical definition.

## Framework (2)

### ATOM-SOURCE-20260114-001-0004
**Lines**: 44-46
**Context**: consensus / claim
**Tension**: novelty=0.50, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.20, actionability=0.60, epistemic_stability=0.70

> An AI that can figure things out requires baseline knowledge (pre-training), the ability to reason over that knowledge (inference-time compute), and the ability to iterate its way to the answer (long-horizon agents).

### ATOM-SOURCE-20260114-001-0010
**Lines**: 97-100
**Context**: method / claim
**Tension**: novelty=0.50, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.30, actionability=0.70, epistemic_stability=0.70

> Two technical approaches for coaxing models to think for longer are reinforcement learning (teaching models intrinsically to maintain focus) and agent harnesses (designing scaffolding around model limitations like memory hand-offs).

## Prediction (5)

### ATOM-SOURCE-20260114-001-0002
**Lines**: 22-22
**Context**: speculation / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.90, actionability=0.40, epistemic_stability=0.30

> Long-horizon agents are functionally AGI, and 2026 will be their year.

### ATOM-SOURCE-20260114-001-0013
**Lines**: 102-104
**Context**: speculation / claim
**Tension**: novelty=0.60, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.80, actionability=0.40, epistemic_stability=0.30

> AI applications in 2026 and 2027 will be "doers" that feel like colleagues, leading to all-day, every-day usage with multiple instances running in parallel.

### ATOM-SOURCE-20260114-001-0014
**Lines**: 104-106
**Context**: speculation / claim
**Tension**: novelty=0.70, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.90, actionability=0.50, epistemic_stability=0.20

> Users of future AI applications will transition from working as individual contributors to managing teams of AI agents.

### ATOM-SOURCE-20260114-001-0015
**Lines**: 107-107
**Context**: speculation / claim
**Tension**: novelty=0.80, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.90, actionability=0.30, epistemic_stability=0.20

> The performance of long-horizon agents is on an exponential curve, doubling every ~7 months. By 2028, agents should reliably complete tasks that take human experts a full day; by 2034, a full year; and by 2037, a full century.

### ATOM-SOURCE-20260114-001-0017
**Lines**: 122-123
**Context**: speculation / claim
**Tension**: novelty=0.80, consensus_pressure=0.10, contradiction_load=0.10, speculation_risk=0.90, actionability=0.60, epistemic_stability=0.10

> Agents will soon be able to perform a day's worth of work reliably, and eventually a century's worth of work.
