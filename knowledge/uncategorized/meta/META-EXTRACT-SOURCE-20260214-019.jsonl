{"atom_id": "ATOM-SOURCE-20260214-019-0001", "source_id": "SOURCE-20260214-019", "category": "claim", "content": "Stanford and Caltech researchers have published the first comprehensive taxonomy of how Large Language Models (LLMs) fail at reasoning.", "line_start": 2, "line_end": 3, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.8, 0.5, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260214-019-0002", "source_id": "SOURCE-20260214-019", "category": "framework", "content": "A novel categorization framework for LLM reasoning failures distinguishes reasoning into embodied and non-embodied types, with non-embodied further subdivided into informal (intuitive) and formal (logical) reasoning. Failures are classified along a complementary axis into three types: fundamental failures (intrinsic to LLM architectures), application-specific limitations, and robustness issues (inconsistent performance across minor variations).", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.9, 0.4, 0.1, 0.1, 0.6, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260214-019-0003", "source_id": "SOURCE-20260214-019", "category": "claim", "content": "The research provides a structured perspective on systemic weaknesses in LLM reasoning, offering insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities.", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.7, 0.6, 0.1, 0.2, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260214-019-0004", "source_id": "SOURCE-20260214-019", "category": "praxis_hook", "content": "A comprehensive collection of research works on LLM reasoning failures is available as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures.", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.0, 0.0, 0.9, 0.9], "opposes_atom_ids": []}, "extensions": {}}
