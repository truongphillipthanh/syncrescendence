# Scaling AI Through Energy-Efficient Hardware: A New Computing Paradigm

The AI revolution we've been experiencing over the past few years is extraordinary. We've discovered how to convert energy into essentially arbitrary amounts of intelligence—or so the prevailing belief goes. But there's a fundamental problem that most people don't grasp when they casually use ChatGPT on their phones: the underlying computation is orders of magnitude more complex than something like a Google search. 

For the first time in history, the average person has become a significant consumer of the world's high-performance computing resources. And what follows from this energy intensity is inescapable: very soon, energy will become the primary constraint in scaling computing. For decades, the limiting factors were speed and capability—how fast is the computer, what can it do? In the coming years, the question becomes: how efficient is it?

This shift opens a massive opportunity. The market for AI has become truly staggering—just look at Nvidia's market capitalization growth over the past few years, representing several trillion dollars. This convergence creates space for seemingly far-fetched approaches to energy-efficient computing. At Extropic, we foresaw this constraint a couple of years ago and have been working on it since. Today, I'm going to talk about what we're building, why we're building it, and how we're building it—though the last part ventures fairly quickly into PhD-level physics and machine learning.

## What We're Building

In the simplest terms: we're building a new type of integrated circuit processor that leverages probabilistic circuits to run new generative AI algorithms. Let me unpack that.

An integrated circuit processor is made from transistors, just like any CPU or GPU from Nvidia or Intel. We manufacture these at foundries like Intel, Samsung, or TSMC. We're not doing anything exotic like quantum computing or photonics—just conventional electronics with a different architectural approach.

The novel element we're introducing is something called *probabilistic circuits*. Rather than computing simple deterministic functions like 0 AND 1 = 0, these circuits sample from mathematically well-defined probability distributions. We build circuits that sample instead of compute. And once you have that primitive, you need to build new AI algorithms that leverage this hardware efficiently to accomplish something comparable to ChatGPT or Midjourney. Transformers and current neural networks were designed specifically to exploit what GPUs are good at. Build new hardware, and you must build new algorithms to match.

Where are we today? Over the past two years, we've built version zero of everything I just described. We've created chips that let us test these probabilistic circuits in the lab—this isn't theory. We've actually built them and validated that they work as predicted. That was significantly harder than it sounds, because while all circuits have noise, it's another matter entirely to harness that noise for reliable, useful computation. We've integrated these chips into simple systems, packaging them onto boards and building interfaces to FPGAs for hybrid algorithm development. We've also done extensive work on the machine learning side, figuring out how to leverage this new processor type for useful generative AI. All of this in two years.

The result? Simulations of our next chip predict it could be approximately 10,000 times more efficient than a VAE running on a GPU for simple generative AI benchmarks. Now we're focused on scaling. We've built these small systems; now we must scale both the hardware and algorithms to raise our system's capabilities to something more comparable to today's large language models. Everything we've done so far has been extremely small-scale. The company is only around 15 people, distributed across integrated circuit design, statistical physics research, and machine learning.

## Why We're Building It: The Energy Scaling Problem

I probably don't need to belabor the capability side: computers are getting remarkably smart. We're building AI systems that win gold medals at the IOI competition—something that would have seemed impossible just years ago. The current belief in Silicon Valley is that this capability growth will continue, and I certainly hope that's true.

The problem emerges when you look at claims about *deploying* that capability universally. All the major tech CEOs universally say the same thing: everyone will have their own advanced AI they'll use all day, constantly, for everything. While I want to believe this, the physicist in me knows it's simply not possible with current technology.

Let me walk through the argument. First, I'll project from today to see how hard it would be to make this universal AI future real using exactly the technology we already have. To do this calculation, I use a fairly simple model. The fundamental equation is: number of FLOPs = 2 × parameters × tokens. For something like an H100, you can easily derive the joules-per-flop number—the theoretical maximum efficiency of the accelerator. Using these two numbers, you can assign an energy number to any scenario.

The model has a few parameters. P is the number of parameters in the LLM—roughly a couple hundred billion for the largest models today. N is the total number of input and output tokens the model processes. M is what I call the reasoning multiplier—it captures how many internal tokens the model generates per I/O token (especially relevant for reasoning models that think internally). And epsilon is the energy per FLOP. For an H100, that's about 0.7 picojoules per FLOP.

Let's run some scenarios through this model. First, today's scenario: everyone sends a couple thousand tokens through a model that doesn't do much reasoning—think GPT-5 Pro levels on average. The calculation yields that AI is consuming around 0.5% of the grid currently, which matches real-world estimates. In absolute terms, that's about 3 gigawatts. This validates our model's ballpark accuracy.

Now consider the most basic version of an AI-enabled future: a personal assistant that interacts with everything you do each day. It reads your emails, manages your schedule, keeps your social connections alive by helping you text friends. If this system had about the same reasoning capability as today's GPT-5 Pro, energy consumption would spike to approximately 20% of the total grid—100 gigawatts. For context, one gigawatt data center costs about $10 billion. This is already a trillion-dollar scale problem for what is, frankly, not that useful.

If you upgraded this assistant to see video at 1 FPS—essentially like Mark Zuckerberg's metaverse glasses concept—back-of-the-envelope calculations show you'd need to expand the grid by about 10x just to accommodate this for everyone. But wait, there's more. If you took the text assistant and upgraded it to reason at the level of today's models that solve ARC AGI benchmarks (essentially advanced IQ tests), you'd need to expand the grid by 10x in total, or about 5 terawatts in absolute terms. That's absurd. And you can keep escalating. If everyone in the world got a few queries to an expert-level system like those used in the IOI competition, you'd need to expand the grid by 100x just for those queries. This assumes OpenAI used only 100 GPUs for their IOI systems, which is obviously a huge underestimate. For a true expert-level chatbot—something like an AI coworker—you're looking at astronomical numbers.

These aren't just abstract numbers. Look at what's happening: everyone in power is doing this calculation, but they're not talking about it publicly. How else would Sam Altman arrive at "one gigawatt data center per week"? That's roughly the infrastructure needed for the text-assistant scenario. Converting our power numbers to costs shows the reality. The US grid capacity has grown linearly over the past 75 years, which itself is somewhat surprising—I'd have expected exponential growth. Sam Altman's plan of one gigawatt per week would require 3 to 5 times the current rate of energy production. That's already nearly impossible given that energy ultimately limits everything. More absurdly, it would require Nvidia to roughly 10x their current output.

But those 10-100 terawatt scenarios, which are what's actually needed to make AI universally useful, reveal the true scale. We're talking about building 10 terawatts in 20 years. On our scale, that's a vertical line—completely impractical. Nvidia would need to 100x their output. The infrastructure cost in terms of solar panels alone? You'd essentially need to cover Nevada with panels, at a cost roughly equal to the world's entire GDP.

Your reaction should be: "Trevor, you're an idiot. GPUs are still improving. Models are getting smaller through distillation. These numbers are never going to be real." I genuinely wish you were right. But unfortunately, physics and historical data support this fairly soundly. Every direction forward looks hard, which typically signals you're at a local minimum of your technology. When you want to escape a local minimum, you take a high-temperature sample and try something completely different. That's what we did. We looked at the roots of generative AI and asked: what if we built hardware optimized for what generative AI fundamentally does?

## How We're Building It: The Hardware and Physics

Generative AI is, fundamentally, about sampling. So why not build hardware that does sampling natively, efficiently, and then profit? That's the core insight.

Let me be concrete about what a thermodynamic sampling unit actually is. It's called "thermodynamic" because a TSU produces samples from an energy-based model—a probability distribution defined by an energy function. Rather than going through convoluted denoisng processes, we directly parameterize a probability distribution via an energy function we learn. When I adjust the parameters, the shape of the distribution changes. This is an extremely literal interpretation of machine learning: you want to fit a probability distribution, so learn its shape directly.

In practice, TSUs are giant arrays of random sampling circuits. What makes this possible is an ancient algorithm called Gibbs sampling. This algorithm breaks down the task of sampling from a complicated energy function into smaller tasks that simple sampling cores can complete. If your energy function is structured as a sum of terms where each term depends only on a subset of variables, you can sample from it by sampling from simpler energy-based models for each degree of freedom, where each simpler model only depends on terms relevant to that variable. Gibbs sampling lets you modularize sampling.

When a TSU runs Gibbs sampling in practice, you cycle through cells one at a time or in blocks. Each cell receives state information from its neighbors, computes the parameters of its local distribution based on those neighbor states, and then a probabilistic circuit samples from that conditional distribution. You have this giant array of random sampling circuits working in parallel to produce samples from an energy-based model. This is actually a concept from the 1980s—we didn't invent it. If you understand Gibbs sampling and have any physics background, it becomes obvious that you should build this hardware. Several papers from academic researchers have explored it.

So if this idea has existed for decades and academics have studied it, why are we the first to actually build it? First and most importantly, we figured out how to use TSUs efficiently. You can define sampling problems to run on a TSU, but that doesn't automatically mean they can do anything useful in the real world. In academia, people have primarily tried using these for optimization problems: graph coloring, traveling salesman, and similar constraint satisfaction problems. The market for optimization is tens or hundreds of billions—not enough to launch a novel computing paradigm. What's far more exciting is using these for generative AI, which is actually the original form of machine learning, predating neural networks. The market here is massive.

The real algorithmic challenge is that despite TSUs running Gibbs sampling very efficiently, sampling from complicated distributions can still be difficult because sampling algorithms tend to get stuck in energy valleys. If your energy landscape is rugged with many valleys separated by tall barriers, the sampler struggles to cross them. This is enormous for machine learning because real datasets are highly multimodal—they have many valleys separated by high barriers. In an image dataset, you're unlikely to find a dog-airplane hybrid, so there are regions of space with very high energy that samplers can't cross. You don't even need a complex ML problem to see this. It happens with simple double-well potentials: when the barrier is high, particles get stuck; when it drops, they can cross.

The solution is to return to a denoising procedure. But now, because we have hardware that can sample from distributions much more complicated than Gaussians, we can use far fewer steps in the reverse process. We skip steps. Instead of progressing through many neural networks and Gaussians as in a traditional diffusion model, we sample from an energy-based model at each step. We run the sampler for a while, reach a state, feed it to another EBM, run the sampler again, reach a new state, and continue until we get useful output. This is what we call a denoising thermodynamic model (DTM).

## Understanding the Physics: Transistors as Probabilistic Computers

To understand why this is possible, you have to think really hard about noise in transistors and how to engineer transistor circuits to perform useful sampling. When you dive into the physics, you discover that transistors are actually an extremely natural substrate for probabilistic computing—specifically, transistors operating at very low voltages. At low voltages, charge dynamics in the transistor are driven almost completely by thermal fluctuations. Current in transistors at low voltages is entirely driven by noise, which is exactly what you want for probabilistic computing. You're not wasting current on deterministic operations.

Think about transistors in this low-voltage regime using the same double-well picture we discussed earlier. When the gate voltage applied to a transistor is very low, you have the source and drain—the two terminals where current flows—separated by a really high energy barrier. This high barrier makes it hard for charge to cross, which is how you turn a transistor off. As you increase the gate voltage, you drop this barrier, allowing charge to flow from one side to the other. That's how you turn a transistor on. When you have a low barrier and apply a bias, current can flow. This is exactly how to think about low-voltage transistors. We've built very rigorous physical models of this process and use them to design our circuits. This key theoretical insight let us design probabilistic circuits using only transistors, no exotic devices.

With these models, you can build a family of probabilistic circuits that do useful things. Consider a probabilistic bit—just a source of random ones and zeros. You can control the bias of this random bitstream by applying a control voltage. You can build it so it's more likely to produce zero, one, or something in the middle. You can build more complex circuits too: one with four possible states that moves randomly between them, sampling from a categorical distribution with arbitrarily controlled probabilities. You can even build circuits that sample from continuous distributions—we've built ones sampling from Gaussian mixture models, where you control the position of the modes and the weights of the mixture by applying different voltages. You really can control noise in transistors with good fidelity and use it to do something useful.

Is this sensitive to ambient temperature? Yes, somewhat. But the temperature dependence is much simpler in these low-voltage regimes because everything is thermally activated. It's very sensitive but also very predictable, which means you can manage it.

Once you understand how to build probabilistic circuits from transistors, there's nothing else in a TSU that's unconventional. You can immediately scale it. We're now building a much larger TSU—actually much larger than anything we can reasonably simulate on a GPU. Once we have this chip, that's when the joint machine learning and hardware research will really accelerate.

## Results, Scaling, and What's Next

When you combine a denoising thermodynamic model running on an energy-efficient TSU, you get remarkable performance. Simulations predict that a TSU can generate samples from a simple dataset using around 10,000 times less energy than the most efficient algorithms running on a GPU. To be clear: this research isn't about raw capability—the dataset is very simple. The point is head-to-head energy efficiency: 10,000 times better than a GPU.

The real question is how to scale this. The dataset in our paper was too simple to represent real machine learning problems. Critically, denoising thermodynamic models aren't full machine learning models on their own—they should be used as primitives within larger systems. The way to scale, I believe, is by intelligently combining these DTMs with conventional neural networks running on GPUs. You're not spending zero energy on deterministic compute, but you're spending far less.

In a paper we're publishing shortly, we did a simple version of this: we used a denoising model for image generation and found you end up using far fewer deterministic FLOPs than using a standard GAN alone. This is extremely early research, but it's very promising. It's quite likely that using hybrid algorithms, we could build a system that does something much more complicated—like image generation—using two orders of magnitude less FLOPs than what's possible today. How close we can get to the 10,000x asymptotic efficiency we see in pure probabilistic benchmarks remains an open question, but getting there is why I'm here today. This is only the beginning.

The fact that we've accomplished all of this—novel transistor models, probabilistic circuits, hardware testing, algorithmic development—with just 15 people is genuinely remarkable. From here, we're building a much larger team, doing more research, and scaling this technology to make scaling machine learning far easier than it would be otherwise.