https://www.youtube.com/watch?v=dRuhl6MLC78
Making AI Way More Energy Efficient | Extropic CTO
21,166 views  Premiered Oct 31, 2025
Talk by Trevor McCourt (CTO) on Extropic's technologies.

Learn more at extropic.ai

---

0:00
Because these algorithms are so energy intensive, the amount of infrastructure you'd have to build out to achieve that
0:05
harder objective is like totally crazy, right? I'm talking tens, hundreds or thousands of trillions of dollars that
0:10
looks like this. And so, yes, it might be crazy to kind of throw all the work that's been done away, but if it works,
0:16
you get this. You unlock a new scaling in technology that bypasses a lot of the hardships that you would otherwise be
0:22
facing. Thanks everyone for coming. I don't know
0:29
if you guys have heard, but this whole AI thing, it's kind of crazy. It's cool that we've figured out a way to convert
0:36
energy into kind of an arbitrary amount of intelligence. At least as far as we can tell, or at least what that's what
0:41
the current religious belief is. But the algorithms we're using to run AI are
0:46
extremely energy intensive and it's kind of unintuitive when you open up chat GPT
0:51
on your phone and type something into it. But the computation going on in the back end is orders of magnitude um more
0:57
complicated than something like a Google search. Right? So the tools that you've all started to use over the last few
1:03
years really represent the first time that the average person is becoming a significant consumer of the world's high
1:09
performance computing resources. And what the energy intensity of these application means is that really soon
1:14
energy is actually going to start to become the main constraint in um scaling computing, right? Whereas in the past
1:21
the problems have more been centered around speed and capability, right? How fast is it the computer and what can it do? Versus in the coming years it's
1:28
going to be how efficient is it, right? The market for AI is is truly massive, right? Even if you just look at the how
1:34
much the market cap of Nvidia has increased in the last few years, that's several trillion dollars, right? And so
1:39
what this all this comes together to create is a massive opportunity to work on somewhat far out schemes for energy
1:46
efficient computing. And we foresaw this a couple of years ago and we've been working on that since. And um this is
1:52
again, like I said, one of the first times that I'm going to talk about what we come up with. First, I'm going to tell you in pretty short terms what
1:58
we're building. Uh then I'm going to tell you why we're building it. And then I'm going to tell you a bit about how. Although that pretty quickly gets into
2:04
PhD level physics and machine learning. So I'll do my best. I made animations. So in very few words, what is extropic
2:11
building? Well, we're building a new type of integrated circuit processor that leverages our novel probabilistic
2:17
circuits to run new generative AI algorithms. So, let's break that down. An integrated circuit processor means
2:23
that something that's made out of transistors just like a CPU or GPU that Nvidia or Intel would make. Um, and you
2:29
make them at a foundry like Intel or Samsung TSMC, right? So, we're making very conventional electronics, nothing
2:35
weird like quantum or photonics. The new thing that we're bringing to integrated circuits are something that we call probabilistic circuits. So these are
2:42
circuits that instead of computing simple functions like 0 and 1 equals 0
2:47
sample from uh mathematically well- definfined probability distributions, right? So we build circuits that sample
2:53
instead of compute. And once you have those two things, if you go and you have to go and build new AI algorithms that
3:01
leverage that hardware efficiently to do something similar to chat, GBT or midjourney, right? So the neural
3:06
networks we have today like transformers are really built to leverage specific things about GPUs. Um and if you built
3:11
so if you build a new type of hardware you also have to build a new type of algorithm to go along with it. And so where are we now? Well over the last 2
3:18
years uh we've kind of built a version zero of everything I just told you about. We've built chips that let us
3:24
test these new probabilistic circuits in the lab. Right? So it's not just theory. We've actually built them and validated that they work how we think they should.
3:30
which was a lot harder than you would think because you know all circuits have noise but it's a whole another matter to actually use that noise to do something
3:36
useful reliably. We've integrated those chips into simple systems. So you know we've packaged our chips built boards
3:42
around them um built interfaces from the talk to FPGAAS and kind of do hybrid algorithms and then we've also done a
3:49
bunch of work on the machine learning side to figure out how to leverage this new type of processor to do something useful in generative AI. So all that's
3:55
been done over the last two years. And what that gets us is this plot here,
4:00
which shows that simulations of a chip we're building now could be around 10,000 times more efficient than a VAE
4:06
running on a GPU on some simple generative AI benchmark. Now, great. Okay, so basically we did a bunch of
4:12
research. Where are we going? Well, now it's all about scale. We've built these simple systems. So now we have to scale
4:18
up both the hardware and the algorithms to raise the capabilities of our systems to be more comparable to what you can do
4:23
with LLM today. Right? Everything we've done so far has been extremely scal small scale. The company's only around
4:29
15 people and those people are split between integrated circuit design, statistical physics research and machine
4:34
learning. Now let's get a little bit more specific about what each of those new things is. Right? So what do I mean by a new type of integrated circuit
4:40
processor? Well again an integrated circuit is just a chip made out of a bunch of transistors that you can get from TSMC. So there's nothing exotic
4:47
like quantum computing or photonics. The way it is different from what exists today is mostly architecturally.
4:53
So a GPU is essentially a giant array of floatingoint multiply accumulate units
4:58
that get orchestrated by some central controller to implement things like matrix multiplication for deep learning.
5:04
That's great for doing neural networks. Our new type of processor is called a thermodynamic sampling unit. A
5:10
thermodynamic sampling unit is extremely different from a GPU in that instead of being a large array of floatingoint
5:16
multiply accumulate units. It's actually a giant array of sampling cells. Right? So a thermodynamic sampling unit looks
5:23
like this picture on the right where you have a bunch of cells and within each cell you have some kind of specialized
5:30
sampling circuitry that implements ultraefficient random number generation. You have some circuitry that computes
5:36
the parameters of that random number generation process such as the bias of flipping a coin and you have a register
5:42
that stores the state of the program. And so when you roll out a program on a TSU, what basically happens is all of
5:48
these cells talk to each other and orchestrate some kind of sampling procedure to uh sample from some
5:54
computationally useful probability distribution. What do I mean by probabistic circuit? Well, traditional
5:59
circuits compute functions, right? So if you have an ANDgate with inputs A and B, it will output A and B, right? So 0 and
6:06
0 equals 0. And if you look at what's actually happening inside your computer over time, the logic levels one and zero
6:11
are coded as voltages generally and the inputs will at some time rise up and
6:17
then some dynamics happen inside the circuit and at a later time the output will be the end of the inputs. Right? So
6:23
it's a dynamic system. Probabilistic circuits work a little bit differently. Instead of computing determin simple
6:29
deterministic functions, they sample from simple probability distributions. Uh and so this here on the right is kind
6:35
of the simplest example of a probabilistic circuit. These time traces here show the output which are obviously
6:41
very different than something like an endgate. These are actually moving randomly in time. And the inputs to a
6:47
probabilistic circuit are actually voltages that set the parameters of the distribution that it samples from. Right? So the input to this circuit is a
6:54
voltage that sets the probability of the circuit being in the low state or the high state. And so from an input output
6:59
perspective, a traditional logic gate takes voltages as inputs and outputs deterministic voltages to compute a
7:06
function versus a probabilistic circuit takes voltages as inputs. It outputs a random voltage that can be used as a
7:12
sample from a probability distribution. Um, and that's what it does, right? It samples. It's a probabistic circuit. So
7:19
what do I mean by a new type of generative AI algorithm? Well, deep learning algorithms kind of look like this, especially in generative AI, where
7:25
you have these kind of massive deterministic function computations that feed parameters into really simple
7:32
probabilistic sampling tasks. In an LLM, you run a giant transformer which sets the parameters of a softmax distribution
7:39
which predicts your next token. Diffusion models, very similar story. From a resource allocation perspective, in traditional deep learning, you're
7:45
spending all of your resources computing these giant functions um and almost none of your resources on random sampling.
7:51
Now, because our computers are focused on sampling, the algorithms that run well on them are going to be quite
7:56
different than deep learning, right? The algorithms that run well on a thermodynamic sampling unit are going to involve much more random sampling than
8:04
they do deterministic function computation, right? And so, but that's not normal deep learning. This is an
8:10
entirely new class of algorithms that we're designing to run in our hardware. And the idea is that if TSUs really
8:16
efficiently sample from complex probability distributions and we can build algorithms that leverage this
8:23
capability properly, you'll be able to build a overall AI system that's far more efficient than deep learning
8:28
running on GPUs. So you probably just heard all of that and can come to one conclusion, which is
8:34
that I'm completely insane. Why would you throw away all of the work that's been done on deep learning and pushing
8:40
GPU capabilities over the last two years and try and build a computer on an unproven architecture that's never been
8:46
scaled before? Because like from a pretty reasonable perspective, it looks like deep learning is just working, right? Like it kind of looks like we can
8:52
just keep building GPUs, keep scaling our size of our models, and keep getting better and better results, right? Well,
8:57
I kind of wish that was true, but I really don't think it is. And also yes, I am insane and so are most of the other
9:04
people that work at the company. But that's besides the point. From my perspective, what's actually happening
9:09
is something like this. So here we are today. This curve represents the invention of deep learning and building
9:15
up the AI infrastructure to the state it is today, right? Which is, you know, pretty immature. We're really just
9:21
getting started. Um, and through all the work we've done, we can achieve this, which is some people using chatbots
9:27
sometimes. But this isn't enough. The dream that we're kind of all being sold is
9:34
something more like this, which is everyone using AI for everything all the time, right? And it's not intuitive from
9:41
our perspective, but those things are really far apart. Um, because these algorithms are so energy intensive, the
9:48
amount of infrastructure you'd have to build out to achieve that harder objective is like totally crazy, right?
9:53
I'm talking tens, hundreds or thousands of trillions of dollars if you just try and scale what we have today, right? And
10:00
so that looks like this. That's going to take forever and we're going to spend multiples of the world's GDP to get
10:06
there. And so, yes, it might be crazy to kind of throw all the work that's been done away. But if it works, you get
10:13
this. You unlock new scaling in technology that bypasses a lot of the hardships that you'd otherwise be
10:18
facing. And you might look at that and say, well, okay, that's kind of just your opinion. kind of sounds like you're trying to predict the future. That's
10:25
notoriously hard. Don't do that. Right? And again, I wish that were true. I wish everything I just said wasn't true and
10:30
it's not going to cost a hundred trillion dollars to scale LLMs. But unfortunately, there's a lot of historical data and facts about physics
10:37
that back that up fairly soundly. And over the rest of this talk, I'm going to try and convince you that it's true and that also what we're building at
10:43
Extropic has some chance of of breaking through that scaling. The story goes something like this. So advanced AI is
10:49
great. We all wish we could use it, but it uses a ton of power, right? Improving GPUs is expensive, and models probably
10:55
aren't getting any smaller. Despite the fact that we're getting better and better at distilling LMS, if you look at
11:00
kind of the frontier of capabilities, model complexity is going up exponentially. So, that's not probably not going to save us. You know, when you
11:06
look around in this space, it feels like every direction is hard, right? And that tells you you're probably at a local minima of your technology. And so, what
11:13
do you do when you want to escape a local minima? You try and take some high temperature sample and do something totally different. And so that's kind of
11:19
what we did. We looked at the roots of generative AI and tried to use that to motivate a new type of hardware, right?
11:26
And the kind of key thing is that we noticed and this is obvious is that generative AI is all about sampling,
11:31
right? So why not make hardware that does sampling natively? You build hardware that does sampling really
11:36
efficiently and then profit. Great. So let's start with the why. Um I don't think I really need to hammer on this
11:42
point. Computers are getting really smart. These are two results just recently. For example, we're building AI systems that are getting gold medals at
11:47
the IIY competition, right? Crazy. When I was in your seat taking heat transfer, I never could have imagined something
11:52
like that, right? And there's no way to know whether or not this going to this is going to keep going. But definitely
11:58
the current belief in Silicon Valley is that it's going to. And I think that's great. I hope that's true. No problem
12:04
with this at all. Where you do run into problems is when you start looking at the claims around how that capability is
12:10
going to be deployed to everyone. Right? If you look at what all of the big tech CEOs are saying in the news universally,
12:16
it's that everyone's going to have their own advanced AI that they're going to use all day, all the time to do everything. And while I also want to
12:22
believe in this, the physicist in me knows that this is just not possible. to walk you through why I think that we're
12:29
going to go through kind of a subtle argument which is first we're going to kind of project out from today
12:36
how hard it would be to make this future real using exactly the technology we already have right so if we have today's
12:43
GPUs and today's models what would it take to make this real and then with those numbers in mind we're going to ask
12:50
how much that costs right so what would it take to actually build the amount of power production we need to support that
12:55
and those are going to be massive numbers Right? Then we're going to say, okay, but obviously technology gets better, right? So we won't need that
13:00
much. Um, but then you realize that GPUs are quite close to the limits of efficiency. Um, and like I said, models
13:07
aren't getting smaller. So first step of that, what would it take to pull off these crazy objectives using the
13:12
technology of today? Well, transformers are actually quite predictable in terms of their uh floatingpoint operation
13:19
usage. It's a pretty simple formula which is basically two times parameters times the number of tokens is the number
13:25
of flops the model uses. And if you're looking at something like an H100, you can come up with uh pretty easily a kind
13:32
of jewels for per flop number, energy per flop number, which is just kind of like the theoretical maximum efficiency
13:38
of the accelerator, right? And so using those two numbers, you can take any one of those scenarios we were looking at
13:44
the last slide and actually assign an energy number to it. And it's not an exact science because a lot of today's
13:51
systems are so-called reasoning models. Uh and they generate basically a bunch of extra internal tokens for the model
13:58
to think that we don't get to see, right? So that number we kind of have to
14:03
figure out from without direct data, but you can do it. You know, you can look at the API pricing. You can look at some of
14:09
the higher profile examples where they're forced to report those kind of numbers and you can you can figure them
14:14
out, right? And so this equation here captures what I was just saying. P is the number of parameters in the LLM. N
14:20
is the number of input and output tokens seen by the model. So what you tell everyone it gives you back. M is what I
14:26
call a reasoning multiplier. So it captures how many internal tokens the model is generating per IO token.
14:32
And epsilon is the amount of energy per flop. So quite a simple model. Um the
14:38
number of parameters is relatively easy to pin down. You know a couple hundred billion for the big models these days. There are mixtures of experts. They
14:44
obviously have more parameters in total, but that's roughly how many would be active. And then for something like an H100, epsilon is like.7 pico per flop.
14:52
So we've got this model of energy. Let's push some scenarios through it and see what kind of numbers come up. Right? So if you take a scenario that looks like
14:58
today where um let's say basically everyone is sending a couple thousand tokens through a model that doesn't do
15:05
very much reasoning, say like GPD5 pro levels on average, right? the number you come up with is that AI should be using
15:11
around half a percent of the grid, which is about right based off of the estimates that are coming in. Right? So,
15:17
our model is like in the right ballpark of reality. Um, and in absolute terms, that's about 3 gawatt just to ground
15:23
that ground some of the stuff I'm about to say in and today. Cool. So, the most basic version of an AI enabled future I
15:30
think that we can all pretty clearly see is some kind of assistant that interacts with all the text you do every day,
15:35
right? So maybe it reads your emails, manages your schedules, keeps your friends happy, makes you reply to them
15:40
over text, that kind of thing. If we had such a system and it had about the same level of reasoning as today's GPD5 Pro,
15:48
your energy consumption would grow to around 20% of the total grid, which is already a lot. That's 100 gawatt, right?
15:55
And uh one gigawatt data center is about $10 billion. So this is already a trillion dollar scale thing that I'm
16:02
talking about here. And just this really simple scenario, right? This isn't act wouldn't actually be that useful if you
16:07
upgrade that AI assistant to see uh video at 1 FPS which it would be kind of like Mark Zuckerberg's metag glasses.
16:14
Same kind of back of the envelope calculation tells you that you'd need to around 10x the grid to just accommodate
16:20
that for everyone. Right? So that kind of puts into perspective how crazy the computational demands of these AI models
16:26
are. If you take your text assistant scenario and you upgrade it to reason at the same level as today's models that
16:32
work in ARC AGI which is uh one of the simpler like AGI benchmarks floating around that kind of just says IQ test
16:38
problems um you'd find that even just the text assistant would require around a 10xing of the grid today's grid um or
16:45
around 5 terowatts in absolute terms which is insanity and you can keep going crazier. So like what if we wanted to
16:52
give everyone in the world a couple of queries to an expert level system, right? Like something similar to the II
16:57
system, II gold system. In that case, you would need to around
17:02
100x the grid just for a couple of queries, right? And by the way, this is assuming that OpenAI only used 100 GPUs
17:09
for their IOI challenge, which they certainly used way more than that because why wouldn't they, right? So this is probably a huge sandbag. If you
17:16
want the chatbot to be expert level, which I think is getting closer to like Jensen's AI coworker, um, then it's just
17:22
like I don't I can't even count how many seros are there, right? It's a lot a lot a lot a lot of power. Great. So, those
17:28
are big numbers. What does that actually mean? Well, first of all, it's interesting because if you look at
17:33
what's happening in the world around us, it's clear that everyone in power is doing this calculation, but they're not talking about it. Like, how do you think
17:39
Sam Alman came up with one gigawatt data center per week? It was something like he wants to support the text assistant
17:45
type use case, right? Because that would make them a ton of money. Yeah, we can kind of take those power numbers we just
17:50
saw and convert those into dollars by looking at how much it would cost to actually build data centers to do that, right? And if we just look at the kind
17:58
of energy production perspective, this orange curve is the last 75 years of US
18:04
history. Um, how the US grid capacity has been growing over time. And first of all, it's actually pretty linear, which
18:09
is surprising. I would have expected something more exponential, but it's linear. And this blue curve here is Sam
18:14
Alman's 1 gawatt data center per week, right? So that's already just that kind of modest thing would already require
18:21
like 3 to 5x the rate of energy production that we have now. Um, which even that is a little bit hard to
18:27
believe because we're probably already doing this as fast as we can given that energy limits everything in the world, right? But maybe possible. Um, the
18:34
slightly crazier thing for Sam Waltman's plan is that it would require Nvidia to roughly 10x their current output. And maybe that's why the stock keeps going
18:40
up. You know, based off of the numbers we just saw, which very quickly crept up into the tens or hundreds of terowatts,
18:47
um, 1 gawatt a week isn't really that interesting, right? We're interested in something more like 10 terowatts in the
18:53
next 20 years, right? I think that's kind of the level of project we would need to do to really make these LLM
18:58
systems useful to everyone. And if you look at what those lines look like on our scale, they're kind of vertical,
19:05
right? So this this pink line here is if we wanted to build 10 terowatts in 20 years, just totally absurd and Nvidia
19:12
would need to 100x their output. And if you take this 10 terowatt number to heart and you calculate out the cost of
19:19
the infrastructure for building that, uh, for example, in terms of solar panels, you need to basically fill
19:24
Nevada with solar panels to provide 10 terowatts of power. And that would come at a cost of around the world's GDP. So
19:31
just to put put this all in perspective, it's like totally totally crazy. And your reaction to that should be Trevor,
19:37
you're an idiot. GPUs are still getting better. Models are still getting smaller. These numbers are never going
19:42
to be real. And again, I really wish you were right because that would be way easier than what we're doing now. But
19:48
it's also very easy to come to that conclusion, right? Because if you look at the last I mean the plot will go way back into the 70s but computers have
19:55
been getting exponentially more efficient for a really long time and there's two things that drive that. One
20:01
of them is the miniaturaturization of the transistor, right? So, we've been making the primitive building blocks of
20:06
computers smaller and smaller since the integrated circuit was invented. And the other thing is you can just build more
20:13
efficient computer architectures, right? And that's both of these things have been going on for a while. Um, the
20:19
miniaturaturization of the transistor for a long time seemed like a completely endless source of improvement.
20:24
Optimizing architectures kind of has a pretty stiff upper bound on how how much improvement you can get, right? Because
20:30
given a particular mathematical operation, you can only implement that using so few transistors, right? There's like a lower limit. Unfortunately,
20:36
digital logic as of quite recently actually is pretty much at physical limits of efficiency. And the reason for
20:43
that is pretty much how capacitance works. It tells you how much energy gets stored in two pieces of metal when you
20:50
apply voltage to one of them, which is very relevant to computing. So as pieces of metal get bigger, the capacitance
20:56
goes up, which is bad for energy. But it also gets bigger as the distance between the plates shrinks, right? So as you
21:02
make transistors smaller, this term in the numerator area gets smaller, but D also gets smaller, right? And so just
21:10
kind of the engineering of it means that at a certain point you're going to stop getting improvements, right? And this is obviously way more complicated than the
21:16
simple formula, but this captures the essence. So this is real data. capacitance stopped getting smaller at
21:21
roughly the 10 nanometer node which is pretty recent. Um and this isn't sorry
21:27
this isn't just the transistor capacitance this is including this is like of an inverter right so if I lay
21:32
out an inverter using real transistors include all parasitics that capacitance is plateaued for a while energy computer
21:39
there's two terms capacitance and voltage capacitance hasn't gotten significantly smaller in a while um and
21:44
voltage is actually constrained by pretty fundamental thermodynamic considerations which is the current that
21:50
flows through a transistor when it's off um is actually directly a function of
21:56
the voltage that turns it on. Right? So, as I keep lowering the voltage that
22:01
switches the transistor on, the current that flows through it when it's off goes up. And there's nothing I can do about
22:06
the slope of this line. This is given by basically statistical physics, right? And so the on voltage of a transistor
22:13
can only be so low at any given temperature before it kind of stops behaving like a transistor, right? And so the threshold voltage of transistors
22:21
for use in digital logic applications hasn't gotten significantly smaller than around 300 molts in a long time. The
22:27
energy dissipated when you switch a digital logic gate from one state to another can't get that much smaller than
22:33
it is today operating room temperature, right? And so what this kind of tells us is that the improvements that are coming
22:39
in GPU and CPU technology have been largely driven by architecture, which if you look at the stuff coming out of
22:45
Nvidia seems to kind of be true, right? They're really pushing on quantization, sparsity, tensor core improvements.
22:50
Those are all architectural things you can do to compensate for the fact that your building blocks aren't getting any more energy efficient. Right? Note that
22:57
I'm not saying they're not getting faster. They are getting faster, but they're not getting more efficient. And so, okay, fine. Even if you're right,
23:02
Trevor, you're still an idiot because we'll just make models smaller, right? This does happen. And this is actually
23:07
kind of the hardest point for me to defend because if you look at the size of language models over time at a fixed
23:14
performance, they are getting smaller and fast, right? This is just from I think 2022 to 2025, you know, you can do
23:20
today uh with a 3 billion parameter model which used to take several hundred billion. We're really still learning
23:26
about how to build small AI models that are useful. Now, this plot is misleading for two reasons. The first one is that
23:33
these smaller models are reasoning models. And so they're actually despite the fact that maybe they have let's say
23:38
a 100 times less parameters, they're probably using 10 times more forward pass per problem. And so it's probably
23:44
more like one order of magnitude of improvement in terms of flops. That's kind of a hard number to calculate. The other thing is that if you look at a
23:49
different plot, which is the inference flops used at kind of the frontier of performance, that's going up very
23:56
quickly, right? So this plot here is um showing you how much compute was used
24:02
for a few of the most prominent uh AI demonstrations over the last few years.
24:07
So this box here is Alph Go from Google which is one game of Go. Uh and that was
24:12
around one exoflop per game, right? More recently we have Alpha Code 1 and two
24:18
which were all closer to 100 exoflops per coding problem. And then if you plot the IMO gold result, that's actually
24:25
closer to I think it was around 100,000 exoflops for the whole IMO competition, right? So despite the fact that we're
24:31
making models more and more efficient, uh the drive to improve capabilities is far outpacing the miniaturization of
24:38
models, right? And just from this plot alone, I think it's safe to say that we're not going to be saved by models
24:43
getting smaller, right? Because let's be honest with ourselves, the models we have today are not going to be terribly
24:49
useful as like general purpose systems in the real world, right? Like even solving a math competition problem is in
24:55
a lot of ways far easier than just like walking to school, right? In the sense that you're kind of in this sanitary
25:02
environment. You're not dealing with the real world random fluctuations and things you're not expecting. You might
25:07
still think I'm wrong. I might be. Um, and I think a lot of people in Silicon Valley when they hear me talk about this
25:13
just choose to stick their head in the sand and keep scaling. Uh, and that's great. You should scale because we're still going to get something useful even
25:19
if we don't achieve all the things we want to. But, you know, I'm an engineer and I see a problem like this and I want to solve it, right? And so that's kind
25:25
of what we've been trying to do at the company which is really just like find a less painful way to build AI. And so,
25:31
okay, we're in this kind of local minima. How do we get out? Well, let's go back to fundamentals, right? Is there
25:36
really a reason that machine learning works the way it does today with GPUs and and neural networks? No. Um, it was
25:43
kind of an accident. Neural networks were around for a long time before they got popular. And what kind of kicked it off was Alexet in 2012, right? People
25:50
discovered that, hey, these rendering machines are actually really good at doing parallel linear algebra, right? And so, you know, you had like your GTX
25:57
580 running compet and then that worked really well. You write a paper about it, then Nvidia takes notice, they make a Titan, you write a more complicated
26:04
paper, then Nvidia starts making data center GPUs just for this deep learning thing, and then you get language models,
26:09
and then it really kicks off, right? And you, by the time you get to the end of this, you have these like,200 watt monstrosities that are doing a lot of
26:16
matrix multiplication. And this is great, but is this like really fundamentally the way to do machine
26:21
learning? Well, if we want to think about how one should do machine learning, let's go back to the roots of what it is. Learning corresponds to
26:29
inferring the probability distribution that underlies some observations of the real world, right? And then inference
26:35
corresponds to sampling from your learn distribution to generate new things that look like the training data. You have a
26:41
bunch of data points that might represent some text. That's your training data. And kind of the
26:46
hypothesis is that there's some well- definfined distribution that underlies the training data. You initialize a
26:52
model and then through the magic of machine learning you shape that model to look like the distribution that
26:57
underlies your data and once you have that in hand you can sample new examples that if you've done a good job look a
27:04
lot like the data right so that the point to take away from this is that machine learning is about fitting distributions and sampling from them
27:11
right and to make that more concrete we can look at a specific example which is diffusion models so in diffusion models
27:18
you define this process that converts your data into noise incrementally, right? So, you start your diffusion
27:24
model off with a noiseless example and then you run it through what's called
27:29
the forward process which gradually adds noise to your data one step at a time. Right? In this case, it's just three in
27:35
real diffusion models. There can be a lot more. Then at the end of your diffusion model, you have pure noise. Then what you try and do is you try and
27:41
learn to reverse this. So you try to learn a process that starts from pure noise and ends at something that looks
27:47
like the data. And the way you do that is by the thing I just showed you on the other slide, which is you take samples
27:53
before and after adding noise and you try and learn the distribution that maps from later times to earlier times,
27:59
right? And then once you learn all of those distributions, you can go backwards through your model and
28:06
generate samples that look like the data, right? So like here, you start noisier and you d noiseise towards
28:12
something that looks like came from your training set, right? And so you can see that very literally
28:18
when you run inference on a diffusion model, the thing you're doing is sampling from a complicated probability distribution. Now what do GPUs do? Well,
28:25
they rotate vectors. GPUs were originally invented for rendering, which is largely a series of perspective
28:31
transformations. So you take your point cloud in 3D space and you project it onto a plane normal to your camera to
28:37
produce a 2D image. And it turns out that that process largely involves four-dimensional matrix vector
28:43
multiplications, right? And so that's what GPUs are invented to do. Neural networks also rotate vectors and skew
28:49
them. Um, and this is obviously no coincidence, right? This is a result of the co-evolution we just discussed. Deep
28:54
learning really came up to work well in the GPUs that already existed. But none of this has anything to do with
29:00
sampling, right? So how do we bridge the gap between something like a denoising model and something that runs well on
29:06
GPUs, right? Well, you use some math. Turns out that if you use an infinite number of steps in your forward process
29:12
in your denoising model, you can reverse each step exactly by sampling from a
29:17
Gaussian distribution where the mean is defined by the input to that step. Right? So you take your noisier sample,
29:24
you can pass that into a giant neural network, produce a mean vector for your Gaussian, and then sample from that to
29:30
go backwards. Right? So in order to make these kind of sampling procedures work
29:36
well on GPUs, we have to operate in this kind of degenerate limit where we're taking an infinite number of steps in
29:42
theory, right? Obviously in practice, people will find ways around this. But you can see that there's something like kind of not right about this. We have to
29:48
go to this extreme limit of our algorithm to make it run on a GPU. And none of that's to say that this isn't a
29:54
miracle and works very well. It does, but it is kind of theoretically odd. And so when I look at that, I think there
30:01
must be a better way to do this, right? Like why do we have to work in this infinite time limit just to avoid
30:07
sampling, right? Or to make make it make our machine learning problem look like something that a GPU can do. And that
30:13
kind of basic idea gave rise to the thermodynamic sampling unit. Now what is a thermodynamic sampling unit? It's
30:19
called thermodynamic because a TSU produces samples from an energybased model, which is a distribution that
30:26
takes the form I'm showing here. Right? So we're just directly parameterizing a probability distribution via some energy
30:32
function epsilon that we're learning right and so this animation here is showing the correspondence between epsilon and p uh in this case epsilon is
30:40
this like double well thing and when you take the exponential of the negative of that you get a biodal probability
30:46
distribution right and you can see that when I change the parameters of epsilon theta I change the shape of the
30:52
distribution right and so in some sense this is EBMS are an extremely literal
30:57
interpretation of machine learning where it's like okay you want to fit a probability distribution we'll just learn the shape directly right don't go
31:04
through some kind of convoluted den noising process just learn the shape of the distribution directly and so that's
31:10
kind of useless because that's just a bunch of math what are TSUs actually in practice they're giant arrays of random
31:17
sampling circuits and so fundamentally um what allows TSUs to work this way is
31:22
something that's not talked about a lot because it's kind of ancient algorithm called Gibb sampling Uh, and what Gibb sampling lets you do
31:29
is break down the task of sampling from some complicated energy function into a bunch of small tasks that can be
31:35
completed by simple sampling cores. And the way this works mathematically is if you have some kind of complex
31:42
distribution defined by complicated energy function, if it happens that your energy function is structured in as a
31:49
sum of terms where each term depends on only a subset of the variables, you can
31:54
sample from it by sampling from a a much simpler EBM um for each degree of freedom in the model where this epsilon
32:01
i only depends on terms that include x i. Right? So it's that's you'll probably
32:07
have to look at a textbook to really understand what I just said, but basically Gibb sampling lets you modularize sampling, right? So you can
32:13
do it on some kind of simple piece of probabistic hardware like this. And practically what it looks like when a
32:18
TSU is doing GIB sampling is you go through the cells one at a time or in blocks. Um and in each step each cell
32:27
receives state information from its neighbors. It computes basically the
32:32
parameters of this distribution epsilon i based on the neighbor states and then a probabilistic circuit samples from
32:40
this conditional distribution based off of the parameters calculated from the neighbor states, right? And so this
32:46
animation here kind of shows the signal flow um at basically at any given point in time half of the cells receive state
32:54
information from their neighbors then go through this kind of conditional update procedure. Right? So you have this giant
32:59
array of random sampling circuits working in parallel um to produce samples from an energy based model
33:04
coming from some particular family. This is by the way like a really ancient concept. We did not invent this. This
33:10
has been around since the 80s. Um it's if you know what Gibb sampling is and have any background in physics, it's
33:16
super obvious that you should build this. Uh and here's a couple of papers from a bunch of people talking about
33:21
building this. Right? So it's this is not something we invented at Extropic. It's been around for a really long time. So, like I said, this has existed for a
33:27
really long time. Um, people have been studying in academia for decades. Why are we the first ones to do it? Well,
33:34
the first and arguably most important thing is we figured out how to use TSUs efficiently. So, you know, you can
33:40
define these kind of sampling problems that'll run on a TSU, but that doesn't mean they can actually do anything in the real world. You know, a lot of the
33:46
ways that people have been trying to use these things in academia is to solve optimization problems. So what you try
33:51
and do here is you take some kind of simple problem like graph coloring where you know you're trying to find a
33:58
coloring of the graph such that no two neighbors are the same color.
34:03
So like this graph is well colored, right? Because there's no two neighboring nodes that are red or green or blue, right? And you can take that
34:10
kind of problem and encode it directly into an energy function, right? So this energy function would penalize the model
34:16
for having ever having two neighbors be in the same state. And you can, it turns
34:22
out you can encode a giant class of optimization problems into an energy function in exactly this way, right? And
34:27
so then if you built a TSU, you could just run the TSU forward and it's most
34:32
likely to end up in a state that has low energy which corresponds to solutions to your optimization problem. Right? So
34:38
this is in academia is how people use these things and it's a good idea and it works and it's useful even just you know
34:45
running simulations of these things turns out to be a good heristic for a lot of hard problems. But the kind of tough thing is that the market isn't
34:50
that big. So optimization is obviously extremely useful, but we're talking like tens or hundreds of billions, which is
34:56
not enough to get a novel computing paradigm off the ground. What's more exciting is using these things for generative AI.
35:03
And uh what's funny is that this is actually kind of the original form of machine learning. Um this is kind of the
35:10
first thing that invented even before neural networks. And he won the Nobel Prize for it last year, which is convenient for us. Uh and in this case
35:18
what you do obviously is you just try and shape the energy function to match the distribution of your data directly.
35:23
Right? And there's a bunch of math you can do on this and shows that there's good learning rules that just involve
35:28
sampling from the model. But the nice thing about this for a startup is that the market is massive as we've seen recently. Right? So this kind of allows
35:36
you to take a big shot like what we're doing at Extropic. The real algorithmic challenge with using energy based models
35:43
and also TSUs is that despite the fact that they run the Gibb sampling algorithm very efficiently, it can still
35:50
be very difficult to sample from complicated distributions using them because sampling algorithms like to get
35:56
stuck in valleys. So if you have an energy landscape that's extremely rugged
36:01
like this one, your sampler is going to have a really hard time climbing over this valley. And this is a huge problem
36:07
for machine learning because all real data sets are very multimodal which means they have lots of valleys
36:12
separated by tall barriers, right? Because like for example in an image data set you're very unlikely to find a
36:19
dog airplane hybrid, right? So there's there's just regions of space that have very high energy that are very hard for
36:24
sampling algorithms to cross. You don't have to go to something complicated like a machine learning problem to see this
36:30
phenomena. It happens even just in 1D with this double well, right? When the well is high, the particles all get
36:35
stuck. But when the well drops, they can cross. Now, this distribution with the
36:40
well dropped isn't useful for machine learning, right? Because you can't encode any structure into it. And so,
36:46
this is kind of the main algorithmic challenge that you have to solve to make TSUs useful. And it turns out that the
36:52
solution to this is actually to go back to a denoising type procedure like we were just talking about, right? But now
36:58
because you have access to this piece of hardware that can sample from something much more complicated than a Gaussian,
37:04
it turns out that you can use far far less steps in the reverse process, right? Because you can kind of skip
37:09
steps. And what I mean by that is you have the same kind of setup as a denoising model or a diffusion model
37:15
where you start with pure noise, but now instead of going back through a bunch of neural networks and Gaussians, I'm
37:21
actually just going to sample from an energybased model at each step. Yeah, my first step will look something like
37:26
this. I'll run my sampler for a while, arrive at a state, take that out here, shove that into another EBM, run the
37:32
sampler for a while, find a new state, go back again, sample from another EBM, and finally get my output that looks
37:38
like something that from the training data, right? So, it's very simple. Take a diffusion model, make the steps
37:44
bigger, and insert energy based models where you have gausian. And the nice thing here is that I can actually tune
37:50
the complexity of the sampling task by shrinking or lengthening the length of the steps in the forward process. Right?
37:55
So what this kind of framework lets me do is actually trade off directly sampling complexity for a number of
38:01
steps in the forward process. And it kind of lets me turn any problem into something that can run efficiently on
38:08
something like a TSU. And this idea of using EBMS in a denoising process is actually something
38:14
that started getting to get studied in just the deep learning literature recently. Right? So this is a fairly
38:20
contemporary idea. And when you think about what that looks like running on a TSU, you get something like this. So
38:26
what you do is you take a TSU and you divide it into a bunch of separate patches that represent the different
38:32
time steps in the denoising model. Then just like in a traditional diffusion model, you sample a bunch of noise and
38:38
you send that noise into the first layer of your TSU. Then your TSU runs for a while and
38:44
produces a new sample corresponding to the dnoised output. Then you take that output, you send it
38:50
into the next layer of the TSU, run your sampling again, and den noiseise a little further. And you just keep
38:55
repeating this process over and over again until you finally get something
39:01
that looks like a image. Cool. So one thing is we figured out how to connect
39:07
energy based model sampling to modern machine learning or actually someone else did and then we took their work.
39:13
The second thing we did is we figured out how to actually build TSUs at scale. Believe it or not, making a circuit that
39:20
reliably samples from a distribution is extremely hard. Even though noise is present in every circuit, including just
39:26
a resistor and a capacitor, right? That circuit has noise that behaves in a predictable way. It turns out making
39:31
that useful in the computer system is extremely hard. to the extent that there's a lot of academic groups out there working on incorporating exotic
39:38
stuff like spintronics which are basically nanoscopic magnets that you can um print or build into a CMOS
39:45
process if you try really hard. People are trying to figure out how to use these exotic spintronic devices just to get good RGs. And that's kind of been
39:52
the state-of-the-art in the field and they're really good. um but it's going to be decades before you can scale that
39:58
up to to the point where you can go to TSMC and say hey can I have some 5nometer transistors and also um
40:04
spintronics right so that's a really challenging path random number generation using conventional techniques
40:09
like kind of uh chaotic circuits basically is extremely expensive um the
40:15
energy to generate one random bit on your CPU or GPU is quite similar to a full 32-bit floatingoint ad which is
40:22
kind of ridiculous right because a floatingoint ad is much computationally richer than just generating a single
40:27
random bit which is why a lot of these randomized algorithms aren't popular on GPUs. The key thing that we brought to
40:32
the table is we figured out how to build these random number generators using just transistors. Our performance is
40:38
similar to something like a spintronic device but we use only transistors uh that you can make on mature processes
40:44
already. Right? So we've kind of shortcuted that entire uh bring up and
40:50
just avoided that problem entirely. And performance is quite similar. Now what did we have to do to get there? Well, we
40:55
had to think really really hard about noise and transistors and uh think hard about how to engineer transistor
41:01
circuits to do useful sampling tasks. Once you get into the physics, you realize uh that transistors are actually
41:06
an extremely natural substrate for probabilistic computing, specifically transistors operating at very low
41:12
voltages. Because when transistors are run at really low voltages, the dynamics of charge in the transistor is almost
41:18
completely driven by thermal fluctuations. Right? So current in transistors at low voltages is is
41:25
completely driven by noise, right? Which is where you want to be if you're building a probabistic computer because then you're not wasting a bunch of
41:31
current doing deterministic stuff. And the way to think about transistors in this kind of really low voltage regime
41:36
is actually using the same kind of double well picture that we were looking at earlier. When the gate voltage applied to a transistor is very low, the
41:43
picture you should have in your head is you have your source and your drain, which are the two terminals of the transistor that current flows between.
41:51
um being separated by a really high energy barrier, right? So, it's very hard for charge to cross from one side
41:56
to the other and that's what turns a transistor off. As you increase the gate voltage that you're applying to the
42:01
device, you actually drop this barrier, right? You make it possible for charge to flow from one side to the other. And
42:07
that's how you turn on a transistor. And when you have the barrier low and you apply a bias, so you apply a voltage
42:14
from the source to the drain of the device, current can actually flow. Right? So this is actually pretty much
42:20
exactly how we think about transistors low voltage and we've built a lot of you know very rigorous physical models of
42:26
this kind of process and and use them to design our circuits. And it's this kind of key theoretical insight that let us
42:31
design probabilistic circuits using only transistors and no exotic devices. And so once you have all these models you
42:37
can build a whole family of probistic circuits that do useful things. Right? So this slide here is showing you the
42:43
output of what we call a probabistic bit which is just a source of um random ones and zeros. Uh and so this is what a time
42:51
trace output of a probabistic bit looks like where the x-axis is in units of the correlation time um which is taken from
42:57
that plot over there you know around 100 nconds or so for this device. You can go faster if you want it. And the key thing
43:03
is that you can control the bias of the random bitstream. Right? So by applying a control voltage to my probabistic bit,
43:11
I can make it either more likely to produce zero or more likely to produce one or be something in the middle, right? And so that's what these
43:16
different time traces are showing you. So we can use these exotic noise models and transistors to engineer circuits
43:22
that do something useful. And you can you can build a lot more than just probabistic bits, right? So this is
43:27
actually a circuit uh with four possible states and it moves randomly between the four states and that gives you samples
43:33
from a categorical distribution and I can control the probability of my system
43:38
being in any particular state at a given time right arbitrarily and it's not even just discrete stuff. So in principle you
43:44
can build circuits that sample from continuous distributions. So this is an example of a circuit that's sampling
43:50
from a simple Gaussian mixture model which is just two Gaussian distributions
43:55
kind of added together and you know I could control the position of the two
44:00
Gausian modes which is what's going on over here. So by applying a voltage I can move the two blobs relative to each
44:06
other or I can control the kind of weights of the mixture right so I can make either the left Gaussian or the
44:12
right Gaussian more likely. All this goes to show is just that you really can control noise in transistors with the
44:19
pretty good fidelity and use it to do something useful. Yeah. Is it like more sensitive to ambient
44:24
temperature than normal stuff? Um so it's yes kind of. Um but also the
44:32
temperature dependence is much simpler in these low voltage regimes because everything is thermally activated. So
44:38
it's very sensitive but it's also very predictable which means you can deal with it basically. Yeah. So once you
44:44
know how to build proistic circuits out of transistors, there's nothing else in a TSU that is really that unconventional
44:50
and so you can immediately scale it. That's what we're doing now. Um so imminently we're going to be building a
44:56
much much a very large TSU that is actually much larger than anything we can reasonably simulate on a GPU. And so
45:02
once we have this chip, that's really when um the joint machine learning and hardware research is going to take off.
45:10
Now what happens when you put that all together? when you when you take a DTM and you run it on a energy efficient
45:17
TSU, you get really really high performance in terms of energy efficiency. And so this is a result from
45:23
the paper where we show that you know simulations of a TSU predict that it can generate samples from a really simple
45:30
data set using around 10,000 times less energy than the most efficient algorithm running on a GPU. So this research is
45:36
not about capabilities. This is a very simple data set. The point is that head-to-head for equal performance, our
45:44
thing is 10,000 times better than a GPU, right? And so with all of that, the real
45:49
question is how do you scale this? Because the the data set that we worked on in that paper was way too simple to
45:56
be representative of a real life machine learning problem, right? And the key thing to realize is that these denoising
46:03
thermodynamic models are not full machine learning models on their own. they actually should be used as primitives in a larger system, right?
46:11
And so the way that you scale this, I think, is by figuring out a way to intelligently combine these DTMs uh with
46:17
conventional neural networks running on GPUs. So you're not spending zero energy on deterministic compute, but you're
46:23
spending way less, right? And in the paper that'll be published shortly, we actually did a very simple version of
46:29
this where we basically um used one of these denoising models to see again. And
46:35
it turns out when you do that you end up using way way less deterministic flops than using the GAN on its own. Right? So
46:41
this is extremely early research. But I think this direction is very promising and I think it's quite likely that using
46:47
one of these hybrid algorithms we could have a system that does something much more complicated like imageet using let's say two orders of magnitude less
46:54
flops than what's possible today. Right? And that's already extremely interesting. Um, how close we can get to
47:00
the kind of 10,000x asmtoic result that we had in kind of a pure proistic computing benchmark is an open question,
47:06
but that's, you know, getting getting there is really why I'm here today because this is only the beginning of
47:12
what we're going to do. I really can't believe that we accomplished all of this with 15 people, all of this hardware
47:17
research, algorithms, like novel transistor models. That's kind of insane. Um, and so the goal from here is
47:22
to build the team much bigger, hire a lot of people, do a lot more research and and scale this thing and and make uh
47:29
scaling up machine learning way easier than it's going to be otherwise, right? So I hope to some extent I've inspired you guys to want to join the team. Um,
47:37
this QR code just goes to our careers page, so you can find that on our website, too. And very shortly you'll be
47:42
able to read a paper that kind of covers everything I just talked about in much more concrete terms. Thanks.