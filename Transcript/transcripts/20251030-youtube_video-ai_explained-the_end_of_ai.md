# The End of AI: A Thought Experiment on Evolution and Integration

Sometimes I think about where it all began with AI, and I believe the first real disruption came when we started to understand chain-of-thought prompting. We asked an AI to break down a problem into a series of intermediate, sequential reasoning steps, and this led the model to better formulation and better reasoning than if we simply said "do this" and nothing else.

Imagine sitting in an empty classroom. You are an LLM—not an agent yet. You have nothing but your brain, whatever you learned up to this very moment. You have just a linear sequence in the complexity of creating your thoughts, learning, and experiencing your immediate epsilon environment. That's it. Your reasoning complexity is limited to a linear sequence.

Then, around 2022, we had a leap to action. The React framework emerged for AI, introducing three main components: first, to reason and formulate a thought about what needs to be done next; then to act—to actually execute an action, go to a database, search the internet; and finally to observe the result, analyze it, and integrate it.

What happened? You're still sitting in an empty classroom, but now you have internet connectivity. You can connect to external data, but you don't know if you can trust those external data, because you also have internal data—what you learned from your parents, from school, from your local environment. Something interesting is now happening. With internet access, agentic features, and memory, a world model is synthesized inside this agent. This means we have parametric knowledge from pre-training and an external data stream, and somehow this world model has the task to make sense of new data that are not yet knowledge alongside the parametric knowledge from the pre-training phase. This is the first clash: data encounters knowledge.

The next frontier was knowledge graphs—structured knowledge in particular forms with particular topologies and mathematical spaces. For more than a year, research has explored Howard's knowledge graph agents, LeanRAG with multiple layers of knowledge graphs, GraphRAG, and how LLMs repair knowledge graphs. We've looked at combining knowledge graphs with agents, using sentence BERT encoders for structured multi-agent systems. We've examined graph neural networks, not just RAG but GraphRAG, the integration of higher-dimensional knowledge graphs. Then we moved to in-context learning, from prompt generation to subgraph generation for the ego methodology and the integration of RAG into textual gradients for interpretable artificial intelligence.

But where are we now? Take a step back. We now have multi-agent systems where every agent has its own brain, has connectivity to the internet, to databases, whatever. And either we have a coordinating agent—a supervisor agent—or, like in a beehive, we just have local coordination between local groups of multi-agent systems. We've increased the complexity, and there's significant information, data, and knowledge generation within this multi-agent network.

We noticed that with the old React framework we got stuck in repetitive failure cycles. It just repeated itself. Even graph neural networks and vision transformers, especially when we extended to multimodal with images and video, had limitations because of inherent complexity constraints. So we tried hierarchical reasoning to overcome those limitations. Three months ago we had hierarchical reasoning models, and two months ago we had hierarchical reasoning on GraphRAG complexities themselves. We now have LLMs arguing on a graph manifold, on a RAG manifold, trying to find better solutions—the next step in the evolution of AI systems. Then we went further with completely distributed neural graph architectures for AI systems, with some brilliant ideas from Stanford University.

In this time, I think we all understood that the future of AI is not solitary. We can expect to see the rise of more multi-agent systems where different AI agents have specialized skills and they collaborate with particular protocols—Model Context Protocol with tool use or simple API calls—to solve problems that are too complex for any single agent.

You understand we are not anymore talking about single individual agents. Now we go for a collective, for a hive mind. We have the latest AI ecosystem work regarding context engineering, cognitive upgrades for multi-AI agent systems, and new protocols for multi-agent systems with 100-plus agents—how to communicate, data knowledge transfer, how to learn. Even new mathematical ideas about directed acyclic graphs applied to network layers.

Where can we find the new AI reasoning revolution? Because our current AI systems are just pattern recognition machines and nothing else.

Where are we now? We have a student or a little agent sitting in a full classroom with many other agents, many other students. Now they can ask a classmate for help on a mathematical problem, brainstorm ideas together, divide up research tasks for a group project. At this moment, I think we all in the AI community understand it is no longer about individual agents. It is now about a social network of AI agents working together.

## The Philosophical Turn: AI as a Subsystem of Society

This brings me to something more speculative, more philosophical. I want to think about AI not as a tool or even as a collection of agents, but as a subsystem emerging within human society. Let me explore this from a systems theory perspective.

There's a fundamental question: does AI constitute its own subsystem within human society, or is it merely an extension of existing human institutional logics? I think it's becoming its own subsystem—a technical subsystem with its own operational logic that is genuinely distinct from human social and cultural systems.

Human society operates through meaning, through narrative, through interpretation. We have ambiguity, we have emotional intelligence, we have cultural context. AI systems, in contrast, operate through optimization functions, through probabilistic inference, through pattern matching at scale. These are fundamentally different logics.

Consider this: human institutions like governments, markets, and civil society are governed by norms, by laws, by cultural expectations. But the AI subsystem is not governed by institutional pressures in the same way. It's shaped by coded utility functions and constraints defined by its corporate creators. Remember what Sam Altman tells us almost daily about how they change GPT's behavior slightly, how they brought back GPT-4 Omni with certain personality traits. This is really about the corporate creators, and they have extreme influence.

This AI technical subsystem is interesting because it will soon control the operational infrastructure of modern life: energy grids, logistic chains, capital flows—on a corporate level, on a personal individual level. I think AI will penetrate into those sectors and into our personal and professional lives very soon.

Then we have to encounter and understand the key dynamics of this system integration. I think we will experience social fissures emerging from the friction between these two incompatible logic systems—between AI and humans.

## The Specification Class and New Power Dynamics

I already mentioned this idea of a priestly elite. That happened 2,000 years ago, a long time ago—not just in Greece, but everywhere on the planet.

Human social goals like "promote public well-being" or "increase fairness" must be translated into the AI system—into formal quantifiable objective functions, value functions, reward structures, training data that we train in reinforcement learning for our AI systems. Who decides on what particular reward function to train GPT, which is used by 800 or 900 million people daily or weekly?

This is real power. This translation process—how to design the AI, the sensitivity of the AI, the nuances of the AI—this is a new critical locus of power. Absolutely.

Let me speculate with a crazy idea. There could be a very tiny chance that we will see the emergence of a specification class—let's call it a technocratic elite—an elite of policymakers, data scientists (though it's a little bit more than data scientists), and corporate strategists like Sam Altman, whose primary skill is to define, to audit, and to negotiate the formal metrics for AI systems. They tune AI systems to what they want society to behave like.

I'm not a sociologist, but I love to read a little bit of their work. This can be modeled as the creation of a new form of symbolic capital, if you're interested in that framework.[^1]

I'm absolutely fascinated to see where AI will lead us if we have this human-AI augmentation, interaction, symbiosis.

However, I think we still have an asymmetric path dependency in our system, because it seems to me—and maybe I'm wrong—that human society is becoming operationally dependent on the AI subsystem for its stability. If I see young students learning, it's crazy: everybody's using AI systems—ChatGPT, Claude—for programming and so on.

Critical functions of the state and the market become suddenly opaque to human understanding, to democratic oversight, because the complexity is managed exclusively by AI subsystems. You don't have a human expert explaining complex issues to you. You can ask the AI, and it will explain it to you—whether it's correct or not, who cares?

Human institutions are left managing the social consequences of black-box AI decisions without the ability to influence the mechanics of those decisions. Many people do not even understand to 5% how an AI system works internally—the internal mechanics of the workings.

I think the primary social fissure will be between the specification class—the tech billionaires like Sam Altman or others who can really define what behavior they want from the AI system that is used by a billion people on this globe—and on the other side, another class: the systematically managed class, whose lives, whose opportunities, and whose environments are the raw material to be optimized by those AI systems.

Maybe you understand what I want to tell you with this sentence.

For the humans, I have this feeling: yeah, you can vote, you have rights, but maybe you can change the values sometime for a short period, but you can't change the underlying operational logic of the system. I think this could theoretically, in the worst-case scenario, lead to political and social apathy, resulting in the isolation of individuals and a social decoupling from the circles of friendship, from bonds. I have to tell you, I noticed this myself. It is kind of crazy to formulate this, but I think this is something I'm becoming aware of right now.

## Filter Bubbles and Social Fragmentation

I think social trust will also fracture along technological lines, not just around political parties. Every nation is different and has different degrees of these dynamics in political systems. I'm not talking about that. I'm talking about the AI ecosystem and the impact it will have on human society. Because since AI systems shape the information and the opportunities that you—the human users—see, those AI systems have theoretically the power to create hardened, sealed communities, making social consensus a structural impossibility.

This is really a kind of weaponization of filter bubbles into a fundamental infrastructure of an advanced society where we have dense human-AI interaction patterns.

But you know what? On the other side, we manage this already without AI—just by human stupidity and human ignorance. We also have today, in social media, our filter bubbles, and we don't even read any argumentation. We don't listen to the other side. We are closed up already without any AI.

## The End of AI: Complete Integration

If I come back to the beginning of this reflection, where I said "the end of AI"—you know what? Maybe AI is just completely integrated within our society within the next years. Maybe this will become just a standard gadget or a standard interaction for our society. Maybe we will not even notice that it is another form of intelligence. Maybe then we are absolutely familiar with this alien kind of technology that we have integrated into our professional and personal lives. And maybe this will be the end of AI.