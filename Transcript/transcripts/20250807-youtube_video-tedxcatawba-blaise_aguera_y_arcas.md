# The Intelligence of Us: Rethinking Minds in the Age of AI

My background is in computational neuroscience and applied math. I went to Google about 12 years ago, and my first several years there, the main project was to add AI features to Android and Pixel phones. Most of the AI in those phones over those years came from teams that I built. Some of these features are things like face unlocking and music recognition—this feature called Now Playing. We built predictors of next words for the keyboard, the Google keyboard Gboard. These things were called AI, but they weren't really AI in the sense that researchers meant when they invented that term back in the 1950s. When we thought about AI as kids, it probably wasn't face recognizers or handrail recognizers or something like that.

That was why the terms artificial narrow intelligence and artificial general intelligence were invented—in order to draw that distinction. Artificial narrow intelligence is models that are trained for doing specific tasks like these. Artificial general intelligence is a robot you can have a conversation with. We didn't have artificial general intelligence, but we did have these models for doing things like next word prediction. We always assumed that these statistical models for next word prediction were inherently going to be very limited because if the first word is Humpty, then it's easy to statistically predict that the next word is going to be Dumpty. But if the previous words are a mathematical word problem or a story, and the next word is about how a character in that story feels at this moment—of course, statistics won't be enough to answer that. Those are problems that are called AI-complete.

So it was quite a shock when we basically scaled up the same sorts of models and built things like Meena[^1] and LaMDA[^2] in 2020-2021, and found that they could actually answer those kinds of questions. They were able to solve all of those kinds of general AI problems. This was a bit of a shock because they really are, in some sense, just autocomplete.

But the neuroscientist in me maybe shouldn't have been so shocked, because there is at least one theory in neuroscience that goes back a long way—at least to Helmholtz in the 19th century, certainly to the cyberneticists in the 1940s—that brains basically are autocomplete. The reason we develop nervous systems is to predict the future given the past, in order to be able to affect it. So there is some sense in which the fact that these models that just predict the next word end up being intelligent is validation of an old idea in neuroscience.

This was quite a shock, but I began around the same time to write books. A lot of these books—in fact, all of them—are questions. Even the first one, which was fiction in 2022, *Ubi Sunt*[^3] (Latin for "where are they?"), and the other ones all have question marks at the end. It's taken me a while to figure out what the underlying theme or question was behind all of these books that I began writing around this time—that it started to become clear what AI was really about and what it told us about ourselves. The last of these books is just coming out in September. *What Is Intelligence?* addresses that one head-on, but the other ones are, as I said, fiction, social science, and theoretical biology.

I think that if there's an underlying theme behind all of these, it's the idea of major evolutionary transitions—a concept that was introduced by two evolutionary biologists, Eörs Szathmáry and John Maynard Smith, in the 1990s. The question they were asking is: Why does life become more complex over time? Why is it that we began on Earth with single-celled organisms, and then we got multicellular organisms? Somewhere in there, the cells became more complex when mitochondria made their way into archaea and we got eukaryotes. Why do those multicellular animals become more intelligent over time and form societies and eventually launch spacecraft and do all kinds of crazy things like this, and even develop AI? They called these major evolutionary transitions. Their initial list includes some of the events that I've just described, and they've added a few since then. I think we're going through another of these right now, which is the emergence of AI.

So what do these transitions have in common? Why do they happen? Is life actually becoming more complex, and why is that taking place? And are we in the end times?

No, I don't think we're in the end times. Yes, life is becoming more complex. And I think the answer to the question of why is a very counterintuitive one, and one that I've only come to appreciate in the last year or so, which is that I think life is computational. It's kind of right there in Smith and Szathmáry's paper when they say these changes involved changes in the way information is stored and transmitted—which is computation. Indeed, the original theoretical model for computation, for a computer, which was introduced by Alan Turing in the 1930s, was modeled on the process of biological evolution. That was what he was trying to build a mathematical model of when he invented what we now call the Turing machine.

So the idea that life is fundamentally computational is actually quite an old one, but I think it's one that hasn't really sunk in or hasn't been widely appreciated. The way I now think about this is: What is biology? Biology is matter trying to find an order that resists entropy. You have this rule, the second law of thermodynamics, that tells you that things become more disordered with time. Biology is a way of resisting that, of maintaining order. And the way you maintain order is through computation—through a kind of active control and prediction.

Let me show you a very simple example of this. This is a toy model of the origin of life that I built a few years ago. It's called the computational soup. What it is, is a string of random bytes—random bits of information—and you have rules for how these bytes interact with each other. The rules are very simple: bytes that are next to each other can interact, and when they interact, they can copy each other or they can destroy each other or various other things can happen. You start with random noise, and you let it run. What you find after running this for a while is that patterns emerge—patterns that can replicate themselves.

Now, when we think about Darwinian evolution, we tend to think about it as this process that takes place over millions or billions of years. But in this simple model, you can watch it happen in seconds or minutes. And what's striking about it is how inevitable it seems. Once you set up these rules, once you have this computational substrate, life-like patterns emerge spontaneously. They don't emerge in a random way from that starting point—it looks almost like intelligent design. Why is that happening?

This is, by the way, what that looks like in terms of amount of computation happening in that soup. I've drawn here the first 10 million interactions in a particular run. Each interaction is a dot. The x-axis is time, and the y-axis is how much computation took place in that interaction. What you can see is that there is a discontinuous transition right at 6 million interactions in this particular case, where suddenly the soup starts to compute—because the process of copying or replicating is a computational process. You can think about this almost like two phases of matter. On the left is a phase of matter that is disordered, like a gas. On the right is a phase of matter that you could call life. It's purposive.

All right, so why does it become more complex over time? Well, Lynn Margulis, I think, came up with the answer back in 1969. She was the one who figured out that eukaryotes—the complex cells like the ones that make up our bodies—are made out of mitochondria that made their way inside other cells in an act that she called symbiogenesis. In other words, a symbiosis between two parts that made something more complex than those parts—made a whole more complex than those parts. And it turns out that that's exactly the same thing that's happening in the soup.

When you first start running this thing, you occasionally get a single byte that manages to replicate itself. What happens is that those individual bytes sometimes team up. That is to say, they manage to find themselves next to each other, and the pair can reproduce better than one of them could on its own. And that process repeats and ladders up, and that's how you get complexity. It's basically all about teamwork. That form of teamwork is just as natural as evolutionary selection itself, and it's a key part of the way evolution works, we now believe.

And that's the same process that has led to the human intelligence explosion. When we look at how it is that we went from being individual stupid animals to collectively brilliant animals as we are today, it really has nothing to do with what's going on in individual brains and everything to do with how we collaborate together. I sometimes joke that if you take an average Manhattanite out of their apartment and ask them why the toilet flushes when you push the button, they won't know, right? Individually, we don't know a lot, but collectively, through teamwork, we know a great deal. It's that collective intelligence that we really are talking about when we talk about human intelligence—not what we can do individually.

And there's a kind of arms race—a friendly arms race, if you like—that happens when people are trying to predict each other. As you try to predict others, as you try to model their minds, your brain has to become more complex. But then when you're the subject of that prediction, then somebody else has to be more complex in order to predict your mind, and so on. That's the kind of one-upsmanship that has led to these intelligence explosions. We can see that the brains of different animals become larger as their troop sizes grow—or looked at another way, as the troop sizes grow, their brains have to become bigger. We've seen these social intelligence explosions not only in humans but also in whales and in the smarter birds like parrots, and in a couple of other places.

So scaling cooperation and competition is the way intelligence rises, the way complexity rises. It's all a part of the same process. And this is the connection between the last two books that I've written: *What Is Life?* and *What Is Intelligence?* I've come to think about life as a self-constructing computational phase of matter that complexifies through symbiogenesis, and of intelligence as the ability to predict and influence one's future, which complexifies through symbiogenesis. They're kind of one and the same thing.

All right, so I just want to leave you with a few points to end.

First, we have been thinking about AI as this kind of alien invader, this other that comes to us in the 2020s. I've increasingly come to realize that that's not the case. For one thing, we got AI when we began to train it on human experience. And so it is about as human as it gets from an information standpoint. It also is computational, just like us. And in that sense, it's a part of that same evolutionary process that's been going on on Earth for more than four billion years. Biology is computational too.

It's been a long time since individual humans were the pinnacle of intelligence on Earth. It's human society as a whole collective that is intelligent in that way. And AI is increasingly a part of that collective intelligent fabric. In that sense also, it's not alien. It's actually a part of what we are in some bigger sense.

Humanity is collectively brilliant. Humanity plus AI is going to be even more collectively brilliant.

And finally, this symbiosis that's emerging between AI and humanity—I don't want to be too Pollyannaish about it. Symbiosis can be hard. They can be disruptive. They can lead to complexity. They can lead to civilizational troubles of various kinds. But ultimately, they're rich and positive. That's how they've been for the last four billion years, and I have no reason to expect that this next one won't be as well.