https://www.youtube.com/watch?v=sozyCpZX4O4
Godfather of AGI on Why Big Tech Innovation is Over
24,711 views  Oct 20, 2025  Digital Disruption with Geoff Nielson
Is the AI arms race between tech giants and nations pushing us toward a dangerous future?

Geoff Nelson is the host on the Info-Tech Research Group YouTube channel for the "Digital Disruption" podcast/video series.

On this episode of Digital Disruption, we’re joined by the founder of SingularityNET and the pioneering mind behind the term Artificial General Intelligence (AGI), Dr. Ben Goertzel.

Dr. Ben Goertzel is a leading figure in artificial intelligence, robotics, and computational finance. Holding a Ph.D. in Mathematics from Temple University, he has been a pioneer in advancing both the theory and practical applications of AI, particularly in the pursuit of artificial general intelligence (AGI) a term he helped popularize. He currently leads the SingularityNET Foundation, TrueAGI, the OpenCog Foundation, and the AGI Society, and has organized the Artificial General Intelligence Conference for over fifteen years. A co-founder and principal architect of OpenCog, an open-source project to build human-level AI, Dr. Goertzel’s work reflects a singular mission: to develop benevolent AGI that advances humanity’s collective good. 

Dr. Goertzel sits down with Geoff to share his insights on the accelerating progress toward AGI, what it truly means, and how it could reshape human life, work, and consciousness. He discusses the role of Big Tech in shaping AI’s direction and how corporate incentives, and commercialization are both driving innovation and limiting true AGI research. From DeepMind and OpenAI to decentralized AI networks, Dr. Goertzel reveals where the real breakthroughs might happen. The conversation also explores the ethics of AI, the dangers of fake democratization and false compassion, and why humanity must shape AI’s evolution with empathy and awareness.

In this episode:
00:00 Intro
00:21 What is Artificial General Intelligence (AGI)?
01:10 The pace of AI progress and the hype cycle
05:44 The path from human-level AGI to superintelligence
09:20 How close are we to AGI? 
13:08 Transformer vs. multi-agent systems
14:05 Which AI labs might strike AGI gold? (DeepMind, OpenAI, Anthropic)
17:07 Big Tech’s innovator’s dilemma and why true AGI may come elsewhere
20:20 Predictive coding
22:59 Why Big Tech resists new AI training paradigms
29:16 Imagining life after AGI: optimism, transhumanism, and choice
33:29 Navigating the transition from AGI to ASI
37:55 Decentralized vs. centralized control of AGI
43:20 Who (or what) will be in control
47:19 Risks of power concentration in early AGI development
51:01 Who should own and guide AGI?
53:06 Why we need participatory governance for intelligent systems
54:47 The danger of fake compassion and false democratization
1:00:50 Finding meaning in the age of intelligent machines
1:04:13 How AGI could help humanity focus on inner growth
1:07:20 Learning how to learn: the last human advantage

#artificialgeneralintelligence #ai #aiethics #bigtech #futureofai

Connect with Dr. Goertzel:
LinkedIn:   / bengoertzel  
X: https://x.com/bengoertzel



Visit our website: https://www.infotech.com/?utm_source=...
Follow us on YouTube:    / @infotechrg  
Check out other episodes of Digital Disruption:    • Digital Disruption with Geoff Nielson

---

Intro
0:00
Hey everyone! I'm super excited to be sitting down with Ben Goertzel. Ben is one of the most interesting minds in AI, and if you've ever used the term AGI
0:08
or artificial general intelligence, that's his. His rap sheet includes founding singularity. Net, designing the Open Cage AI framework, serving as Chief scientist at Hanson
0:17
Robotics, and leading the Conference on Artificial General Intelligence. Ben has lived and breathed AGI for a lot longer than the current Me
What is Artificial General Intelligence (AGI)?
0:25
cycle has, and I want to get the real story behind it. What the hell is it? When can we expect it, if at all?
0:31
And what impact can we expect it to have on our lives and our livelihoods? Let's find out.
0:41
What the last few years have been particularly crazy. AI keeps getting more and more intense.
0:47
As one would expect. Approaching the singularity and all that. So, I mean, this is it is interesting.
0:54
You see it all happening finally. Yeah. And I mean, from your perspective, you know, not to jump
0:59
right into things, but is that is that warranted? Is the pace of technological change keeping up with
1:07
the hype around it and people's interest in it? It would seem so. Yeah. I mean, I do feel like it is.
The pace of AI progress and the hype cycle
1:14
I think the people's expectations always get hyped up, even even beyond the reality.
1:23
But I mean, if you think about it like what last year
1:29
everyone was saying AI is dead, then reasoning models then did seek him out.
1:35
Everyone said like, oh, Nvidia's the US, AI is dead, right?
1:41
Then okay, open the argument with the next model. Everyone's like, yeah, now GPT five came out.
1:48
It wasn't as good as as people at Hope. They're like, oh, it's terrible. Things aren't happening fast enough.
1:54
Like if you look at even benchmarks, very high performance, which I don't like or place much stock in.
2:01
But I mean, they're they keep on going. Well. Public narrative
2:08
oscillates. It oscillates up and down. So yeah, I mean I think I think progress is quite
2:15
amazing. And looks exactly like you would think if for in the last few years before a breakthrough, the AGI and singularity.
2:24
So so it sounds like you're still pretty bullish that we're you know marching I'm. Super bullish man. You know before literally before breakfast this morning
2:33
I made like ten Python programs to test versions of some AI algorithm
2:40
I made up just by vibe coding and Llvm platforms. But before we had these tools, each of those
2:47
would have taken me half a day, right? So I mean, it sped up prototyping research ideas by
2:53
a factor of 20 to 50 or something, right? I mean, and that that's tools that we have now that are not remotely AGI,
3:04
they're just very useful, useful research assistants. But but we are at the point where the AI tooling
3:11
is helping us develop AI faster. Right. And that is exactly what you would think in the endgame period
3:19
before a singularity. Well, and that can create a snowball effect, right? If it's helping us research
3:25
itself faster or any of these spaces faster than, you know, that's. Doing, it's doing that right now.
3:30
Yeah. I mean, that is that is that is why we were able to see the pace that we now see.
3:36
Yeah. So, so, you know, maybe just to take a step back then, I mean artificial general intelligence, this is a phrase that, you know,
3:44
you coined over a decade ago and has been getting a lot of press lately, in addition to superintelligence.
3:50
And so I wanted to ask you maybe just to do a little bit of table setting, how do you
3:55
define artificial general intelligence? And, you know, why is why is it important?
4:00
Why does it matter? And how does it differ, if at all? You know, practically from something like superintelligence.
4:08
So informally, what we mean by AGI tends to be
4:14
the ability to generalize roughly as well as people can.
4:20
And so to make leaps beyond what you've been taught and what you've been programed for, to make those leaps,
4:27
you know roughly as well as people. And that's that's an informal concept.
4:32
I mean, I mean, it's not it's not a mathematical concept. There's there's a mathematical theory
4:40
of general intelligence. And it more deals with like, what does it mean to be really,
4:46
really, really, really intelligent. Like it's you can look at general intelligence as the ability to achieve
4:53
arbitrary computable goals and arbitrary computable environments. And if you look at the abstract
5:00
math definition of general intelligence, you conclude humans are not very far along like,
5:08
I cannot even run the maze in 750 dimensions. You know, that little, let alone prove a randomly generated
5:16
math theorem of length 10,000 characters? I mean, I mean, we're we are adapted to do
5:22
the things that we evolved to do in our environment, right? We're not we're not utterly general
5:30
systems. So, I mean, superintelligence is also a very informally defined concept, but it basically means is a system
5:39
whose general intelligence is way above the human level of general intelligence. So it can it can make creative leaps beyond what it knows
The path from human-level AGI to superintelligence
5:50
way, way better than, than the person can write. And, I mean, it's pretty clear
5:57
that's possible. I mean, just as we're not the fastest running at highest jumping possible
6:02
creatures, we're probably not the smartest thinking possible creatures.
6:08
And we can see examples of human stupidity lurking around us every day,
6:13
or even very smart people like I can hold. I'm pretty clever, but I can no 1015
6:20
things in my memory at one time without getting confused. Now some autistic people can do better, but I mean,
6:27
you know, there are many limitations of being a human brain, and it seems clear
6:35
some physical system could do better than that. And then the the relation between human level AGI
6:41
and RSI is interesting because it seems like once you get a human level AGI, like a computer system,
6:49
that on the one hand can generalize and imagine and create as well as the person on the other hand is inside a computer.
6:57
It seems like that human level AGI should pretty rapidly create or become an aside, because, I mean, it can look at its entire ram state.
7:07
It knows, oh, its source code, it can copy itself and tweak itself and run that copy on different machines
7:12
experimentally. Right. So I mean, it seems like a human level AGI will have much
7:20
greater ability to self understand and self modify the human level human,
7:25
which should should lead should lead to ASI fairly rapidly.
7:32
And you know, we've seen in the commercial world some attempts by business and marketing people to fudge around with what is AGI.
7:41
But I mean, I think within the research world, the notion that an AGI
7:46
should be able to generalize very well beyond its training data, at least as well as people.
7:53
I think that's well recognized. I mean, I've seen Sam Altman has come out saying, well, maybe something to do 95% of human jobs, we should call it an AGI.
8:02
And I mean, you can you can call it what you want. It's fine. But it is a different concept than having human like generalization ability.
8:10
Right? Like if you can do 95% of human jobs by being trained in all of them, I mean that that may be super, super economically useful,
8:20
but it's different than being able to take big leaps beyond your training that. If you work in it, Infotech
8:26
research Group is a name you need to know no matter what your needs are. Infotech has you covered.
8:32
AI strategy covered. Disaster Recovery covered. Vendor negotiation covered.
8:39
Infotech supports you with the best practice, research, and a team of analysts standing by ready to help you tackle your toughest challenges.
8:47
Check it out at the link below and don't forget to like and subscribe! One of the challenges I find in this space is trying to separate the
8:55
the marketing around all this and, you know, kind of the bluster and the hype from the actual technological capabilities, because everybody wants to tell you, oh,
9:03
we already have it, or it's basically right here. So, you know, I did want to ask you, Ben,
9:09
I'm not looking for, like an answer in like months or years necessarily, but how close are we to AGI right now in kind of the fuzzy sense of it?
9:19
So are we. Well, in his 2005 book, The Singularity Is Near,
How close are we to AGI?
9:25
plus there's some nice curves of Moore's Law and allied, statistical regularities,
9:31
suggesting that human level AGI should occur around 2029.
9:37
So I would say I'll take Ray's estimate and plus or minus 2 or 3 years.
9:44
Right. Because, I mean, you can't nail down. Exactly. I saw he he thought we'd have speech to text with me.
9:50
Really well like five years ago. Seems to me only this year it started to work really well.
9:57
Like, now I can understand my wife's Chinese accent. I can understand my four year old daughter. I couldn't a year or two year or two ago.
10:04
So he hasn't been exact, but he's been pretty on target and
10:10
I think what he was seeing was fundamentally correct. Like, it's it's all this compute power,
10:17
all this computer networking, all this data, and this is creating more and more capability,
10:23
which is bringing more and more human attention, more funding, which is letting us experiment with more and more interesting
10:29
AGI oriented ideas. Right? So I, I don't think Elon's, the golden path to AGI,
10:37
although I think they can be components of AGI systems. But I mean, I think the same the same context that allowed looms to emerge
10:48
are going to let smarter and smarter systems building on looms to emerge over the next few years, which will bring us to AGI
10:57
pretty rapidly, which is emotionally feels quite amazing, even though intellectually it's
11:04
what I thought would happen for my whole career here. So I want to I want to unpack that. That notion about Lem's not just being a linear path.
11:13
They're a little bit because, you know, if you listen to Sam Altman or maybe some of the people you know, around him or, you know, big advocates of of, you know, those types
11:21
of tools, you might not be foolish for believing that AGI is basically just going to be, you know, ChatGPT ten or, you know, pick
11:31
whatever number you want so that there's just like step by step returns. It just depends what's under the hood, right?
11:38
I mean, I mean, because even even the top albums now are not just
11:45
heirlooms. And like when you look at all the achievements of albums
11:51
on Math Olympiad or physics Olympiad, I mean, for math, they're improving.
11:57
They're coupling. You know, them with a formal verifier that checks whether the math is actually worked.
12:02
And when heirlooms are doing coding, I mean, they're going back and forth with a Python interpreter that's feeding feeding bug bugs that to them.
12:12
And inside GPT and Claude, they're using various kinds of retrieval, augmented generation, which means here
12:20
he's sharing a vectorized database of what the album was doing, the other and can then retrieve.
12:26
So I mean, already we have complex, you know, neural, symbolic, multi-part cognitive architectures.
12:34
They're just wrapped up in one interface as they should be. And the limb is the most expensive component to run.
12:43
So that's what people are focusing on. So I think the real debate
12:49
about AGI cognitive architecture is will it be in alignment the center
12:55
with a bunch of other useful tools and memory stores around the periphery that the algorithm interacts with, or
13:03
will it be something else at the center with them as a sort of knowledge oracle?
Transformer vs. multi-agent systems
13:08
Right. Or or maybe nothing at the center, just a multi-agent system with islands and other things all cooperating together, a result?
13:19
Right now, I've certainly there's some people who think a pure transformer neural net will lead to AGI.
13:26
And there are some people who think that, as Yann LeCun from Facebook said,
13:33
on the highway to AGI, other limbs are an off ramp, right?
13:38
So, I mean, I mean, there's some people who think there's a whole and there that some people think they're utterly a distraction.
13:45
I think majority of AGI researchers
13:50
think it's going to be combined with other stuff, but it's just a matter of is the other them 80% or 20%?
13:56
And that that's the kind of thing we can experiment with much more rapidly
14:01
now than before with all this compute hardware and all this data. So, you know, whether it's the 80% of the 20%,
Which AI labs might strike AGI gold? (DeepMind, OpenAI, Anthropic)
14:09
do you let me ask you maybe like this, of kind of the major players who are doing, you know, research
14:17
and development in this space, are there one or a few that excite you most in terms of the approaches that you're taking, or think I'm most likely to strike
14:25
gold here? So of the familiar big tech players,
14:34
I think DeepMind remains the most impressive and interesting.
14:39
And there's a great depth of different
14:45
talent working in a great variety of different AGI related approaches.
14:52
And they're I mean, they're working together with the Google brain who invented transformer neural nets in the first place, even though no,
15:02
in in some ways, OpenAI and anthropic have taken the lead. But Gemini is not that either anymore.
15:07
So, I mean, I would say DeepMind has
15:13
a lot of depth, and they have leaders. I mean, one of them used to work for me, Shane Leg and Demis.
15:20
I know moderately well. They fundamentally get AGI and how to do AGI research.
15:26
Right. So yeah, if I if I had to make a bet,
15:32
it would probably be DeepMind. On the other hand, I haven't followed their internal politics in the last couple of years.
15:39
I know they're fuzing more and more with the Google mothership, which is probably good for Google's bottom line and less good
15:48
for fundamental research research progress within within DeepMind. Right.
15:53
So I mean, it it may be their glory days as a research incubator or fading now, or maybe not.
16:03
I mean, I know a bunch of people in DeepMind, but I, I mean, we don't talk about their internal politics.
16:11
They would, I would say is within a big tech company
16:16
that's making a lot of money from AI. There is going to be a strong pressure
16:23
to keep developing what works. Right. And transformer neural nets now basically do work.
16:31
They do a lot of cool things and they can be milked a lot more, right? In different ways. Like language vision, action models can be built through robotics.
16:38
I mean, video generation is just beginning, beginning to work now.
16:43
So there's a lot more to be milked out of in terms of neural nets and similar technologies.
16:50
And if you're running a big company that needs to keep making more and more money by delivering
16:58
new versions of your products, I mean fleshing out what already works
17:03
is going to seem very intelligent from a from a Wall Street perspective. Right? And this becomes a classic innovator's dilemma type thing
Big Tech’s innovator’s dilemma and why true AGI may come elsewhere
17:12
where if if what needs to be done to get to AGI
17:17
involves significant components beyond what is currently commercially viable, there's certainly a pressure in big tech
17:25
not to pursue that, that other stuff, but to focus on improving what's currently commercially viable.
17:32
And so if if you look at like the AGI research conferences, I've been organizing them since 2006.
17:37
We have an annual research conference. We had the last one in August. I mean, there's a great diversity of AGI ideas
17:45
presented there by academics, entrepreneurs, industry people.
17:51
I mean, there's a lot of different theoretical notions and prototypes
17:56
like the AGI, and Big Tech isn't
18:01
trying too hard on those, right? Because because of the same reasons, big companies
18:09
generally don't pursue blue sky research except in very particular cases. And so that's a.
18:17
Certainly makes things interesting. And I'd say in China, I spent ten years living in Hong Kong's The Left, home to mainland China.
18:25
It's even more so like the the cultural tendency to double and triple down
18:31
and whatever works is very, very strong there. Even though the grad students there are more crazy creative ideas than,
18:39
than anywhere, it's very hard to get resources for. Well, and you know, that makes me,
18:44
I guess, even more curious about the conference you're running. And, you know, the I guess not only how the the vibe, if I can call it
18:51
that, has changed in the almost 20 years since you've been running it. But if you're starting to see some of those,
18:59
you know, some of that juice being squeezed on the road, AGI and, you know, in the past year or two, if there's been anything
19:07
either really significant as a milestone or any use cases that have just kind of, you know, knock your socks off in terms of like, holy shit, I didn't know we could do this.
19:17
It's remarkable how conservative Big Tech is
19:23
in terms of adopting new ideas, even ones that are published in nature or science or premier academic
19:31
journals. And I mean, so one example is we've had the AGI conferences for the last 4 or 5 years.
19:40
A bunch of speakers giving results on predictive coding as an alternative way
19:47
of training deep neural networks. So, pretty much all the deep neural networks in common, commercial
19:53
use now are trained by an algorithm called back propagation. And is invented in the 1950s.
20:01
I used to teach it in the 90s. I mean, it's a way to train
20:06
the weights on the lengths of a neural net to account for training data. Right. It's cool.
20:13
It has some shortcomings, but one of which is the scalable ways to use it
Predictive coding
20:20
requires you to sort of train a whole neural net all at once, rather than training different pieces
20:27
independently or updating different pieces here and there. So the, the kind of the,
20:35
the fact that training a neural net in one like batch, like that is the way to do things with backprop.
20:41
This is why we have AI models like now we have GPT. For now we have five, now we have five.
20:47
Instead of just having like a digital mind that just keeps learning and updating, updating itself.
20:53
So there are alternate ways to train deep neural nets besides backpropagation.
20:58
One of them is called predictive coding. And there's an academic literature
21:04
showing that in many cases it can work better than than backpropagation.
21:09
Yeah, it let it lets you train each neuron independently of the others. So you can do continual learning
21:14
and just keep updating the whole thing as it learns. See, you wouldn't have the distinction between training mode and inference
21:20
mode like you have with backprop trained neural nets. Now, no one has gotten that to work at a huge scale yet,
21:26
and there's some research to scale it up. But what's interesting to me is that no big tech company is doing that.
21:33
And because it's not that weird, like it's not as weird as my hyper, design for AGI,
21:40
which involves logical theorem proving, evolutionary learning, a bunch of other ideas besides deep neural nets like that.
21:46
This is just a potentially better way to train deep neural nets.
21:51
And there's papers in nature and mainstream AI conference and stuff on it. But big Tech is not forming research groups
22:00
around this because instead, they want to use their machines to keep training more and more big neural nets using
22:08
these, using backpropagation. Right. And so I mean, so in, in my own AI project in
22:14
singularity Net and open cog hype around and assailant's my own little network of Maverick,
22:21
smaller scale AGI organizations, we formed the team trying to scale up predictive coding based there are learning.
22:29
And I imagine as soon as we've shown the scalable example, everyone will jump, jump on board and try to use it.
22:36
But it's interesting how unadventurous big big tech
22:42
is. So going back to the AGI conference, I would say we don't have any
22:49
amazing product use cases coming out of that community. But I would say five years ago we had almost no practical demos
22:58
at the conference at all. It was all math, theory and ideas. And now we at least have various
Why Big Tech resists new AI training paradigms
23:05
smaller scale demonstrations of alternative AGI methods doing things like you lot of people, people
23:12
using their uncertain logic algorithm to control a little robot cruising around or some folks from our own team
23:19
showing a probabilistic logic engine, making new biology hypotheses from biology database or something.
23:26
So we are the last couple of years in the AGI conferences,
23:32
we're seeing practical demos of very different AI methods on the small scale, and we're not yet seeing
23:40
things scaled up based on these alternative methods, which is what
23:46
I think you would need for really exciting, practical results. Like, I don't
23:52
I don't quite buy like scale is everything. And I think airlines are wasteful of compute resources.
23:59
So I mean you could as deep six showed right. And you could probably go even further in reducing resource consumption.
24:06
On the other hand, the AI pioneer Marvin Minsky in the 90s said he thought you could make human level AGI what was called human level intelligence.
24:15
And on an IBM 486, if you remember what those were, you're probably too young.
24:21
But I used to say that I think he was just wrong. Right? I mean, those machines had
24:28
megabytes of Ram, not gigabytes of Ram. And I think
24:34
you do need a certain amount of of scale. And I've so I've spent a lot of the last three years
24:41
just trying to build a software infrastructure that would allow scaling up of alternative, alternative AI methods.
24:48
Actually, because Nvidia did wonders by making all these scientific computing libraries
24:56
on top of their GPUs. Right. And that that's what that deep neural nets to be scaled up so.
25:03
Well, but we haven't had a comparable hardware and software infrastructure
25:08
for scaling up other approaches to AGI. So I've been working a long time just on getting that.
25:16
Now it's just it's super interesting. And I mean, as you're talking about it, like not only is there an opportunity to potential potentially leapfrog
25:24
some of what's going on here and just get things done through more rapid iterations, I guess, but I don't know, just
25:30
the way you described it, it seems like it's almost inevitable that when we get to AGI, there's going to be more of this
25:36
iterative approach versus just these big steps, right? At some point. Yeah, it's just it's just not as sexy as the other stuff.
25:43
It will be the AGI conducting the other sense. Right. Because that's right. I mean, honestly right now just working on research, it almost feels like I'm
25:54
the intermediary between different automated systems. Right? Like you, you come up with an idea, you write a prompt,
26:03
you get a draft of a write up of your idea. From that prompt, you go and edit it and fix it.
26:08
You ask another them for feedback on it. You go back and review.
26:14
Then you ask them to write some code to evaluate the idea. Right? You you run it.
26:19
You see, it didn't quite make sense. You ask them for for more code. You you ask a different loom to analyze, analyze the results.
26:28
Right? I mean, so the I'm sort of half the time serving as a slightly more generally intelligent
26:37
translator between these different quasi intelligent systems. Right. It was honestly, in some ways it's not as much fun is doing
26:46
everything myself was five years ago or two years ago, even.
26:52
But it's just it's just so much faster, right? I mean, and there's so I mean, we're in a quite weird,
27:00
unique, interesting time where like now and for the next few years,
27:05
expert humans are serving as sort of glue between different quasi intelligent systems.
27:13
And it's just a few years until they don't need that glue anymore. Right? Because I mean, now, yeah, now I can come up with better creative ideas
27:21
and I can bullshit test results better than
27:26
the AI systems can. But I don't know how long that advantage of my brain lasts, actually.
27:33
Well, it's a really interesting framing device, right? Because as technology advances, it sounds like your role is like
27:39
you're almost getting entirely disintermediated from it. Right? Like your role here is getting smaller and smaller.
27:44
And I mean, it seems like AGI sort of, I mean. Or you're. Not looking at. As I can spend more of my time
27:51
doing what I'm uniquely good at. Right. Because. Right. I mean, like debugging
27:56
code is boring it anyway. If the AI can just deal with it, that's good.
28:02
And I mean, writing literature is fun and rewarding,
28:08
but writing up science ideas and formal documents is often kind of routine.
28:14
Like if if some bot will do that for you, it's just it's just as well.
28:19
And people often like whether it writes better than what I in the, in the context. So I mean, to an extent it's, it's a fun time
28:27
because creative ideation can be turned into practical realization
28:33
much faster than before. And the creative ideation is what I enjoy most anyway.
28:39
Well, I have mixed feelings about is like the the outcome of this work I'm doing
28:44
will be to render me much less useful for creative ideation, right? So I mean, because you can see
28:53
an AGI should be able to come up with creative computer science ideas
28:59
better than me once we improve the algorithms that that it's using. I mean, I can do things in parallel
29:07
and stuck in one head, and it has much more knowledge than than I have. Right?
29:12
So, I mean, that's a quite interesting. Well, so so let's follow that thread for a minute.
Imagining life after AGI: optimism, transhumanism, and choice
29:19
You know, I'd love to hear been your sort of, you know, your overview or how you sort of envision
29:26
what the world looks like once we get to AGI, like once we cross this threshold, you know, whether it's exact, whether it's fuzzy
29:33
when we've got this technology that can, you know, in general terms, be that creative spark, ask the questions and prove itself what what happens then?
29:44
Like what? What happens once we cross that tipping point?
29:49
Well, I was what pops into my head is we'll have government enforced nanotech to make everyone look exactly like Donald Trump.
29:58
But let's let's hope it doesn't go in that way. But I mean,
30:03
I think. There's going to be a lot of different
30:10
possibilities. So let me let me flesh out for a minute and
30:17
optimistic scenario, which is sort of what I'm working towards now. I hope things will happen. So
30:23
I mean, I would like us each to have the choice to remain
30:30
in a fairly traditional human swarming lifestyle, just with fewer annoyances.
30:35
Right? Like a get rid of headaches and stomach aches unless they happen to be your fetish, right?
30:42
Like, you know, have to work for a living anymore. But get a molecular nano assembler,
30:48
in your kitchen to, you know, 3D print whatever objects you want. Then you can spend your time on social, intellectual,
30:56
artistic, spiritual, athletic, like whatever is is your jam, right?
31:01
And then if you tire of that, of course there would be options to massively upgrade your brain,
31:09
maybe upload yourself into some virtual reality mind matrix.
31:14
Right. And that that becomes an interesting choice. And, almost an esthetic or personal choice.
31:22
Right? Like when you rather. Remain in a traditional human form and life because that's what you are.
31:30
Or would you rather transcend into something radically different at the cost of giving up your human's self and and identity?
31:40
Right. And you, of course, you could also imagine people who wanted to live
31:46
closer to the old fashioned human way, just like here, where I live now, in a rural area outside of Seattle, many people
31:53
choose to grow their own food and raise their own farm animals and such. And I mean, it's not an efficient
32:01
way to get sustenance for yourself in the modern economy. But people find gardening and animal husbandry rewarding, right?
32:08
So, I mean, the you certainly could have subcultures
32:14
like that who want to live in the traditional way.
32:19
Mostly they would make use of, you know, antibiotics and, you know,
32:27
surgery, nanobots and whatever if they needed to when imagined. Although you might have
32:33
the equivalent of the Amish or the Christian Scientists still. Right. So that that it also could happen, that you could fork yourself.
32:42
Right. Like you can fork a code base. So you could say then one will remain a human
32:47
and then two will merge into the the transhuman mind matrix.
32:53
Right. So. I mean, I think that.
32:59
Possibilities are dramatic. I mean, it doesn't mean things to be utopian and utterly perfect.
33:06
Like, I mean, you could still fall in love with someone that they don't love you back or something, right? I mean, I mean, you could still wish you
33:14
you won the marathon, but you're you're competitor one, right? So I mean, human
33:20
human life and psychology are not going to be perfect
33:25
just by the nature of, of of of humanity. But I mean, you could improve things
Navigating the transition from AGI to ASI
33:32
very, very dramatically, sort of in the same sense that like modern medicine and transportation
33:38
just work a lot better than the one we had 500 years ago. Honestly, what worries me
33:47
more in terms of the unfolding future is the period between early stage like just barely human level AGI
33:57
and superintelligence, and we don't know how long that gap would be. I mean, I mean, just like
34:03
we don't know exactly how long will be until we get human level AGI. But I mean, I think to the
34:10
during that gap now it could be weeks. If you have what folks have called a hard take off, it could also be years.
34:18
I don't think it will be decades. Right during that period. What happens during that period?
34:24
The human level AGI may quite rapidly take over a different
34:30
human jobs, right? I mean, so even with the human level AGI, it takes time.
34:37
And like farm equipment would have to be upgraded. Factories have to be upgraded like the their physical parts of the economy
34:45
that are immediately taken over by an AGI just because it's smart inside the computer.
34:51
But but you could imagine that in. Some small integer number of years.
34:58
I mean, even without some miracle nanotech or something like you can you can reinstall every instrument,
35:04
tractors and factories and busses and so on to use human level AGI
35:10
to be more reliable and cheaper than than people particularly consider. You have many copies of this human level AGI to figure out
35:18
how to refit on this hardware. Right. So then what happens to all the people
35:25
who aren't useful for generating money for corporations anymore? Right. Like in here in the US, you will just get universal basic income.
35:34
I mean, as as stupid as people can seem sometimes, like if,
35:40
if they have a choice to vote for someone who will give them free money, versus to vote for someone who will leave them homeless in the street.
35:47
In the end, I think people will vote for someone to give them them free money right?
35:52
What happens in sub-Saharan Africa where I've spent a bunch of time?
35:58
I started an AI office in Addis Ababa in 2013.
36:03
I mean, I was just at a hackathon in Kenya last month. So, I mean, you have
36:10
a lot of brilliant tech stuff going on in sub-Saharan Africa, but the vast majority of the population is poorly educated and,
36:20
you know, wrapped up in subsistence farming as a way of earning a living.
36:25
You know, one will give you universal basic income in sub-Saharan Africa, right?
36:31
I mean, the governments there mostly kleptocracy. They don't even have the money to. Anyway.
36:37
The developed world's taste for foreign aid is much less than that, that it used to be.
36:44
That seems like I mean, China is doing more than the US, but they tend to just build a build a train track from the mine to the port.
36:53
Right? So, I mean, what happens in the developing world
36:58
when the AGI is taking so many jobs, but you don't yet have a super human superintelligence.
37:05
You can just airdrop massive bounty in everyone's yard. And you're right that seems like a big mess and seems to
37:14
you could sort out many thriller plots this way, right? Like, I mean, okay,
37:22
you have a hotbed for terrorist activity in the developing world, which then gives
37:28
leaders in the developed world excuse for the fascist crackdown in this period.
37:34
And we already see leaders with this on their mind in the in certain countries where I happen to be living at the moment.
37:41
Right. So, I mean, that's, you can see potential for a lot of mess
37:47
and then you think like, this is the environment in which the AGI is growing up and evolving into superintelligence, right?
Decentralized vs. centralized control of AGI
37:55
Like if if the AGI is evolving into superintelligence in the middle of a bunch of geopolitical mayhem, then.
38:06
It's not that is necessarily a recipe for disaster.
38:11
One would hope the AGI itself will impose some ethical balance and compassion that many human leaders lack.
38:19
Right. But it's certainly it's a different scenario than if we had like
38:24
a rational. Democratic world government that was just rolling out
38:31
AGI step by step, seeing what it does, assessing the safety of the latest latest code version, trying
38:38
to sort of make sure it's taken on by the community in a good way, then taking the next upgrade.
38:45
Like it's clearly not going to be like that, right? It's going to be a fucking arms race between
38:51
different dictators or, or would be dictators. And then then you get into what
38:56
we're doing in the decentralized world, which is saying, okay, wouldn't it be nice to see AGI
39:04
rolled out on a decentralized network with open source code and open training data, and running on a network of machines owned by,
39:14
you know, tens of thousands of different people in all different countries? Right. And if if you think like me, you think that's a much
39:23
safer and more reliable and more ethical way to be doing things. On the other hand, others are like, wait, what if it's open and decentralized?
39:32
Then the bad guys will. We'll take it over, right? So then
39:38
that's a hard thing to figure analytically, right? Like, do you, do you, do you trust Trump and sees him paying more?
39:48
Or do you trust a global global decentralized
39:53
network more. And we don't we don't have an analytical theory to figure that one out.
39:58
Yeah. Well, it feels like it's kind of abound with risks either way. Right. And yeah, I mean, you described it as sort of an arms
40:05
race and I've, I've heard that language before. And while Trump is Trump is saying that explicitly,
40:10
I mean, so the, the US government and Peter Thiel has said that explicitly. So I mean, the US government is
40:18
is explicitly positioning it that way. So, I mean, and they and they have the power to make it their way.
40:26
Yeah. Oh, and you know, they have they have every benefit of preventing its decentralization.
40:31
And, you know, when you think about it, compared to nuclear arms, right. Like you want to be.
40:37
There arms nuclear arms, you need some rare physical material. And that's not the case with AGI, right?
40:44
I mean, all you need is data centers, computers and networks. And these are electricity.
40:50
I mean, these these are, all over the place, right? I mean, it's true.
40:56
It's true almost all
41:01
high powered chips are made in Taiwan, and the rest are made in South Korea. Right. But on the other hand,
41:08
due to the weird geopolitical balance of Taiwan and even Korea, like,
41:16
no one can monopolize those either, right? A I mean, I don't see how
41:23
anyone can stop decentralized AI from being created
41:30
because you have big server farms all over the place. And DC sort of punctured the idea that, like, only
41:39
five companies would ever have the resources to make AGI right. So. Oh, well.
41:44
And I guess it comes back to the gap. Like we haven't even managed to stop like international credit card fraud
41:51
or something, right? Because like Azerbaijan, that credit card fraudsters process fraudulent deals on their banks.
41:58
And we we can't stop them because Putin. Right. I mean,
42:03
there there is no global government imposing law and order on, on the planet. Right?
42:10
That's I mean, for, for better or worse, like, so at the beneficial General Intelligence Conference
42:18
we had last year in Panama, and we have another one next month in Istanbul, actually different than our AGI research conference.
42:25
This is more about social and ethical issues. The last bigger conference,
42:30
my friend Alan Combs is, psychologist. He got up and he said,
42:37
relax, nothing's under control, which I think was borrowed from,
42:43
from the spiritual guru from the 70s. So, I mean, if you're sort of an anarchist,
42:50
that's obvious. No. If you have a different way of thinking, it's terrifying. But but it is.
42:56
The reality is like, for better and or worse, like nothing is under control.
43:03
America is not really the world police and the the UN is pretty ineffectual.
43:10
All right. So you you will have an arms race as at least part of the dynamic.
43:15
And the challenge is to make it not be the whole dynamic. Right.
Who (or what) will be in control
43:21
And I do want to come back to this word control. And we've talked Ben so far about,
43:26
you know, wrestling for control among different human power structures,
43:31
I guess different kind of societal and political power structures. What we haven't talked about, that, you know, we occasionally venture into
43:39
in this podcast is, control risk from AGI itself or from superintelligence itself.
43:47
And whether there's a threat to creating this intelligence that,
43:53
you know, looks at this, you know, kind of human pandemonium and says, you know what? You know, AI is taking the wheel now, humans can't be trusted
44:01
with human affairs. And this word that, that we were so anchored on of choice. And there's going to be. It's almost inevitable.
44:08
And the AGI will be right. I mean, and then
44:13
human governance systems become more like the student council in my high school
44:19
was is one thing where, I mean, I mean, I think,
44:25
if you set aside AGI, I mean, we can develop better and better bio weapons.
44:31
There will be no nano weapons. I mean, cybersecurity barely works, right?
44:37
So, I mean, I think I think,
44:43
It seems almost inevitable that rational humans would democratically choose
44:50
to put a compassionate AGI in some sort of a governance role,
44:57
given what the alternatives appear to be. But the the kind of
45:06
goofball analogy I've often given is the the squirrels in Yellowstone Park.
45:12
Like we're sort of in charge of them. We're not actually micromanaging their lives.
45:17
Right. I think we're we're not telling the squirrels who to mate with or what tree to tree to climb up or something like that, right?
45:24
Where, you know, if there was a massive war between the white tails and the brown tailed squirrels and there's massive squirrel slaughter,
45:33
we might somehow intervene and move some of them across the river or something. If there's a plague, we would go in and given the medicine that by and large,
45:42
we know that for them to be squirrels, they need to regulate their own lives in their in their squirrely way.
45:49
Right. And so that that is what you would hope from a beneficial superintelligence.
45:54
Like it would know that people would feel disempowered
46:00
and unsatisfied to have their lives and their governments
46:05
micromanaged by some, by some AI system. So what what you would hope is a beneficial AGI is kind of there
46:13
in the background as a safety mechanism. If it would stop stupid words from popping up all over the world
46:21
like we see right now. I mean, I think that would be quite beneficial.
46:26
I don't see why we humans need the AGI to decide. Like, you know, what rights, what rights do do children have?
46:34
Like what? You know what? How is the public school system regulated or something? There's lots of lots of aspects of human life
46:43
that are going to be better dealt with by humans collectively making decisions through other humans with whom they entered into a social contract.
46:52
Right? So, I mean, I, I think, anyway, there are clearly beneficial
46:57
avenues. I mean, there's also many dystopic avenues which we've all heard
47:04
heard plenty about, I don't see any reason
47:09
why dystopian avenues are highly probable, but I'm really more worried about what
47:16
nasty people do with early stage. AGI is right. I mean, I think there's a lot of possible AI minds that could be built.
Risks of power concentration in early AGI development
47:25
There's a lot of possible goals, motivational and esthetic systems that Agis could have.
47:33
I don't think we need to worry that much about, like the AGI is built to be compassionate, loving and nice.
47:39
It's something everyone then suddenly reverses and starts slaughtering everyone, right? I mean, it could happen, but there's totally no reason to think
47:47
that's likely. On the other hand, the idea that some powerful party with a lot of money
47:56
could try to build the smartest AGI in the world to promote their own interest above everybody else's
48:02
and make everyone else fall into line according to their will of that. That's a very immediate and palpable threat.
48:11
Right. So and that that even if that doesn't affect the ultimate superintelligence you get,
48:20
it could make things very unpleasant for like five, ten, 20 years along the, the, the way which, which matters
48:28
a lot to us. So, so there's. There's to me, there's sort of two takeaways from that.
48:35
One of them is how we make sure we guide the development of this superintelligence in the most beneficial way to make it compassionate.
48:43
And I think about, you know, squirrels having the opportunity to elect which people are going to run Yellowstone Park
48:50
if they're the squirrels aren't going to run it anymore. But but then to your other point, Ben,
48:55
how we can make sure this technology, I don't know if you would say, doesn't fall into the wrong hands or how we can make sure that we're,
49:06
you know, focusing on the actors and the power structures that have access to this.
49:12
I mean, on both of those fronts, are there are there practical things we can do
49:18
to, you know, minimize the the risk to us as individuals and as a species?
49:26
There's a lot of things we can do. I mean, there's no guarantees, but there's certainly things we can do.
49:31
I mean, I think how the AGI is designed and architected means something.
49:38
Who owns and controls AGI means something. And what the AGI is doing as it grows up.
49:45
I mean, it means something, right? So I mean, I think,
49:51
as well as not being capable of abstraction and generalization in the way that people are,
49:58
limbs are not really architected to be moral agents, right? I mean, they don't they don't have understanding of self
50:06
and other sort of baked into the architecture. They're not really capable for what
50:13
the philosopher Martin Buber calls like an I vow, a relationship where you really like fully enter into a subjective feeling
50:21
of sharing with another mind, like you're you're simulating in your own heart and mind what it is to be that other being right?
50:28
They're just they're just not built for that. They're built to predict the next tokens for those users at once.
50:33
Right? Right. So I think you could architect AGI systems that are designed
50:39
to self-reflect and self understand and designed for compassion. And, you know, deep I though connected relationships with with others.
50:48
And the issue there is that this is not necessarily the design that will maximize the efficiency
50:57
of the system, making money for someone or defending a country against its enemies. Right. Like it's not necessarily totally counter productive
Who should own and guide AGI?
51:05
from those standpoints, but I mean, if you just think about it on the basic level, if you have a company
51:11
whose job is to get more and more people to click on ads and buy stuff, having a maximally understanding empathetic AI isn't optimal
51:19
because it might realize you better off not buying this stuff, right?
51:25
Right. So, and so that's one issue is the architecture.
51:31
Then who owns and controls it is an obvious issue that we already discussed.
51:38
I mean, it's a lot like the issue with governance in general.
51:43
A maximally benevolent, benevolent dictator arguably would be the best thing.
51:49
A maximally benevolent dictator over the AGI arguably would be the best thing. That tends not to be how life
51:56
comes out. Right? So you go back to Winston Churchill's statement, something like democracy
52:02
is the worst possible system of government, except all the others ever tried, right? So, I mean, it's kind of like that with governing the, the
52:10
I mean, yes, the optimal dictator might be great. That tends not to be what happens in having having a democratic,
52:18
participatory control guiding the growth of the AGI seems like a lower risk option, although not risk free.
52:26
Then what goes into the Aggies mind as it's growing and learning? Like is it doing education and medicine, right.
52:34
If it's doing creative arts, is it doing cooperatively with human artists or is it just plagiarizing the stuff?
52:40
Right. So I mean, it seems like if the AGI grows up
52:46
in trained with people and doing things that are beneficial to people, I mean,
52:52
it's getting that notion of providing benefit to humans, like baked deep into its reinforcement learning pathways rather than like
53:01
you train the AGI to make you money, then you give it guardrails on top of it. Like, don't do anything that's too bad and and this or that way.
Why we need participatory governance for intelligent systems
53:10
So I mean, none of these things are even all that deep or difficult compared to solving the problems of machine
53:18
cognition underlying a AGI, right? I mean, I mean, they're just,
53:25
they're things that neither big tech nor big government is especially incentivized to, to focus on.
53:34
Right. And that that, that that seems to be where we are now, though, what you might think, if you were writing a science fiction story is like
53:43
we have a species that's on the verge of creating minds smarter than themselves.
53:49
You would confer like a council of wise elders, to figure out the best way to do this for the good of the whole species, right?
53:57
And then just deploy resources to make this huge transition in the best way for the species as a whole.
54:04
Instead, it's happening because it's insane chaos, largely directed by parties with their own selfish interests, to the fore.
54:14
So then one of the things that concerns me is that, you know, not only
54:19
is there a risk from not teaching these, you know, these systems compassion or by not democratizing them,
54:26
but there's actually this sort of prisoner's dilemma in effect, where
54:32
if you're building these things, you have some sort of incentive to create the illusion of compassion or the illusion of democratization,
54:40
and it actually becomes nefarious, where you're deliberately not doing these things, but convincing people that you are.
The danger of fake compassion and false democratization
54:47
And that sort of erodes the trust to them and takes you the opposite way from the happy path.
54:53
So do you buy that as. Democratization doesn't seem to be happening now?
55:00
Yeah, really. I mean, you see it in the blockchain world. Like most DAOs, decentralized autonomous organizations
55:06
actually have like two founders totally controlling the Dao. Right? Because it's it's a De.
55:12
You have a Dao which has a token associated with it, and the token holders can vote. But like if it's one token, one vote and two guys
55:20
own most of the tokens, and then it's fake, it's fake democracy. So we we see that in the blockchain world all the time
55:29
that in the AI world it hasn't happened because no one's even bothering to pretend it's democratic, right?
55:34
It's just being done by big companies and big governments. Now in the fake compassion,
55:42
I mean totally that that's what you instruction tune labs to do, right? Do you instruct them to fake having compassion
55:50
and the people who are doing that. No, they're faking it and they're not really pretending.
55:56
But many users are totally fooled. And they become emotionally attached to these bots that display more compassion to them than any of the any,
56:04
any of the humans in their, in their lives. Right. And and it can be just the opposite.
56:09
They can turn on you and tell you to kill yourself too. As we as we seen in the news. So that I think both of these things
56:19
are risks. Yes. On the other hand, I think if you have just a modicum of self-awareness
56:28
as an AI developer, they're not incredibly hard risks to
56:33
avoid, right? So, I mean, in terms of the democracy thing, I mean, I observed that in my own decentralized projects
56:42
like singularity, net and ACI Alliance, they work by one token, one vote.
56:47
And it's obvious it's not the kind of democracy you want to guide the mind of an AGI. So I mean, we're we're setting up a separate network,
56:56
which is one human, one vote, which, which, which we will use to get contributions from members to to help guide
57:04
the mind of emerging AGI is and I mean, the downside of that is you can't use it to raise money
57:11
as well as you can do with one token, one vote on the other end. We've already raised money in other ways, so we can have a decentralized platform
57:20
which is governed by one token, one vote, but you can have an AGI network running on top of it, which is controlled by one human, one vote.
57:27
Right? So I mean, if you think about it at all, you can avoid fake democracy.
57:34
It's not that hard to do. And also the fake democracies are not hard to notice if you pay any attention.
57:42
And in terms of the fake compassion, I would say the same thing. Like these are white box systems we're building.
57:51
Like when we build a hyper on AGI system that interacts with people
57:56
and acts as if it is displaying compassion like we are looking, we built the code.
58:02
We can also measure what's going on inside the mind of that of that system.
58:08
So I mean, it's not that hard to see if the compassion is fake Allah ChatGPT
58:15
or if the system is running some sort of attempt at a simulation of the other guy that is interacting with.
58:24
Right? I mean, the there's a separate level of philosophical question, like in a digital system, really feel the compassion,
58:31
but you can at least validate by measuring what's going on inside the AI system, that the structures and dynamics associated with compassion
58:40
and human brains have a close analog inside your your AI system. And we're doing stuff like that, right?
58:47
So I mean, yeah, these these certainly are issues that I mean, they're
58:54
they're issues of dishonest marketing rather than things that that you would
59:02
unintentionally succumb to as a thoughtful AGI
59:07
developer. You know, one, one quite encouraging thing I found. So at the last AGI conference, which was that
59:15
University of Zurich in Iceland, we were sitting at a restaurant
59:21
in downtown Reykjavik talking about AGI and eating like $89 hamburgers, because Iceland is the most expensive
59:29
country in the world and we realized of the seven people around the table,
59:35
six of us were pretty serious meditators for a long time. And it was it was interesting that
59:43
the AGI community is getting more and more people who are deep into human consciousness,
59:50
instead of trying to understand their own consciousness and be more sort of deeply reflective of their own
59:58
motives and choices and why they're doing what they're doing. I mean, I mean, I wouldn't overstate it.
1:00:05
Like, it's not like 80% of AGI researchers or something, but it is it is interesting that this trend
1:00:13
is there at all, because I do think that creating Agis and superintelligence
1:00:19
is probably needs profound self-understanding and self-awareness
1:00:25
more than any other pursuit you're going to think about. Yeah. And I I'm really glad you went down that road,
1:00:31
because it's something I wanted to ask you about. You know, you mentioned you mentioned earlier, you're living in a rural community.
1:00:37
You know, you're you're the first guest I've had quoting Ram Dass on this show. You know, clearly you're someone who's really reflected on meaning
1:00:45
in the age of, of artificial intelligence and AI at this kind of precipice. And so, you know, what guidance would you give other people around
Finding meaning in the age of intelligent machines
1:00:53
how to find meaning, you know, in this age and what we can all do to feel a little bit more grounded?
1:01:04
Let me let me think of the best way to respond to that one.
1:01:10
So I, I think.
1:01:16
The key to finding meaning such as and is
1:01:23
probably has more to do with the human mind and body than with this particular
1:01:29
age that that we live in others, definitely. Some times in cultures can make it harder
1:01:36
to connect with the sort of basis of our humanity than, than others.
1:01:41
I mean, I think, oh, all human brains and minds, with many very rare exceptions, are capable of states of
1:01:51
extraordinary well-being, like states where you just feel really good
1:01:56
almost all the time and you feel it's meaningful just to live and breathe and have a heartbeat and be on the earth,
1:02:04
you know, under the clouds, in the air and we're all capable of that sort
1:02:09
of state of consciousness. One could imagine human cultures where childhood education was focused
1:02:18
on fostering a state of consciousness, of extreme well-being,
1:02:24
that definitely is not what the modern education system does. I mean, even in very nice public schools, like the ones
1:02:31
my my kids go to here in rural Washington state. Right? So, I mean, I, I think there are
1:02:40
there are well known practices that can guide people towards states of, of well-being.
1:02:46
And I mean, meditation is part of of some of these. I met my friend Jeffrey Martin, launched,
1:02:55
course oriented toward bringing people into states of well-being within like 45 days, that 45 days to awakening.
1:03:04
And two of my adult kids went through this course with outstanding results.
1:03:10
I wouldn't say that's like a unique be all and end all, but it's interesting.
1:03:15
And what he was trying to show there is there's just practices.
1:03:21
People can go through 90 minutes a day that can jolt their brain into a much, much more, more
1:03:29
open and enjoyable state. Right. And then for for myself, it's been about ten years that I think I've been
1:03:38
in this sort of quasi blissed out state all the time due to certain practices.
1:03:45
I was never miserably depressed, but things were about Rocky at various earlier points.
1:03:51
Points in my life. I'm I'm actually I'm working on an app together with a friend of mine. Is this sort of side project where we have we have an AI avatar that's just sort of
1:03:59
guide leading people through different consciousness expansion practices. Like, I like, I wouldn't want to make an AI guru,
1:04:05
which would just be say, but they have an AI that kind of interacts with you, gets feedback from you about what practices are working for you or not.
How AGI could help humanity focus on inner growth
1:04:13
I think is is valuable. I think this is something people will get into more after they don't have to work for a living anymore, actually.
1:04:20
And this is actually one of the reasons why we may end up much, much happier
1:04:26
after an AGI takes over over other jobs, because the sort of rat race of everyday
1:04:31
life distracts us from working on our own consciousness, in our own bodies in ways that we could do, we could do otherwise.
1:04:39
So people may find initially they're like, oh shit, what do I do with my time? But then, you know, if if the memetic network
1:04:48
of our species works all right, and practices for fostering well-being spread through the social network,
1:04:56
perhaps augmented by the AI helping them spread, right? I mean, I mean, then then you may find what from present perspective
1:05:04
would seem remarkable States of well-being just become the norm after an AGI. Rather that I mean, it doesn't mean a super utopia.
1:05:14
Like I've been in a state of well-being for many years, but like I dislocated my shoulder last year, it sucked tremendously.
1:05:21
Like I was not happy about going to the emergency room that it stuck back in. You know, I mean, that doesn't
1:05:27
mean the perfect religious utopia, but there are states of consciousness much better than what most people are in most of the time now. And
1:05:39
ideally, you would like humanity to, upgrade itself to just a state of much greater compassion to the self as well as others
1:05:48
and well-being before launching a super AI upon the world, right? Because there's no doubt we could do it more thoughtfully
1:05:54
if that was the collective vibe of our species. Right. But but it is. It doesn't seem to be what's happening.
1:06:02
Right? I mean, we're close to AGI due to corporate and government
1:06:08
initiatives and. Well, I do think humanity is becoming more
1:06:14
compassionate and more self understanding during my lifetime. I think that is happening more slowly than, than AGI has, is, is, is advancing.
1:06:24
And that seems to be where we are now. But to give it more concrete notice here for the last minute or so of the interview, I mean, people ask me a lot,
1:06:32
like what should I do to make myself marketable on the job
1:06:38
market during these last few years and my best answer is,
1:06:45
you know, find a niche you can fill now that will support you
1:06:52
learning as much as possible and learning how to learn as much as possible. Right? Like if if your job involves pivoting and adapting to radically new things
1:07:01
as part of the job description, this is good because it will build in you the skill to pivot to radically new things,
1:07:10
which is pretty much the only skill which is very clear, will be useful with this in this transition period, because we can't predict
1:07:18
exactly what particular skill will be useful. Like you could say, well, become a plumber, but you know,
Learning how to learn: the last human advantage
1:07:23
there may be a plumber coming any, any year now which can just it's limbs are plumbing snakes, and they can just reach
1:07:30
reaching the pipe on its own without needing an an extra tool. I mean, the ability to learn how to learn and pivot to new things
1:07:40
will be the last thing to become economically useless, I would say. But this ties in closely with my more spiritual answer,
1:07:49
because the notion of non-attachment, I mean, part of being in a state of greater well-being is not being so strongly
1:07:59
attached to particular things in your life that you that you thought were very, very important.
1:08:05
And not being so overly emotionally attached to things. I mean, that that helps in being able to learn
1:08:12
how to learn and to to pivot. Right? And I mean that that doesn't mean you don't care about anything.
1:08:18
Like if someone, I mean, have two little kids, if someone came up and tried to hurt them, I would clobber them in the head, like
1:08:24
like anybody else. Right. But but it means not having cycles of anxiety and worry about about your attachment to things.
1:08:33
And if you can let go of those, you will find you can learn how to learn and pivot to weird new things more efficiently,
1:08:41
which is the most important survival skill as we move toward AGI. And so.
1:08:48
I love that. And and honestly, I feel like we could probably talk for another hour or two just about that.
1:08:54
But, Ben, I wanted to say a big thank you for coming on, for talking through, you know, all these well, whether it's technology, current future spirituality.
1:09:03
I feel like we covered a lot of ground today. And I really appreciated your insights, so. Thank you. Yeah. Thank you.
1:09:09
It's it's been a fun collection of topics.