https://www.youtube.com/watch?v=DOWqEWdwQtM
This New Al Workflow Replaces Hours of VFX with a Single Prompt
8,140 views  Sep 22, 2025
What used to take hours in After Effects now takes just one text prompt.  Tools like Google's Nano Banana, Seedream 4, Runwayâ€™s Aleph, and others are pioneering instruction-based editing, a breakthrough that collapses complex, multi-step VFX workflows into a single, implicit direction.

Chapters:
00:00 Introduction
00:54 Image Models
01:47 Video Models
03:24 What does this mean?
05:53 Practical Uses Today
09:42 Conclusion

Links & Resources:
World Models Video:    â€¢ Why Is Nobody Talking About AI World Models?  
New Qwen Image Editing Model: https://x.com/bilawalsidhu/status/197...
New Reve Image Editor: https://x.com/bilawalsidhu/status/196...
Seedance Multi-Shot Video Generation: https://x.com/bilawalsidhu/status/197...
Nano Banana: https://x.com/GeminiApp/status/196034...
Seedream 4.0: https://seed.bytedance.com/en/seedrea...
GPT Image 1 (LMArena): https://lmarena.ai/leaderboard/text-t...
WAN 2.2: https://x.com/_akhaliq/status/1968900...
Flux Kontext: https://bfl.ai/models/flux-kontext
Runway Aleph: https://x.com/runwayml/status/1948786...
Luma Modify Video: https://x.com/LumaLabsAI/status/19552...
Higgsfield: https://x.com/higgsfield_ai/status/19...
Claude Motion Graphics Example: https://claude.ai/public/artifacts/3d...
NVIDIA Gen-3C: https://research.nvidia.com/labs/toro...
BlenderFusion (AI + 3D integration): https://x.com/bilawalsidhu/status/194...
VGGT: https://vgg-t.github.io/
Google DeepLight:    â€¢ DeepLight: Learning Illumination for Uncon...  

Subscribe for more in-depth AI & creative tech videos! ðŸ‘‰ @bilawalsidhu 

Join My Newsletter: https://spatialintelligence.ai
Connect with me on X/Twitter here: https://x.com/bilawalsidhu
Everywhere else here: https://bilawal.ai
Business inquiries: team@metaversity.us 

Bio: 
Bilawal Sidhu is a creator, engineer, and product builder obsessed with blending reality and imagination using art and science. Bilawal is the technology curator for TED Talks, and a venture scout for Andreessen Horowitz. With more than a decade of experience in the tech industry, he spent six years as a product manager at Google, where he worked on spatial computing and 3D maps. His work has been featured in major publications including Bloomberg, Forbes, BBC, CNBC, and Fortune, among others. Bilawalâ€™s journey into computer graphics began at 11, when he fell in love with seamlessly blending 3D into real life footage. Since then, he's captivated over 1.5M subscribers, garnering more than 500M+ views across his platforms. Driven by a mission to empower the next generation of artists and entrepreneurs, Bilawal openly shares AI-assisted workflows and industry insights on social media. When heâ€™s not working, you can find Bilawal expanding his collection of electric guitars.

TED: https://www.ted.com/speakers/bilawal_...

--- 

Introduction
What used to take hours in Adobe After
Effects or even Comfy UI is now
collapsed down to a single text prompt.
Tools like Nano Banana, Cadream 4,
Runaway ALF, and so many more are
introducing this new paradigm of
instructionbased editing. It's a whole
new way of creating. Instead of motion
tracking something, building geometry,
estimating the lighting in the scene,
and all the other steps you had to take
before for visual effects, you can just
tell the AI model exactly what you want,
and it does the rest. It's quite
literally like standing over the
shoulder of a talented VFX artist and
having them execute on your vision
immediately. These advances are
collapsing that explicit multi-step
pipeline into one implicit step. It's
making professional-grade scene
reconstruction and visual effects
editing more accessible to content
creators. So, let's break down the
latest tools, what they do, and why
they're a big deal. All right, let's
Image Models
talk about the image models and then the
video models. On the image side, you've
got Google's Nano Banana. Lightning fast
instruction based edits with text and
image references. Bite Dance recently
dropped Cadream 4.0. It's very similar
to Nano Banana, but much higher
resolution and even better prompt
adherance. Kind of crazy. The unreleased
model that you probably haven't heard of
is GPT Image 1 Highfidelity. This is
unreleased but coming very soon as it is
already available on LM Marina. From
Blackforce Labs, you've got Flux
Context, which again is all about
instruction based editing. And of
course, the model that started it all,
GPT40 image. On the open source side,
you've got Quen imageedit, which is
absolutely amazing. And it's very close
cousin on the video side, WAN and vase,
which I'll get to a little bit later.
Now, all these image editing models,
what makes them different? What's the
instruction-based aspect? On the video
Video Models
side, you've got an equally killer
lineup. You've got runways ALF and
Luma's modified video. These are closest
to what I'm talking about in terms of
instructionbased editing. You provide in
a video, describe what you want to
change in it, and voila, you get an
output. You've also got tools like
Higsfield that are making it easier to
do things like insert props into video.
Now, tools like PA have been able to do
this, but Higsfield hits a higher
quality threshold for consumer
creations. On the video side, I would
also include large language models like
Claude is amazing for motion graphics.
So you can use it to generate HTML, CSS,
and JavaScript to create animations that
you screen record and have explicit
control over. And a great example of
instructionbased editing. If you want to
just move the camera around, Nvidia has
an amazing model called Gen 3C. It's
basically the Ken Burns effect on
steroids. You take a still image and
then you can animate the camera exactly
how you want. And then, as I mentioned
earlier, on the open source side, you've
got the cousin of WAN 2.0 called Vase or
all-in-one video creation and editing.
If you're a Comfy UI user, you got to
look into this tool. I mean, the
capabilities are very easy to explain.
You've got the ability to move anything,
swap anything, or reference anything,
expand anything, and animate anything.
Essentially, all of the closed source
tools have these features to various
degrees, and you have it all in one with
this particular release. I think a good
way to think about this is like you can
also rerender videos very easily
including content preservation,
structure preservation, subject
preservation, all the things that you
might have otherwise used like
multicontrol network flows for but
optimized for video in this case. So
everything is very temporally and
structurally coherent. So what does all
What does this mean?
of this mean? This is extremely exciting
because essentially you're collapsing
down the complexity of a multi-step
Photoshop, After Effects, Nuke, Cinema
4D workflow down to a handful of steps.
And again, if you know these tools, you
are at even more of an advantage because
a you know exactly how to prompt these
systems to get really good results
because you know how to describe it and
you can blend the best of old and new
techniques to get the job done. To give
you a perfect example of taking all
these instruction based primitives
together, check out this video a
talented visual effects artist made
reskinning that iconic scene from Superb
Bad where Mcleven presents his fake ID
for the first time. He swapped this out
to look like the characters from the
Phantom Menace. And it looks so good
because this is an experienced artist
combining geni tools like runway with
things like Photoshop and Nuke for
compositing. But this also has amazing
takeaways for less savvy proumer
creators, right? Like the beauty of this
stuff is the images serve as your
starting seed point. But this is also
amazing for the less initiated and
aspiring creator, right? Like suddenly
you have a really good way of creating
all the key frames, the start and end
frames you need to throw into your video
model of choice. If you made a
generation and you want to swap out the
face to have consistent characters, you
could do that. You don't need to use a
taskspecific model for face replacement
or background segmentation and all these
different tasks anymore. You could just
do it with a handful of primitives that
are far more general purpose. So you
could take the image models we talked
about and use it to make your start and
end key frames. Then throw it into your
video models of choice. You can take the
output of that video, throw into some of
these instruct based videos to modified
further. And suddenly you're bypassing
After Effects completely. These
Higsfield examples of object insertion
just show how amazing this is. Like
think of how much effort it would be to
go motion track this in Mocha, for
example, and then painstakingly adjust
the lighting. This is all one shot now.
So the power here is quite immense. It
means that we've got these generative
blackbox models where you can provide a
set of inputs and get some really cool
shots or elements that you can composite
together uh depending on how you look at
it and how sophisticated of a creator
you are. Honestly, it even replaces 3D
animation and motion graphics for you to
a certain extent. Check out this amazing
example of going into Nano Banana to
make a bunch of motion graphic key
frames and then animating them with your
video model of choice. Since you got the
start and end key frame, you can just
keep repeating it again and again,
essentially doing extended length
generations to an theoretically infinite
duration. Now, look, it is not quite
Practical Uses Today
ready for professional grade productions
yet, right? Like, if you're professional
grade production, you probably want some
sort of a mezzanine codec. You're
probably looking for high depth color.
You're looking for a log color curve,
all these types of things. It's not
quite there yet. But for internet scale
production, for making content on
YouTube or other social media platforms,
I think these tools very much have a
place in your workflow today. Now, I
suspect where this is going to go in the
future is we're going to have these two
bifurcated lanes of development. Some of
the models will be common, but we'll
have, you know, far more accessible
tools like Higsfield and Runway on one
end that cater to a broad swath of
users, including some professionals. And
then on the other side, we're going to
see a lot more of these hybrid workflows
emerge. As I'm creating this video,
Adobe actually listened to the feedback
and is going to integrate Nano Banana
directly into Photoshop. This is the
first third-party model that Adobe is
putting in their crown jewels. Even on
the After Effects side, you're going to
see a bunch of cool new plugins. Now,
back in my day when I used to work at
Google, we made this thing called deep
light for AR Core. We went and recorded
a bunch of these shiny balls essentially
to collect a bunch of training data. So
a camera could take just the feed that
it sees, which is this like vertical
limited field of view image, and imagine
what the light probe for the whole scene
might look like. This is child's play
today with machine learning. In many
ways, you can bypass it all together
with these really large models that have
just seen enough of the content on the
internet to be able to do this kind of
lighting estimation implicitly. On the
compositing end, with tools like Nuke,
you'll see even other amazing options.
You know, you can take things like depth
estimation and already use it to figure
out how to place volutric lighting in a
scene and have your scene and structure
respond very very believably. Some of
these results are absolutely fantastic.
And this is just using machine learning
based depth estimation as a primitive.
You're going to start seeing just like
you have really powerful nodes inside of
Comfy UI, these are going to start
coming into tools like Nuke and After
Effects. So phase swaps become a single
instruction. Relighting a scene becomes
a single image reference. Getting a
motion track, coarse grain geometry, and
feature tracking points all becomes one
single step once we start plugging in
models like VGGT directly inside of
these compositing tools. One last
prediction I'll make here is on this
theme of industry standard tools playing
with the new models. One really cool
line of research to look at is perhaps
exemplified by Blender Fusion. So this
paper is very interesting. It basically
says screw trying to describe 3D edits
through text and just use Blender. And
the idea is very straightforward, right?
Like instead of trying to cram 3D
understanding into a diffusion model,
you use depth estimation and
segmentation to take these 2D images and
project them into 2 and 1/2D meshes,
edit them in actual 3D software, and
then use a very fine-tuned diffusion
model to make the results
photorealistic. Again, this is very much
the approach that Nvidia is taking with
Cosmos and Omniverse, which we've
discussed in previous videos. So, this
is very cool, right? Like, if you're a
less sophisticated creator and you just
care about the end result and you want
to use natural language images and
scribbles to describe what you want,
you're going to have a really great path
for you. On the other hand, if you're a
professional and you want basically to
turn videos into these editable scene
graphs where every single thing is
controllable, all the writing is on the
wall. You're going to have that, too.
And personally, this has me very excited
because I grew up learning visual
effects. So, there are many times I
actually find it a little bit easier to
go into something like After Effects and
Maya and do it myself. I want to have
the best of both worlds, but there are
other times I'm making a ship hosting
meme for Twitter and I don't want to go
through all of that effort. In those
situations, I really like the consumer
proumer ccentric workflows. So, I think
there's an ability for creators to take
advantage of both of these lines of
development that will continue to
Conclusion
flourish. Where does this leave us?
Instruction-based video editing is
clearly the start of something bigger.
You're collapsing these complicated,
explicit pipelines into text, image, and
video prompts with very, very simple
instructions. On the other hand, thanks
to the magic of machine learning, you
can basically take a video and get back
those explicit pipelines where
everything is controllable. This is the
magic of multimodal in-n-out models. So
ultimately where all of this is going is
that some kid sitting in a basement is
going to have access to the capabilities
that James Cameron could only have
dreamt of. Want to use that camera to
record everything with your friends in
real life? You could do that. Want to
make it all inside your computer? You
could do that, too. With all this
mundane drudgery, these multi-step
workflows getting collapsed down into a
handful of Lego pieces you can use in
concert with each other. It really comes
down to the vision in your head and how
you turn your mind inside out. I call
this tech ILM in a box. You get a visual
effects studio in this black box. Maybe
even the offline hollow deck. Now, if
you want the real hol deck, I'll see you
in my next video about world models over
here.