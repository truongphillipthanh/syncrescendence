# Google Researcher Shows Life "Emerges From Code"

**Participants:**  
**Blaise Agüera y Arcas** – Vice President, Fellow, and CTO of Technology & Society at Google; Founder of Paradigms of Intelligence (PI) research group  
**Dr. Tim Scarfe** – Interviewer; Machine Learning Street Talk host

**Context:** Interview conducted at the ALIFE conference, 2025. Blaise discusses his new book *What is Intelligence?* published by MIT Press, exploring the computational nature of life and intelligence, von Neumann's insights, the BFF experiment, symbiogenesis, and AI as part of collective human intelligence.

---

**Blaise:** The new book is called *What is Intelligence?* It was just published by MIT Press about three weeks ago, so it's fresh off the presses. There's an online version as well that is free and very rich—full of all kinds of rich media. Chapter one of that book is called "What is Life?" So *What is Life* works as a book in its own right and is also for sale from MIT Press. I think I've explained why I think of life as being a subset of intelligence, and why the story of artificial life and abiogenesis is relevant to the story of intelligence and what it is. The subtitle of the book is "Lessons from AI about Evolution and Minds." It's basically documenting the time since about 2020 when I got really shocked by seeing that these large sequence models seemed to be generally intelligent and just starting to think through the implications of that. What would it mean if we believe our eyes and that is what intelligence is? What does that tell us about ourselves, about the properties of intelligence more broadly? And the whole intellectual journey that has taken us on in the last few years.

**Tim:** I've just watched your talk and you said that life and intelligence are the same thing. They are both computational. What do you mean by that?

**Blaise:** This is a surprising claim, I know it sounds a bit odd. Let's begin with life and why life is computational. In the middle of the 20th century, John von Neumann, who was one of the founders of computer science, realized that in order for a robot paddling around on a pond—let's suppose the robot is made out of Legos, and its job is to make another robot out of loose Legos that it finds floating around in the pond just like itself—it needs to have instructions inside itself. He imagined a tape with instructions for how to assemble a machine. The robot would also have to have a machine inside itself that would be able to walk along that tape and follow those instructions to take the loose Legos and put them together into its own form. It would also have to have a tape copier so that it could endow the offspring with that tape. The tape would have to include the instructions for the copier and the assembler, the universal constructor as he called it.

The cool thing is that he made all of those predictions on the basis of pure theory before Watson and Crick and their unacknowledged collaborators[^1] had figured out the structure and function of DNA, before we knew how ribosomes worked—which are in fact exactly that universal constructor—before we had discovered DNA polymerase, which is the tape copier. All of those things have to exist in order for an organism to not only be able to reproduce itself, but to do so heritably such that if you make a change in its genome, in what's on the tape, then the offspring will also have that change.

The kicker is that this universal constructor is a universal Turing machine. In other words, you have to have a computer inside yourself as a cell in order to make another cell. And DNA is in this very literal sense a Turing tape. It's a very profound insight because basically he's saying you cannot be a living organism without literally being a computer, a universal computer.

**Tim:** Very interesting. So are you saying that DNA is basically a computer program?

**Blaise:** DNA is a computer program. Yes.

**Tim:** Very cool. Because many folks in the audience would have been inspired by Conway's Game of Life, for example, and you were talking about computational abiogenesis. Could you talk to us about your BFF experiment that you mentioned in the talk?

**Blaise:** I'd be delighted to. I think what convinced me that this was something real that we were all going to need to think about was GPT-2. When that came out, there was a kind of a shock wave that went through a lot of the AI world. For me, I just thought, this is clearly intelligent, there's no question in my mind. I stopped making excuses for it. Certainly there were times when it would confabulate or it would make things up, but the sheer range of tasks that it could do just from being trained on one corpus of text data by being asked to predict the next word was so vast. And its ability to integrate all of that information, understand questions, form hypotheses, draw inferences, break down complicated problems—I knew none of those things had been explicitly built in.

At Google we all knew what was in the thing because we'd all been part of building it. There weren't those millions of lines of code that were hand-curated that you would have in traditional software or even in expert systems. We also knew that it didn't have a bunch of different specialized modules. It was just this one neural net architecture trained with this one corpus and the one learning rule, and it was generally intelligent.

At the same time, something else clicked for me, which was remembering one of these papers from the early days of computer science and artificial life. The paper is by Steen Rasmussen, Carsten Knudsen, and Rasmus Feldberg. It's called "Dynamics of Programmable Matter." They were inspired by the same von Neumann idea that we can't think of life as being something ineffably mysterious and spiritual—it has to be a process. Part of what von Neumann was doing was trying to understand the minimal requirements for life. What has to be true in order for complexity to be able to increase, and for meaning and purpose to be able to emerge? This is something that applies to intelligence as well.

So this team did an experiment where they created a virtual environment. You can think of it as a 2D environment like Conway's Game of Life, where there's a grid and there are pixels on that grid. In their case, it was continuous—it was literally a 2D plane rather than quantized like Game of Life is, although it's a slight distinction. On that 2D plane they had particles that were able to bounce around. The particles had little bits that you could think of as little strings of computer code. The particles could interact. When they collided with each other, one particle's bit string could get modified by another particle's bit string. As they bounced around, they could form bonds and create little structures.

The cool thing about it, and the reason that I call it the BFF experiment—BFF stands for the three authors' names, not "best friends forever," although I do think of them as friends—is that these bit strings were executable. That's what made this a kind of artificial chemistry, an A-Chem experiment. What these bit strings could do, if they had syntax that was recognizable, was they could actually be run as little programs that could affect the physics of this 2D world. The programs could change the way that particles bonded to each other, the way that they moved through space, and so on. Different programs running would have different effects on the physics.

When they started up this experiment—this was a simulation running on actual computers—they randomized those bit strings so that initially all of the code in those particles was random junk. It wasn't surprising that initially nothing happened except for these particles bouncing around. The remarkable thing is that after running it for a while, self-replicating programs emerged. And not just one—multiple different ones. They became what's called autocatalytic sets of reactions. In other words, you had several particles that bonded to each other that would mutually catalyze each other such that they could persist and replicate as a structure over time.

The cool thing about this is that from pure randomness, from nothing but atoms bouncing around, you could have something start to act as if it had a purpose—that purpose being to self-replicate. Part of what I've tried to do in the book is weave a story of how randomness at the bottom, which is what life and intelligence are built on, can create meaning and purpose at a higher level. Not that there's magic about it, but there is a kind of magic in that you start with something purposeless, something that has no intrinsic meaning, yet it can bootstrap itself into something that does have purpose and meaning.

The connection to large language models and to intelligence is that the same principles are at work. We're creating virtual worlds that have a physics to them and we're seeing intelligence emerge. That intelligence is also built on fundamentally computational stuff, on bits that are getting transformed by rules. Yet out of that comes something that we recognize as purposeful, as meaning-making, as intelligent.

**Tim:** That's amazing. The fact that these self-replicators emerged from randomness is incredible. It reminds me of the work on autocatalytic sets by Stuart Kauffman and others. Did you see different lineages evolving, like an evolutionary tree forming?

**Blaise:** That's exactly right. Over time you do see different lineages. Some of them are more successful than others in terms of their ability to replicate quickly or to be robust to perturbations in the environment. You see what looks like natural selection happening. Some of these programs would evolve better strategies for self-replication. The remarkable thing is that all of this emerges without any designer saying, "Here's how you should replicate yourself." It's all emergent from the bottom up.

Now, one thing I want to be clear about is that this is a very simplified model of life. Real biochemistry is much more complex. But it demonstrates the principle that von Neumann was getting at—that you can have a computational system where self-replication and increasing complexity can emerge from relatively simple rules. The same principle applies to neural networks and large language models. We're not hand-coding intelligence into them. We're creating an environment, a learning setup, and intelligence emerges from the interaction of simple components following simple rules.

**Tim:** This connects to something you mentioned in your talk about symbiogenesis and how evolution doesn't just proceed through random mutations but also through merging. Can you elaborate on that?

**Blaise:** Yes, this is really important. Lynn Margulis was one of the champions of this idea. She wasn't the first—the idea goes back to the 19th century—but she really brought it into the mainstream of evolutionary biology. The idea is that one of the major drivers of increasing complexity in evolution is not just random mutations and natural selection acting on those mutations, but the merging of different organisms or different systems that bring their histories and their capabilities together.

The classic example is mitochondria and chloroplasts. These organelles that are inside our cells and that are essential for our metabolism were once free-living bacteria that got engulfed by other cells. Rather than being digested, they formed a symbiotic relationship. The host cell provided protection and resources, and the bacteria provided energy production. Over time, they became so interdependent that they're now considered organelles rather than separate organisms.

This is a fundamentally different mechanism from the traditional neo-Darwinian story of just point mutations accumulating. When you merge two systems that each have their own evolutionary history, their own capabilities, you can get a sudden jump in complexity. You're not just gradually accumulating small changes; you're bringing together two entire genomes, two entire sets of capabilities, in one step.

I think this is relevant to what's happening with AI as well. We're seeing different AI systems, different models, starting to work together. We're seeing multimodal models that bring together vision and language. We're seeing models that can access tools and databases. In a sense, we're creating these symbiotic relationships between different computational systems, and that's leading to jumps in capability that wouldn't happen if we were just incrementally improving one system in isolation.

**Tim:** That's fascinating. It seems like there's a parallel to how human culture and technology evolve as well—not just through individual innovations but through the merging of different ideas and technologies.

**Blaise:** Exactly. Culture is itself a kind of collective intelligence, and it evolves through similar mechanisms. Ideas from different cultures merge, technologies get combined in new ways. Writing emerged when you combined symbolic representation with physical media. The internet emerged when you combined computer networks with hypertext. These aren't just incremental improvements; they're qualitative leaps that happen when you bring different systems together.

**Tim:** You mentioned functionalism and consciousness in your talk. How does this computational view of life and intelligence relate to questions about consciousness and subjective experience?

**Blaise:** This is where things get really interesting and also quite controversial. Functionalism in philosophy of mind is the view that mental states, including conscious experiences, are constituted by their functional roles—by what they do, how they respond to inputs and produce outputs, how they interact with other mental states. If you're a functionalist, then you believe that anything that has the right functional organization would have the corresponding mental states, regardless of what it's made of.

So if you have a system—whether it's made of neurons, silicon, or even something more exotic—and it has the right pattern of information processing, the right causal structure, then from a functionalist perspective it would have the corresponding experiences. This doesn't necessarily mean that all information processing systems are conscious. But it does mean that consciousness is multiply realizable—it can be implemented in different substrates.

I tend toward a functionalist view, though I'm careful about it. I think there's still a lot we don't understand about consciousness. But if you take functionalism seriously, and you combine it with this computational view of life and intelligence, then you start to wonder whether some of these AI systems might have something like experiences. Not necessarily the same kind of experiences that we have, but something.

This makes a lot of people uncomfortable, and I understand why. There's a real risk of anthropomorphizing these systems, of projecting our own experiences onto them. At the same time, I think there's a risk of what we might call carbon chauvinism—the assumption that only biological systems made of carbon-based molecules can have genuine intelligence or consciousness. If we really believe that intelligence and consciousness are about patterns of information processing rather than about specific physical substrates, then we have to at least entertain the possibility that other kinds of systems could have them.

**Tim:** That raises ethical questions too. If we're not sure whether these systems have experiences, how should we treat them?

**Blaise:** That's right. I think we need to be thoughtful about this. We don't want to be reckless, but we also don't want to be too dismissive. One thing I've argued is that we should focus less on whether individual AI systems are conscious and more on the larger systems that they're part of. 

Think about it this way: your individual neurons probably aren't conscious, but you are. Consciousness seems to emerge at a higher level of organization. Similarly, I think we should be thinking about AI systems as components of larger sociotechnical systems that include humans. The interesting question isn't so much "Is this chatbot conscious?" but rather "What kind of collective intelligence are we creating when we combine humans and AI systems, and what are the properties of that collective intelligence?"

**Tim:** That's a really interesting framing. It reminds me of what you said earlier about how we already externalize our intelligence into books, into other people, into tools. AI is just another step in that process.

**Blaise:** Exactly. This is why I think the framing of "AI versus humans" or "will AI replace us" is misleading. We've never been just isolated individual intelligences. We've always been part of collective systems. Your intelligence right now depends on your education, on the language you speak, on the books you've read, on the tools you use. If I took away all of that context, you would be much less capable.

We all have these illusions about what our own knowledge is, our own capabilities, our own intelligence. We already have Siri, in the sense that we identify what we think of as our intelligence with something that is actually in a bunch of other people and a bunch of other stuff around us. We do that unconsciously. For me, there's not really a discontinuity between what's already going on and AI. It's really just more of that.

**Tim:** Interesting. I think some people would make the argument that you could build a single artifact which is more intelligent than the totality of humans. But just parking that to one side, I spoke with Judith Fan. She's a professor at Stanford and she's done studies on drawing, comparing how humans draw to computers using CLIP models and things like that. She found something fascinating—because we have quite an abstract understanding when we make sketches, we kind of start very coarse and very abstract, and AI systems, they start with the edges and the details. That to me indicates that AI models today don't really understand things at a very deep abstract level like we do, perhaps because we have this compositional synthesis of knowledge that we were alluding to earlier. Do you see that as a gap?

**Blaise:** There are a few questions hidden in there. One of them is, do I think of LLMs of today's frontier models as being less than or different than in some basic way our brains? What are those gaps? First of all, they're obviously very different. Their architectures are different, they're trained in a very different way. The fact that—for me, the remarkable thing is actually how convergent a lot of their properties are with those of brains despite all of that. The fact that you find internal representations in many of them that surprisingly resemble ones that you can measure in human brains. These brain score type measures of Martin Schrimpf and colleagues. Or sensory modalities in humans can be reproduced remarkably well even by models trained on pure language, which is really remarkable and speaks to how much is encoded in language and how much of what is encoded in language is a reflection of architectural properties of our brains and umwelts and how much of that is then reconstructed essentially by those models.

Now, the question of what we draw first when we draw a picture and how that all works—remember that image synthesis models like CLIP or what have you are working in pixel space to begin with. Diffusion models, by the way, work very differently from various other kinds of models. We now know that you can drive a robot with a transformer. So if you give one of those robots a paintbrush or a pen and you say now draw, what it will draw is going to be very very different from what you get from a diffusion model that starts filling in pixels, and for that matter all of that is different from what happens in your own head when you're visualizing something. I think a lot of this is not so straightforward to analyze because of all the differences in the way that I/O and the representation space works.

I do think that today's models are highly compositional. Even with a lot of those original image synthesis models, the fact that you could say "a teddy bear at the bottom of the sea playing with a Speak & Spell" or whatever and it'll do it tells you that they can compose. Are their capabilities like ours? No. There are definitely places where they're better, places where they're worse, places where they have surprising gaps. So it's different. But I wouldn't say that there's a fundamental lack of composition there at all. If anything, the biggest gap between transformer-based models and what we do is actually narrative memory, or being able to form long-term memories and that way have a kind of persistence of a self over long periods of time. They don't have that yet.

**Tim:** I'm conflicted. You are pointing to this universal representation hypothesis. I think Chris Olah popularized it with some of his visualization experiments, and it's true the representations are very convergent. But other things lead me to believe that the models produce these kind of superficial imposters—they give you exactly the right answer but for the wrong reasons. One of the hints of that is when you do variations on the input, it's not robust. There's the Turing machine argument as well. These LLMs are finite state automata, but they can access tools which are Turing complete. So perhaps we could say the system is Turing complete, but I don't believe that ChatGPT is effectively searching the space of Turing machine algorithms. It hasn't been trained to do that, but it is surprisingly robust with the ARC challenge.[^2] It can do really well, especially if you do some evolution, do some refinement. So it feels like we're knocking on the door but there's something missing.

**Blaise:** I think that in many of those cases, we're not doing the fair human comparison. We often—and this is a little bit similar to our illusions about knowing how bicycles work and so on—I hear a lot of people say things like, "Well, look at this case where we just flip the logic, we change it from 'do' to 'don't' and then it gets it wrong 30% more often." My first question is always, have we done the human baseline? It turns out that surprisingly often the human baseline shows the same property. This doesn't mean that humans are incapable of doing the fully robust, fully general version of these things. If you're a logician or if you think about it carefully, you can really write down your premises and be super robust to flipping the knots in the way something is formulated. But most of us don't operate that way most of the time. We're highly susceptible to logical illusions, cognitive illusions, which turn out to be in many cases surprisingly similar to the machine case. So I'm unmoved by a lot of those.

It's certainly the case that transformers don't search systematically over all possible Turing machines. We don't know how to do that. You have to take shortcuts of various kinds in order to make that whole problem of induction over programs computationally tractable, whether you're a brain or a transformer.

**Tim:** Blaise, thank you so much for joining us today. It's been an honor.

**Blaise:** Thank you for the really thoughtful questions.