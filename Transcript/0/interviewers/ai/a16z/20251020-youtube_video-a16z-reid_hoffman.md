# Reid Hoffman on AI, Consciousness, and the Future of Labor

**Participants:**  
- **Reid Hoffman** – Co-founder of LinkedIn, investor in OpenAI, AI entrepreneur  
- **Erik Torenberg** – Host, a16z Podcast  
- **Alex Rampell** – Co-host, General Partner at a16z

**Context:** Reid Hoffman discusses AI's impact on work, science, and humanity, covering investment frameworks, Silicon Valley's blind spots, consciousness in AI, LinkedIn's durability, and the nature of friendship in an era of intelligent machines.

---

**Erik:** Welcome to the podcast. It's great to be here. Reed, you're one of the most successful Web 2.0 investors of that era—Facebook, LinkedIn obviously which you co-created, Airbnb, many others. And you had several frameworks that helped you do that, one of which was the Seven Deadly Sins, which we talk about often and love. As you're thinking about AI investing, what's a framework or worldview that you take to your AI investing?

**Reid:** Obviously we're all looking through a glass darkly, looking through a fog with strobe lights that are hard to understand. So we're all navigating this new universe. I don't know if I have as crystallized a framework, but the Seven Deadly Sins still work because that's a question of what is infrastructural psychological infrastructure across all 8 billion-plus human beings.

But I'd say there's a couple things. The first is there is going to be a set of things that are the obvious line of sight—chatbots, productivity tools, coding assistance. And by the way, that's still worth investing in, but obvious line of sight means it's obvious to everybody's line of sight. So doing a differential investment is harder.

The second area is, well, what does this mean? Because too often people say in an area of disruption that everything changes, as opposed to significant things change. So you were mentioning Web 2.0 and LinkedIn. And obviously, part of this with a platform change, you go, okay, well, are there now new LinkedIns that are possible because of AI or something like that? And obviously, given my own heritage, I would love LinkedIn to be that, but it's whatever—I'm always pro innovation, entrepreneurship, best possible thing for humanity. But what are the kind of more traditional things that haven't changed? Network effects, enterprise integration, other kinds of things that the new platform upsets the apple cart, but you're still going to be putting that apple cart back together in some way. And what is that?

And then the third, which is probably where I've been putting most of my time, has been what I think of as Silicon Valley blind spots. Because Silicon Valley is one of the most amazing places in the world. There's a network of intense coopetition, learning, invention, building new things, which is just great. But we also have our cannons, we have our blind spots. And a classic one for us tends to be, well, everything should be done in CS, everything should be done in software, everything should be done in bits. And that's the most relevant thing because, by the way, it's a great area to invest. But it was like, okay, what are the areas where the AI revolution will be magical but won't be within the Silicon Valley blind spots?

And that's probably where I've been putting the majority of my co-founding time, invention time, investment time. Because a blind spot on something that's very, very big is precisely the kinds of things that you go, okay, you have a long runway to create something that could be another one of the iconic companies.

**Erik:** Let's go deeper on that, because we were talking just before this about how people focus so much on the productivity side, the workflow sides, but they're missing other elements. Say more about other things that you find more interesting now.

**Reid:** So one of the things I told my partners back at Greylock in 2015, so 10 years ago, I said, "Look, there's going to be a bunch of different things on productivity around AI. I'll help." Like, you have companies you want me to work with that you're doing? Great. I'll do the reference calls. If you have questions, ask me. If you want me to help with founders, ask me. But I said, "Look, I want to invest my time and energy in drug discovery and creating new therapeutics." Because this is the kind of thing that AI has been having amazing results on. And it's one of these things, where the difference between having a drug that works and a drug that doesn't work is life and death. It's incalculable.

And so that's where I've been doing most of my work. So I co-founded Manusis[^1] with Siddhartha Mukherjee, who's the CEO, author of *The Emperor of All Maladies*, inventor of some T-cell therapies. And we're going after autoimmune. We're trying to re-engineer the immune system to be more efficient at dealing with immune-mediated diseases—a whole category of diseases that we don't really have good solutions for.

And that was what I said when I went to my partners 10 years ago. I was like, this is an area where we have this amazing AI technology, and it's going to disrupt and change this whole way in which we've been addressing basically most modern medicine. Most modern medicine has been, "What's the symptom? How do we address the symptom?" As opposed to, "What's the underlying cause? How do we address that?" Because we never had the techniques and tools to do it. And now we have the techniques and tools, and that's a fundamentally transformative area. And to me, that's way more interesting than another set of new productivity workflows—although those are important too.

**Alex:** So your point, if I understand it correctly, is basically that there'll be incremental improvements in coding or whatever these things are, but then when it's matter of life or death, not just it's more serious morally, but the value of it is so high that even if the improvements are also maybe incremental, the dollar value or impact value is far greater.

**Reid:** Yes, exactly. And this is actually one of the things that I think people don't realize about Silicon Valley. You start with, what's the amazing thing that you can suddenly create? Lots of these companies, you go, "What's your business model?" They go, "I don't know." They're like, "We're going to try to work it out, but I can create something amazing here." And that's actually one of the fundamental—call it the religion of Silicon Valley and the knowledge of Silicon Valley—that I so much love and admire and embody.

Like, we didn't know what the monetization model for LinkedIn was. In fact, one of the things we went through was like, we probably have to play with five or 10 things, and we'll figure out which three to six actually work. When we first launched LinkedIn, the only thing we had was you could pay for contacting people outside your network—kind of this initial theory around social capital. And it wasn't until a couple years later that we came up with subscriptions. And then it was a couple years after that that we came up with advertising. And then it was a couple years after that that we came up with the real driver, which was talent solutions, recruiting, and hiring.

So it's this thing that you go, "Okay, I've got something amazing. I've created a whole new network of hundreds of millions of professionals that every single professional in the world should be on." Now, how do we make it work as a business? And that's the kind of Silicon Valley, "Trust us, we'll figure it out."

And so similarly here, you go, "Okay, we can create a whole new way of dealing with autoimmune diseases." This has huge implications for hundreds of millions of people globally. Huge implications for health care costs and a bunch of other things. Figure out the business model. It's really easy because you've got this amazing thing. And so I think sometimes Silicon Valley forgets that when it gets into a more mature phase around business models. And it's like, no, no, actually go back to the thing—what's the amazing thing you can create? And then the business model becomes obvious because you've created this amazing thing.

**Erik:** And to the extent—I mean, you're doing this through Inflection, through Manusis, through other companies. Do you think the medical profession is in some sense going away? Or what do you think the role of doctors should be? Because they're obviously a form of credentialism as well.

**Reid:** Well, so I think AI doctors will be a thing. I don't think that'll be the only thing. And I think human doctors will still be a thing. Partially because I think the practice of medicine will change as a result of AI, where it will no longer be the way that we currently memorize a bunch of things and try to apply them. It'll be, "Oh, I have access to this amazing diagnostic tool and therapeutic set of suggestions that does a bunch of work for me, and then I add my humanity, my compassion, my presence, my being there to help you through this."

And that's actually part of what I think will change. But I think that there will be a category of AI doctors that are valuable because, for example, they might be the best diagnosticians. They might be like, "Look, I've seen every case ever. Here's what I think is going on with you. Here's what I think you should be doing." And you might go to that AI doctor first before you go to the human doctor because you want to understand, "What do I need to ask my human doctor about?" Or, "What should I be orienting to?"

And I think that's actually part of the future. And I think the credentialism thing is something where we should be very careful about because I think credentialism should be a heuristic. It should be a starting point. It shouldn't be, "Oh, you don't have the credential, therefore you can't do it." It should be, "You don't have the credential. Well, what's the way that you're proving that you can do this thing?" And I think that's the shift that needs to happen.

**Alex:** The challenge though is you don't want the person who doesn't have the credentials doing your surgery, at least not yet.

**Reid:** Well, so that's where I think the AI thing comes in, which is like, look, you might have the AI doing the surgery with a human supervisor. You might have a less credentialed person doing the surgery because the AI is doing most of the work and they're there as the manual dexterity. And I think that's actually part of what the transformation will be.

But I think the really important thing is we need to be thinking about competence, not just credentials. And credentials are a useful heuristic for competence, but they're not the only heuristic. And I think we've over-rotated on credentials in part because it's been a useful way of creating barriers to entry, creating economic rents, and other things that are not necessarily in the best interest of society.

**Erik:** Let's talk about some of the technical limitations or bottlenecks in AI today. Where do you think the current models are most limited? And what needs to happen to get past those limitations?

**Reid:** Well, so I think the current models are most limited in reasoning. They're good at pattern matching. They're good at statistical inference. They're good at, "I've seen this before, here's what typically happens next." But they're not great at true reasoning—multi-step logical inference, planning over long horizons, understanding deep causal relationships.

Now, there's a bunch of work happening on this. OpenAI has their O1 model, which is starting to get better at reasoning. There's other research happening. But I think that's still one of the fundamental limitations. And I think until we crack that, we're going to have AI that's really good at a lot of things but still limited in its ability to do truly novel problem-solving.

The second limitation is context. These models can only hold so much in their context window. They can only reason over so much information at once. And while that's getting better—we've gone from 2,000 tokens to 8,000 to 32,000 to now models that can do 200,000 or a million tokens—there's still a fundamental limitation there. And I think we need to figure out how to give these models better memory, better ability to reason over large bodies of information.

The third limitation is what I'd call groundedness in the physical world. These models are trained on text and images, but they don't have direct experience with the physical world. They don't understand physics in the way that even a toddler understands physics from interacting with the world. And I think that's going to be a limitation for applications that require that kind of understanding—robotics, physical manipulation, understanding how things work in the real world.

**Alex:** On that last point about robotics and bits versus atoms, one thing that's interesting is software can be reproduced at zero marginal cost, but atoms can't. And so even if you have the perfect AI that can design the perfect robot, you still have to manufacture the robot. You still have to deal with supply chains and logistics. Do you think that's a fundamental limit on how quickly AI can transform the physical world?

**Reid:** I think it's a real constraint, but I don't think it's a fundamental limit. I think what will happen is we'll get better at manufacturing, better at logistics, better at supply chains, in part because AI will help us optimize those things. But you're right that there's a physical bottleneck that doesn't exist in pure software.

And I think that's part of why I'm bullish on areas like synthetic biology, where you can use biological systems as manufacturing—you can literally grow things. You can have cells that are manufacturing therapeutics or materials or other things. Because there, you have this hybrid of bits and atoms where the bits are programming the atoms to manufacture themselves, and you get some of the advantages of both.

But you're right that pure robotics, pure physical manufacturing, there's going to be a constraint there that's different from pure software. And I think that's going to mean that the transformation of the physical world happens more slowly than the transformation of the digital world. But it still happens, and it's still going to be profound.

**Erik:** I'm curious about this idea of AI as savant. Like, you could imagine an AI that's incredibly good at one narrow thing but terrible at everything else. Do you think that's where we're headed? Or do you think we're going to get more general intelligence?

**Reid:** I think we're going to get both. I think we're going to have specialized models that are incredible at one thing—whether that's drug discovery or protein folding or whatever. And then we're going to have more general models that are good at a lot of things but maybe not the absolute best at any one thing.

And I think there's going to be a question of how you compose these. How do you have a general orchestrator that knows when to call on the specialized model? How do you have models that can work together? And I think that's actually one of the interesting research directions—not just making individual models better, but making systems of models that can collaborate.

But I do think we're going to get more general intelligence. I don't think we're stuck at the current level of generality. I think we're going to keep pushing toward more and more general capabilities. And the question is, how far can we push that? Can we get to human-level general intelligence? Can we get beyond that? I think those are open questions, but I'm optimistic that we'll make progress.

**Alex:** On software eating labor—there's this idea that software has eaten a lot of jobs, but there are certain jobs that seem resistant. What's your mental model for thinking about which jobs or which parts of jobs are going to be automated versus which are going to remain human?

**Reid:** So I have this heuristic that I call "lazy and rich." And the idea is, what are the things that rich people pay other people to do because they're too lazy to do it themselves? And those are often the things that are going to be hard to automate because they require either physical presence, emotional labor, status signaling, or some combination thereof.

So like, a rich person might pay someone to walk their dog, not because they can't walk the dog, but because they want the experience of having someone do it for them, or they want the status of having staff, or whatever. And those kinds of jobs are going to be harder to automate.

Whereas the things that people are doing because they have to, because there's no other option, because they're pure information processing or pure routine physical labor—those are the things that are going to get automated more quickly.

And I think the question is, what does that mean for the economy? Because if you automate all the jobs that people don't want to do, and you're left with the jobs that people either want to do or that require human presence, what does that economy look like? And I think that's actually a more optimistic vision than the dystopian "all the jobs go away" vision. Because it's like, okay, we're automating the drudgery, we're automating the things that people didn't want to do anyway, and we're left with work that's more meaningful, more human, more about connection and creativity and things that we actually value.

But we have to make sure the economic distribution works out, that people can still support themselves, that we have the social systems in place to handle that transition. And I think that's the real challenge, not the technology itself.

**Erik:** Let's talk about scaling laws. There's been this incredible predictability in how these models improve with more compute, more data. Do you think that continues? Or do we hit some kind of wall?

**Reid:** I think it continues for longer than most people think. I think there are still a lot of gains to be had from scaling. Now, there might be other things that are required in addition to scaling—better architectures, better training techniques, better data quality. But I think the fundamental scaling laws still hold.

And I think part of the reason people are skeptical is because they're looking at it from the outside and going, "Well, it can't just be that simple. It can't just be you throw more compute at it and it gets better." But that does seem to be how it works, at least for now. And I think we're going to see that continue.

Now, at some point, you might hit diminishing returns. You might hit a point where the cost of the next increment of improvement is so high that it's not worth it. But I don't think we're there yet. I think we're still in the phase where every order of magnitude of compute gives you meaningful improvements in capability.

And I think that's why there's so much investment going into AI infrastructure, into building bigger clusters, into getting more efficient hardware. Because people realize that this is still a viable path to better models.

**Alex:** There's also this question of whether the models are truly understanding or just doing sophisticated pattern matching. And I know you've thought about consciousness and intelligence. Do you think understanding or consciousness is necessary for the kinds of intelligence that we're seeing?

**Reid:** No, I don't think consciousness is necessary for intelligence. I think you can have very intelligent systems that are not conscious. And in fact, I think most of the AI we're building is exactly that—intelligent but not conscious.

Now, the question of whether they truly understand is a philosophical question. What does it mean to understand? If a system can predict what's going to happen next, can answer questions correctly, can explain its reasoning, does that count as understanding? Or is there something more required?

And I think this gets into questions of phenomenology, qualia, subjective experience. And those are hard questions. I don't think we have good answers to them. But I also don't think they're necessary to answer in order to build useful AI.

I think we can build AI that's incredibly capable, that can do things that we would describe as intelligent, without needing to answer the question of whether it's conscious or whether it truly understands in some deep philosophical sense.

And I think that's actually liberating because it means we don't have to solve the hard problem of consciousness in order to make progress on AI. We can make progress on AI and then maybe come back to those questions later.

**Erik:** But do you think consciousness will emerge? Or is it something that would have to be explicitly designed?

**Reid:** I think it's possible that consciousness could emerge, but I don't think it's likely with current architectures. I think current architectures are fundamentally doing computation, doing information processing, and I don't think that necessarily gives rise to subjective experience.

Now, could you design a system that is conscious? Maybe. But I think that would require understanding consciousness better than we currently do. And I think we're very far from understanding consciousness.

And I think there's also a question of whether we should build conscious AI. Because if you build a conscious AI, you might have moral obligations to it. It might have rights. It might have interests that conflict with ours. And I think those are questions we should think carefully about before we go down that path.

**Alex:** Let's shift gears and talk about LinkedIn. It's been around for over 20 years now. It's outlasted many other social networks. What do you think makes it durable? And do you think that durability continues, or is there something that could disrupt it?

**Reid:** I think the fundamental reason LinkedIn is durable is because it's built on a real-world network that doesn't change that quickly. Your professional identity, your work history, your skills—those things are relatively stable. They evolve over time, but they don't change as fast as your social relationships or your interests.

And so LinkedIn is built on top of this underlying reality that is relatively stable. And that gives it durability. It means that the network effects are strong. It means that once you're on LinkedIn, there's a reason to stay on LinkedIn because that's where your professional network is.

Now, could something disrupt it? Sure. I think if someone built a fundamentally better way of doing professional networking, or if the nature of work changed so much that the current model of LinkedIn didn't make sense anymore, then yeah, it could be disrupted.

But I think those are both hard to do. I think building a better professional network is hard because you're competing against existing network effects. And I think the nature of work, while it is changing, is not changing so quickly that the fundamental model of "here's my professional identity, here's my work history, here's how you can find me" stops making sense.

So I think LinkedIn has durability, but it's not permanent. Nothing is permanent. And I think the question is, how does LinkedIn evolve to stay relevant as the world of work changes? How does it incorporate AI? How does it help people navigate careers in an AI-augmented world? And I think those are the questions that the team there is working on.

**Erik:** One interesting thing about LinkedIn is that it's not particularly good at negative references. Like, if I want to check on someone, I have to do backchannel. There's no way to see negative information on the platform. Is that a feature or a bug?

**Reid:** It's a feature. And the reason it's a feature is because we thought about building that, and we realized that if you have negative references on the platform, you create a whole set of problems. You create legal liability. You create social complications. You create incentives for people to retaliate against each other. You create a much more negative environment overall.

And we thought, better to have LinkedIn be a place where you present your professional identity, and then if someone wants to check on you, they do that through backchannel. They reach out to mutual connections. They ask around.

And actually, LinkedIn makes that easier because you can see who might know a person. You can see the connections. And I have a standard email that I send where I ask people to rate someone from 1 to 10 or reply "call me." And when you get a "call me," you know there's something there. When you get a 10, you're like, "Really? Best person you've ever worked with?" But when you get a bunch of 8s and 9s, you're like, "Okay, this person is solid."

And I think that system works better than having negative references directly on the platform. It puts the burden on the person doing the checking, but it avoids creating a toxic environment on the platform itself.

**Alex:** We have about 10 minutes left, just logistics check. Is there anything you wanted to make sure we covered?

**Reid:** No, but we can do this again. This is always fun.

**Erik:** I'm curious, Reid, as you've continued to uplevel in your career and have more opportunities that seem to compound, especially post-selling LinkedIn, how have you decided where is the highest leverage use for your time? Where can you have the biggest impact? What's your mental framework?

**Reid:** One of the things that I'm sure I speak for all three of us—it's an amazing time to be alive. This AI transformation of what it means for evolving homo techne, and what is possible in life and society and work, is just amazing. And so I stay as involved with that as I possibly can. It has to be something that's so important that I will stop doing that.

Now, within that, part of that was co-founding Manusis with Siddhartha Mukherjee. For example, getting an instruction from him on the FDA process—that's the kind of thing that makes us all run screaming for the hills. But also, one of the things I think is really important is, as technology drives more and more of everything that's going on in society, how do we make government more intelligent on technology?

And so for every well-ordered Western democracy, I've been doing this for at least 20 to 25 years. If a minister or senior person from a democracy comes and asks for advice, I give it to them. Just last week, I was in France talking with Macron because he's trying to figure out, "How do I help French industry, French society, French people? What are the things I need to be doing?" If all the frontier models are going to be built in the US and maybe China, what does that mean for how I help our people?

And he's doing the exact right thing, which is, "I understand that I have a potential challenge. What do I do to help my people? How do I reach out? How do I talk?" Sure, they've got Mistral. They've got some other things. But how do I maximally help what I'm doing? And so putting time into that as well.

**Erik:** I remember seeing your calendar, and it was what seemed like seven days a week, meetings absolutely stacked.

**Reid:** I've gone to six and a half days.

**Erik:** Okay, I'm glad you've calmed down. One of the ways in which you're able to do that—one, it's important problems, but two, you work on projects with friends, sometimes over decades. Maybe we'll close here. You've thought a lot about friendship. You've written about it, spoken about it. I'm curious what you found most remarkable or most surprising about friendship, especially as we enter this AI era where people are questioning what the next generation's relationship to friends will be.

**Reid:** I'm actually going to write a bunch about this specifically because AI is now bringing some very important things that people need to understand. Friendship is a joint relationship. It's not, "Oh, you're just loyal to me," or "Oh, you just do things for me," or "This person does things for me." Well, there's a lot of people who do things for you. Your bus driver does things for you, but that doesn't mean you're friends.

Friends—like, for example, a classic way of putting it is, "I had a really bad day, and I show up at my friend Alex's, and I want to talk to him. And then Alex is like, 'Oh my god, here's my day.' I'm like, 'Oh, your day is much worse.' We're going to talk about your day versus my day." That's the kind of thing that happens because what I think fundamentally happens with friends is two people agree to help each other become the best possible versions of themselves.

And by the way, sometimes that leads to friendship conversations that are tough love. They're like, "Yeah, you're screwing this up, and I need to talk to you about it." It's not the sycophancy thing. It's like, how do I help you?

But part of the thing that I gave in the commencement speech at Vanderbilt a few years back was on friendship, and part of it was to say, look, part of friends is not just "does Alex help me?" but "Alex allows me to help him." And as part of that, that's part of how I become a deeper friend. I learn things from helping Alex. That joint relationship is really important.

And you're going to see all kinds of people saying, "Oh, I have your AI friend right here." No, you don't. It's not a bidirectional relationship. Maybe an awesome companion—spectacular—but it's not a friend. And you need to understand that part of friendship is part of when we begin to realize that life's not just about us, that it's a team sport, that we go into it together. That sometimes friendship conversations are wonderful and difficult.

And I think that's what's really important. And now that we've got this blurriness that AI has created, I have to go write some of this very soon so that people understand how to navigate it and why they should not think about AI anytime soon as friends.

**Erik:** One thing I've always appreciated about you as well is you're able to be friends with people with whom you have disagreements, or people you're not close to for a few years but you can reconnect. That ability to—

**Reid:** Yeah, it's about us making each other better versions of ourselves. And sometimes those go through rough patches.

**Erik:** I think it's a great place to close, Reid. Thanks so much for coming on the podcast.

**Reid:** My pleasure, and I hope we do this again.

**Erik:** Excellent.