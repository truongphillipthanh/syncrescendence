# Marc Andreessen and Ben Horowitz on the State of AI

**Host:** Erik Torenberg  
**Panelists:** Marc Andreessen, Ben Horowitz  
**Context:** Closing keynote from a16z's Runtime conference, October 31, 2025

---

**Erik:** Marc, there's been a lot of talk lately about the limitations of LLMs—that they can't do true invention of new science, that they can't do true creative genius, that it's just combining or packaging. You have thoughts here. What say you?

**Marc:** Yeah. So for me, you get all these questions and they usually come in either: are language models intelligent in the sense of can they actually process information and have conceptual breakthroughs the way that people can? And then there's: are language models or video models creative? Can they create new art? Can they actually have genuine creative breakthroughs? 

And of course my answer to both of those is: well, can people do those things? 

I think there's two questions there. Even if some people are quote unquote intelligent—as in having original conceptual breakthroughs and not just regurgitating the training set or following scripts—what percentage of people can actually do that? I've only met a few, some of them are here in the room, but not that many. Most people never do. And then creativity—I mean, how many people are actually genuinely creative? You point to a Beethoven or Van Gogh or something like that and you're like, okay, that's creativity. And yeah, that's creativity. And then how many Beethovens and Van Goghs are there? Obviously, not very many. 

So one is just: if these things clear the bar of 99.99% of humanity, that's pretty interesting just in and of itself. But then you dig into it further and you're like, okay, how many actual real conceptual breakthroughs have there ever been in human history as compared to remixing ideas? 

If you look at the history of technology, it's almost always the case that the big breakthroughs are the result of usually at least 40 years of work ahead of time—four decades, right? In fact, language models themselves are the culmination of eight decades of previous work. And so there's remixing. And then in the arts it's the exact same thing—novels and music and everything. There are clearly creative leaps, but there's just tremendous amounts of influence that came in from people who came before. Even if you think about somebody with the creativity of a Beethoven—there's a lot of Beethoven in Mozart and Haydn and in the composers that came before. So there's just tremendous amounts of remixing and combination. 

And so it's a little bit of an angel dancing on the head of a pin question, which is: if you can get within 0.01% of world-beating generational creativity and intelligence, you're probably all the way there. So emotionally I want to hold out hope that there is still something special about human creativity, and I certainly believe that and I very much want to believe that. But I don't know—when I use these things I'm like, wow, they seem to be awfully smart and awfully creative. So I'm pretty convinced that they're going to clear the bar.

**Erik:** Yeah. That seems to be a common theme in your analysis when people talk about the limitations of LLMs. Can they do transfer learning or just learning in general? You seem to ask: can people do this?

**Marc:** Yes. Can people do these things? It's like lateral thinking, right? So it's like reasoning in or out of distribution. Okay, I know a lot of people who are very good at reasoning inside distribution. How many people do I actually know who are good at reasoning outside of distribution and doing transfer learning? And the answer is I know a handful. I know a few people where I'm like, okay, that person is legitimately capable of lateral leaps of thinking. And so then it's like, okay, if the AI can do that, then clearly it can do what people can do, because most people can't. 

The other thing I would say is just on the creativity question specifically—creativity is inherently social, right? And so it's like, you define creativity in the act of creating something, you put it out into the world, and then it gets evaluated. There's this crowdsourcing aspect to it and then there's a durability question. Is it still considered creative in a hundred years? And so I think the thing on AI and creativity is: we're going to find out. The models are going to be able to create all kinds of things. And then the question is going to be—is it good? Do people like it? Does it stand the test of time? That will be the ultimate answer.

And then I think—look, there's many possible futures on that. One possible future is we just get completely flooded and there's so much AI content that we don't know what to do with any of it and it's just all overwhelming and terrible. There's that future. There's another future where it turns out to be absolutely fantastic and we love it and it's completely integrated into culture. And then there's a whole spectrum in between. My suspicion is it's going to be that middle spectrum where some of it is really great and a lot of it is not. Which is exactly like human creativity—you know, 99.999% of all music ever created is not worth listening to. But the 0.001% that is worth listening to is really special and important.

**Ben:** I'll throw in a hip-hop perspective on this—

**Marc:** Of course you will.

**Ben:** —which is that hip-hop is entirely built on remixing. The entire art form is built on sampling, which is taking somebody else's creativity and then putting it into a new context. And the question that always comes up in hip-hop is: is that creative? Is that original? And the answer from the hip-hop community is absolutely yes, it's extremely creative and original, even though it's literally taking pieces of other people's work.

The example I always use is *Paul's Boutique* by the Beastie Boys, which is considered one of the most creative albums ever made in hip-hop. And it's 100% samples—I think there are 300 samples on that album. Every single sound comes from somewhere else. And yet the album as a whole is completely original. Nobody had ever heard anything like it before or since. It's a masterpiece of creativity.

So I think the question of whether AI can be creative by remixing is—yeah, of course it can. That's what creativity is. The Beatles were remixing Chuck Berry and Little Richard. Led Zeppelin was remixing blues music. Everybody's remixing. The question is: are you remixing in an interesting way? Are you creating something that feels new? And I think AI absolutely can do that.

**Marc:** And by the way, to your point about hip-hop—one of the things that's really interesting about hip-hop is it developed as an art form in a context where the musicians didn't have access to traditional instruments or traditional musical training. They couldn't afford guitar lessons. They couldn't afford a piano. What they could afford was turntables and records. And so they created an entirely new art form based on the technology they had access to. 

And I think that's actually a really good analogy for what's happening with AI. We're creating a new form of creativity based on the technology we have access to. And it's going to be different from traditional creativity in the same way hip-hop was different from traditional music. But that doesn't make it less valid or less creative.

**Erik:** So let's shift gears a bit. There's been this ongoing debate about intelligence versus power—the idea that just because you're highly intelligent doesn't necessarily mean you're going to be the person who leads or has the most impact. How do you guys think about this in the context of AI?

**Marc:** Yeah, so this is a really important question. There's this assumption that intelligence equals power or intelligence equals leadership or intelligence equals impact. And I think that's just fundamentally wrong. 

If you look at human history, the correlation between pure cognitive intelligence—let's say IQ—and actual leadership or power or impact is not that strong. There are plenty of extremely intelligent people who have zero impact on the world. And there are plenty of people who are maybe above average intelligence but not genius level who have enormous impact.

The reason is that leadership and power and impact require a lot more than just intelligence. They require emotional intelligence. They require theory of mind—the ability to understand what other people are thinking and feeling. They require charisma. They require the ability to motivate people. They require the ability to make decisions under uncertainty. They require courage. They require persistence. They require all of these other qualities that are not captured by traditional measures of intelligence.

And so I think when people worry about AI becoming super intelligent and therefore taking over the world or having all the power—I think that's based on a flawed assumption. Just because an AI is really smart doesn't mean it's going to be able to lead people or motivate people or understand people or build coalitions or do any of the things that are actually required to have real power in the world.

**Ben:** Yeah, I think that's exactly right. And I'll add to that: one of the things we see in companies all the time is that the most intelligent person in the room is often not the CEO. The CEO is often not even in the top quartile of intelligence in the company. But the CEO is the person who can make decisions, who can motivate people, who can set a vision, who can execute.

And the reason for that is—intelligence is table stakes. You need to be smart enough to understand what's going on. But beyond a certain threshold, more intelligence doesn't necessarily help you. In fact, it can sometimes hurt you because you can overthink things. You can see too many angles. You can get paralyzed by analysis.

What you really need is judgment, which is different from intelligence. Judgment is the ability to make good decisions with incomplete information. And that's something that comes from experience, from understanding people, from having seen patterns before. It's not something that comes purely from being able to process information quickly.

**Marc:** And this is why I'm actually not that worried about the sort of AI takeover scenario where super intelligent AI decides it wants to rule the world. Because ruling the world requires a lot more than just being smart. It requires understanding human psychology at a deep level. It requires being able to motivate millions of people. It requires being able to build institutions. It requires being able to navigate incredibly complex social and political dynamics.

And those are all things that—sure, AI will get better at them over time. But they're not things that just automatically come with intelligence. They're separate skills that need to be developed separately.

**Erik:** So on that theory of mind question—how advanced do you think current AI systems actually are at understanding what humans are thinking and feeling?

**Marc:** So this is really interesting. The current generation of LLMs actually has pretty good theory of mind in a narrow sense. They can do quite well on theory of mind tests. They can understand—if you give them a scenario with multiple people who have different knowledge states, they can reason about what each person knows and doesn't know pretty accurately.

Where they struggle is with the emotional component of theory of mind. Understanding not just what someone knows, but what they feel, what they care about, what motivates them at a deeper level. That's harder for them.

But I think that's going to improve rapidly. Because what these models are training on is enormous amounts of human conversation, human writing, human interaction. They're seeing patterns in how humans behave emotionally over and over and over again across billions of examples. And so they're going to get better at understanding emotional patterns.

The question is whether that understanding is genuine or whether it's just pattern matching. And I think that's going to be a philosophical debate that we're going to have for a long time. But from a practical standpoint, if the AI can accurately predict how a human is going to feel in a given situation and respond appropriately, does it matter whether the AI actually "feels" anything itself? I'm not sure it does.

**Ben:** I think the other thing to add here is that theory of mind is not one thing. There are different levels of theory of mind. There's basic theory of mind, which is: I understand that you can see things I can't see, or know things I don't know. That's fairly straightforward.

But then there's much more sophisticated theory of mind, which is: I understand your motivations, your fears, your insecurities, your aspirations, how you're going to react not just to what I say but to how I say it, what you're going to infer from my tone of voice or my body language. That's much more complex.

And I think current AI systems are pretty good at the basic level. They're getting better at the intermediate level. The really sophisticated level—that's still very much a work in progress. But it's improving fast.

**Marc:** And by the way, most humans aren't that good at the sophisticated level either. That's why there are so many misunderstandings and conflicts. Most people have pretty limited theory of mind when it comes to people who are different from them or who come from different backgrounds or who have different values.

**Erik:** Let's talk about embodied intelligence for a moment. There's this question of whether intelligence that exists purely in language or purely in digital space is really the same as intelligence that exists in a physical body that has to interact with the physical world. What do you guys think about that?

**Marc:** So this is the mind-body question, right? And it's been debated by philosophers for thousands of years. Does the mind need a body? Does consciousness require embodiment? Does intelligence require physical experience?

My view is: probably not in principle, but probably yes in practice. Which is to say, I don't think there's anything magical about having a physical body that creates consciousness or intelligence. I think intelligence is fundamentally about information processing, and information processing can happen in silicon just as well as it can happen in neurons.

But in practice, a huge amount of human intelligence is grounded in physical experience. We learn about the world by interacting with it physically. We develop intuitions about causality and physics and object permanence and all of these things through physical manipulation of objects as children. And that gives us a deep understanding of how the world works that purely abstract reasoning doesn't give you.

So I think for AI to really match human intelligence across all domains, it probably does need some form of embodiment. It needs to interact with the physical world in some way. That doesn't necessarily mean it needs a humanoid robot body. But it probably needs some way of getting feedback from physical reality.

**Ben:** Although I'd push back a little bit on that, because humans do an enormous amount of abstract reasoning that's not grounded in physical experience. We reason about abstract concepts—justice, love, mathematics, philosophy. None of those things are physical. You can't touch them. You can't manipulate them physically. And yet we're capable of reasoning about them quite deeply.

So I think the question is not whether intelligence requires embodiment, but whether certain types of intelligence require embodiment. Common sense reasoning about the physical world probably requires some form of physical grounding. But abstract reasoning might not.

**Marc:** That's fair. Although I would say even our abstract reasoning is often grounded in physical metaphors. We talk about "grasping" concepts, ideas "flowing," arguments having "weight." A lot of our abstract reasoning uses physical intuitions as a foundation.

But your point is well taken. I think the jury's still out on how much embodiment is actually necessary for general intelligence.

**Erik:** So let's shift to the question everyone seems to be asking right now: are we in an AI bubble? You've both been in Silicon Valley through multiple boom and bust cycles. How do you think about the current level of investment and hype around AI?

**Marc:** So I have a very strong view on this, which is: no, we are not in a bubble. And the reason I say that very confidently is because bubbles are characterized by a disconnect between fundamentals and valuation. In a bubble, companies get valued at absurd multiples despite having no revenue, no product, no customers, no path to profitability.

What we're seeing in AI right now is the exact opposite. We're seeing massive capital expenditures by some of the largest, most sophisticated companies in the world—Microsoft, Google, Meta, Amazon. These companies are spending tens of billions of dollars building out AI infrastructure. And they're not doing it because they're caught up in hype. They're doing it because they're seeing real results. They're seeing real revenue. They're seeing real improvements in their products.

Google is completely rebuilding search around AI. Microsoft is completely rebuilding Office around AI. Meta is integrating AI into all of its products. These are not speculative bets. These are companies responding to a fundamental platform shift that they see happening in real time.

And then you look at the application layer—companies are already building real businesses with real revenue using these AI models. We're seeing companies go from zero to tens of millions in revenue in 12 to 18 months using AI. That's not a bubble. That's a real technological revolution creating real economic value.

Now, are there going to be some companies that are overvalued? Sure. Are there going to be some companies that fail? Absolutely. That's true of any wave of innovation. But the fundamental technology is real, the business models are real, the value creation is real. This is not a bubble.

**Ben:** And I'll add to that: one way to think about whether something is a bubble is to ask: are people making irrational decisions? And what I would say is that the decisions being made right now are actually extremely rational given the potential upside.

If you're Google and you think there's even a 20% chance that AI fundamentally changes how search works, you have to invest heavily in it. Because if you don't and your competitors do, you could lose your entire business. That's not irrational exuberance. That's rational risk management.

Same thing for Microsoft with Office, or Meta with its social products. The potential downside of not investing heavily in AI is existential. The potential upside of investing heavily and getting it right is enormous. So even if you're not certain it's going to work, it's still rational to invest aggressively.

That's very different from the dot-com bubble where people were investing in companies that had no business model, no revenue, no path to profitability, just because stock prices were going up. That was irrational. This is not.

**Marc:** And then the other thing I'd say is: look at the capital expenditures. These companies are spending $50 billion, $100 billion building data centers, buying chips, building infrastructure. That is not financial speculation. That is real investment in productive capacity.

In a financial bubble, the money goes into bidding up asset prices. In a real technological revolution, the money goes into building real infrastructure that creates real productive capacity. And that's what we're seeing here.

Now, could there be a shakeout where some of these investments don't pay off as quickly as people hope? Sure. Could there be a period where the market gets disappointed and valuations come down? Possibly. But that's very different from a bubble popping. That's just the normal cycle of innovation where initial expectations get recalibrated based on reality.

**Erik:** Let's talk about platform shifts. We've seen several major platform shifts in the history of computing—mainframes to PCs, PCs to web, web to mobile. How do you see AI as a platform shift? And what does that mean for existing companies, particularly someone like Google?

**Marc:** So I think AI is absolutely a platform shift on the scale of any of those previous shifts. And actually it might be the biggest platform shift we've ever seen because it's not just changing how we interact with computers, it's changing what computers can do.

Previous platform shifts were mostly about interface and distribution. Mainframes to PCs was about making computing accessible to individuals. PCs to web was about connecting computers together. Web to mobile was about making computing portable and always-on. Those were all huge shifts, but they didn't fundamentally change what computers could do. They changed who could use them and how, but the underlying capabilities were similar.

AI is different. AI is fundamentally expanding the set of things computers can do. Things that required human intelligence—writing, reasoning, creativity, understanding language, understanding images—computers can now do. That's a much more fundamental shift.

And that means the implications are much more far-reaching. Every application, every product, every business process that involves any kind of cognitive work is potentially up for grabs. Which is why you see every major tech company scrambling to integrate AI into everything they do.

For Google specifically—yeah, this is a massive challenge. Search has been Google's core product for 25 years. It's generated hundreds of billions of dollars in revenue. And the entire paradigm of search is: you type in a query, you get back ten blue links, you click on a link, you see an ad. That entire paradigm is now being questioned.

With AI, you can have a different paradigm where you ask a question and you get a direct answer. You don't need the ten blue links. You don't need to click through to websites. The AI just gives you the answer. And that's existentially threatening to Google's business model because the ads are on the links. If people don't click the links, the ads don't get seen.

Now, Google is obviously aware of this and they're working hard to adapt. But it's a huge challenge when you have a business that generates $200 billion a year in revenue and you have to fundamentally rethink how it works. That's not easy.

**Ben:** And I think the thing that's interesting about this platform shift is that we don't yet know the shape and form of the ultimate products. Just one obvious historical analogy is the personal computer from invention in 1975 through to basically 1992 was a text prompt system—17 years in, the whole industry took a left turn into GUIs and never looked back. And then five years after that the industry took a left turn into web browsers and never looked back.

And I'm sure there will be chatbots 20 years from now, but I'm pretty confident that both the current chatbot companies and many new companies are going to figure out many kinds of user experiences that are radically different that we don't even know about yet.

So I think we're very early in understanding what the right product form factors are for AI. Chatbot is one. But there's going to be many others. And I think the companies that figure out those new form factors are going to be the big winners.

**Marc:** And the other thing I'd say about platform shifts is they create this interesting dynamic where the incumbents have huge advantages but also huge disadvantages. The advantages are: they have users, they have distribution, they have resources, they have talent. The disadvantages are: they have legacy systems, they have legacy business models, they have organizational inertia, they have a huge installed base that they're afraid to disrupt.

And so you get this race where the incumbents are trying to adapt and new entrants are trying to build new things from scratch. And which one wins depends on a lot of factors—how fast the technology is moving, how strong the network effects are, how much the business model needs to change.

In the case of AI, my sense is that both are going to win. Some incumbents are going to successfully adapt and some new companies are going to become huge. But there's going to be a lot of change and a lot of value created and destroyed in the process.

**Erik:** Let's talk about coaching founders in this era. You guys work with a lot of AI startups. How is coaching founders in AI different from coaching founders in previous technology waves?

**Marc:** So it's actually really interesting because in some ways it's very similar and in other ways it's completely different. 

The similar part is: all the fundamentals of building a great company still apply. You still need to have a great team. You still need to have a clear vision. You still need to understand your customers. You still need to build a great product. You still need to figure out distribution. You still need to manage your cash. All of that stuff is the same.

What's different is the pace of change in the underlying technology. In most previous waves of innovation, the underlying technology was relatively stable by the time startups were building on top of it. The web standards were pretty stable by the time most web companies were getting built. Mobile platforms were pretty stable by the time most mobile companies were getting built.

But with AI, the underlying technology is changing month by month. Every few months there's a new model that's 10x better than the previous one. Every few months there's a new capability that didn't exist before. And so you're trying to build a company on top of a foundation that's shifting under your feet.

That creates both huge opportunities and huge challenges. The opportunity is that if you're fast, you can do things today that were impossible six months ago. The challenge is that what you built six months ago might be completely obsolete today.

And so the key thing we tell founders is: be extremely customer-focused and problem-focused, not technology-focused. Don't build a company around a specific model or a specific capability. Build a company around solving a real customer problem. And then use whatever technology is available to solve that problem, recognizing that the technology is going to keep getting better.

If you do that, then as the technology improves, your product just gets better and better. But if you build around a specific technical approach, you might find that approach is obsolete in six months.

**Ben:** Yeah, and the other thing I'd add is: in this wave, distribution is even more important than usual. Because the technology is so accessible—anybody can access these models, anybody can build on top of them. The barrier to entry is extremely low.

And so the companies that win are going to be the companies that figure out distribution. How do you get customers? How do you build a brand? How do you create network effects? How do you create lock-in? Those are the things that matter.

We see a lot of founders who are very focused on their model performance or their prompt engineering or their fine-tuning approach. And those things matter, but they're not sufficient. You need to be just as focused, if not more focused, on: how are you going to get customers and keep them?

**Marc:** And the other thing we tell founders is: be prepared for the value chain to shift. Right now, a lot of the value is captured at the model layer—OpenAI, Anthropic, Google, these companies that are building the foundation models. But over time, value is going to shift up the stack into the application layer.

This is what always happens in technology. Initially, the value is captured by whoever controls the underlying technology. But over time, as that technology becomes commoditized, value shifts up into the applications and the user experiences that sit on top of it.

And so if you're building an AI company, you need to be thinking about: what is the sustainable moat? What is the thing that's going to protect you as the underlying technology becomes cheaper and more accessible? Usually the answer is: some combination of data, distribution, and network effects.

**Erik:** Let's talk about the talent market and the chip market, because those are two big bottlenecks right now. What's your sense of where those are headed?

**Marc:** So on talent—right now there's obviously a huge shortage of people who know how to train these models and work with them at a high level. And so there's this bidding war for talent. People are getting paid extraordinary amounts of money. Companies are getting into talent wars with each other.

I think that's going to ease over time for a couple of reasons. One is just: there are more and more people learning these skills. You're seeing excellent models coming out of China now from Deepseek and Qwen and Kimi. And it is striking how the teams that are making those are not the name brand people with their names on all the papers. China is successfully decoding how to take young people and train them up in the field.

**Ben:** And XAI to a large extent too.

**Marc:** Yeah, exactly. And so I think that there's going to be—and look, it makes sense. For a while it's going to be this super esoteric skill set and people are going to pay through the nose for it. But there's no question the information is being transferred into the environment. People are learning how to do this. College kids are figuring it out. So I don't think there's ever going to be a talent glut per se, but there's going to be a lot more people in the future who know how to build these things.

And then also, AI building AI, right? So the tools themselves are going to get better at contributing to that. And I think that's good because the current level of shortage of engineers and researchers is too constraining.

And then on the chip side—I don't want to call it specifically, but it's never been the case in the chip industry that there's ever—every shortage in the chip industry has always resulted in a glut because the profit pool of a shortage, the margins get too big, the incentive for other people to come in and figure out how to commoditize the function gets too big. Nvidia has the best position probably anybody's ever had in chips. But notwithstanding that, I find it hard to believe that there's going to be this level of pressure on infrastructure in five years.

**Ben:** Yeah. And even if the bottleneck within the infrastructure moves—so if it becomes power, if it becomes cooling or anything else—then you'll have a chip glut for sure.

**Marc:** So I would just say this: it's likely the challenges that we all have in five years from now are going to be different challenges. Like don't definitely—this industry of all industries—don't look at us as static. The positions could change very, very fast.

**Erik:** Let's actually close on more of this macro note. Marc, you mentioned China. Last month we were in DC, and one of the big questions the senators have is: how should we make sense of the state of the AI race vis-à-vis China? Do you want to share just the high-level summary of what you shared with them?

**Marc:** Yeah. So my sense of things—and I think if you just observe currently, specifically Deepseek and Qwen and these models coming out of China—my sense basically is: the US specifically, and the West generally, but more and more specifically the US, is where the conceptual innovations have been coming out. The big conceptual breakthroughs are coming out of the US, coming out of the West. 

China is extremely good at picking up ideas and implementing them and scaling them and commoditizing them. They do that obviously throughout the manufacturing world, and they're doing it now very successfully in AI. So I would say they're running the catch-up game really well.

And then there's always this question of how much of that is being done authentically through hard work and smart people, and then how much is being done with maybe a little bit of help—maybe a little USB stick in the middle of the night kind of help. Okay, so there's always a little bit of a question. But like either way, they're doing a great job. 

Obviously they aspire to more than that. And there are many very smart and creative people in China. And so it will be interesting now to see the level to which the conceptual breakthroughs start to come from there and whether they pull ahead.

But what we tell people in Washington is: look, this is now a full-on race. It's a foot race. It's a game of inches. We're not going to have a five-year lead. We're going to have maybe a six-month lead. We have to run fast. We have to win. We can't put constraints on our companies that the Chinese government isn't putting on their own companies. And so we'll just lose. And do you really want to wake up in the morning and live in a world really controlled and run by Chinese AI? Most of us would say no, we don't want to live in that world.

So there's that. And I would say I feel moderately good about that just because I think we're really good at software. The minute this goes into embodied AI in the form of robotics, I think things get a lot scarier.

And this is the thing I'm now spending time in DC trying to really educate people on, which is: the US and the West have chosen to deindustrialize to the extent that we have over the last 40 years. China specifically now has this giant industrial ecosystem for building mechanical, electrical, and semiconductor and now software devices of all kinds—including phones and drones and cars and robots.

And so there's going to be a phase two to the AI revolution. It's going to be robotics. It's going to happen pretty quickly here I think. And when it does, even if the US stays ahead in software, the robots have got to get built. And that's not an easy thing. And it's not just a company that does that. It's got to be an entire ecosystem. It's going to be—the car industry was not three car companies. It was thousands and thousands of component suppliers building all the parts. And it's been the same thing for airplanes and the same thing for computers and everything else. It's going to be the same thing for robotics.

And by default, sitting here today, that's all going to happen in China. So even if they never quite catch us in software, they might just lap us in hardware and that'll be that.

The good news is I think there's a growing awareness—there's a growing awareness I would say across the political spectrum in the US—that deindustrialization went too far. And there's a growing desire to kind of figure out how to reverse that. And I say I'm guardedly optimistic that we'll be making progress on that, but I think there's a lot of work to be done on that call to arms.

**Erik:** Let's wrap. Thank you Marc and Ben.

**Marc and Ben:** Thank you, everybody.