---
id: SOURCE-20251029-youtube-interview-openai-sam_jakub_wojciech
title: "OpenAI Structure Change and AI Timelines"
creator: OpenAI
guest: Sam Altman, Jakub Pachocki
date_published: 2025-10-28
date_processed: 2026-01-05
signal_tier: paradigm
status: processed
chain_relevance: Intelligence
integration_targets: [CANON-00004-EVOLUTION, CANON-30000-INTELLIGENCE]
---

# OpenAI Structure Change and AI Timelines

## Executive Summary
OpenAI's leadership presents their internal timeline projections and structural reorganization. Jakub Pachocki (Chief Scientist) states they believe superintelligence—systems smarter than humans on most critical axes—may be less than a decade away. Current models operate at ~5-hour task horizons; they project AI research interns by September 2026, autonomous AI researchers by September 2027, Nobel-level breakthroughs in 2-3 years. The company restructures as a Public Benefit Corporation controlled by a nonprofit OpenAI Foundation.

## Key Insights

### Superintelligence Timeline
OpenAI explicitly states they believe deep learning systems are less than a decade away from superintelligence—defined as systems smarter than all humans on most critical axes. This represents the clearest public timeline commitment from a major lab.

### Task Horizon Metric
Progress measured by task horizon—how long a task would take humans to complete that models can now match. Current generation: ~5 hours (matching top competitive programmers at IOI). This horizon extends rapidly with algorithmic innovation and test-time compute scaling.

### Test-Time Compute Scaling
"Orders and orders of magnitude to go" on in-context compute (test-time reasoning). Current models spend limited thinking time. For problems that matter (scientific breakthroughs), using entire data centers for inference is justified. This is a new scaling axis beyond training compute.

### AI Research Intern Timeline
- September 2026: AI research interns that meaningfully accelerate researchers
- September 2027: Systems that autonomously deliver on core research tasks
- 2027: Graduate-level autonomous science research
- 2-3 years: Nobel-level scientific breakthroughs

### Infrastructure Scale
Comfortable spending $1-1.5 trillion on infrastructure over coming years. Stargate partnership with SoftBank for massive data center buildout. Goal: make AI free or nearly free for everyone through infrastructure scale.

### Structural Reorganization
OpenAI Foundation (nonprofit) controls OpenAI Group PBC (for-profit). Foundation holds ~$130B equity in PBC. $25B committed to health (disease eradication) and AI resilience (broad benefit distribution). Independent expert panel will verify AGI claims.

### Alignment at Scale
Human-level systems can be verified by humans. Superhuman systems require new oversight techniques—AI systems overseeing other AI systems. Alignment is both a technical and governance problem: "aligned with what, for whom?"

## Quotable Passages
> "We believe that it is possible that deep learning systems are less than a decade away from superintelligence—systems that are smarter than all of us on a large number of critical axes." — Jakub Pachocki

> "Where the current generation of models is at right now is about five hours. You can see this by looking at the models matching the best people in competitions such as the International Olympiad in Informatics." — Jakub Pachocki

> "If you think about how much compute—how much time—you would like to spend on problems that really matter, such as scientific breakthroughs, you should be okay using entire data centers. There is really quite a way to go there." — Jakub Pachocki

## Integration Notes
- Connects to CANON-00004-EVOLUTION: Explicit timeline commitments from major lab; test-time compute as new scaling axis
- Connects to CANON-30000-INTELLIGENCE: Task horizon metric; autonomous research timeline
- Novel contribution: Public superintelligence timeline; task horizon measurement; trillion-dollar infrastructure projection

## Metadata
- Duration: ~1 hour (Q&A format)
- Quality: Official corporate communication with extended Q&A
- Processing notes: Paradigm-tier for explicit timeline commitments and infrastructure scale projections
