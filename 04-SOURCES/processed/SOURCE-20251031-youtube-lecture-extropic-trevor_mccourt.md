# SOURCE: Trevor McCourt - Thermodynamic Computing and AI Energy Scaling

---
title: "Trevor McCourt on Probabilistic Circuits and the AI Energy Crisis"
source_type: youtube_video
format: lecture
creator: Extropic
date: 2025-10-31
url: null
duration: ~45 minutes
signal_tier: paradigm
chain_relevance: Intelligence|Expertise
processing_status: processed
processed_date: 2026-01-05
integration_targets:
  - CANON-30300-TECH_STACK
  - CANON-30000-INTELLIGENCE
  - CANON-00004-EVOLUTION
---

## Key Concepts

### The Energy Scaling Problem
- AI has discovered how to convert energy into intelligence, but scaling is physically impossible
- Current AI consumes ~0.5% of US grid (3 gigawatts)
- Basic personal AI assistant: 20% of grid (100 gigawatts)
- Video-enabled assistant: 10x grid expansion needed
- Expert-level AI for all: 100x grid expansion (impossible)

### The Math That Forces Innovation
Formula: `FLOPs = 2 × parameters × tokens`
- H100 efficiency: ~0.7 picojoules per FLOP
- Sam Altman's "1 gigawatt data center per week" = just the text assistant scenario
- The 10-100 terawatt scenarios needed for useful AI would require covering Nevada with solar panels

### Thermodynamic Sampling Units (TSUs)
- Build hardware that samples from probability distributions natively
- Transistors at low voltage are naturally probabilistic (thermal fluctuations dominate)
- Uses Gibbs sampling to break complex distributions into modular operations
- 10,000x more energy efficient than GPUs for generative AI benchmarks

### Probabilistic Circuits Physics
- Low-voltage transistor operation uses thermal noise as compute primitive
- Energy barrier controlled by gate voltage creates controllable random sampling
- Can build circuits sampling from categorical distributions, Gaussian mixtures
- Temperature dependence is predictable and manageable

## Paradigm-Level Insights

1. **Energy becomes THE constraint** - For first time, average person consumes significant HPC resources; efficiency now matters more than speed

2. **Local minimum escape** - Current GPU architecture is optimal for current algorithms; new paradigm requires both new hardware AND new algorithms

3. **Physics dictates economics** - The trillion-dollar infrastructure needed for universal AI is physically impossible without breakthrough efficiency

4. **Generative AI is fundamentally sampling** - Build hardware that does what AI actually does, not what deterministic computing does

5. **Hybrid architectures are transition path** - Combine probabilistic cores with conventional neural networks for practical scaling

## Integration Notes

- Energy scaling analysis essential for TECH_STACK infrastructure planning
- Thermodynamic computing represents alternative intelligence substrate for INTELLIGENCE chain
- Local minimum / paradigm shift pattern relevant to EVOLUTION framework
- 10,000x efficiency claim demands verification and tracking
