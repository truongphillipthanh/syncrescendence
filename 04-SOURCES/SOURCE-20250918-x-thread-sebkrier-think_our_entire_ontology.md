---
id: SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology
platform: x
format: thread
creator: sebkrier
title: think our entire ontology
status: triaged
original_filename: "20250918-x_thread-think_our_entire_ontology-@sebkrier.md"
url: https://x.com/sebkrier/status/1968753358216302894
author: "Sèb Krier (@sebkrier)"
captured_date: 2026-02-04
signal_tier: strategic
topics:
  - "ai-engineering"
  - "philosophy"
  - "economics"
  - "opinion"
teleology: contextualize
notebooklm_category: ai-engineering
aliases:
  - "Krier - AGI Ontology Confusion"
synopsis: "Seb Krier argues our entire ontology for conceptualizing AI is confused. AGI will be a distributed ecosystem of models from different actors—saying we need to prepare for AGI is like saying we need to prepare for The Economy. Alignment is governance, not a one-off technical fix."
key_insights:
  - "Talking about AGI as a monolithic entity is a category error—it will be a distributed ecosystem of different models with different capabilities and incentive structures"
  - "Solving alignment in the broad sense is equivalent to solving truth or solving conflict—it is perpetual governance, not a one-off technical solution"
  - "The discourse risks cementing bad memes (like nuclear energy stigma) that take decades to undo"
---
# AGI Ontology and Conceptualization Thread

## Post 1: Main Thread (Sep 18, 2025)

I think our entire ontology for how we talk about and conceptualise AI is confused. And I wouldn't be surprised if in ten years we will look back at the discourse today and laugh at how primitive some ideas are. A few hot and uncertain takes:

The way people talk about future AIs/AGIs feels like a category error. Sometimes they reify future systems as self-sovereign entities with their own goals and incentives, a different species that we need to learn to co-exist with. I think that's not impossible, and I used to be a lot more sympathetic to this view, but I'm a lot less certain now and it's certainly not self-evident. Agents can still be tools, and tool agents that operate along timelines don't need to necessarily be 'separate species'-like.

Other times they abstract away so much that claims about AGI or ASI are incredibly hard to really parse and falsify. A bit like saying "Finance will do x/y/z". Sure it's helpful in some contexts, but it does away with all of the complexity and moving parts. Ultimately this rarely feels like the right level to start designing actionable or useful prescriptions, unless you want to be forever stuck at "we need to balance the risks and the opportunities of [complex systems]" or "society needs to prepare."

To me at least, AGI will likely be a distributed ecosystem of different models, built by different companies and state actors, with different capabilities, architectures, and incentive structures. Saying "we need to prepare for AGI" is like saying "we need to prepare for The Economy." Like, sure we do - but if you want to productively contribute to preparing then you'll need to be a lot more precise and focus on many things that aren't 'AI'.

Similarly, I sometimes feel like the way some people talk about 'solving alignment' (in the broad sense) feels equivalent to talking about 'solving truth' or 'solving conflict'. Ultimately this is all about governance, politics, and power; AI just forces these dynamics to the forefront. Anyone expecting a clear one-off *ex ante* 'fix' today is likely wrong about the type of problem we're facing. It will be a messy, perpetual process of negotiation, regulation, and adaptation, much like law, democracy, or international relations.

Lastly, there's a severe dearth of imagination across the board. Somehow we imagine AI solving cancer and revolutionizing R&D, but we're still stuck with today-level solutions for governing it? If you had asked anyone at the dawn of the Industrial Revolution "what do you think the world is like in 50 years and what do you think we should do?" I would bet that they'd be off by a lot.

Ultimately I think it's good that we're having these conversations, and at least the discourse has served a useful purpose in forcing the conversation into the mainstream. I'm just wary of cementing bad memes that are hard to undo, just like we've been plagued with unhelpful memes about nuclear energy for decades now and only *starting* to gradually undo them.

(Description: Isometric pixel art illustration of a dense urban cityscape integrated with natural landscape elements, featuring tall buildings, forests with autumn foliage in reds and browns, water features, and white clouds above)

---

## Post 2: Reply to FleetingBits (Sep 18, 2025)

Pretty much yes. My view on (1) is that it's easier than people assumed in the past, and (2) is actually harder b/c politics. But I'm very uncertain about (1) and happy for people to argue against it!

---

## Post 3: Reply to David Manheim (Sep 19, 2025)

I agree! But nothing is 'solved' – we just make continuous progress through good incentives, institutions, norms, laws etc. I think something like this applies to alignment too:

(Description: Black and white diagram with stylized curved line patterns and small icons in the center. Text at bottom reads "Why do people believe true things?" - sourced from conspicuouscognition.com)

---

## Post 4: Reply to Paul Triolo (Jan 23)

I think something AGI like two years off is plausible, though that's not my prediction, at least for a strong version of AGI. But I think the way people think about what AGI 'is' and what it actually implies is often poor, and the associated policy prescriptions even worse…

---

## Post 5: Reply to goog (Sep 18, 2025)

"I have not seen a single researcher from a single lab talk like this." Then you need to engage with more researchers, plenty of people who do in fact talk/think like this.

And I'm not dismissing the technical questions here, which is why I specified the 'broad' and normative

---

## Post 6: Reply to Oleg Eterevsky (Sep 18, 2025)

I think so too! And increasingly it seems like it's not just developing models that matters, but deploying them – and that's a whole other ball game too.

---

## Post 7: Reply to Bhishmaraj S (Sep 19, 2025)

Much would change with the latter and I would index less on the developments of the last few years. But equally depends a lot on the architecture's characteristics, how goals are defined and set etc.

---