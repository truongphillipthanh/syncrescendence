---
url: https://x.com/vasuman/status/2021362047980892628
author: vas (@vasuman)
captured_date: 2026-02-13
---

# AI for Enterprise Finance

(Description: Black and white sketch-style illustration depicting a busy corporate office scene. Multiple professionals in business attire are working at desks with computers, stacks of papers, and documents scattered throughout. The scene conveys controlled chaos and intensive collaborative work, with figures leaning over desks, gesturing while communicating, and managing extensive paperwork. A bright light source emanates from the upper right, symbolizing insight or clarity emerging from complexity. The art style suggests both the mechanical nature of routine work and human effort involved in enterprise operations.)

## Overview

As most CFOs know, finance feels more bloated than ever. ERPs, close management platforms, AP automations, expense tools, treasury systems, and more. The software stack is deep, yet most CFOs I talk to still describe closing the books as a nightmare, with long hours, spreadsheet reconciliations, chasing people for approvals, and fixing the same data mismatches over and over.

Your software (i.e. Concur/SAP/Bill.com/etc) handles big ticket items. It automated the core ledger, moved invoices into a digital workflow, gave everyone a dashboard. But between those systems, there are still hundreds of small tasks and workflows that require a human to copy a value from one screen, paste it into another, check whether two numbers match, send a follow-up email when they don't, escalate when nobody responds, etc. Humans are still the glue holding these systems together, and this makes scaling a nightmare.

That's the gap AI should fill. Don't replace your ERP or build a hypothetical futuristic finance brain. Just pick up the manual, repetitive connective tissue that your team spends 60% of their time on. Let AI sit on top and in between the software platforms your team already use and heavily rely on. And to be clear, that is where ALL of the ROI is. That's how you bring month end close from 12 days to 5 days (we did that).

For context, I'm an ex-Meta software engineer. I started Varick Agents, where we embed with enterprise teams and deploy AI agents that operate inside their existing tools. We work across verticals, but finance departments are where we've seen some of the most immediate, measurable impact. The problems are well-defined, the processes are repetitive, and the cost of manual work is easy to quantify. I've spent the last two years watching what actually works in production (and what doesn't), and that's what this article is about. If you're a CFO looking to bring AI efficiency to your department, this article is for you.

## What Should Bother You

Before I get into the specifics, it's worth grounding this in data: the gap between where finance teams are and where the tools could take them is wider than most people realize.

**50% of finance teams still take over a week to close their books each month.** That's not a technology problem at this point. The software to close faster exists. It's a workflow problem, a human bottleneck problem. The close drags because upstream exceptions didn't get resolved, because reconciliations require manual investigation, because someone's waiting on an approval that's sitting in an inbox.

**94% of finance teams still rely on Excel for close processes.** Even teams running BlackLine or FloQast end up pulling data into spreadsheets for ad hoc analysis, tie-outs, or because the reconciliation template in the system doesn't quite match how they actually do the work. Excel isn't the problem, the problem is that your actual process doesn't live in your system.

**On the AP side, only 32.6% of invoices are processed without any human touch.** That means roughly two-thirds of every invoice that comes through the door requires someone to look at it, fix something, or route it somewhere. 14% of invoices require manual exception handling, which is the expensive kind of human involvement. And the cost adds up: the average company spends $9.40 (in human hours) to process a single invoice manually. For a company processing 10,000 invoices a month, that's nearly $100K a month in processing cost alone, and most of that cost is people doing repetitive, low-judgment work. That's insane, respectfully.

**The stat that matters most, though, is this one: 87% of AI pilots never reach production.** That's from Gartner, and it tracks with what I've seen. Companies run a proof of concept, it works in a demo, everyone gets excited, and then it dies. It's not usually because the technology failed, but because the implementation approach was wrong from the start. I'll come back to that.

## Where AI for Finance Really Works

I'm going to walk through five areas where we've deployed AI agents in finance departments and seen real, sustained results. For each one, I'll describe what happens today in most teams and what it looks like when you put an AI agent on it.

### 1. Exception Resolution

This is the single highest-ROI use case in finance, and it's the one most people overlook because it doesn't sound glamorous.

**What happens today:** An invoice comes in with no PO number. Someone on the AP team has to figure out who ordered it, find the PO, match it, and push the invoice through. A payment comes in that doesn't match any open invoice exactly, maybe it's short by $47, maybe the vendor combined two invoices into one payment. Someone has to investigate. A price variance exceeds the threshold on a three-way match. An approval has been sitting with a department head for six days. These exceptions pile up constantly, and each one requires a human to open multiple systems, investigate, and resolve.

**What it looks like with AI:** An agent monitors the exception queue in real time. When a missing PO exception hits, the agent searches the PO system by vendor, amount range, and date. If it finds a likely match, it attaches the PO and moves the invoice forward. If the match is ambiguous, it sends a Slack message to the requester with the two most likely POs and asks them to confirm. For price variances, the agent pulls the contract terms and the PO line item, calculates whether the variance falls within the agreed tolerance, and either auto-resolves or escalates with full context attached. For stalled approvals, the agent sends a reminder after the configured threshold, then escalates to the backup approver if there's no response within 24 hours.

Crucially: the agent doesn't just flag the exception. It investigates, gathers context, and either resolves it or hands it to a human with everything they need to make a decision in 30 seconds instead of 15 minutes.

### 2. Month-End Close Acceleration

**What happens today:** Close takes 7 to 12 business days. The controller has a close checklist (often in Excel) with 80 to 150 tasks. The first few days are spent resolving the backlog of exceptions from the month, things that should've been fixed in real time but weren't. Then the team starts running reconciliations, most of which involve pulling data from two systems and comparing line by line. Journal entries get prepared, reviewed, posted. Flux analysis gets done manually for anything outside threshold. The whole thing is sequential because downstream tasks depend on upstream ones being done.

**What it looks like with AI:** The biggest unlock isn't automating the close itself. It's eliminating the exception backlog that creates the bottleneck. When exceptions are resolved in real time throughout the month (see above), the close starts with clean data. Beyond that, an agent can run reconciliation comparisons automatically, flag variances, pre-prepare journal entries for recurring accruals, and generate first-draft flux commentary by comparing current period to prior period and budget. The controller's job shifts from doing the work to reviewing the work, which is what it should be.

We've seen teams cut close time by 30 to 40% just by resolving exceptions continuously instead of batching them at month-end. The close isn't slow because closing is slow. The close is slow because everything upstream is messy.

### 3. Cross-System Data Management

**What happens today:** Bank reconciliation means pulling the bank statement (or logging into the bank portal), pulling the GL, and matching transactions. For a company with multiple bank accounts across entities, this is hours of work. Subledger-to-GL tie-outs require pulling trial balances from the subledger systems and comparing to the GL, then investigating any differences. Multi-entity consolidation involves pulling data from multiple ERPs (or multiple instances of the same ERP), standardizing the chart of accounts, eliminating intercompany transactions, and producing consolidated financials. Intercompany elimination alone is a multi-day process for some teams.

**What it looks like with AI:** An agent pulls bank data via API (most banks support this now, or you use a service like Plaid for commercial accounts), pulls the GL, and runs the matching. Exact matches clear automatically. Partial matches (timing differences, batched deposits) get flagged with suggested matches and the agent's reasoning. The human reviews the exceptions, not the whole reconciliation. For consolidation, the agent pulls trial balances from each entity, applies the mapping rules, identifies intercompany balances, proposes elimination entries, and surfaces any imbalances. The finance team reviews and approves rather than building the consolidation from scratch each month.

This is the area where the "glue work" metaphor is most literal. These tasks are almost entirely about moving data between systems and comparing it. There's very little judgment involved in 90% of the work. The 10% that requires judgment (investigating a real discrepancy, deciding how to classify an unusual transaction) is where you want your people spending their time.

### 4. External Communication

**What happens today:** Someone on the team is spending hours every week sending emails to vendors. Requesting W-9s before year-end. Following up on invoices that were submitted without required documentation. Responding to vendor inquiries about payment status ("when are we getting paid?"). Sending collection notices on overdue receivables. These are necessary, time-sensitive communications that follow predictable patterns, and they're almost always done manually because the ERP doesn't have a built-in "chase this vendor for a W-9" workflow.

**What it looks like with AI:** An agent monitors the vendor master for missing W-9s and sends a templated request email, then follows up on a schedule (day 3, day 7, day 14) with escalating urgency. For missing invoice documentation, the agent sends a specific request ("Invoice #4521 is missing the PO reference, can you provide it?") and monitors for the response. Payment status inquiries get auto-responded to by pulling the payment status from the ERP and generating a response ("Payment of $12,450 for Invoice #3892 was processed on 2/3 via ACH, you should see it within 2 business days"). Collections follow a defined escalation schedule, with the agent sending notices and only pulling in a human when the account hits a threshold or the vendor disputes.

The volume of outbound vendor communication in a mid-market finance team is staggering. I've seen AP teams where one full-time person does nothing but send follow-up emails. That's a perfect use case for an agent.

### 5. Audit and Compliance

**What happens today:** External audit starts with a PBC (prepared by client) list. The auditors send over a list of 50 to 200 items they need: specific reports, samples of transactions, policy documents, screenshots of system configurations. Someone (usually the most senior person who shouldn't be doing this) spends days pulling these items from various systems, organizing them, and uploading them to the audit portal. SOX testing requires pulling samples, documenting that controls were executed, and providing evidence. This is tedious, manual, and happens every single quarter.

**What it looks like with AI:** An agent parses the PBC list, maps each item to the system it lives in, and pulls what it can automatically (reports, transaction samples, account listings). For items that require judgment (selecting the "right" sample or providing context), the agent drafts a response and routes it to the appropriate person for review. For SOX testing, the agent can pull the sample transactions, verify that the control evidence exists (approval timestamps, segregation of duties verification, reconciliation sign-offs), and pre-populate the testing workpaper. The human reviews and signs off instead of building the workpaper from scratch.

Audit prep is one of those tasks that everyone hates, that takes senior people away from actual analysis, and that follows the exact same pattern every time. It's a textbook case for AI agents.

## How to Implement

As I mentioned before, 87% of AI pilots fail. The technology isn't usually the issue. The implementation approach is. I've seen the same mistakes kill projects at companies of every size, and I've also seen what works. It comes down to five things.

**Don't add to your software bloat.** Your finance department has 100+ workflows, and if you have a separate agent/automation for every single one of those, you're creating a tech-debt hell-hole that is impossible to dig yourself out of. Instead, approach AI agents from the key principle of on-top and in-between. That means: (1) have a single pane of glass over all of your existing software, where the AI bubbles insights to the top, and (2) have your AI agents that run each individual software piece, passing data back and forth between them with high accuracy.

**Embed with the team and map the real process.** This is the one I feel strongest about. Every finance team has a documented process (the SOP, the flowchart on the wall, the close checklist). And every finance team has the actual process, which is what people really do. The actual process includes the workarounds, the "I always check this spreadsheet first," the "I email Sarah directly because the system notification doesn't work." If you build automation on top of the documented process, you'll automate the wrong thing. You have to sit with the people doing the work, watch them do it, and map what actually happens. This takes time. It's worth it.

**Build inside existing tools.** Your team uses NetSuite, or SAP, or BlackLine, or Stampli, or some combination. The AI agent should operate inside those tools, the same way a new hire would. It logs into the system, reads the data, takes actions through the UI or API. It doesn't require your team to learn a new interface or change how they work. The best AI implementation is one where the team barely notices it's there, they just notice that exceptions are getting resolved faster and the close is shorter.

**Build agents that do the work, not dashboards that display it.** This is a critical distinction. Most "AI for finance" products I see are really analytics tools with an AI label. They give you a dashboard showing your exception rate, your average close time, your invoice processing metrics. That's fine, but it doesn't reduce anyone's workload. An agent doesn't show you that you have 47 unresolved exceptions. It resolves 40 of them and shows you the 7 that need your attention. The difference between a dashboard and an agent is the difference between a report that says "you have a problem" and a colleague who fixes the problem.

**Escalate to humans for genuine judgment calls.** The goal isn't to remove humans from the process. It's to remove humans from the parts of the process that don't require human judgment. An agent should resolve the straightforward cases (which are typically 70 to 85% of volume) and escalate the rest with full context. The human should be making decisions, not gathering information. When you design the escalation paths well, your team ends up doing more interesting, higher-value work and the output quality goes up because the routine stuff isn't getting half-attention anymore. And the best part is that this feedback should then train the agent to approach 99.9% accuracy within just a few weeks.

## What Kills Most Finance AI Projects

I want to be specific about the failure modes because they're predictable and avoidable.

**Trying to automate everything separately.** I've seen companies try to deploy AI across AP, AR, close, treasury, and FP&A one after the other. The rationale is usually "we want to go step by step." What actually happens is that every workstream conflicts with the previous one, nothing reaches production quality, the team gets overwhelmed, and six months later the whole initiative gets shelved. You need to design with the entire organization in mind. The issue is that your average operator will pick up a vibe-coding tool, make a 'demo' that they think helps automate some work, but it's impossible to scale across the entire organization.

**Building a "finance chatbot" instead of an operator.** Chatbots are the most common first attempt, and they almost always disappoint. You build a chatbot that can answer questions about financial data: "What's our AP aging over 90 days?" or "What was revenue last quarter?" That's barely useful, because it doesn't reduce anyone's workload. Your team doesn't need a new way to query data. They need something that does the work. The shift from "AI that answers questions" to "AI that takes actions" is the entire difference between a pilot that gets abandoned and a deployment that sticks.

**Treating it as an IT project instead of an ops project.** When AI implementation gets handed to IT, the focus shifts to infrastructure, security reviews, vendor evaluations, and architecture diagrams. Those things matter, but they're not the hard part. The hard part is understanding the finance team's actual workflow well enough to build something that fits into it. The project should be owned by the finance team (usually the controller or VP of Finance) with IT as a supporting function, not the other way around. The people who know the process should be driving the requirements.

**Automating the documented process instead of the real one.** I mentioned this above but it deserves its own callout because it's the single most common reason AI projects deliver technically correct results that nobody uses. The documented process says "invoices are matched to POs in the system." The real process is "invoices are matched to POs in the system, except when the PO was never created, in which case Brittany emails the department head to get a retroactive PO, unless it's under $500, in which case she just codes it to the department's general expense line and flags it for review later." If your agent only handles the documented process, it breaks the first time it encounters a case Brittany handles with tribal knowledge. You have to map the real process, edge cases and all.

## Where to Start

If you're a CFO or controller reading this and thinking about where to begin, my advice is simple. Pick the process your team complains about the most. It's probably exception handling, or vendor communication, or some reconciliation that eats two days every month. Map exactly how it works today, including the workarounds. Then figure out what percentage of that work is pattern-matching versus genuine judgment. If it's 70% or more pattern-matching (and it usually is), that's your starting point.

Don't buy a platform. Don't hire a data science team. Don't run a six-month evaluation. Find someone who can sit with your team, understand the real workflow, and build an agent that operates inside your existing systems. Then measure the results after 30 days.

If you want a quick run-through of what that looks like in practice, check this link out: demo.varickagents.com/cfo

This is what we do at Varick Agents. We embed with finance teams at companies doing 100M+ ARR and build AI agents that actually operate their tools. If you want to talk about what this looks like for your team, schedule some time at varickagents.com.