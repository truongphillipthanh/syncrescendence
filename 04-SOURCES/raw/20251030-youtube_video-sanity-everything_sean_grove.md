# Riding the Coming Token Wave: How Cognitive Power Will Transform Our Work

In the next five to ten years, trillions of dollars will be invested in generating tokens at great scale. Alongside those dollars, millions of hours from hugely capable, motivated researchers and engineers will be poured into this work for our benefit. This means that per token, over the next couple of years, the intelligence, speed, and ability to represent the things we care about in our world is going to skyrocket. At the same time, the cost, latency, and waste per token is going to drop.

The net effect is that there will be waves of tokens washing through data centers and across the internet. Just like any other wave, if you're able to capture and harness the power in that wave, it's a greatly empowering feature. Just as we've done with rivers, hydropower, steam power, and solar power, we're entering an age where we have an abundant source of cognitive power. But the challenge is: how do you actually harness that to make things better? How do you move forward with that? Can we turn tokens into a form of propulsion?

## The Steam Engine Analogy

Consider the steam engine—an analogy particularly apt given this venue. Steam trains literally transformed society. But it's ironic because steam power by itself is very impotent. It can't do anything on its own. It only works because humanity literally changed the shape of the earth to accommodate it, and then changed their problems to fit into a train shape.

First we covered the earth in tracks. We drilled holes through mountains to make it so that trains could actually move quickly. Then, as a follow-up, we changed all of our problems—the problems that we work on—to fit into a train shape: containers adhering to weight restrictions and whatnot. We iterated through this so much that at some point, if you just added more steam, that turned into more goods or people delivered.

With token-powered systems, we have to do the same thing. We have to build the infrastructure that allows the token power to access our world: connectors that allow these tokens to perceive our data and to modify it in ways that we care about. We have to be able to monitor, to understand the bottlenecks, to identify problems. And then once we get there, we have to figure out how to shape our problems in a way that can be pushed forward on this new infrastructure.

The encouraging part is that AI labs are putting tremendous resources into understanding what that shared infrastructure looks like—working hard to unhobble the LLMs at scale. That means your leverage is going to be in understanding how to unhobble an LLM in your specific domain. How do you make it so that the LLM understands what you're trying to do and can power the changes you want to see happen?

We need to get to the point in each of our domains—and it's all of our responsibilities—where we can add more tokens, add more compute, and get better and better solutions. This is important because we have to shape our problems in a way that can accommodate tokens. In the same way that taking a train, loading it with containers, and running it over rocky mountain terrain without rails wouldn't do much good, we have to figure out how to add more tokens to a problem and get a better solution.

## Building the Infrastructure

What does this infrastructure look like? A few years back, there was this shift where everyone was talking about low-code platforms. The promise was that we'd all become citizen developers—that you wouldn't need programmers anymore because everyone would just drag and drop and build their own apps. But that didn't pan out, largely because the domain of computer science is actually quite hard. You can't just wave a magic wand and become an expert.

What we've seen instead is that specialized roles emerged: professional coders who understand compilers, type systems, deployment, monitoring, security, and all the deep technical knowledge required. These aren't things that just disappear because you have a visual interface.

But with AI systems, something different is happening. We're not trying to turn everyone into programmers in the traditional sense. Instead, we're building systems that allow people to express their intent in natural language and have that translated into working solutions. The key difference is that the AI handles the translation between human intent and technical implementation—it manages the complexity of the domain.

This creates new leverage points. The question becomes: what infrastructure do we need to make this work reliably at scale? We need systems that can take ambiguous human intent and refine it into precise specifications. We need verification systems that can prove the solutions actually work. We need trust frameworks that let us confidently deploy AI-generated solutions.

## From Coders to Conductors

The role transformation happening here is profound. We're shifting from being coders—people who manually translate ideas into instructions—to being conductors. A conductor doesn't play every instrument. They don't need to be the best violinist or the best percussionist. But they need to understand the music, communicate the vision, and coordinate all the pieces into a coherent whole.

That's increasingly what working with AI looks like. You're not writing every line of code. You're expressing what you want to achieve, shaping the solution through iteration and feedback, proving that it works correctly, and building trust through evidence.

This isn't about laziness or taking shortcuts. It's about operating at a higher level of abstraction. Just as we don't write in assembly language anymore—we use higher-level languages that let us express our intent more clearly—we're moving to an even higher level where natural language becomes the programming interface.

## The Four Core Skills

When you work with AI systems at this level, four core skills become essential:

**Express**: Being able to articulate what you want clearly. This is harder than it sounds. Most of us have fuzzy ideas that seem clear in our heads but fall apart when we try to describe them precisely. AI systems can help by asking clarifying questions, surfacing assumptions, and forcing us to think through edge cases we hadn't considered.

**Shape**: Iteratively refining the solution through feedback. You express an initial intent, the system generates something, you see what's wrong or what's missing, and you guide it toward what you actually want. This is a conversation, not a one-shot command. The system helps you discover what you really want through the process of building it.

**Prove**: Demonstrating that the solution actually works. This isn't about vibes or trust—it's about evidence. You need to be able to verify that the code runs, that it handles edge cases, that it meets the requirements. This is where automated testing, type checking, and formal verification become powerful tools.

**Trust**: Building confidence that you can rely on the solution. Trust comes from evidence, not hope. When you can see the reasoning, inspect the implementation, and verify the behavior, you can trust the system to work correctly even when you're not watching it.

## A Practical Demonstration

Let me walk through what this looks like in practice. Suppose I want to build a drawing app for my daughter. I might start by just expressing that intent: "I want a simple drawing app."

The system immediately starts asking questions. What tech stack? What about user accounts? What about safety features? Privacy concerns? These are questions I might not have thought through, but they're important. The system is helping me express my intent more fully by surfacing the gaps in my thinking.

As we go back and forth, the specification gets more precise. Maybe I decide I don't care about the tech stack—the system can choose whatever makes sense. But I care deeply about safety features since this is for my daughter. That's information the system needs to make good decisions.

Once we have a clearer specification, the system can generate mockups, write code, set up the infrastructure. But I'm not done—now I need to shape it. I look at the mockup and realize I want different colors, or a different layout, or additional features I hadn't thought of initially. The iteration continues.

Throughout this process, I'm proving that it works. I'm running the code, testing the interface, checking that it behaves the way I expect. The system can help here too—generating tests, checking for common errors, validating against the specification.

Finally, I build trust. I've seen the code. I've tested the functionality. I have evidence that it works. I can deploy it confidently because I'm not just hoping it works—I know it works.

This entire process—express, shape, prove, trust—is the new interface for working with AI systems. It's not about telling the computer exactly what to do step by step. It's about communicating intent, refining through iteration, and building confidence through evidence.

## The Evidence Layer

One of the most important shifts here is moving from trust based on credentials to trust based on evidence. Traditionally, you might trust code because it was written by a senior engineer with years of experience. But how do you trust code written by an AI?

The answer is that you demand evidence. The system needs to show its work. It needs to provide tests that demonstrate correctness. It needs to make its reasoning transparent. This is actually a higher standard than we often apply to human-written code, and that's a good thing.

When you have robust evidence, it doesn't matter whether the code was written by a human or an AI. You can verify that it works. You can understand what it does and why. You can modify it with confidence because you can re-verify after making changes.

This evidence-based approach scales in ways that credential-based trust doesn't. You can't personally review every line of code in a large system. But you can have comprehensive automated tests. You can have formal verification for critical components. You can have monitoring and observability that shows the system is behaving correctly in production.

## Scaling Without Fear

This brings us to one of the most profound implications: the ability to scale without fear. Right now, scaling software systems is risky. More code means more bugs. More complexity means more things that can go wrong. More features means more maintenance burden.

But when you have systems that can reliably generate correct code, that can comprehensively test their outputs, that provide strong evidence of correctness—scaling becomes less risky. You can add features faster. You can experiment more freely. You can maintain larger systems with smaller teams.

This doesn't mean eliminating all risk. Software development will always involve uncertainty and trade-offs. But it shifts the risk profile dramatically. The bottleneck moves from "can we write this code without introducing bugs?" to "can we clearly express what we want and verify that we got it?"

## The Human Element

There's a natural question here about what happens to traditional roles. If AI can write code, what do programmers do? If AI can design interfaces, what do designers do?

I think there's definitely a blending, but it's more about amplification of each individual than replacement. In my demonstration, I'm generating mockups of screens because I'm not good at making visual mockups. But a designer might be incredibly skilled at expressing what they want in Figma—they can import that, but they might not be sure about the tech stack, the distribution method, or the technical partnerships. The system enables them to work beyond their traditional constraints, but through the lens they feel most comfortable with.

This reminds me of leadership. As a leader, you're driving toward outcomes, going about it in a structured way, trying to realize what you're trying to do—not necessarily the mechanics of doing it. Leaders enable teams to build things without micromanaging every detail. They communicate vision and intent, not implementation specifics.

In a world where you might have a hundred thousand agents or a million agents working for you, you're by definition a leader—one of the most powerful leaders in the history of humanity. We all become leaders in this world. You have to think about impact. What impact are you going to have with this effort?

The common thread through all of this is that the biggest bottleneck and the most precious resource is human life, human time, human attention. If you can speak anything into existence, it makes you think about what you want to spend your life doing. What is the impact you want to have? That is the definition of being a leader.

## On Joy and Purpose

Not everyone is cut out to be or wants to step into leadership roles—thinking about difficult questions of why we're doing things and to what end. Some people just enjoy building things, picking up tickets, laboring over code they can be proud of. That's what gives them joy in life.

I think it's very important to do the things that give you joy. One of my favorite terms is "amateur," though we use it as a pejorative now. But the meaning is beautiful: someone who does something for the love of it. If you love programming, if you love the Norwegian demo scene or whatever your passion is, you can throw yourself into that and really enjoy it.

But a professional is someone who gets paid to do something for some outcome. I think it's valuable that those things become separated. If you don't want to do this professionally, then don't do it professionally. But it will probably be somewhat difficult to compete as an amateur.

There are industries where this works—artisanally made hats, belts, couture. I don't know if the market will exist for artisanal code. It's possible. But if you enjoy it, do it for the love of it. And if you want to have an impact with it, then do it professionally.

## The Path Forward

We're at the beginning of something profound. The infrastructure for token-powered systems is being built. The AI labs are investing massive resources into making these systems more capable, more reliable, and more accessible. Our job is to figure out how to shape our problems to take advantage of this new form of power.

This isn't about replacing human creativity or judgment. It's about amplification. It's about giving each person the leverage to have the impact of a team, or a company, or even more. But that leverage comes with responsibility. You have to think more carefully about what you want to achieve. You have to be more precise about your intent. You have to demand evidence and build trust.

The future is one where cognitive power becomes abundant, where the bottleneck shifts from execution to vision, where the question isn't "can we build this?" but "should we build this?" That's a future worth working toward.