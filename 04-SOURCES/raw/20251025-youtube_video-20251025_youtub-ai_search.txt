https://www.youtube.com/watch?v=uQiqKFK5_0w
Infinite AI video, 4K images, realtime videos, DeepSeek breakthrough, Google’s quantum leap: AI NEWS
63,480 views  Oct 25, 2025  #ai #ainews #aitools
INSANE AI NEWS: DeepSeek OCR, ChatGPT Atlas, Google Earth AI, Stable Video Infinity, Unitree H2, UltraGen, Krea Realtime #ai #ainews #aitools #aivideo #agi

Thanks to our sponsor Abacus AI. Try ChatLLM & DeepAgent here: http://chatllm.abacus.ai/?token=aisearch

Sources in order of mention:
https://3d-models.hunyuan.tencent.com...
https://holo-cine.github.io/
https://noamissachar.github.io/DyPE/
https://visual-ai.github.io/inpaint4d...
https://huggingface.co/krea/krea-real...
https://sjtuplayer.github.io/projects...
Ditto tutorial:    • This new AI edits any video! FREE & offline  
https://jamesyjl.github.io/Nano3D/
https://g-vista.github.io/
https://github.com/vita-epfl/Stable-V...
https://blog.google/technology/resear...
https://blog.google/technology/resear...
https://openai.com/index/introducing-...
http://github.com/deepseek-ai/DeepSee...
https://gmargo11.github.io/softmimic/


0:00 AI news intro
0:57 Hunyuan World Mirror
3:40 HoloCine
7:20 DyPE
9:09 Inpaint4Drag
10:38 Krea realtime 14B
12:39 UltraGen
15:23 Ditto
16:19 Nano3D
17:55 Google Vista
22:12 ChatLLM
23:06 Unitree H2
24:16 Origin M1
25:22 Stable Video Infinity
28:11 Quantum breakthrough
30:33 Google Earth AI
32:46 ChatGPT Atlas
35:43 DeepSeek OCR
40:30 SoftMimic


Newsletter: https://aisearch.substack.com/
Find AI tools & jobs: https://ai-search.io/
Support: https://ko-fi.com/aisearch

Here's my equipment, in case you're wondering:
Lenovo Thinkbook: https://amzn.to/4jWeKwH
Dell Precision 5690: https://www.dell.com/en-us/dt/ai-tech... 
GPU: Nvidia RTX 5000 Ada https://nvda.ws/3zfqGqS
Mic: Shure SM7B https://amzn.to/3DErjt1
Audio interface: Scarlett Solo https://amzn.to/3qELMeu

---

AI news intro
0:00
AI never sleeps and this week has been absolutely insane. This AI can generate extremely
0:08
highresolution images natively. And instead of images, we have another AI that can create 4K videos, and these are
0:15
all free and open- source. Deepseek releases a banger that might change how we design future AI models. Google
0:22
achieves a huge breakthrough in quantum computing. Plus, they released some super powerful updates for Google Earth
0:28
AI. OpenAI launches their new web browser Chad GPT Atlas that plans to challenge Google Chrome. We have a new
0:36
semantic 3D model editor where you can just edit 3D models with a text prompt and also a real-time open-source video
0:43
generator. Plus, we have not one but two new open-source video models that can
0:48
generate long videos which are actually consistent. We have some new and moderately creepy humanoid robot demos
0:55
and a lot more. So, let's jump right in. First up, Tencent releases Hunyan World
Hunyuan World Mirror
1:01
Mirror. This has got to be the most flexible open-source 3D world generator
1:06
you can use right now. So, here's how it works. You can input an image to use as
1:11
a reference to create the 3D world, but you can also add in camera intrinsics or
1:17
depth maps or even poses or any combination of the above. And it'll basically map all your controls together
1:25
to generate your 3D world. But not only that, it can also generate the camera
1:30
parameters, the depth maps, and the normal estimation. This is insanely powerful. Here are some image to 3D
1:37
world examples. We can upload two photos like this, and it can basically create a
1:43
3D model of these two photos stitched together. Plus, notice the boxes over
1:49
here. These are where it estimates the cameras were placed. Or you can upload
1:54
four photos of this imaginary room like this and it can stitch them together. As
2:00
you can see, here is its estimate of the camera positions where the four photos were taken. And here's the
2:05
reconstruction of the room. Very cool. Or here's another example where we can take multiple zoomed shots of this room,
2:13
each a bit closer to the table. And here's your result. Notice that for these examples, these are basically 3D
2:19
point maps. So, they're kind of like dots scattered across this 3D space. And there are some parts that are missing or
2:26
disconnected, especially along the walls and the edges of these objects. And that's because it couldn't really detect
2:32
any data from our input images. But in addition to point maps like this diagram
2:37
shows, this can also generate basically a complete 3D scene. So, for example, if
2:43
you upload this image, here's your resulting 3D scene. Or here's another example. We can upload this tunnel photo
2:51
again. And here's the fully reconstructed gausian scene. Or here's another example of this lighthouse.
2:57
Notice the details are all there, including the windows and the fences and the American flag. This is very good
3:03
quality. The awesome thing is if you scroll up to the top of the page, they've actually released the models on
3:09
HuggingFace already. Plus, if you click on this GitHub repo and you scroll down a bit here, it contains all the
3:15
instructions on how to download and run this locally on your computer. Notice that you do need a CUDA GPU to run this.
3:23
However, over here, they didn't specify the minimum VRAM requirements for this. But if you look at the hugging face
3:30
repo, the size of all the models are only 5 GB. So, you should be able to run this using most consumer grade GPUs.
3:37
Anyways, if you're interested in reading further, I'll link to this main page in the description below. Next up, Ant
HoloCine
3:43
Groupoup releases another super useful tool this week. It's called Hollow Scene, and this allows you to generate
3:50
entire multi-shot long videos just with the text prompt. You see, the problem
3:55
with traditional video models is they can only generate isolated clips of only like 5 to 10 seconds at a time. But
4:02
finally, this open- source tool allows you to generate much longer videos. and they actually remain coherent. So here
4:09
are some examples for your reference. First notice here is how you would format the prompt. So we first have a
4:15
global caption which defines the overall scene of the entire video and then you
4:20
would denote each character with a number like this. So if you have multiple characters then it's going to
4:25
be character one, character two, etc. And then here you can specify that the scene contains five shots and then you
4:32
can list out each shot like this. So, the first shot is a wide shot of the
4:38
majestic sunrise over the ocean. Character one as a small silhouette. And then the next shot is a medium shot of
4:44
character one from the side as she looks out at the horizon. And then we cut to a
4:50
closeup of her face. And then we cut to a close-up of her hand gripping a small
4:56
weathered stone. And then we cut to a medium shot of her turning her head slightly and closing her eyes. And
5:02
here's the result. Notice that everything looks super cinematic and it follows the prompt exactly and the scene
5:09
remains consistent, including the background, the colors, and the appearance of the character. Or here's
5:15
another example for your reference. Here's the global caption. And then we have character one, which is a
5:21
scientist, and character 2, which is an android, learning to tend a garden of synthetic light emmitting plants. And
5:28
then this scene contains six shots. As you can see over here, the first shot is a medium shot of the android gently
5:34
touching the pedal of a plant. And then it cuts to the scientist watching on a
5:40
monitor. And then it cuts to the android tilting its head. Then we have a closeup of the gentle's metallic fingers as it
5:46
prunes a leaf. Then we have a rack focus from the android's peaceful work to the
5:51
scientist who smiles. And then we have a final shot showing the glowing garden. And that's indeed what we get. This new
5:59
AI is really good at actually following everything you specify in your prompt.
6:04
Or here are some other examples for your reference. These are pretty long prompts, so if you want, you can replay
6:10
these parts and read the prompts yourself to confirm that the video is actually generating each shot that was
6:16
specified. And again, everything looks super consistent and seamless, as if it
6:21
was indeed the same character and the same scene. Here are some other examples for your reference. Now, notice that
6:28
this hollow scene is mostly good at generating cinematic shots like this. It
6:34
might not be as good with like highaction shots or scenes that are physically challenging like gymnastics.
6:40
The awesome thing is if you scroll up to the top of the page, they've released the models already. And then if you
6:45
click on this GitHub repo, this contains all the instructions on how to download and run this locally on your computer.
6:52
Notice that this is based off of 1 2.214b. So, this is the latest and best
6:57
open-source version of one. And down here, they give you some instructions on how you should format your prompt. Like
7:04
I showed you earlier, you need to include a global caption and then also shot captions. And then you can also
7:09
include the number of frames. The default here is 10 seconds at 24 frames per second, but you can make this longer
7:15
if you want. Anyways, if you're interested in reading further, I'll link to this main page in the description
DyPE
7:20
below. Next up, this AI is super powerful. It's called DYP and this is an
7:27
open- source AI that can finally generate super high resolution images natively. So, here are some examples. I
7:34
mean, if I zoom into this, notice the details of everything. This is super high, like 4K resolution. Or here's
7:41
another example. If I zoom in, notice that you can even see the details of the
7:46
dude's face and the grass and his fingers. I mean, this is insanely detailed. Here are some other examples.
7:53
Let's zoom in on this image and note how detailed everything is. Even this
7:59
building in the background, like everything is just very sharp and detailed. Or here's another example.
8:05
Let's zoom in on her armor. Everything is just very sharp and detailed. Now, here's the thing. You could use an
8:11
existing open- source model like Flux and try to get it to generate a 4K image
8:17
like this. But here's your result. However, after you plug in dip, then the
8:22
result looks a lot better. As you can see here, I mean, this is truly a 4K
8:27
image. Or here's another example. Here's the same prompt, but on the left is just using Flux. Notice that the details
8:34
aren't great, but on the right is dye. And notice just how much more detailed
8:41
everything is. Or here's another example. Notice that at such a high resolution, Flux often gets some parts
8:47
wrong, like over here. But if you add dye again, it just adds so much more detail into this photo. The awesome
8:54
thing is if you scroll up to the top of the page, they've released the code to this already. So on this GitHub repo,
9:00
here are all the instructions on how you can download and run this locally on your computer. Anyways, if you're
9:06
interested in reading further, I'll link to this main page in the description below. Next up, this AI is really cool.
Inpaint4Drag
9:12
It's called In Paint for Drag, and here's how it works. You would first start by painting over the areas of the
9:19
image which you want to edit. And then it would automatically select or mask out these areas. And then step two is
9:26
you would draw arrows that kind of control how you want to move these objects. And then afterwards it's going
9:32
to actually move these objects very roughly like this. But then use AI to stitch everything together seamlessly.
9:40
And here is your final result. Here are some more examples. So, you can select this wing and move the wing up a bit and
9:46
then the AI will stitch this together and here's what you get. Or here's another example where we can select this
9:53
panda and move it to the left a bit and here's your result. Or here's another example where we can select this arm and
10:00
move it up a bit and the AI will stitch everything together seamlessly like this. So, you know, this is kind of like
10:06
Nano Banana, but instead of editing the image with a text prompt, you actually
10:12
draw over the image and dictate how you want to move specific parts of the image. Anyways, if you scroll up to the
10:18
top of the page, they've actually released the code for this already. So, if you click on this GitHub repo, this
10:24
contains all the instructions on how to install this and run it on your computer. Plus, it looks like they've
10:29
also released a collab demo for you to try this out online. Anyways, if you're interested in reading further, I'll link
10:36
to this main page in the description below. Next up, we have a new opensource
Krea realtime 14B
10:41
video model that can potentially generate videos in real time. So, this is called Crea Realtime 14b, and this is
10:49
based off of Alibaba's 2.114b, but they've added this new self-forcing
10:55
technique. And here it says it can achieve texttovideo inference at a speed of 11 frames per second on a single
11:02
Nvidia B200. So I guess the caveat here is you still need a beast of a GPU in
11:08
order to actually generate real-time videos. You're not going to be able to do real time on a consumer GPU. However,
11:15
that being said, you can still generate videos a lot faster with this new Crea Realtime model. Now the awesome thing
11:22
about Crea Real Time is you can do video to video. So for example, you can take
11:27
this video on the left where it's just a very rough composition of how you want to place objects in the video and then
11:33
it can generate an entire race car scene of it on the right. Now, this doesn't have to be in real time, but if you can
11:40
do this in real time, then this unlocks a ton of cool creative possibilities where you can basically, you know, just
11:46
film yourself on your webcam, but then turn the output into a completely different video. Or here's another
11:51
example where we can take the top video and plug it through this AI to generate a completely different car and scene.
11:59
Here are some other examples of video to video where on the top is the original video, but you can change the characters
12:06
like this. Now, before you get too excited, here's the thing here. It says their model is over 10 times larger than
12:13
existing real-time video models. In fact, if you click on this hugging face, notice that the total size of everything
12:19
is almost 60 GB in size. And that's why they mentioned that you need a B200 to actually run this in real time. So, it
12:26
doesn't look like this can be run on most consumer-grade devices, but I mean, if you're really interested in trying
12:32
this out, you could rent a GPU cloud to set this up. Anyways, if you're interested in reading further, I'll link
12:38
to this main page in the description below. Next up, this AI is super useful.
UltraGen
12:43
So, it's called Ultragen, and this allows you to generate highresolution videos up to 4K. In fact, this is the
12:50
first model that I'm aware of that allows you to generate native 4K videos. Here are some comparisons with the
12:57
leading open-source models out there, including Hunyan Video and Juan. But as
13:02
you can see, this new one by Ultragen is a bit more detailed, and that's because it can generate well 4K resolution,
13:10
especially notice the details of the mountains. I mean, the resolution is just a lot clearer and better. Here are
13:18
some other comparisons for your reference. So, here's a dramatic volcano eruption. Here is a coral reef and then
13:25
here's a river. Everything is just a lot more detailed. So here they compared the
13:31
generation time of Juan and Hunen and this Ultragen. If you try to generate videos at all these different
13:37
resolutions. Notice that if you get Juan, which is like the leading open-source model to generate even a
13:43
1020p resolution video, it'll take 35 minutes. Whereas for Ultra Gen, it
13:49
requires less than half the time. And then if you try to get it to generate 4K, well, first of all, Juan doesn't
13:54
really natively support 4K. So you get a super blurry image like this. Plus, it
14:00
takes like almost 9 hours to generate this. Whereas for Ultragen, it took less
14:06
than 2 hours. And the result also looks amazing. Here are some other results for
14:11
your reference. So the bottom row is this new Ultragen. Notice, especially for this prompt, a snowy village at
14:18
Dusk, none of the other video generators could actually get the building details correct. If you zoom into the buildings,
14:25
however, notice how detailed the generation from Ultra Genen is. You can even make out the details of the
14:31
windows. And then here are some quality benchmarks for your reference. Note that for both 1080p and 4K videos, Ultragen
14:38
just beats the rest of the leading video models for most of these benchmarks. Really quickly, here's how it works. So,
14:45
Ultragen uses a special attention mechanism that breaks down the video into both global and local models. So,
14:54
the global model is in charge of generating the overall scene, but the local model is focused on the fine
15:00
details. And if you mix both of them together, this creates both a consistent and detailed scene. Now, if you scroll
15:07
up to the top of the page, it looks like they've released a technical paper and a GitHub repo. And then on this GitHub
15:13
repo, it says the code will be released soon, so stay tuned for that. Anyways, if you're interested in reading further,
15:20
I'll link to this main page in the description below. Next up, we have a really cool AI video tool called Ditto.
Ditto
15:28
And this allows you to edit existing videos just with a text prompt. So, for example, you can completely change the
15:35
character or the background of the video like this. Or you can also easily insert
15:41
objects into a video just with a text prompt as you can see here. And one of the coolest features is they've also
15:48
released a fine-tuned model which allows you to turn any anime scene into a realistic scene like this. Best of all,
15:55
this is completely open- source and you can use this right now. In fact, I already did a full installation tutorial
16:02
this week on how you can set this up on your computer and it's actually pretty small. So, each model is only 6 GB in
16:09
size, so you should be able to run this on most consumer- grade GPUs. Anyways, I'm not going to repeat too much here.
16:15
See this video if you want to check out Ditto further. Next up, this is a pretty cool 3D model editor. So, this allows
Nano3D
16:22
you to microedit existing 3D models just with a text prompt. So, it's kind of
16:27
like Nano Banana, but for 3D models. Here are some examples. On the left is the original model, and you can prompt
16:33
it to make the backpack bigger. And on the right is your result. Notice that it
16:38
keeps the rest of the character consistent. Or here's another example. We can replace the blue jacket with the
16:44
brown down jacket. And here's what we get. You can also remove objects. So let's get it to remove the chimney. And
16:50
here's your result. Or we can do the opposite and add a satellite dish on top of this house. Or we can get this dragon
16:57
to hold a sword like this. Or we can remove the wings of this dragon. Basically, as you can see, it's able to
17:04
add or remove things that you prompt it with without affecting the rest of the model. Really quickly, here's how it
17:10
works. So, it uses a combination of techniques, including FlowEedit, which is a 3D model editor, which I've went
17:17
over on my channel before, and it integrates this into Trellis, which is the 3D model generator. And this allows
17:22
you to create local edits without even needing to apply a mask or basically select or draw over the area which you
17:30
want to edit. it can automatically detect this just with your text prompt. Anyways, if you scroll up here, it looks
17:36
like they are planning to release some demo or data set and the code for this. Right now, if you click on this GitHub
17:42
repo, it doesn't seem like they've released the code yet, but it looks like they are planning to release the data
17:48
set and a Gradio demo very soon. Anyways, if you're interested in reading further, I'll link to this main page in
17:54
the description below. Next up, Google releases a super insightful AI system.
Google Vista
17:59
It's called Vista and this is basically an agentic system that is designed to
18:05
improve video generations automatically. So here's how it works. It's basically an agentic loop like this where it first
18:13
takes the user's prompt and then it generates multiple videos from that prompt and then it goes through this
18:19
criteria and multiple rounds much like a competition to basically select the best
18:25
video from these generations. And then afterwards it gets multiple AI agents to
18:31
critique the videos further. And the user can also add feedback here on which videos they prefer. So for example,
18:37
these specialized agents will analyze the videos like visual fidelity, motion,
18:43
dynamics, etc. And also how good the audio is and then how good the context
18:48
is. And then afterwards this feedback is then fed through this reasoning agent to
18:53
rewrite and improve the original video prompt from the user. And then this prompt can also be fed back into this
19:01
loop. So this cycle basically repeats and for each loop it analyzes the video.
19:06
It critiques everything and it gets this deep thinking agent to rewrite the prompt to improve it for the next
19:12
generation. So, in theory, this loop should generate a much better looking video than if the user just plugged in a
19:19
text prompt without any further refinement. So, here are some examples for your reference. Here's the original
19:25
prompt. It's just a spaceship entering hyperdrive, which actually does not look bad already. But then after plugging it
19:31
through Vista, here is the entire prompt that this agent has created. And here's
19:36
the resulting video. And look at the visual effects of everything. It just looks way better, especially that tunnel
19:43
effect at the end. Or here's another example where here is the original prompt. It's basically this block of ice
19:50
slowly melting into this drink, which already looks pretty good.
20:01
But then after plugging the prompt through Vista, here is the entire prompt that Vista has created. Notice how
20:07
detailed this is. And here's the result.
20:17
Let me know in the comments which one you prefer. Or here's another example. Here is the original prompt. And here's
20:23
the video.
20:31
It's already not bad. It does follow this prompt pretty well. But after plugging it through Vista, here's what
20:37
we get.
20:45
Notice the scene looks a lot more cinematic. It even has a nice camera orbit effect. And the dude does kind of
20:52
seem more distressed. Or here's another example where we have a couple running through a sudden downpour, laughing and
20:58
splashing in puddles. Let's go. Let's go.
21:08
That generation was pretty bad. After plugging the prompt through Vista, here's what we get.
21:14
I can't believe it's raining this hard. Yeah. Yeah. Let's keep going.
21:21
Notice the scene is a lot more coherent. Plus, it just looks way better and more realistic. Or here's another example
21:27
where we have an aerial view of this lush green forest. Let's play the original one first.
21:41
which honestly already looks pretty good. But here is Vista's generation.
21:53
So, it just looks way more cinematic with some nice extra camera motions. Plus, the colors and the sound are also
22:00
a bit better than the original generation. Anyways, for now they've only released a technical paper on this,
22:06
but if you're interested in reading further or checking out more examples, I'll link to this main page in the
22:11
description below. Let me tell you about this awesome tool called Chat LLM by
ChatLLM
22:17
Abacus AI, the sponsor of this video. Chat LLM is an all-in-one platform for
22:22
you to use the best AI models out there. You can seamlessly switch between different models. Plus, you can use the
22:28
best image generators out there and the best video generators out there all in one integrated platform. Plus, if you're
22:34
coding something, they have a really useful artifacts feature so that you can preview your generations side by side.
22:40
Plus, they have a deep agent feature which can do really complex tasks all autonomously like creating powerpoints,
22:46
websites, and research reports. It's going to supercharge your productivity. You can access all these AI models and
22:53
image and video generators and Deep Agent for only $10 a month. This is way
22:58
cheaper than if you paid for each tool separately. Definitely check out Chat LLM that comes with Deep Agent in the
23:05
description below. In other news, we have a new humanoid robot from Unree
Unitree H2
23:10
Robotics. Now, in the past, they've released the G1, which has gotten a lot of attention. This is by far the most
23:16
acrobatic and flexible robot out there. Well, this week they released something even crazier. It's called the Uni Tree
23:23
H2. And here are some demo videos of it moving. I mean, look how flexible and
23:29
natural its movements are. This looks even a bit more humanlike than the Unitere G1. So, here are some specs for
23:35
your reference. This stands at approximately 180 cm and weighs around 70 kg. It has 31 degrees of freedom,
23:43
including a waist with three degrees of freedom and a neck with two degrees of freedom. And it has really smooth and
23:50
lifelike movements like dancing and it can even do kung fu and ballay spins. As
23:55
you can see here, the arms also have seven degrees of freedom each for really dextrous manipulation. And they've also
24:02
decided to add what they call a bionic human face to make it even more lifelike
24:07
and relatable. But honestly, I think adding that face makes it pretty creepy. Let me know in the comments what you
24:13
think. Now, in addition to the Uni Tree H2 with that creepy face, we have another robot face, this time called the
Origin M1
24:21
Origin M1 by this Shanghai based company called a head form. Unfortunately, they've only created a male face. I
24:28
would like to have a cute, you know, catgirl face instead. But nevertheless, this is super realistic, as you can see
24:34
here. And that's the whole point of this. It's designed with ultra realistic facial expressions, and it's equipped
24:40
with up to 25 brushless micro motors beneath its synthetic skin to create
24:46
some really subtle and lifelike movements like blinking and raising its eyebrows or tilting its head. And this
24:53
Origin M1 also has some embedded cameras inside its eyes for gaze tracking and
24:59
environmental perception. So, you know, it can actually move its eyes around and look around just like a human. This
25:05
system can recognize and understand speech and also respond in real time. In
25:10
fact, it has real-time lip syncs, so it can actually speak out its response back to you with its mouth. Again, honestly,
25:17
I would prefer a cute catgirl face, but here's what we have right now. All right, next up, we have another super
Stable Video Infinity
25:23
useful video tool called Stable Video Infinity. Apparently, this can create
25:28
infinite length videos with consistent scenes and smooth transitions. So, here
25:34
are some examples for your reference. At the bottom is the prompt and on the right is stable video infinity and this
25:41
clip is over 40 seconds long. If you compare this with the other two competitors, notice that the videos kind
25:47
of deform and just completely mess up as soon as you start generating longer videos. But for stable video infinity,
25:55
the video remains consistent throughout this 40-second clip. Here's another example for your reference. So, this
26:01
clip is over 30 seconds long, and as you can see, only stable video infinity was
26:07
able to keep the video consistent and error-free across the entire generation.
26:12
In fact, they claim that Stable Video Infinity is able to generate videos of up to 10 minutes long without losing
26:19
quality. Here's another example for your reference. Here, this clip is again over
26:24
30 seconds long. And only stable video infinity on the right was able to generate a consistent and error-free
26:32
video without any errors. And here's another awesome thing. You can also plug audio into the system for it to
26:39
lip-sync. Now, if we start with the same input image of Obama talking, well, we
26:44
already have tools like multi-alk which can get him to talk. But notice that the face of Obama kind of changes over time,
26:51
especially if you generate a super long video of him talking. But for stable video infinity, Obama remains Obama,
26:58
even though the video is over a minute long. Far less. All of us take offense to anyone who
27:05
reaps the rewards of living in America without taking on the responsibilities of living in America. and undocumented
27:12
immigrants who desperately want to embrace those responsibilities see little option but to remain in the
27:18
shadows or risk their families being torn apart. And as you can see or hear, the lipsync
27:24
is very good. And I mean, what the hell is going on with the video on the left? But I mean, there you have it. Here is
27:30
finally a tool that can generate long videos even with audio and lip sync. Anyways, there are a ton of examples on
27:37
this page. In the interest of time, I'm not going to go over everything, but if you scroll up to the top of the page,
27:43
they have released the code for this. And down here, they give you all the instructions on how to run this on your
27:50
computer. Here, they did not specify the minimum VRAM requirements for this, but they did say that they used an A100 with
27:57
80 GB. I don't suspect you need that much because this is only based off of 1 2.114b, which can already be run on most
28:04
consumer grade GPUs. Anyways, if you're interested in reading further, I'll link to this main page in the description
28:10
below. Next up, Google continues to drop some huge breakthroughs in AI. So, this
Quantum breakthrough
28:16
week they've released this blog where they claim that their Willow quantum
28:21
chip just demonstrated a huge breakthrough in quantum computing. So, quantum computing is quite a complex
28:28
subject. So, let me first explain this to you in simple terms. You see, regular computers are just made of a ton of
28:35
different light switches that either turn on or off. So, it's either zero or one. But a quantum computer is like a
28:41
dimmer switch where it's not just on or off, but it can also be everything in between. And this basically opens up a
28:48
ton of possibilities. This allows us to have way more powerful compute. However, the challenge of designing a computer of
28:56
continuous values instead of just 0 and one is that it makes them very sensitive and prone to errors. However, Google's
29:02
Willow chip is this advanced quantum computer that actually works. And for the first time, it ran a complex quantum
29:11
algorithm and it did this way faster than classical supercomputers. In fact, it did this 13,000 times faster. So this
29:18
really complex algorithm is what they call quantum echoes. And this works like
29:24
sending a special signal into the quantum system and then reversing that signal and then measuring the echo that
29:30
comes back. And this echo helps scientists understand well how disturbances spread in this quantum
29:36
system. It's very precise and reliable because the results can be repeated and verified. So in simple terms, this new
29:43
technique basically makes their calculation super precise. And you might be wondering, well, why should we care?
29:49
What impact does this actually have? Well, because of this new precision, we can use this tool to study the universe
29:55
way better. For example, it can be used to learn the structure of systems from molecules to magnets to black holes. And
30:03
the ability to really precisely model molecules. I mean, these are basically
30:08
some of the smallest components of our universe. If we can measure and understand how these work, well, this is
30:14
fundamental to our understanding of chemistry, biology, and material science. So, this can help us with new
30:20
breakthroughs in, for example, biotechnology to solar energy to nuclear fusion. Anyways, it's quite a technical
30:28
article, but this is a huge breakthrough. If you're interested in learning more, I'll link to this main
Google Earth AI
30:33
announcement page in the description below. Now, in addition to Google's new quantum breakthrough, they've also
30:39
updated Google Earth AI, which is also incredibly powerful. So, Google Earth is
30:45
basically a tool that allows you to look at many different layers and geospatial data of planet Earth. This is built on
30:53
decades of Earth modeling data. But now on top of that, if you add in the advanced reasoning abilities of Gemini,
31:00
then well, you can just ask an AI about anything on Earth and it would automatically and autonomously help you,
31:07
you know, like pull the correct information and answer your question or do an analysis for you. This is super
31:13
important for tackling some global challenges like environmental monitoring or disaster response. This new framework
31:20
called geospatial reasoning which is powered by Gemini can automatically link various earth data models like weather
31:27
or population or satellite imagery to answer any complex questions that the
31:32
user may have such as identifying vulnerable communities or areas that are at risk of disasters or perhaps
31:39
rainforests that are most at risk of deforestation or climate change. As you can see here, Google Earth's new AI
31:46
features allow you to instantly find objects and patterns in satellite imagery. So, instead of you having to
31:52
like swipe around the globe and zoom in and out to pinpoint the exact location, you can just prompt your query in this
31:59
chatbot and it would automatically find the stuff for you. By now, you might be wondering, well, how can I access this
32:05
here? They say that this will be available in the US in the coming weeks, whatever that means. and only to Google
32:13
Earth professional and professional advanced users. So, it's mostly targeted towards like enterprises or research
32:18
labs, people who are actually serious about you know this geospatial research. And then also starting today, it seems
32:24
like Google AI Pro and Ultra subscribers in the US can also access Gemini
32:30
capabilities in Google Earth with higher limits. So, they're just starting to roll this out, but I'm sure that in the
32:36
near future, we are going to have like AI powered Google Earth and Google Maps. Anyways, if you're interested in reading
32:43
further, I'll link to this main announcement page in the description below. Also, this week, OpenAI just
ChatGPT Atlas
32:48
released Chat GPT Atlas. This is essentially a web browser that's powered
32:53
by ChatGpt. So, actually, it's kind of just a Chromium wrapper. In other words, it's a clone of Google Chrome, but
33:01
they've just added a right side window where you can pull up chat GPT and ask
33:06
it questions. In fact, it's kind of very similar to Perplexity's Comet browser, which was released a few months ago. So,
33:13
there's nothing revolutionary here. But anyways, here are some examples of what it can do. For example, let's say you're
33:20
on this web page. Well, you can pull up Chachi PT on this sidebar and then ask
33:25
it about anything on this page. For example, you can ask it, "Are there any good hikes nearby?" and it will answer
33:31
you. Or here are some other use cases for your reference. If you're buying a flight ticket, you can also ask
33:37
questions about the airport like this. Or here's another really useful application. If you're writing
33:43
something, like if you're replying to an email, well, you can just highlight any text and then there's going to be this
33:48
chat GPT button that pops up and you can ask it to refine the text further like this. However, notice that at least for
33:55
me, I already have this on my Android phone where I can just hover over any text and then get Gemini to edit or
34:02
rewrite the text for me. Or here's another cool feature. So, this Chad GPC Atlas has something called browser
34:08
memories, which is optional. So, you don't have to turn this on, but if you do, Chad GPT basically remembers your
34:14
browsing history and context. So, if you later prompt it to open the Halloween
34:20
decorations I was looking at last week in some new tabs, it can actually inspect your history and then open what
34:26
you browsed in a new tab. They also have this agent mode which lets chat GPT
34:31
basically do tasks autonomously on your browser such as shopping. So, for example, you can get it to purchase
34:39
stuff on this site and it would autonomously add the items for you into your shopping cart and then check out.
34:46
So those are the main capabilities of Chat GPZ Atlas. Again, it's nothing revolutionary. Actually, we've already
34:52
seen similar browsers before like Perplexity Comet or even like way back
34:57
last year, I did another video on this simple Chrome extension, which basically does the same thing. It can automate my
35:04
shopping. It can automatically write and reply to emails for me. And this was like 10 months ago, and it's just a
35:09
Chrome extension. So, if you want to learn more, definitely check out this video. Anyways, back to here. Notice that Chat GPT Atlas is only available on
35:17
MacOSS today for free and other users. And then here they say experiences for
35:22
Windows, iOS, and Android are coming soon. Also note that agent mode again this is where you can get ChatBT to
35:29
autonomously do tasks for you. This is launching in preview only to plus pro
35:34
and business users. This is still an early experience, so expect it to make mistakes on complex workflows. Anyways,
35:41
if you're interested in reading further, I'll link to this main page in the description below. Also, this week, the
DeepSeek OCR
35:47
whale Deep Seek continues to release some huge AI breakthroughs. This week,
35:52
they published a paper on Deepseek OCR, and this might trigger a paradigm shift
35:58
in how AI models work. You see, for computers, they don't actually understand text like this. For
36:04
computers, you need to feed it numbers. So traditional large language models what they need to do is actually convert
36:10
all this text into tokens or basically numbers to be fed through the AI model
36:15
for training and inference. The problem is using these text tokens especially for extremely long text like if you want
36:22
to feed it an entire novel then it becomes very computationally expensive
36:27
very quickly like the amount of compute to process long text just shoots up exponentially. So the solution here that
36:34
DeepC proposed is instead of converting all this text into well text tokens,
36:40
what if we just like take a screenshot of like each page and send that through the AI model as an image? In other
36:46
words, what if we converted all of this into visual tokens instead and then just use like some optical recognition models
36:54
to basically process the data. Now this is a super technical scientific paper, but let me try to explain this in simple
37:01
terms. Here is their proposed architecture. Basically, instead of inputting text, you would take a
37:06
screenshot of whatever you want to input. So, it's basically an image and then it would break this down into
37:12
chunks at a time like this. And then these chunks will be fed through this deep encoder component. This consists of
37:20
this SAM or segment anything model. And this is in charge of basically focusing
37:25
on small local areas of the image to capture fine details. And then after
37:31
capturing the fine details, the data is then downsampled or basically compressed 16 times through this convolutional
37:38
neural network. And then afterwards we are left with these compressed vision tokens which then get fed through this
37:45
clip component which is focused on understanding the context and relationships across the entire image.
37:52
All right. So this is also called global attention. So again, SAM is focused on local attention or capturing the fine
37:59
details of the image whereas clip is focused on global attention or trying to
38:04
understand the context and relationships between everything in the image. And then after this deep encoder component,
38:10
the data is then fed through this very tiny deepseek 3 billion parameter model.
38:16
This is a mixture of experts model with only 570 million active parameters. And
38:21
this is basically used to decode these vision tokens back into text. So here
38:28
they give you a pretty complicated table on their results. But here's the takeaway. Now with this OCR model, you
38:35
can compress the text down into different ratios. And if you compress this with a 10x compression ratio, which
38:42
basically means if you have like a,000 text tokens, you would just convert it into 100 vision tokens. So it's 10 times
38:49
fewer tokens. If you do that, then the model achieves very high decoding precision at around 97%. So it's nearly
38:58
a 10 times lossless compression. And if you want to compress this further, so for example, if you want to compress it
39:04
20 times, going back to our example, if you have a,000 text tokens, you can compress this down into just 50 vision
39:12
tokens. Well, the precision can still approach 60%. which is not great, but
39:17
here it's kind of a trade-off between efficiency and accuracy. And here's the advantage of using this OCR instead of
39:24
just breaking text down into text tokens because we are essentially taking a screenshot and plugging image data
39:31
through the model. Well, this can also learn to analyze tables and diagrams and charts and other non-ext stuff. So, in
39:37
fact, here they've shown that DeepS OCR can indeed parse much more than just text. It can do images, charts, tables,
39:45
chemical formulas, math formulas, and even geometric figures. Plus, this also has multilingual recognition for almost
39:52
100 languages. This is a very long and technical paper, but in a nutshell, that is what DeepS OCR does. I think this
40:01
might trigger a shift in how we design our future AI models. Instead of using
40:06
text tokens, maybe vision tokens might be the next big thing. Anyways, as before, the awesome team at DeepSeek has
40:13
released everything already. So, if you click on this GitHub repo, this contains all the instructions on how you can
40:19
actually install and run DeepSseek OCR on your computer, so you can test it out
40:25
yourself. Anyways, if you're interested in reading further, I'll link to this main page in the description below. Next
SoftMimic
40:31
up, this new AI helps robots learn to move more safely and smoothly, just like
40:36
humans. So, it's called soft mimic and this basically takes a video of a human
40:43
moving as the input and it can apply this movement data to train these robots
40:49
to mimic this movement while being safe and adaptable. So, here's the thing. Traditional motion tracking methods are
40:56
pretty stiff. So, when you train robots using these traditional methods, the
41:01
movements are pretty awful. As you can see here, like if you get this robot to spread out his arms, it's not really
41:08
balancing very well. And if you get it to do the same thing next to this Lego tower, it ends up just breaking the
41:14
tower. And then here again, if you try to get it to pick up this box in this simulation, it can't really do this well
41:21
because the motion tracking data that it was trained on is very rigid and errorprone. But here's how soft mimic
41:29
works. It takes this human motion data, but then it also adds this technique called inverse kinematic solver, which
41:36
basically helps the robot learn to move in a more safe and compliant way. And this creates an improved data set of all
41:43
possible movements. And then this data set is used to train these robots in a simulation using reinforcement learning
41:50
techniques. So basically, the robot underos like tens of thousands of iterations of training using this data.
41:57
And as I'll show you in a second, you can also actually control the stiffness of these movements. And then from that,
42:04
these robots are able to adapt to new situations while being a lot more flexible and natural in their movements.
42:10
So here's an example where you can control the stiffness. On the left, the stiffness is a lot lower. So this allows
42:17
the robot to be moved around quite flexibly while maintaining balance. Notice how smooth its movements are,
42:24
even if the dude tries to like drag it down. And then on the right, the stiffness is increased a lot more. So
42:31
it's a lot harder to move the robot even when it's applied force. And so here is
42:36
the benefit of training this robot with this new more compliant data set. If you try to get the robot to spread its arms
42:44
out, now this robot won't actually break this Lego tower. And apparently it's also a lot better at picking up boxes
42:51
even of different dimensions. So notice the different widths of these boxes. But the robot is able to handle all of these
42:58
very well. Here are some other examples where you can get this robot to walk around while carrying objects or while
43:05
facing some obstructions, but it's still able to handle all these scenarios very well. In fact, it's kind of cute how it
43:11
kind of walks super cautiously like this, as if it's afraid to break anything. Here's another cool example
43:17
where on the left the stiffness is very high. So if you try to drag it down
43:23
while it's pouring liquid, then it's very jittery. It's spilling the contents everywhere. Whereas on the right, if you
43:30
decrease the stiffness, then even if you try to like pull the robot down, it doesn't really affect the liquid pouring
43:36
very much. Unfortunately, if you scroll up to the top of the page, they've only released a technical paper for now.
43:42
There's no indication whether they will release the code for this or not. But if you're interested in reading further,
43:48
I'll link to this main page in the description below. And that sums up all the highlights in AI this week. Let me
43:55
know in the comments what you think of all of this. Which piece of news was your favorite? And which tool are you
44:01
most looking forward to trying out? As always, I will be on the lookout for the
44:06
top AI news and tools to share with you. So, if you enjoyed this video, remember to like, share, subscribe, and stay
44:13
tuned for more content. Also, there's just so much happening in the world of AI every week. I can't possibly cover
44:19
everything on my YouTube channel. So, to really stay up to date with all that's going on in AI, be sure to subscribe to
44:27
my free weekly newsletter. The link to that will be in the description below. Thanks for watching and I'll see you in
44:33
the next one.