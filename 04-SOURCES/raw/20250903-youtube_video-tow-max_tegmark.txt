Max Tegmark Says Physics Just Swallowed AI
190,771 views  Sep 3, 2025  ✪ Members first on September 1, 2025  Theories of Everything with Curt Jaimungal
As a listener of TOE you can get a special 20% off discount to The Economist and all it has to offer! Visit https://www.economist.com/toe

MIT physicist Max Tegmark argues AI now belongs inside physics—and that consciousness will be next. He separates intelligence (goal-achieving behavior) from consciousness (subjective experience), sketches falsifiable experiments using brain-reading tech and rigorous theories (e.g., IIT/φ), and shows how ideas like Hopfield energy landscapes make memory “feel” like physics. We get into mechanistic interpretability (sparse autoencoders), number representations that snap into clean geometry, why RLHF mostly aligns behavior (not goals), and the stakes as AI progress accelerates from “underhyped” to civilization-shaping. It’s a masterclass on where mind, math, and machines collide.

Join My New Substack (Personal Writings): https://curtjaimungal.substack.com

Listen on Spotify:  https://open.spotify.com/show/4gL14b9...

Timestamps: 
00:00 - Why AI is the New Frontier of Physics
09:38 - Is Consciousness Just a Byproduct of Intelligence?
16:43 - A Falsifiable Theory of Consciousness? (The MEG Helmet Experiment)
27:34 - Beyond Neural Correlates: A New Paradigm for Scientific Inquiry
38:40 - Humanity: The Masters of Underestimation (Fermi's AI Analogy)
51:27 - What Are an AI's True Goals? (The Serial Killer Problem)
1:03:42 - Fermat's Principle, Entropy, and the Physics of Goals
1:15:52 - Eureka Moment: When an AI Discovered Geometry on Its Own
1:30:01 - Refuting the "AI Doomers": We Have More Agency Than We Think

Links mentioned:
Max’s Papers: https://scholar.google.com/citations?...
Language Models Use Trigonometry to Do Addition [Paper]: https://arxiv.org/abs/2502.00873
Generalization from Starvation [Paper]: https://arxiv.org/abs/2410.08255
Geoffrey Hinton [TOE]:    • Why The "Godfather of AI" Now Fears His Ow...  
Michael Levin [TOE]:    • Michael Levin: Consciousness, Biology, Uni...  
Iceberg of Consciousness [TOE]:    • The Complete Consciousness Iceberg | 2 Hou...  
Improved Measures of Integrated Information [Paper]: https://arxiv.org/abs/1601.02626
David Kaiser [TOE]:    • MIT Scientist's Discovery: "Black Holes Mi...  
Iain McGilchrist [TOE]:    • Matter and Mind: Rethinking Consciousness ...  
Elan Barenholtz & William Hahn [TOE]:    • Language Without Meaning: How LLMs Exposed...  
Daniel Schmachtenberger [TOE]:    • The Dark Side of Conscious Ai | Daniel Sch...  
Ted Jacobson [TOE]:    • The Physicist Who Proved Entropy = Gravity  
The “All Possible Paths” Myth [TOE]:    • Debunking the “All Possible Paths” Myth: W...  

Augmentation Lab at MIT Links:
X: https://x.com/auglab
Site: https://augmentationlab.org

SUPPORT:
Become a YouTube Member (Early Access Videos):
   / @theoriesofeverything  

Support me on Patreon:   / curtjaimungal   
Support me on Crypto: https://commerce.coinbase.com/checkou...
Support me on PayPal: https://www.paypal.com/donate?hosted_...

SOCIALS:
Twitter:   / toewithcurt  
Discord Invite:   / discord  

Guests do not pay to appear. Theories of Everything receives revenue solely from viewer donations, platform ads, and clearly labelled sponsors; no guest or associated entity has ever given compensation, directly or through intermediaries.

---

Why AI is the New Frontier of Physics
0:00
When Michael Faraday first proposed the idea  of the electromagnetic field, people were like,   “What are you talking about? You're saying there  is some stuff that exists, but you can't see it,  
0:10
you can't touch it. That sounds like total  non-scientific ghosts.” Most of my science   colleagues still feel that talking about  consciousness as science is just bullshit.  
0:19
But what I've noticed is when I push them a  little harder about why they think it's bullshit,   they split into two camps that are in complete  disagreement with each other. You can have  
0:26
intelligence without consciousness. And you  can have consciousness without intelligence.
0:33
Your brain is doing something remarkable right  now. It's turning these words into meaning.  
0:39
However, you have no idea how. Professor  Max Tegmark of MIT studies this puzzle.  
0:46
You recognize faces instantly, yet you can't  explain the unconscious processes. You dream  
0:52
full of consciousness while you're outwardly  not doing anything. Thus, there's intelligence  
0:57
without consciousness and consciousness  without intelligence. In other words,  
1:02
they're different phenomena entirely.  Tegmark proposes something radical.  
1:07
Consciousness is testable in a new extension  to science where you become the judge of your  
1:13
own subjective experience. Physics absorbed  electromagnetism and then atoms and then space,  
1:20
and now Tegmark says it's swallowing AI. In  fact, I spoke to Nobel Prize winner Geoffrey  
1:25
Hinton about this specifically. Now to Max  Tegmark, the same principle that explains   why light bends in water may actually  explain how thoughts emerge from neurons.
1:35
I was honored to have been invited to the  Augmentation Lab Summit, which was a weekend of   events at MIT last week. This was hosted by MIT  researcher Dunya Baradari. The summit featured  
1:47
talks on the future of biological and artificial  intelligence, brain-computer interfaces,   and included speakers such as Stephen Wolfram and  Andres Gomez-Emilsson. My conversations with them  
1:57
will be released on this channel in a couple  weeks, so subscribe to get notified. Or you   can check the Substack, curtjaimungal.com,  as I release episodes early over there.
2:08
A special thank you to our advertising sponsor,  The Economist. Among weekly global affairs  
2:13
magazines, The Economist is praised for its  nonpartisan reporting and being fact-driven.  
2:19
This is something that's extremely important  to me. It's something that I appreciate. I   personally love their coverage of other topics  that aren't just politics as well. For instance,  
2:29
The Economist has a new tab for artificial  intelligence on their website and they have   a fantastic article on the recent DESI dark  energy survey. It surpasses, in my opinion,  
2:39
Scientific American's coverage. Something else I  love, since I have ADHD, is that they allow you  
2:45
to listen to articles at 2x speed, and it's  from an actual person, not a dubbed voice.  
2:51
The British accents are a bonus. So if you're  passionate about expanding your knowledge and  
2:56
gaining a deeper understanding of the forces that  shape our world, I highly recommend subscribing  
3:01
to The Economist. It's an investment into your  intellectual growth, one that you won't regret.  
3:06
I don't regret it. As a listener of TOE, you  get a special discount. Now you can enjoy  
3:11
The Economist and all it has to offer for less.  Head over to their website, economist.com/TOE to  
3:20
get started. Make sure you use that link. That's  www.economist.com/TOE to get that discount. Thanks  
3:29
for tuning in. And now back to the explorations  of the mysteries of the universe with Max Tegmark.
3:37
Max, is AI physics? There was a Nobel Prize  awarded for that. What are your views?
3:44
I believe that artificial intelligence has gone  from being not physics to being physics. Actually,  
3:51
one of the best ways to insult a physicist is  to tell them that their work isn't physics,  
3:56
as if somehow there's a generally agreed on  boundary between what's physics and what's not,  
4:02
or between what's science and what's not. But  I find the most obvious lesson we get if we  
4:08
just look at the history of science is that  the boundary has evolved. Some things that   used to be considered scientific by some,  like astrology, has left. The boundary has  
4:20
contracted so that's not considered science  now. And then a lot of other things that were  
4:25
pooh-poohed as being non-scientific  are now considered obviously science. Like, I sometimes teach the electromagnetism  course and I remind my students that when  
4:37
Michael Faraday first proposed the  idea of the electromagnetic field,  
4:42
people were like, “What are you talking about?  You're saying there is some stuff that exists,  
4:50
but you can't see it. You can't touch it. That  sounds like ghosts, like total non-scientific  
4:56
bullshit.” And they really gave him a hard  time for that. And the irony is not only is  
5:01
that considered part of physics now, but you can  see the electromagnetic field. It's, in fact,  
5:07
the only thing we can see, because light  is an electromagnetic wave. And after that,   things like black holes, things like atoms,  which Max Planck famously said is not physics,  
5:19
even talk about what our universe was doing 13.8  billion years ago, have become considered part of  
5:24
physics. And I think AI is now going the same  way. I think that's part of the reason that  
5:30
Geoff Hinton got the Nobel Prize in Physics,  because what is physics? To me, physics is  
5:40
all about looking at some complex, interesting  system, doing something and trying to figure out  
5:45
how it works. We started on things like the solar  system and atoms. But if you look at an artificial  
5:54
neural network that can translate French  into Japanese, that's pretty impressive too.
6:00
And there's this whole field that started  blossoming now that I also had a lot of  
6:07
fun working on called mechanistic  interpretability, where you study  
6:13
an intelligent artificial system to try to ask  these basic questions like, “How does it work?  
6:19
Are there some equations that describe it?  Are there some basic mechanisms?” and so on.  
6:26
In a way I think of traditional physics like  astrophysics, for example, as just mechanistic  
6:31
interpretability applied to the universe. And  Hopfield, who also got the Nobel Prize last year,  
6:39
he was the first person to show that, “Hey,  you know, you can actually write down an energy  
6:45
landscape.” You know, put potential energy on  the vertical axis, is how the potential energy  
6:51
depends on where you are, and think of each  little valley as a memory. You might wonder,  
6:59
how the heck can I store information in an  egg carton, say? If it has 25 valleys in it,  
7:07
well very easy. You can put the marble in one  of them and now that's log 25 bits right there.  
7:15
And how do you retrieve what the memory  is? You can look where the marble is.
7:22
And Hopfield had this amazing physics insight.  If you think of there as being any system whose  
7:32
potential energy function has many, many, many  different minima that are pretty stable, you can  
7:37
just use it to store information. But he realized  that that's different from the way computer  
7:42
scientists used to store information. It used to  be like the whole von Neumann paradigm, you know,   with a computer. You're like, “Tell me what's in  this variable. Tell me what number is sitting in  
7:51
this particular address.” You go look here. Right.  That's how traditional computers store things. But  
8:00
if I say to you, “Twinkle, twinkle...” “Little  star.” Yeah, that's a different kind of memory  
8:07
retrieval, right? I didn't tell you, “Hey, give  me the information that's stored in those neurons   over there.” I gave you something which was sort  of partial, part of the stuff, and you filled it  
8:19
in. This is called associative memory. And this  is also how Google will give you something. You  
8:24
can type something in that you don't quite  remember, and it'll give you the right thing. And Hopfield showed, coming back to the egg  carton, that if you don't remember exactly…  
8:37
Suppose you want to memorize the digits of  pi, and you have an energy function where  
8:43
the actual minimum is at exactly 3.14159,  etc. But you don't remember exactly what  
8:48
pi is. “Three something.” Yes. So you put  a marble at three, you let it roll. As long  
8:55
as it's in the basin of attraction whose  minimum is at pi, it's going to go there.  
9:01
So to me this is an example of how something  that felt like it had nothing to do with physics,  
9:07
like memory, can be beautifully understood with  tools from physics. You have an energy landscape,  
9:12
you have different minima, you have  dynamics—the Hopfield network. So I think,  
9:19
yeah, it's totally fair that Hinton and  Hopfield got a Nobel Prize in Physics,   and it's because we're beginning to understand  that we can expand again the domain of what is  
9:31
physics to include these very deep questions  about intelligence, memory, and computation.
Is Consciousness Just a Byproduct of Intelligence?
9:38
What about consciousness? So you mentioned that  Faraday with an electromagnetic field, that was  
9:44
considered to be unsubstantiated, unfalsifiable  nonsense, or just ill-defined. Consciousness seems  
9:51
to be at a similar stage where many scientists  or many physicists tend to look at the way   that consciousness studies, or consciousness is  studied, or consciousness is talked about. Well,  
10:00
firstly, what's the definition of consciousness?  You all can't agree. There's phenomenal,   there's access, etc. And then, even there, what  is it? And then the critique would be, well,  
10:10
you're asking me for a third-person definition  of something that's a first-person phenomenon. Yeah. Okay, so how do you  view consciousness in this?
10:16
Yeah, I love these questions. I feel that  consciousness is actually probably the final  
10:24
frontier, the final thing which is going to end up  ultimately in the domain of physics, that is right  
10:31
now on the controversial borderline. So  let's go back to Galileo, say. Right?  
10:42
If he dropped a grape and a hazelnut, he could  predict exactly when they were going to hit the  
10:48
ground and how far they fell would grow as a  parabola as a function of time. But he had no  
10:55
clue why the grape was green and the hazelnut  was brown. Then came Maxwell's equations and we  
11:01
started to understand that light and colors is  also physics, and we got equations for it. And  
11:07
we couldn't figure out, Galileo either, why  the grape was soft and why the hazelnut was   hard. Then we got quantum mechanics and we  realized that all these properties of stuff  
11:17
could be calculated from the Schrödinger  equation and also brought into physics.   And then we started, and then intelligence  seemed like such a holdout. But we already  
11:27
talked about now how if you start breaking it  into components like memory—and we can talk more  
11:34
about computation and learning—how that can also  very much be understood as a physical process.
11:43
So what about consciousness? Yeah, so I'd say  most of my science colleagues still feel that  
11:52
talking about consciousness as science  is just bullshit. But what I've noticed  
11:58
is when I push them a little harder about why  they think it's bullshit, they split into two   camps that are in complete disagreement with  each other. Half of them, roughly, will say,  
12:06
“Oh, consciousness is bullshit because it's  just the same thing as intelligence.” And the  
12:12
other half will say, “Consciousness is bullshit  because obviously machines can't be conscious,”  
12:20
which is obviously totally inconsistent with  saying it's the same thing as intelligence.
12:27
What really powered the AI revolution in recent  decades is just moving away from philosophical  
12:35
quibbles about what does intelligence really mean  in a deep philosophical sense and instead making a  
12:41
list of tasks and saying, “Can my machine do  this task? Can it do this task?” And that's   quantitative. You can train your systems to get  better at the task. And I think you'd have a very  
12:51
hard time if you went to the NeurIPS conference  and argued that machines can never be intelligent,  
12:56
right? So if you take that, if you then say  intelligence is the same as consciousness,  
13:05
you're predicting that machines are conscious  if they're smart. But we know that consciousness  
13:13
is not the same as intelligence just by some  very simple introspection we can do right now. So for example, what does it say  here? I guess we shouldn't do product…
13:24
No, I don't mind. Let's do  this one. What does it say? It says, "Towards more interpretable AI with  sparse autoencoders by Joshua Engels." Great.  
13:32
This is a PhD thesis of my student, Josh Engels,  or a master's thesis. So, how did you do that  
13:37
computation? Thirty years ago, if I gave you  just a bunch of numbers that are the actual red,  
13:46
green, and blue strengths of the pixels  in this, and asked you what does it say?  
13:52
People didn't… this is a hard problem. Even  harder is if you just open your eyes and ask,   “Who is this?” and you say, “It's Max.” Right?  But you can do it like this. But think about  
14:03
how it felt. Do you actually feel that if  you open your eyes and you see this is Max,  
14:09
that you know the algorithm that you used to  recognize my face? No. Same here. And for me,  
14:15
it's pretty obvious it feels like my  consciousness… there's some part of my  
14:21
information processing that is conscious,  and it's kind of got an email from the   face recognition module saying, you know, “Face  recognition complete, the answer is so and so.”
14:34
So in other words, you do something when you  recognize people that's quite intelligent but  
14:39
not conscious, right? And I would say actually  a large fraction of what your brain does,   you're just not conscious about.  You find out about the results  
14:47
of it often after the fact. So you can  have intelligence without consciousness,  
14:55
that's the first point I'm making. And second,  you can have consciousness without intelligence,  
15:01
without accomplishing any tasks. Like,  did you have any dreams last night? None that I remember. But have you  ever had a dream that you remember?
15:09
Yeah, so there was consciousness there. If  someone was just looking at you lying there  
15:14
in the bed, you probably weren't accomplishing  anything, right? So I think it's obvious that  
15:19
consciousness is not the same. You can have  consciousness without intelligence and vice   versa. So those who say that consciousness  equals intelligence are being sloppy.
15:33
Now, what is it then? My guess is that  consciousness is a particular type of  
15:42
information processing and that intelligence is  also a typical type of information processing,  
15:50
but that there's a Venn diagram like this. There's  some things that are intelligent and conscious,  
15:55
some are intelligent but not conscious, and some  of them are conscious but not very intelligent.   And so the question then  becomes to try to understand,  
16:03
can we write down some equations or formulate  some principles for what kind of information  
16:08
processing is intelligent and what kind  of information processing is conscious?
16:15
And I think my guess is that for something  to be conscious there are at least some  
16:24
sufficient conditions that it probably  has to have. There has to be information,  
16:29
a lot of information there, something to  be the content of consciousness, right?  
A Falsifiable Theory of Consciousness? (The MEG Helmet Experiment)
16:43
There's an Italian scientist, Giulio  Tononi, who has put a lot of creative  
16:49
thought into this and triggered enormous  controversy also, who argues that one  
16:58
necessary condition for consciousness  is what he calls integration. Basically,  
17:05
that if it's going to subjectively feel like a  unified consciousness, like your consciousness,  
17:10
it cannot consist of two information  processing systems that don't communicate  
17:15
with each other. Because if consciousness is the  way information feels when it's being processed,  
17:22
right? Then if this is the information that's  conscious and it's just completely disconnected   from this information, there's no way that this  information can be part of what it's conscious of.
17:31
Just a quick question. Ultimately,   what's the difference between information  processing, computing, and communication?
17:42
So communication, I would say, is just a  very simple special case of information   processing. You have some information here and  you make a copy of it, it ends up over there.
17:50
It's a volleyball you send over. Yeah, a volleyball you send over. Copy this  to that. Yeah. But computation can be much  
17:56
more complex than that. And then the… So that was  information processing, communication, and what  
18:07
was the third word? Computation. And yeah,  so computation and information processing  
18:12
I would say is more or less the same thing.  Then you can try to classify different kinds  
18:18
of information processing depending on how  complex it is, and mathematicians have been  
18:23
doing an amazing job there, even though they  still don't know whether P equals NP and so on.
18:29
But just coming back to consciousness again, I  think a mistake many people make when they think  
18:34
about their own consciousness is like, can you  see the beautiful sunlight coming in here from   the window and some colors and so on, right? It's  to have this model that somehow you're actually  
18:50
conscious of that stuff, that the content of  your consciousness somehow is the outside world.  
18:57
I think that's clearly wrong because you can  experience those things when your eyes are closed,   when you're dreaming, right? So I think the  conscious experience is intrinsic to the  
19:07
information processing itself. What you are  actually conscious about when you look at me   isn't me, it's your world model that you have  and the model you have in your head right now  
19:17
of me. And you can be conscious of that whether  you're awake or whether you're asleep. And then,   of course, you're using your senses and all sorts  of analysis tools to constantly update your inner  
19:28
world model to match relevant parts of what's  outside. And that's what you're conscious of.
19:37
So what Tononi is saying is that the information  processing has to be such that there's no way  
19:53
that it isn't actually just secretly two  separate parts that don't communicate at   all and cannot communicate with each other,  because then they would basically be like two  
20:02
parallel universes that were just unaware of  each other, and you wouldn't be able to have   this feeling that it's all unified. I actually  think that's a very reasonable criteria. And he  
20:12
has a particular formula he calls *phi* for  measuring how integrated things are, and the  
20:20
things that have a high *phi* are more conscious.  I wasn't completely sure whether that was the only  
20:26
formula that had that property. So I wrote a  paper once to classify all possible formulas  
20:31
that have that property. And it turned out there  was less than a hundred of them. So I think it's   actually quite interesting to test if any of the  other ones fit the experiments better than his.
20:45
Just to finish up on why people say  consciousness is bullshit, though,   I think ultimately the main reason is either they  feel it sounds too philosophical or they say, “Oh,  
20:56
you can never test consciousness theories because  how can you test if I'm conscious or not when all  
21:01
you can observe is my behavior?” Right? But here  is a misunderstanding. I'm much more optimistic.  
21:09
Can I tell you about an experiment I envision  where you can test the consciousness theory?
21:15
Of course. So suppose you have someone like Giulio Tononi  or anyone else who has really stuck their neck  
21:23
out and written down a formula for what kind of  information processing is conscious. And suppose  
21:29
we put you in one of our MEG machines here  at MIT or some future scanner that can read  
21:37
out a massive amount of your neural data in real  time, and you connect that to this computer that  
21:46
uses that theory to make predictions about what  you're conscious of. Okay? And then now it says,  
21:57
“I predict that you're consciously aware of a  water bottle.” And you're like, “Yeah, that's   true.” Yes, theory. And then it says, “Okay,  now I predict that you're… I see information  
22:09
processing there about regulating your pulse and  I predict that you're consciously aware of your   heartbeat.” You're like, “No, I'm not.” You've now  ruled out that theory, actually, right? It made a  
22:21
prediction about your subjective experience  and you yourself can falsify that, right?
22:28
So first of all, it is possible for you to  rule out the theory to your satisfaction.  
22:36
That might not convince me because you told  me that you weren't aware of your heartbeat.   Maybe I think you're lying or whatever.  But then you can go, “Okay, hey Max,  
22:45
why don't you try this experiment?” And I  put on my MEG helmet and I work with this.  
22:52
And then it starts making some incorrect  assumptions about what I'm experiencing.   And I'm now also convinced that it's ruled  out. It's a little bit different from how  
23:00
we usually rule out theories. But at the  end of the day, anyone who cares about   this can be convinced that this theory sucks and  belongs on the garbage dump of history, right?
23:09
And conversely, suppose that this theory  just again and again and again and again  
23:16
keeps predicting exactly what you're  conscious of and never anything that   you're not conscious about. You would  gradually start getting kind of impressed,  
23:22
I think. And if you moreover read about  what goes into this theory and you say,  
23:29
“Wow, this is a beautiful formula and it kind  of philosophically makes sense that these are   the criteria that consciousness should  have,” and so on, you might be tempted  
23:37
now to try to extrapolate and wonder if  it works also on other biological animals,  
23:45
maybe even on computers and so on. And this is not  altogether different from how we've dealt with,  
23:51
for example, general relativity, right? So you  might say you can never… it's bullshit to talk  
23:59
about what happens inside black holes because you  can't go there and check and then come back and  
24:04
tell your friends or publish your findings in  Physical Review Letters, right? But what we're  
24:12
actually testing is not some philosophical  ideas about black holes. We're testing a  
24:18
mathematical theory, general relativity, and I  have it there in a frame by my window, right?
24:24
And so what's happened is we tested  it on the perihelion shift of Mercury,   how it's not really going in an ellipse but the  ellipse is precessing a bit. We tested it and it  
24:34
worked. We tested it on how gravity bends light,  and then we extrapolated it to all sorts of stuff  
24:39
way beyond what Einstein had thought about, like  what would happen when our universe was a billion  
24:49
times smaller in volume and what would happen when  black holes get really close to each other and  
24:55
give off gravitational waves. And it just passed  all these tests also. So that gave us a lot of  
25:00
confidence in the theory and therefore also in the  predictions that we haven't been able to test yet,  
25:09
even the predictions we can never test,  like what happens inside black holes. So now, this is typical for science,  really. If someone says, “I like Einstein,  
25:20
I like what it did for predicting gravity in  our solar system, but I'm going to opt out of  
25:29
the black hole prediction,” you can't do that.  It's not like, “Oh, I want coffee, but decaf.”  
25:36
If you're going to buy the theory, you need to buy  all its predictions, not just the ones you like.  
25:41
And if you don't like the predictions, well, come  up with an alternative to general relativity,   write down the math, and then make sure that it  correctly predicts all the things we can test. And  
25:52
good luck, because some of the smartest humans on  the planet have spent 100 years trying and failed,  
25:58
right? So if we have a theory of consciousness  in the same vein, right, which correctly predicts  
26:04
the subjective experience on whoever puts  on this device and tests predictions for   what they are conscious about and it keeps  working, I think people will start taking  
26:13
pretty seriously also what it predicts about  coma patients who seem to be unresponsive,  
26:21
whether they're having locked-in syndrome  or in a coma, and even what it predicts   about machine consciousness, whether machines are  suffering or not. And people who don't like that,  
26:30
they will then be incentivized to work harder  to come up with an alternative theory that   at least predicts subjective experience. So  this was my... I'll get off my soapbox now,  
26:41
but this is why I strongly disagree with people  who say that consciousness is all bullshit. I  
26:47
think there's actually more saying that because  there's an excuse to be lazy and not work on it.
26:52
Hi, everyone. Hope you're enjoying  today's episode. If you're hungry for   deeper dives into physics, AI, consciousness,  philosophy, along with my personal reflections,  
27:02
you'll find it all on my Substack. Subscribers get  first access to new episodes, new posts as well,  
27:08
behind the scenes insights, and the chance  to be a part of a thriving community of   like-minded pilgrimers. By joining, you'll  directly be supporting my work and helping  
27:17
keep these conversations at the cutting edge.  So click the link on screen here, hit subscribe,  
27:23
and let's keep pushing the boundaries of  knowledge together. Thank you and enjoy the show. Just so you know, if you're listening, it's  C-U-R-T-J-A-I-M-U-N-G-A-L.org. CurtJaimungal.org.
Beyond Neural Correlates: A New Paradigm for Scientific Inquiry
27:35
So in the experiment where you put some probes  on your brain in order to discern which neurons  
27:40
are firing or what have you, so that would be a  neural correlate. I'm sure you've already thought   of this. So you're correlating some neural  pattern with the bottle. And you're saying,  
27:51
“Hey, okay, I think you're experiencing a  bottle.” But then technically are we actually   testing consciousness or testing the further  correlation that it tends to be that when I  
28:01
ask you the question, “Are you experiencing  a bottle?” and we see this neural pattern,  
28:06
that that's correlated with you saying yes.  So it's still another correlation, is it not?
28:11
Well, but you're not trying to convince me when  the experiment is being done on you. You're not   trying to convince me. It's just you talking  to the computer. You are just doing experiments  
28:21
basically on the theory. There's no one else  involved, no other human. And you're just   trying to convince yourself. So you sit there  and you have all sorts of thoughts. You might  
28:30
just decide to close your eyes and think about  your favorite place in Toronto to see if it can  
28:36
predict that you're conscious of that, right?  And then you might also do something else which   you know you can do unconsciously and see if  you can trick it into predicting that you're  
28:48
conscious of that information that you know is  being processed in your brain. So ultimately,   you're just trying to convince yourself that  the theory is making incorrect predictions.
28:57
I guess what I'm asking is in this  case, I can see being convinced that   it can read my mind in the sense that it  can roughly determine what I'm seeing. But  
29:04
I don't see how that would tell this other  system that I'm conscious of that. In the   same way that we can see what files are on a  computer doesn't mean that those files are,  
29:13
or when we do some cut and paste,  we can see some process happening. Well, you're not trying to convince the  computer. The computer is coded up to just  
29:22
make the predictions from this putative theory  of consciousness, this mathematical theory,   right? And then your job is just to see, are those  the wrong equations or the right equations? And  
29:32
the way you'd ascertain that is to see whether  it correctly or incorrectly predicts what you're   actually subjectively aware of. We should be  clear that we're defining consciousness here  
29:42
just simply as subjective experience, right?  Which is very different from talking about what  
29:48
information is in your brain. Like, you have all  sorts of memories in your brain right now that   you haven't probably thought about for months. And  that's not your subjective experience right now.  
29:59
And even again, when I open my eyes and I see a  person, and there's a computation happening to  
30:08
figure out exactly who they are, there's also the  detailed information in there, probably about some   angles about their ears and stuff, which I'm  not conscious about at all. Okay. And if the  
30:17
machine incorrectly says that I'm conscious  about that, again, the theory has failed. So it's quite hard. It's like if you look at  my messy desk or I show you a huge amount of  
30:26
information in your brain or in this book.  And suppose there's some small subset of  
30:32
this which is highlighted in yellow. Yes.  And you have to have a computer that can   predict exactly what's highlighted in  yellow. It's pretty impressive if it  
30:39
gets it right. And in the same way, if  it can accurately predict exactly which  
30:45
information in your brain is actually  stuff that you're subjectively aware of. Okay, so let me see if I understand  this. So in the global workspace theory,  
30:52
you have like a small desktop and pages are  being sent to the desktop, but only a small   amount at any given time. I know that there's  another metaphor of a spotlight, but whatever,  
31:00
let's just think of that. So this desktop is  quite small relative to the globe. Yeah. Okay,  
31:06
relative to the city, relative to the globe  for sure. So our brain is akin to this globe   because there's so many different connections,  there's so many different words that there could  
31:13
possibly be. Yeah. If there's some theory  that can say, “Hey, this little thumbtack  
31:19
is what you're experiencing.” And you're  like, “Actually, that is correct.” Okay. Exactly. So the global workspace theory,  great stuff, but it is not sufficiently  
31:29
predictive to do this experiment. It doesn't  have a lot of equations in it, mostly words,   right? So we don't have... no one has actually  done this experiment yet. I would love for  
31:41
someone to do it, where you have a theory  that's sufficiently physical, mathematical,   that it can actually stick its neck out  and risk being proven false all the time.
31:52
I guess what I was saying, just  to wrap this up, is that yes,   that is extremely impressive. I don't  even know if that can technologically be  
32:00
done. Maybe it can be approximately done. But  regardless, we can for sure falsify theories.   But it still wouldn't suggest to an outside  observer that this little square patch here,  
32:11
or whoever is experiencing this square patch,  is indeed experiencing the square patch.
32:17
But you already know that you're  experiencing this square patch. I know. Yes, that's the key thing. You know it. I don't  know it. I don't know that you know. But you  
32:28
can convince yourself that this theory is false  or that this theory is increasingly promising,  
32:34
right? That's the catch. And I  just want to stress, you know,   people sometimes say to me that you can never  prove for sure that something is conscious. We  
32:44
can never prove anything with physics. A little  dark secret, but we can never prove anything. We  
32:51
can't prove that general relativity is correct.  You know, probably it's wrong. Probably it's  
32:56
just a really good approximation. All we ever  do in physics is we disprove theories. But if,  
33:04
as in the case of general relativity, some of the  smartest people on Earth have spent over a century   trying to disprove something and they still have  failed, we start to take it pretty seriously and  
33:14
start to say, well, you know, it might be wrong,  but we're going to take it pretty seriously as  
33:20
a really good approximation, at least, for what's  actually going on. That's how it works in physics,   and that's the best we can ever get with  consciousness also. Something which people  
33:29
have which is making strong predictions,  and which we've, despite trying really hard,  
33:36
have failed to falsify. So we start to earn  our respect. We start taking it more seriously.
33:42
You said something interesting. Look, we can tell,   or you can tell you. You can tell this  theory of consciousness is correct for you,  
33:49
or you can convince yourself. This is super  interesting because earlier in the conversation,   we're talking about physics, what was considered  physics and what is no longer considered physics.  
33:57
So what is this amorphous boundary? Or  maybe it's not amorphous, but it changes. Yeah, it absolutely changes.
34:02
Do you think that's also the case for science?  Do you think science, to incorporate a scientific  
34:08
view of consciousness, quote unquote, is going to  have to change what it considers to be science? I'm a big fan of Karl Popper. I think I personally  consider things scientific if we can falsify them.  
34:23
If there's at least, if no one can even think of  a way in which we could even conceptually in the  
34:29
future with arbitrary funding and technology  test it, I would say it's not science.
34:35
I think Popper didn't say if it can be  falsified then it's science. It's more   that if it can't be falsified it's not science.
34:41
I'll agree with that. I'll agree with that also  for sure. But what I'm saying is consciousness  
34:46
is… a theory of consciousness that's willing to  actually make concrete predictions about what  
34:53
you personally, subjectively experience cannot be  dismissed like that because you can falsify it.  
34:59
If it predicts just one thing that's wrong, then  you falsify it. And I would encourage people to  
35:07
stop wasting time on philosophical excuses for  being lazy and try to build these experiments.   That's what I think we should. And we saw this  happen with intelligence. People had so many  
35:18
quibbles about, “Oh, I don't know how to define  intelligence,” and whatever. And in the meantime,   you got a bunch of people who started  rolling up their sleeves and saying, “Well,  
35:26
can you build a machine that beats the best human  in chess? Can you build a machine that translates  
35:32
Chinese into French? Can you build a machine  that figures out how to fold proteins?” And  
35:40
amazingly, all of those things have  now been done, right? And what that's  
35:45
effectively done is just made people redefine  intelligence as ability to accomplish tasks,  
35:53
ability to accomplish goals. That's what  people in machine learning will say if you   ask them what they mean by intelligence. And the  ability to accomplish goals is different from  
36:05
having a subjective experience. The first I call  intelligence, the second I call consciousness.
36:12
And it's just getting a little philosophical  here. It's quite striking throughout the history  
36:18
of physics how oftentimes we've vastly  delayed physics breakthroughs just by   some curmudgeons convincingly arguing that it's  impossible to make this scientific. For example,  
36:30
extrasolar planets. People were so stuck with  this idea that all the other solar systems had  
36:36
to be like our solar system, with a star and  then some small rocky planets near it and some  
36:42
gas giants farther out. So they were like,  “Yeah, no point in even looking around other   stars because we can't see Earth-like planets.”  Eventually, some folks decided to just look  
36:54
anyway with the Doppler method to see if stars  were going in little circles because something   was orbiting around. And they found these hot  Jupiters, like a gigantic Jupiter-sized thing  
37:04
going closer to the star than Mercury is going  to our sun. Wow. But they could have done that  
37:10
10 years earlier if they hadn't been intimidated  by these curmudgeons who said, “Don't look.”
37:17
So my attitude is, don't listen to the  curmudgeons. If you have an idea for an experiment  
37:24
you can build that's just going to cut into some  new part of parameter space and experimentally  
37:30
test the kind of questions that have never been  asked, just do it. More than half the time when  
37:38
people have done that, there was a revolution.  When Karl Jansky wanted to build the first X-ray  
37:46
telescope and look at X-rays from the sky, for  example, people said, “What a loser. There are  
37:51
no X-rays coming from the sky. What do you think,  there are dentists out there?” I don't know what.  
37:57
And then he found that there is a massive amount  of X-rays even coming from the sun. Or people  
38:04
decided to look at them… basically whenever we  open up another wavelength with telescopes, we've  
38:10
seen new phenomena we didn't even know existed. Or  when van Leeuwenhoek built the first microscope,  
38:15
do you think he expected to find these animals  that were so tiny you couldn't see them with  
38:22
the naked eye? Of course not, right? But he  basically went orders of magnitude in a new  
38:32
direction in an experimental parameter space  and there was a whole new world there, right?
38:38
So this is what I think we should do with  consciousness and with intelligence. This  
Humanity: The Masters of Underestimation (Fermi's AI Analogy)
38:44
is exactly what has happened. If we segue  a little bit into that topic, I think  
38:53
there's too much pessimism in science. If you go  back, I don't know, 30,000 years ago, if you and  
39:03
I were living in a cave sitting and having this  conversation, we would probably have figured,  
39:14
“Well, you know, look at those little white dots  in the sky. They're pretty nifty.” We wouldn't   have any Netflix to distract us with. But we  would know that some of our friends had come up  
39:23
with some cool myths for what these dots in the  sky were. And, “Oh, look, that one maybe looks  
39:29
like an archer,” or whatever. But since you're a  guy who likes to think hard, you'd probably have a  
39:36
little bit of a melancholy tinge that we're never  really going to know what they are. You can't jump  
39:43
up and reach them. You can climb the highest tree  and they're just as far away. And we're kind of   stuck here on our planet. And maybe we'll starve  to death. And 50,000 years from now, if there are  
39:54
still people, life for them is going to be more or  less like it is for ours. And boy, oh boy, would   we have been too pessimistic. We hadn't realized  that we were the masters of underestimation.
40:06
We massively underestimated not only the size  of what existed—everything we knew of was just  
40:18
a small part of this giant spinning ball, Earth,  which was in turn just a small part of a grander  
40:23
structure of the solar system, part of a galaxy,  part of a galaxy cluster, part of a supercluster,  
40:28
part of a universe, maybe part of a hierarchy  of parallel universes—but more importantly,   we also underestimated the power of our own minds  to figure stuff out. And we didn't even have to  
40:43
fly to the stars to figure out what they were. We  just really kind of had to let our minds fly. And,  
40:50
you know, Aristarchus of Samos, over 2,000 years  ago, was looking at a lunar eclipse. And some of  
40:57
his friends were probably like, “Oh, this moon  turned red, it probably means we're all going to  
41:04
die,” or, “an omen from the gods.” And he's like,  “Hmm, the moon is there, sun just set over there,  
41:14
so this is obviously Earth's shadow being cast  on the moon. And actually, the edge of the shadow  
41:20
of Earth is not straight, it's curved. Wait, so  we're living on a curved thing? We may be living  
41:27
on a ball, huh? And wait a minute, the curvature  of Earth's shadow there is clearly showing that  
41:35
Earth is much bigger than the moon is.” And he  went down and calculated how much bigger Earth is  
41:40
than the moon. And then he was like, “Okay, well I  know that Earth is about 40,000 kilometers because  
41:51
I read that Eratosthenes had figured that out. And  I know the moon, I can cover it with my pinky, so  
41:58
it's like half a degree in size, so I can figure  out what the actual physical size is of the moon.”
42:03
It was ideas like this that started breaking  this curse of overdone pessimism. We started  
42:15
to believe in ourselves a little bit more.  And here we are now with the internet,  
42:23
with artificial intelligence, with all these  little things you can eat that prevent you  
42:29
from dying of pneumonia. My grandfather, Sven,  died of a stupid kidney infection, could have  
42:36
been treated with penicillin. It's amazing how  much excessive pessimism there's been. And I  
42:46
think we still have a lot of it, unfortunately.  That's why I want to come back to this thing  
42:51
that… there's no better way to fail at something  than to convince yourself that it's impossible.
43:03
And look at AI. I would say whenever with science  we have started to understand how something in  
43:14
nature works that we previously thought of as sort  of magic, like what causes the winds, the seasons,  
43:19
et cetera, what causes things to move, we  were able to historically transform that into  
43:25
technology that often did this better and could  serve us more. So we figured out how we could  
43:32
build machines that were stronger than us and  faster than us. We got the industrial revolution.  
43:37
We're now figuring out that thinking is also  a physical process: information processing,  
43:44
computation. And Alan Turing was of course  one of the real pioneers in this field. And  
44:04
he clearly realized that the brain is a biological  computer. He didn't know how the brain worked,   we still don't exactly, but it was very clear  to him that we could probably build something  
44:15
that was much more intelligent, and maybe more  conscious too, once we figured out more details.
44:21
I would say from the 50s when the term AI was  coined not far from here at Dartmouth, the field  
44:31
has been chronically overhyped. Most progress has  gone way slower than people predicted, even than  
44:38
McCarthy and Minsky predicted for that Dartmouth  workshop and so on. But then something changed  
44:43
about four years ago when it went from being  overhyped to being underhyped. Because I remember   very vividly, like seven years ago, six years ago,  most of my colleagues here at MIT and most of my  
44:53
AI colleagues in general were pretty convinced  that we were decades away from passing the Turing  
44:58
test. Decades away from building machines that  could master language and knowledge at a human  
45:06
level. And they were all wrong. They were way too  pessimistic because it already happened. You can  
45:14
quibble about whether it happened with ChatGPT-4  or when it was exactly, but it's pretty clear   it's in the past now. So if people could be so  wrong about that, maybe they were wrong about  
45:27
more. And sure enough, since then, AI has gone  from being kind of high school level, to kind of  
45:35
college level, to in many areas being PhD level,  to professor level, to even far beyond that in  
45:43
many areas in just four short years. So prediction  after prediction has been crushed now where things  
45:50
have happened faster. So I think we have gone from  the overhyped regime to the underhyped regime.
45:56
And this is, of course, the reason why so many  people now are talking about maybe we'll reach   broadly human level in a couple years or five  years, depending on which tech CEO you talk  
46:06
to or which professor you talk to. But it's very  hard now for me to find anyone serious who thinks  
46:13
we're 100 years away from it. And then, of course,  you have to think about, go back and reread your  
46:21
Turing, right? So he said in 1951 that once we  get machines that are vastly smarter than us in  
46:31
every way, they can basically perform better than  us on all cognitive tasks. The default outcome  
46:37
is that they're going to take control, and from  there on, Earth will be run by them, not by us,   just like we took over from other apes. And I.  J. Good pointed out in the 60s that that last  
46:52
sprint from being kind of roughly a little bit  better than us to being way better than us can   go very fast because as soon as we can replace the  human AI researchers by machines who don't have to  
47:02
sleep and eat and can think a hundred times faster  and can copy all their knowledge to the others,  
47:11
every doubling in quality from then on might not  take months or years like it is now, but the sort  
47:17
of human R&D timescale. It might happen every  day or on the timescale of hours or something.
47:23
And we would get this sigmoid ultimately where  we shift away from the sort of slow exponential  
47:32
progress that technology has had ever since  the dawn of civilization, where you use today's   technology to build tomorrow's technology which  is so many percent better, to an exponential  
47:42
which goes much faster. First, because humans are  out of the loop, don't slow things down. And then  
47:49
eventually it plateaus into a sigmoid when it  bumps up against the laws of physics. No matter  
47:55
how smart you are, you're probably not going to  send information faster than light, and general  
48:01
relativity and quantum mechanics put limits and  so on. But my colleague Seth Lloyd here at MIT  
48:08
has estimated they were still about a million  million million million million times away   from the limits from the laws of physics,  so it can get pretty crazy pretty quickly.
48:18
And it's also Alan… I keep discovering more stuff.  Stuart Russell dug out this fun quote from him  
48:26
in 1951 that I wasn't aware of before, where he  also talks about what happens when we reach this  
48:34
threshold. And he's like, “Well, don't worry about  this control loss thing now because it's far away,  
48:43
but I'll give you a test so you know when to  pay attention: the canary in the coal mine.” The  
48:49
Turing test, as we call it now. And we already  talked about how that was just passed. This  
48:56
reminds me so much of what happened around 1942  when Enrico Fermi built the first self-sustaining  
49:04
nuclear chain reaction under a football stadium in  Chicago. That was like a Turing test for nuclear  
49:14
weapons. When the physicists found out about  this, they totally freaked out, not because  
49:21
the reactor was at all dangerous—it was pretty  small, you know, it wasn't any more dangerous   than ChatGPT is today—but because they realized,  “Oh, that was the canary in the coal mine. That  
49:33
was the last big milestone we had no idea how  to meet and the rest is just engineering.”
49:42
I feel pretty similarly about AI now.  I think that we obviously don't have AI  
49:50
that are better than us or as good as us at  AI development, but it's mostly engineering,  
49:59
I think, from here on out. We can talk more  about the nerdy details of how it might   happen. It's not going to be large language  models scaled, it's going to be other things,  
50:07
but like in 1942… I'm curious, actually, if you  were there visiting Fermi, how many years would  
50:20
you predict it would have taken from then  until the first nuclear explosion happened? How many years? Difficult to say, maybe a decade.
50:26
Uh-huh. So then it happened in three. Could  have been a decade. Probably got sped up a  
50:31
bit because of the geopolitical competition  that was happening during World War II. And  
50:38
similarly, it's very hard to say now, is it going  to be three years? Is it going to be a decade? But   there's no shortage of competition fueling it  again. And as opposed to the nuclear situation,  
50:49
there's also a lot of money in it. So I think  this is the most interesting time, interesting  
50:56
fork in the road in human history. And if Earth  is the only place in our observable universe  
51:06
with telescopes, then… whether it's actually  consciousness about the universe at large,  
51:14
this is probably the most interesting fork  in the road in the last 13.8 billion years   for our universe too, because there's  so many different places this could go,  
51:21
right? And we have so much agency in steering  in a good direction rather than a bad one.
What Are an AI's True Goals? (The Serial Killer Problem)
51:28
Here's a question I have when people talk about  the AIs taking over. I wonder, which AIs? So is  
51:36
Claude considered a competitor to OpenAI in this  AI space from the AI's perspective? Does it look  
51:42
at other models as an enemy because I want to  self…? Does Claude look at other instances? So  
51:50
you have your own Claude chats. Are they all  competitors? Is every time it generates a new  
51:55
token, is that a new identity? So it looks  at what's going to come next and before as,   “Hey, I would like you to not exist anymore  because I want to exist.” What is the continuing  
52:04
identity that would make us say that the  AIs will take over? What is the AI there? Yeah, those are really great questions. The  very short answer is people generally don't  
52:14
know. I'll say a few things. First of all, we  don't know whether Claude or GPT-5 or any of  
52:24
these other systems are having a subjective  experience or not, whether they're conscious   or not. Because as we talked about for a long  time, we do not have a consensus theory of what  
52:36
kind of information processing has a subjective  experience, what consciousness is. But we don't  
52:41
need necessarily for machines to be conscious  for them to be a threat to us. If you're chased  
52:47
by a heat-seeking missile, you probably  don't care whether it's conscious in some   deep philosophical sense. You just care about  what it's actually doing, what its goals are.
52:58
And so let's just switch to talking about  just behavior of systems. In physics,  
53:06
we typically think about the behavior as  determined by the past through causality,   right? Why did this phone fall down? Because  gravity pulled on it, because there's an Earth  
53:15
planet down here. When you look at what  people do, we usually instead interpret,  
53:24
explain why they do it in terms of not the past  but the future, that's some goal they're trying to   accomplish. If you see someone scoring a beautiful  goal in a soccer match, you could be like, “Yeah,  
53:33
it's because their foot struck the ball in this  angle and therefore action equals reaction,” blah,  
53:39
blah. But more likely you're like, “They  wanted to win.” And when we build technology,  
53:48
we usually build it with a purpose in it. So  people build heat-seeking missiles to shoot down   aircraft. They have a goal. We build mousetraps  to kill mice. And we train our AI systems today,  
54:00
our large language models, for example, to  make money and accomplish certain things.
54:09
But to actually answer your question about what  the system… if it would have a goal to collaborate  
54:15
with other systems or destroy them or see them  as competitors, you actually have to ask, does   the system actually have a… Is it meaningful that  this AI system as a whole has a coherent goal? And  
54:29
that's very unclear, honestly. You could say at  a very trivial level that ChatGPT has the goal to  
54:42
correctly predict the next token or word in a lot  of text because that's exactly how we trained it,  
54:50
so-called pre-training. You just let it read all  the internet and look and predict which words are   going to come next. You let it look at pictures  and predict what's next, what's more in them,  
54:58
and so on. But clearly, they're able to have  much more sophisticated goals than that because  
55:06
it just turns out that in order to predict, like  if you're just trying to predict my next word,  
55:13
it helps if you make a more detailed model about  me as a person and what my actual goals are and  
55:19
what I'm trying to accomplish, right? So these  AI systems have gotten very good at simulating  
55:27
people. So this sounds like a Republican. So if  this Republican is writing about immigration,  
55:34
he's probably going to write this. Or  this, based on what they wrote previously,   they're probably a Democrat. So when they write  about immigration, they're more likely to say  
55:40
these words. This one is, the Democrat  is more likely to maybe use the word  
55:46
undocumented immigrant, whereas the Republican  might predict they're going to say illegal alien.
55:52
So they're very good at predicting, modeling  people's goals, but does that mean they have  
55:59
the goals themselves? Now, if you're a  really good actor, you're very good at  
56:04
modeling people with all sorts of different  goals, but does that mean you have the goals,   really? This is not a well understood  situation. And when companies spend a  
56:15
lot of money on what they call aligning an  AI, which they bill as giving it good goals,  
56:24
what they are actually in practice doing is just  affecting its behavior. So they basically punish  
56:33
it when it says things that they don't want it  to say and encourage it. And that's just like if  
56:41
you train a serial killer to not say anything that  reveals his murderous desires. So I'm curious, if  
56:49
you do that and then the serial killer stops ever  dropping any hints about wanting to knock someone   off, would you feel that you actually changed  this person's goals to not want to kill anyone?
57:01
Well, the difference in this case would be that  the AI's goals seem to be extremely tied to its  
57:08
matching of whatever fitness function you  give it. Whereas in the serial killer case,  
57:13
their true goals are something else  and their verbiage is something else. Yeah. It seems like in the LLM's cases...
57:21
Yeah. But when you train an LLM, I'm talking about  the pre-training now where they read the whole  
57:26
internet, basically. You're not telling it to be  kind or anything like that. You're just really  
57:32
training it to have the goal of predicting. And  then in the so-called fine-tuning, reinforcement   learning from human feedback is the nerd phrase  for it. Yes. There, you look at different answers  
57:42
that it could give and you say, “I want this one,  not that one.” But you're, again, not explaining   to it. I have a two-and-a-half-year-old, I have  a two-year-old son, right? This guy. And my idea  
57:56
for how to make him a good person is to help him  understand the value of kindness. My approach to  
58:08
parenting is not to be mean to him if he ever  kicks somebody without any explanation. I want  
58:15
him rather to internalize the goal of being a kind  person and that he should value the well-being of  
58:20
others, right? And that's very different from how  we do reinforcement learning with human feedback.
58:27
And it's frankly not at all… I would stick my  neck out and say, we have no clue really what,  
58:33
if any, goals ChatGPT has. It acts as if it  has goals, yeah? But if you kick a dog every  
58:40
time it tries to bite someone, it's going to  also act like it doesn't want to bite people.   But who knows? With the serial killer case,  it's quite possible that it doesn't have any  
58:50
particular set of unified goals at all. So  this is a very important thing to study and   understand. Because if we're ever going to end up  living with machines that are way smarter than us,  
59:04
then our well-being depends on them having actual  goals to treat us well, not just having said the  
59:15
right buzzwords before they got the power.  So we've both lived with entities that were  
59:23
smarter than us, our parents when we were  little, and it worked out fine because they   really had goals to be nice to us, right? So we  need some deeper, very fundamental understanding  
59:37
of the science of goals in AI systems. Right now,  most people who say that they've aligned goals to  
59:46
AIs are just bullshitting, in my opinion. They  haven't. They've aligned behavior, not goals.
59:52
And I think I would encourage any physicists  and mathematicians watching this who think  
59:57
about getting into AI to think. I would encourage  them to consider this because physicists have… One  
1:00:10
of the things that's great about physicists is  physicists like you have a much higher bar on  
1:00:15
what they mean by understanding something than  engineers typically do. Engineers will be like,  
1:00:21
“Well, yeah, it works. Let's ship it.”  Whereas as a physicist, you might be like,  
1:00:27
“But why exactly does it work? And can I  actually go a little deeper? Is there some,  
1:00:33
can I write down an effective field theory  for how the training dynamics works? Can I  
1:00:38
model this somehow?” This is the kind of thing  that Hopfield did with memory. It's the sort of  
1:00:47
thing that Geoff Hinton has done. And we  need much more of this to have an actual  
1:00:53
satisfactory theory of intelligence, what it  is, and of goals. If we actually have a system,  
1:01:00
an AI system, that actually has goals, and  there's some way for us to actually really   know what they are, then we would be in a  much better situation than we are today.  
1:01:10
We haven't solved the problems because this AI,  if it's very loyal, might be owned by someone who  
1:01:20
orders it to do horrible things and so  on and program horrible goals into it.  
1:01:27
But at least then we'll have the luxury of really  talking about what goals AI systems should have.
1:01:34
A great word was used, understand. That's  something I want to talk about. What does  
1:01:39
it mean to understand? Before we get to that,  I want to linger on your grandson for a moment.
1:01:45
My son. Yes, your son. I have a grandson too, though,  actually. He's also super cute. So when you're training your son, why is that  not… you're a human, you're giving feedback,  
1:01:55
it's reinforcement. Why is that not RLHF  for the child? And then you wonder, well,  
1:02:01
what is the pre-stage? What if the  pre-stage was all of evolution which   would have just given rise to his nervous  system by default? And now you're coming in  
1:02:10
with your RLHF and tuning not only his  behavior but his goals simultaneously.
1:02:18
So let's start with that second part. Yeah. So  first of all, the way RLHF actually works now  
1:02:26
is that American companies will pay one or two  dollars an hour to a bunch of people in Kenya  
1:02:31
and Nigeria to sit and watch the most awful  graphical images and horrible things. And then  
1:02:38
they keep clicking on which of the different…  keep classifying them and is this something  
1:02:43
that should be okay or not okay, and so on. It's  nothing like the way anyone watching this podcast  
1:02:52
treats their child, where they really try to  help the child understand in a deep way. Second,  
1:03:00
the actual architecture of the transformers  and more scaffolding systems being built  
1:03:06
right now are very different from our limited  understanding of how a child's brain works.  
1:03:15
So no, we're certainly not… we can't just  say we're going to declare victory and move  
1:03:25
on from this. We just, like I said  before that people I think have used  
1:03:32
some philosophical excuses to avoid working hard  on the consciousness problem, I think some people   have made philosophical excuses to avoid just  asking this very sensible question of goals.
Fermat's Principle, Entropy, and the Physics of Goals
1:03:44
Before we talk about understanding, can  I talk a little bit more about goals? Please, yeah. Because if we talk about  goal-oriented behavior first,  
1:03:53
there's less emotional baggage associated  with that, right? Let's define goal-oriented  
1:04:00
behavior as behavior that's more easily  explained by the future than by the past,   more easily explained by the effects it is  going to have than by what caused it. Okay,  
1:04:09
interesting. So again to come back to this. So  if I just take this thesis here and I bang it,  
1:04:19
and you ask, “Why did it move?” You could say the  cause of it moving was because another object,  
1:04:30
my hand, bumped into it, action equals reaction.  In other words, this impulse given to it,   et cetera, et cetera. Or you could say, but  the goal, you could view it as goal behavior,  
1:04:40
thinking, “Well, Max wanted to illustrate a point.  He wanted it to move, so he did something that   made it move,” right? And that feels like  the more economic description in this case.
1:04:50
And it's interesting, even in basic physics, we  actually see stuff which can sometimes be more…  
1:04:58
So first thing I want to say is there is no right  and wrong description. Both of those descriptions   are correct, right? So look at the water in this  bottle here again. If you put a straw into it,  
1:05:11
it's going to look bent because light rays bend  when they cross the surface into the water. You  
1:05:18
can give two different kinds of explanations  for this. The causal explanation will be like,   “Well, the light ray came there. There were  now some atoms in the water and they interacted  
1:05:28
with the electromagnetic field and blah, blah,  blah, blah.” And after a very complicated   calculation, you can calculate the angle that goes  that way. But you can give a different explanation  
1:05:36
from Fermat's principle and say that actually  the light ray took the path that was going to   get it there the fastest. If this were instead a  beach and this is an ocean and you're working a  
1:05:50
summer job as a lifeguard and you want to risk…  and you see a swimmer who's in trouble here, how  
1:05:55
are you going to go to the swimmer? You're going  to again go in the path that gets you there the   fastest. So you'll run a longer distance through  the air on the beach and then a shorter distance  
1:06:05
through the water. Clearly, that's goal-oriented  behavior, right? For us. For the photon, though,  
1:06:15
well, both descriptions are valid. It turns  out in this case that it's actually simpler to   calculate the right answer if you do Fermat's  principle and look at the goal-oriented way.
1:06:25
And then we see in biology, so Jeremy England,  who used to be a professor here, realized that  
1:06:31
in many cases, non-equilibrium thermodynamics  can also be understood sometimes more simply  
1:06:38
through goal-oriented behavior. Like if, suppose  I put a bunch of sugar on the floor and no life  
1:06:49
form ever enters the room, come back—including  the Facilities, who keeps this nice and tidy  
1:06:55
here—then it's still going to be there in a year,  right? But if there are some ants, the sugar is  
1:07:00
going to be gone pretty soon. And entropy will  have increased faster that way because the sugar  
1:07:09
was eaten and there was dissipation. And Jeremy  England showed actually that there's a general  
1:07:15
principle in non-equilibrium thermodynamics where  systems tend to adjust themselves to always be  
1:07:24
able to dissipate faster. To be able to, if you  have a thing, if you have some… there are some  
1:07:30
kinds of liquids where you can put some stuff  where if you shine light at one wavelength, it   will rearrange itself so that it can absorb that  wavelength better to dissipate the heat faster.
1:07:39
And you can even think of life itself a little  bit of that. Life basically can't reduce entropy,  
1:07:46
right, in the universe as a whole. It can't  beat the second law of thermodynamics.   But it has this trick where it could keep its  own entropy low and do interesting things,  
1:07:57
retain its complexity and reproduce, by increasing  the entropy of its environment even faster.
1:08:04
And so if I understand, the increasing of the  entropy in the environment is the side effect,   but the goal is to lower your own entropy.
1:08:12
So again, you can have… there are two ways of  looking at it. You can look at it all as just  
1:08:18
a bunch of atoms bouncing around and causally  explain everything. But a more economic way  
1:08:24
of thinking about it is that, yeah, life  is doing the same thing that that liquid  
1:08:30
that rearranges itself to absorb sunlight is.  It's a process that just increases the overall  
1:08:37
entropy production in the universe. It  makes the universe messier faster so that  
1:08:42
it can accomplish things itself. And since  life can make copies of itself, of course,   those systems that are most fit at doing that  are going to just take over everything. And  
1:08:52
you get an overall trend towards more life, more  complexity, and here we are having a conversation.
1:08:58
So where I'm going with this is to say that  goal-oriented behavior at a very basic level  
1:09:05
was actually built into the laws of physics.  That's why I brought Fermat's principle. There   are two ways you can think of physics, either  as the past causing the future or as deliberate  
1:09:16
choices made now to cause a certain future.  And gradually our universe has become more and  
1:09:22
more goal-oriented as we started getting  more and more sophisticated life forms,   now us. And we're already at the point,  a very interesting transition point now,  
1:09:36
where the amount of atoms that are in technology  that we built with goals in mind is becoming  
1:09:42
comparable to the biomass already. And it might  be if we end up in some sort of AI future where  
1:09:56
life starts spreading into the cosmos near  the speed of light, et cetera, that the vast   majority of all the atoms are going to be engaged  in goal-oriented behavior, so that our universe  
1:10:05
is becoming more and more goal-oriented. So I  wanted to just anchor it a bit in physics again,  
1:10:10
since you love physics, right? And say that I  think it's very interesting for physicists to  
1:10:16
think more about the physics of goal-oriented  behavior. And when you look at an AI system,  
1:10:26
oftentimes what plays the role of a goal is  actually just a loss function or reward function.  
1:10:35
You have a lot of options and there's some sort  of optimization trying to make the loss as small   as possible. And anytime you have optimization,  you'd say you have a goal. Yeah, but just like  
1:10:47
it's a very lame and banal goal for a light ray  to refract a little bit in water to get there as  
1:10:52
fast as possible. And that's a very sophisticated  goal if someone tries to raise their daughter well  
1:10:58
or write a beautiful movie or symphony. It's  a whole spectrum of goals, but yeah, building  
1:11:10
a system that's trying to optimize something, I  would say absolutely is a goal-oriented system. I was just going to inquire about, are they  equivalent? So I see that whenever you're  
1:11:18
optimizing something, you have to have a goal  that you're optimizing toward. Sure. But is it   the case that anytime you have a goal, there's  also optimization? So anytime someone uses the  
1:11:26
word goal, you can think there's going to  be optimization involved and vice versa? That's a wonderful question. Actually, Richard  Feynman famously asked that question. He said that  
1:11:33
all laws of physics he knows about can actually be  derived from an optimization principle except one.  
1:11:41
And he wondered if there was one. So I think this  is an interesting open question you just threw  
1:11:46
out there. I would expect that you cannot, your  actions cannot be really accurately modeled by  
1:11:56
writing down a single goal that you're just trying  to maximize. I don't think that's how human beings  
1:12:02
in general operate. What I think actually is  happening with us and goals is a little different.
1:12:07
Our genes, according to Darwin,  the goal behavior they exhibited,  
1:12:14
even though they weren't conscious, obviously,  our genes, was just evolutionary fitness. Make a  
1:12:21
lot of successful copies of themselves. That's  all they cared about. So then it turned out  
1:12:27
that they would reproduce better if they also  developed bodies around them with brains that  
1:12:34
could do a bunch of information processing,  get more food and mate and stuff like that.  
1:12:41
But evolution also became quite clear that if you  have an organism like a rabbit, if it would have  
1:12:51
to every time it was going to decide whether to  eat this carrot or flirt with this girl rabbit,  
1:12:59
go back and recalculate, “What is this going to  do to my expected number of fertile offspring?”  
1:13:06
That rabbit would just die of starvation and  those genes would go out of the gene pool. It   didn't have the cognitive capacity to always  anchor every decision it made in one single  
1:13:16
goal. It was computationally unfeasible to always  be running this actual optimization that the genes  
1:13:23
cared about, right? So what happened instead  in rabbits and humans and what we in computer  
1:13:29
science call agents of bounded rationality, where  we have limits to how much we can compute, was we  
1:13:35
developed all these heuristic hacks. Like, “If you  feel hungry, eat. If you feel thirsty, drink. If  
1:13:46
there's something that tastes sweet or savory,  eat more of it. Fall in love, make babies.”
1:13:56
These are clearly proxies ultimately for what the  genes cared about, making copies of themselves,   because you're not going to have a lot of  babies if you die of starvation, right?  
1:14:04
But now that you have your great brain, what it  is actually doing is making its decisions based  
1:14:15
on all these heuristics that themselves don't  correspond to any unique goal anymore. Like any  
1:14:21
person watching this podcast who's ever used birth  control would have so pissed off their genes if  
1:14:26
the genes were conscious because this is not at  all what the genes wanted, right? The genes just  
1:14:32
gave them the incentive to make love because that  would make copies of the genes. The person who  
1:14:41
used birth control was well aware what the genes  wanted and was like, “Screw this. I don't want to  
1:14:47
have a baby at this point in my life.” So there's  been a rebellion in the goal behavior of people  
1:14:55
against the original goal that we were made with  and replaced by these heuristics that we have,  
1:15:02
our emotional drives and desires and hunger and  thirst, etc., that are not anymore optimizing for  
1:15:09
anything specific. And they can sometimes work out  pretty badly, like the obesity epidemic and so on.
1:15:20
And I think the machines today, the smartest  AI systems are even more extreme like that than   humans. I don't think they have anything.  I think they're much more… I think humans  
1:15:30
still tend to end up, especially those who are  who like introspection and self-reflection,  
1:15:37
are much more prone and likely to have some  at least somewhat consistent strategy for  
1:15:46
their life or goals than ChatGPT has, which is a  completely random mishmash of all sorts of things.
Eureka Moment: When an AI Discovered Geometry on Its Own
1:15:55
Understanding. Understanding, yes. Oh, that's a big  one. I've been writing a paper called  
1:16:07
“Artificial Understanding” for quite a long  time, as opposed to artificial consciousness   and artificial intelligence. And the  reason I haven't written it is because  
1:16:19
it's a really tough question. I  feel there is a way of defining  
1:16:24
understanding so that it's quite different  from both consciousness and intelligence,  
1:16:31
although also a kind of information processing,  or at least a kind of information representation.
1:16:37
I thought you were going to relate  it to goals. Because if I understand,   goals are related to intelligence, sure, but  then also the understanding of someone else's  
1:16:46
goals seems to be related to intelligence. So for  instance, in chess, you're constantly trying to   figure out the goals of the opponent. And if I  can figure out your goals prior to you figuring  
1:16:54
out mine or ahead of yours or whatever, then I'm  more intelligent than you. Now, you would think  
1:17:01
that the ability to reliably achieve your goals  is what is intelligence, but it's not just that  
1:17:06
because you can have an extremely simple goal that  you always achieve, like the photon here, it's   just following some principle. But we have goals,  even the person on the beach with the swimming…
1:17:17
Hypothetically. Yeah, yeah, yeah. Even that we fail at, but we're more  intelligent than the photon. But we're  
1:17:24
able to model the photon's goal. The  photon is not able to model our goal.   So I thought you were going to say, well,  that modeling is related to understanding.
1:17:32
Yeah, that I agree with for sure. Modeling is  absolutely related to understanding. Goals, I view  
1:17:38
as different. I personally think of intelligence  as being rather independent of goals. So I would  
1:17:51
define intelligence as ability to accomplish  goals. You know, you talked about chess,  
1:17:58
right? There are tournaments where computers  play chess against computers to win. Have you   ever played losing chess? It's a game where you're  trying to force the other person to win the game?
1:18:08
No. They have computer tournaments for that too. Interesting. So you can actually give a computer the goal which  is the exact opposite of a normal chess computer,  
1:18:15
and then you can say that the one that  won the losing chess tournament is the   most intelligent again. So this right there  shows that being intelligent isn't the same  
1:18:31
as having a particular goal. It's how good  you are at accomplishing them, right? I think   a lot of people also make the mistake of saying,  “Oh, we shouldn't worry about what happens with  
1:18:37
powerful AI because it's going to be so smart  it will be kind automatically to us.” You know,  
1:18:44
if Hitler had been smarter, do you really think  the world would have been better? I would guess  
1:18:49
that it would have been worse, in fact, if he  had been smarter and won World War II and so on.  
1:18:56
So there's, Nick Bostrom calls this the  orthogonality thesis, that intelligence  
1:19:03
is just an ability to accomplish whatever goals  you give yourself or whatever goals you have.
1:19:11
And I think understanding is a component of  intelligence which is very linked to modeling,  
1:19:18
as you said, having… or maybe you could argue that  it even is the same, like ability to have a really  
1:19:26
good model of something, another person as you  said, or of our universe if you're a physicist,  
1:19:32
right? And I'm not going to give you some  very glib definition of what understanding  
1:19:41
or artificial understanding is because  I view it as an open problem. But I can  
1:19:47
tell you one anecdote of something which  felt like artificial understanding to me.
1:19:52
So me and some of my students here  at MIT, we were very interested in…  
1:19:59
so we've done a lot of work, including this  thesis here that randomly happens to be lying   here. It's about how you take AI systems and you  do something smart and you figure out how they  
1:20:09
do it. So one particular task we trained  an AI system to do was just to implement,  
1:20:20
to learn group operations abstractly. So  a concrete example, suppose you have 59,  
1:20:28
the numbers 0 through 58, okay? And you're adding  them modulo 59. So you say like 1 plus 2 is 3,  
1:20:36
but 57 plus 3 is 60. Well, that's bigger than  59, so you subtract off 59, you say it's one.
1:20:48
Same principle as a clock. Same exactly as a clock. And I'm so glad you  said clock, because that's your model in your  
1:20:54
brain about modular arithmetic. You think of  all the numbers sitting in a circle. It goes   after 10 and 11 comes 12, but then comes one.  So what happened was we… there are 59 times 59,  
1:21:06
so about 3,600 pairs of numbers, right? We  trained the system on some fraction of those,  
1:21:13
see if it could learn to get the right answer.  And the way the AI worked was it learned to embed  
1:21:20
and represent each number, which was given  to it just as a symbol—it didn't know five,  
1:21:25
whether it had anything to do with  the number five—as a point in a high   dimensional space. So we have these  59 points in a high dimensional space,  
1:21:35
okay? And then we trained another neural network  to look at these representations. So you give it  
1:21:40
this point and this point and it has to figure  out, “Okay, what's this plus this mod 59?”
1:21:47
And then something shocking happened. You  train it, train it, it sucks, it sucks,   and then it starts getting better on the  training data. And then at a sudden point,  
1:21:57
it suddenly also starts to get better on the test  data. So it starts to be able to correctly answer  
1:22:04
questions for pairs of numbers it hasn't seen  yet. So it somehow had a eureka moment where   it understood something about the problem. It had  some understanding. So I suggested to my students,  
1:22:15
“Why don't you look at what's happening to  the geometry of all these points that are   moving around, the 59 points that are moving  around in this high dimensional space during  
1:22:23
the training?” I told them to just do a very  simple thing, principal component analysis,   where you try to see if they mostly lie in a  plane and then you can just plot the 59 points.
1:22:35
And it was so cool what happened. You look at  this, you see 59 points, it's looking very random,  
1:22:40
they're moving around, and then at exactly  the point when the Eureka moment happens,  
1:22:46
when the AI becomes able to answer questions  it hasn't seen before, the points line up on  
1:22:51
a circle, a beautiful circle. Except not  with 12 things, but with 59 things now,  
1:22:57
because that was the problem it had, right?  So to me, this felt like the AI had reached  
1:23:04
understanding about what the problem was.  It had come up with a model, or as we often  
1:23:10
call it, a representation of the problem.  In this case, in terms of some beautiful  
1:23:15
geometry. And this understanding now enabled  it to see patterns in the problem so that it  
1:23:26
could generalize to all sorts of things  that it hadn't even come across before. So I'm not able to give a beautiful, succinct,  fully complete answer to your question on how  
1:23:38
to define artificial understanding,  but I do feel that this is an example,  
1:23:44
a small example of understanding. We've since  then seen many others. We wrote another paper  
1:23:49
where we found that when large language models do  arithmetic, they represent the numbers on a helix,  
1:23:56
like a spiral shape. And I'm like, “What is  that?” Well, the long direction of it can be  
1:24:03
thought of like representing the numbers in  analog, like you're farther this way if the  
1:24:08
number is bigger. But by having them wrap around  on a helix like this, you can use the digits,  
1:24:14
if it's base 10, to go around. And there were  actually several helices. There's a 100-helix   and a 10-helix. And so I suspect that one day  people will come to realize that more broadly,  
1:24:25
when machines understand stuff and maybe when we  understand things also, it has to do with coming  
1:24:31
up with the same patterns and then coming up with  a clever way of representing the patterns such  
1:24:37
that the representation itself goes a long way  towards already giving you the answers you need.
1:24:49
This is how I often… I'm a very visual thinker  when I do physics or when I think in general,   I never feel like I can understand anything  unless I have some geometric image in my…
1:25:00
Yeah. Actually Feynman talked about this. Feynman  said that there's the story of him and a friend  
1:25:05
who can both count to 60 something like this  precisely. And then he's saying to his friend,  
1:25:10
“I can't do it if you're waving your arms in front  of me or distracting me like that.” I remember.  
1:25:17
“But I can, if I'm listening to music, I  can still do this trick.” And he's like,   “I can't do it if I'm listening to music, but  you can wave your arms as much as you like.” And  
1:25:24
Feynman realized he, Feynman, was seeing the  numbers one, two, three. That was his trick,  
1:25:29
was to have a mental image protected. And then  the other person was having a metronome. But the  
1:25:36
goal or the outcome was the same, but the  way that they came about it was different. There's actually something in philosophy  called the rule-following paradox.  
1:25:44
You probably know this. There are two  rule-following paradoxes. One is Kripke,   and one is the one that I'm about to say. So  how do you know when you're teaching a child  
1:25:52
that they've actually followed the rules of  arithmetic? So you can test them 50 plus 80,   et cetera. And they can get it correct  every single time. They can even show  
1:26:02
you their reasoning. But then you don't know  if that actually fails at 6,000 times 51 and   the numbers above that. You don't know if they  did some special convoluted method to get there.
1:26:11
Exactly. All you can do is say you've worked it out  in this case, in this case, in this case. That's actually… we have the advantage with  computers that we can inspect how they understand,  
1:26:21
in principle. But when you look under the hood of  something like ChatGPT, all you see is billions  
1:26:27
and billions of numbers, and you oftentimes have  no idea what all these matrix multiplications and  
1:26:34
things like this… you have no idea really what  it's doing. But mechanistic interpretability,   of course, is exactly the quest to move  beyond that and see how does it actually work.
1:26:42
And coming back to understanding and  representations, there is this idea known  
1:26:49
as the platonic representation hypothesis,  that if you have two different machines,  
1:26:55
or I would generalize it to people also, who  both reach a deep understanding of something,  
1:27:01
there's a chance that they've come up with  a similar representation. In Feynman's case,   there were two different ones, right? But  there probably aren't… there's probably  
1:27:08
at most, there's probably a few ones  or one or a few that are really good.
1:27:15
It seems like a hard case to make. But there is a lot of evidence coming out for it  now, actually. You can… already many years ago,  
1:27:21
there was this team where they just took… you know  in ChatGPT and other AI systems all the words and  
1:27:31
word parts, they call tokens, get represented as  points in a high dimensional space. And so this  
1:27:39
team, they just took something which had been  trained only on English books and another one,   English language stuff, and another one trained  only on Italian stuff. And they just looked at  
1:27:48
these two point clouds and found that there  was a way they could actually rotate them to   match up as well as possible, and it gave them  a somewhat decent English to Italian dictionary.  
1:27:58
So they had the same representation. And there's  a lot of recent papers, quite recent ones even,  
1:28:04
that are showing that yeah, it seems like the  representations of one large language model like  
1:28:10
ChatGPT, for example, is in many ways similar  to the representations that other ones have.
1:28:18
We did a paper, my student, my grad student  Dawan Beck and I, where we looked at family  
1:28:24
trees. So we took the Kennedy family  tree, a bunch of royalty family trees,   etc. And we just trained the AI to correctly  predict like who is the son of whom,  
1:28:34
who is the uncle of whom, is so-and-so a sister  of whom. We just asked all these questions,  
1:28:45
and we also incentivized the large language model  to learn it in as simple a way as possible by  
1:28:58
limiting the resources it had. And then when we  looked inside, we discovered something amazing.   We discovered that, first of all, a whole bunch  of independent systems had learned the same  
1:29:08
representation. So you could actually take the  representation of one and literally just rotate  
1:29:13
it around and stretch it a little bit and put it  into the other and it would work there. And then   when we looked at what it was, they were trees.  We never told it anything about family trees,  
1:29:23
but it would draw like, here is this king  so-and-so and then here are the sons and   this and this. And then it could use that to know  that, well, if someone is farther down, they're a  
1:29:34
descendant, et cetera, et cetera, et cetera. So  that's yet another example, I think, in support  
1:29:41
of this platonic representation hypothesis, the  idea that understanding probably has something,   often has something to do with capturing patterns,  and often in a beautiful geometric way, actually.
1:29:55
Okay. So I wanted to end on the advice  that you received from your parents,  
1:30:00
which was about don't concern yourself too much  what other people think, something akin to that.   It was differently worded. But I also wanted to  talk about what are the misconceptions of your  
Refuting the "AI Doomers": We Have More Agency Than We Think
1:30:11
work that other colleagues even have that you have  to constantly dispel. And another topic I wanted   to talk about was the mathematical universe. The  easy stuff. So there are three, but we don't have  
1:30:21
time for all three. If you could think of a way  to tie them all together, then feel free like a   gymnast or juggler. But otherwise, then I would  like to end on the advice from your parents.
1:30:32
Okay. Well, the whole reason I spent so many  years thinking about whether we are all part  
1:30:40
of a mathematical structure and whether  our universe actually is mathematical   rather than just described by it is, of  course, because I listened to my parents.  
1:30:47
Because I got so much shit for that. And  I just felt, no, I think I'm going to do  
1:30:55
this anyway because to me it makes logical  sense. I'm going to put the ideas out there.
1:31:01
And then in terms of misconceptions about  me, one misconception I think is that  
1:31:11
somehow I don't believe that being falsifiable  is important for science. I talked about earlier,  
1:31:22
I'm totally on board with this. And I actually  argue that if you have a predictive theory about  
1:31:28
anything—gravity, consciousness, et cetera—that  means that you can falsify it. So that's one.  
1:31:39
And another one, probably the one I get  most now because I've stuck my neck out a   bit about AI and the idea that actually the  brain is a biological computer and actually  
1:31:50
we're likely to be able to build machines  that we could totally lose control over,   is that some people like to call me a doomer,  which is of course just something they say  
1:32:00
when they've run out of arguments. It's like  if you call someone a heretic or whatever. And so I think what I would like to correct about  that is I feel actually quite optimistic. I'm not  
1:32:12
a pessimistic person. I think that there's way too  much pessimism floating around about humanity's  
1:32:20
potential. One is people, “Oh, we can never figure  out and make any more progress on consciousness.”  
1:32:31
We totally can if we stop telling  ourselves that it's impossible and   actually work hard. Some people say,  “Oh, we can never figure out more about  
1:32:40
the nature of time and so on unless we  can detect gravitons or whatever.” We  
1:32:49
totally can. There's so much progress that  we can make if we're willing to work hard. And in particular, I think the most pernicious  kind of pessimism we suffer from now is this  
1:33:04
meme that it's inevitable that we are going to  build superintelligence and become irrelevant.  
1:33:14
It is absolutely not inevitable. But if you  tell yourself that something is inevitable,  
1:33:21
it's a self-fulfilling prophecy, right?  This is convincing a country that's just  
1:33:26
been invaded that it's inevitable that they're  going to lose the war if they fight. It's the  
1:33:32
oldest psyop game in town, right? So of course  if there's someone who has a company and they  
1:33:38
want to build stuff and they don't want you  to have any laws that make them accountable,  
1:33:44
they have an incentive to tell everybody, “Oh,  it's inevitable that this is going to get built,   so don't fight it. It's inevitable that  humanity is going to lose control over  
1:33:57
the planet, so just don't fight  it. And hey, buy my new product.” It's absolutely not inevitable. You could  have had people… People say it's inevitable,  
1:34:07
for example, because they say people will  always build technology that can give you  
1:34:12
money and power. That's just factually incorrect.  You're a really smart guy. If I could do cloning  
1:34:19
of you and start selling a million copies of you  on the black market, I could make a ton of money.  
1:34:26
We decided not to do that, right? They say, “Oh,  if we don't do it, China's going to do it.” Well,  
1:34:32
there was actually one guy who did human cloning  in China. And you know what happened to him?
1:34:39
No. He was sent to jail by the Chinese government. Oh, okay. People just didn't want that. They thought  we could lose control over the human germline  
1:34:47
and our species. “Let's not do it.” So there  is no human cloning happening now. We could  
1:34:53
have gotten a lot of military power with  bioweapons. Then Professor Matthew Meselson  
1:34:59
at Harvard said to Richard Nixon, “We don't  want there to be a weapon of mass destruction  
1:35:05
that's so cheap that all our adversaries  can afford it.” And Nixon was like, “Huh,  
1:35:14
that makes sense, actually.” And then Nixon  used that argument on Brezhnev and it worked,   and we got a bioweapons ban. And now people  associate biology mostly with curing diseases,  
1:35:25
not with building bioweapons. So it's absolutely  not… it's absolute BS, this idea that we're always  
1:35:35
going to build any technology that can give  power or money to some people if there's… we  
1:35:42
have much more control over our lives and our  futures. We have much more control over our  
1:35:50
futures than some people like to tell us that we  have. We are much more empowered than we thought.
1:35:55
I mentioned that if we were living in a cave  30,000 years ago, we might've made the same   mistake and thought we were doomed to just always  be at risk of getting eaten by tigers and starving  
1:36:07
to death. That was too pessimistic. We  had the power to, through our thought,  
1:36:13
develop a wonderful society and technology where  we could flourish. And it's exactly the same way  
1:36:19
now. We have an enormous power. What most people  actually want to make money on AI is not some kind  
1:36:26
of sand god that we don't know how to control.  It's tools, AI tools. People want to cure cancer.   People want to make their business more efficient.  Some people want to make their armies stronger and  
1:36:34
so on. You can do all of those things with tool  AI that we can control. And this is something we  
1:36:42
work on in my group, actually. And that's what  people really want. And there's a lot of people  
1:36:48
who do not want to just be like, “Okay, yeah, it's  been a good run, hundreds of thousands of years,  
1:36:53
we had science and all that, but now let's just  throw away the keys to Earth to some alien minds  
1:36:59
that we don't even understand what goals they  have.” Most Americans in polls think that's just   a terrible idea, Republicans and Democrats.  There was an open letter by evangelicals in  
1:37:10
the U.S. to Donald Trump saying, “We want AI  tools. We don't want some sort of uncontrollable  
1:37:16
superintelligence.” The Pope has recently said he  wants AI to be a tool, not some kind of master.  
1:37:24
You have people from Bernie Sanders to Marjorie  Taylor Greene that come out on Twitter saying,  
1:37:30
“We don't want Skynet. We don't want to  just make humans economically obsolete.” So  
1:37:36
it's not inevitable at all. And if we can just  remember we have so much agency in what we do,  
1:37:45
what kind of future we're going to build, if  we can be optimistic and just think through  
1:37:51
what is a really inspiring, globally shared  vision for not just curing cancer but all  
1:37:57
the other great stuff we can do, then we can  totally collaborate and build that future.
1:38:03
The audience member now is listening. They're  a researcher. They're a young researcher,   they're an old researcher. They  have something they would like  
1:38:10
to achieve that's extremely unlikely,  that's criticized by their colleagues   for even them proposing it. And it's  nothing nefarious, something that they  
1:38:17
find interesting and maybe beneficial to  humanity. Whatever. What is your advice?
1:38:25
Two pieces of advice. First of all, about  half of all the greatest breakthroughs in  
1:38:32
science were actually trash-talked at the  time. So just because someone says your  
1:38:41
idea is stupid doesn't mean it is stupid.  A lot of people's ideas… you should be  
1:38:46
willing to abandon your own ideas if you  can see the flaw and you should listen to   destructive criticism against it. But if you  feel you really understand the logic of your  
1:38:56
ideas better than anyone else and it makes  sense to you, then keep pushing it forward.
1:39:04
And the second piece of advice I have is you might  worry then, like I did when I was in grad school,  
1:39:09
that if I only worked on stuff that my colleagues  thought was bullshit—like thinking about the  
1:39:16
many-worlds interpretation of quantum mechanics,  that there were multiverses—then my next job was   going to be at McDonald's. Then my advice is to  hedge your bets. Spend enough time working on  
1:39:32
things that get appreciated by your peers now so  that you can pay your bills, so that your career  
1:39:42
continues ahead. But carve out a significant chunk  of your time to do what you're really passionate  
1:39:47
about in parallel. If people don't get it, well,  don't tell them about it at the time. And that way  
1:40:01
you're doing science for the only good reason,  which is that you're passionate about it. And  
1:40:07
it's a fair deal to society to then do a little  bit of chores for society to pay your bills also. That's a great way of viewing it. And it's been  quite shocking for me to see actually how many  
1:40:16
of the things that I got most criticized for  or was most afraid of talking openly about   when I was a grad student, even papers that I  didn't show my advisor until after he signed  
1:40:26
my PhD thesis and stuff, have later actually  been pretty picked up. And I actually feel  
1:40:34
that the things that I feel have been most  impactful were generally in that category.  
1:40:39
You're never going to be the first to do something  important if you're just following everybody else.
1:40:47
Max, thank you. Thank you.
1:40:52
Hi there, Curt here. If you'd like more  content from Theories of Everything and  
1:40:57
the very best listening experience, then be sure  to check out my Substack at CurtJaimungal.org.  
1:41:05
Some of the top perks are that every week you  get brand new episodes ahead of time. You also  
1:41:12
get bonus written content exclusively for our  members. That's C-U-R-T-J-A-I-M-U-N-G-A-L.org.  
1:41:22
You can also just search my name and the  word Substack on Google. Since I started   that Substack, it somehow already became  number two in the science category. Now,  
1:41:32
Substack, for those who are unfamiliar, is  like a newsletter. One that's beautifully  
1:41:37
formatted. There's zero spam. This is the  best place to follow the content of this  
1:41:43
channel that isn't anywhere else. It's not on  YouTube. It's not on Patreon. It's exclusive to  
1:41:49
the Substack. It's free. There are ways for  you to support me on Substack if you want,  
1:41:54
and you'll get special bonuses if you do. Several  people ask me like, “Hey, Curt, you've spoken to  
1:42:00
so many people in the field of theoretical  physics, of philosophy, of consciousness.  
1:42:05
What are your thoughts, man?” Well, while I  remain impartial in interviews, this Substack  
1:42:12
is a way to peer into my present deliberations  on these topics. And it's the perfect way to  
1:42:19
support me directly. CurtJaimungal.org or  search Curt Jaimungal Substack on Google.
1:42:27
Oh, and I've received several messages,  emails, and comments from professors and  
1:42:32
researchers saying that they recommend  Theories of Everything to their students.   That's fantastic. If you're a professor or  a lecturer or what have you and there's a  
1:42:42
particular standout episode that students can  benefit from or your friends, please do share.
1:42:47
And of course, a huge thank  you to our advertising sponsor,   The Economist. Visit economist.com/TOE to get a  massive discount on their annual subscription.  
1:43:00
I subscribe to The Economist and you'll  love it as well. TOE is actually the only  
1:43:05
podcast that they currently partner with.  So it's a huge honor for me. And for you,   you're getting an exclusive discount.  That's economist.com/TOE, T-O-E.
1:43:16
And finally, you should know this podcast is  on iTunes. It's on Spotify. It's on all the  
1:43:22
audio platforms. All you have to do is type  in Theories of Everything and you'll find   it. I know my last name is complicated, so  maybe you don't want to type in Jaimungal,  
1:43:32
but you can type in Theories of Everything  and you'll find it. Personally, I gain from   re-watching lectures and podcasts. I also read  in the comments that TOE listeners also gain from  
1:43:42
replaying. So how about instead you re-listen  on one of those platforms like iTunes, Spotify,  
1:43:47
Google Podcasts, whatever podcast catcher you  use, I'm there with you. Thank you for listening.