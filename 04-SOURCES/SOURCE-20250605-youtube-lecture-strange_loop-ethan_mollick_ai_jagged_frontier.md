---
id: SOURCE-20250605-001
title: "Ethan Mollick on AI Jagged Frontier, Abundance Mindset, and Knowledge Collapse"
platform: youtube
format: lecture
creator: Strange Loop
date_published: 20250605
status: triaged
url: "https://www.youtube.com/watch?v=KEQjwE7hDjk"
original_filename: processed/SOURCE-20250605-youtube-lecture-strangeloop-ethan_mollick.md
aliases:
  - "Mollick - AI Jagged Frontier"
teleology: strategize
notebooklm_category: ai-engineering
signal_tier: paradigm
chain_relevance: Information|Expertise|Knowledge
integration_targets:
  - CANON-33100-EFFICACY
  - CANON-33110-BIZ_BACKBONE
  - CANON-34000-KNOWLEDGE
synopsis: "Ethan Mollick presents the jagged frontier concept: AI capability is fundamentally uneven across tasks, demanding organizational redesign rather than plug-in adoption. Distinguishes abundance mindset (do new work) from efficiency mindset (do same work cheaper), and warns of knowledge collapse when AI shortcuts bypass apprenticeship."
key_insights:
  - "The jagged frontier means AI brilliance in one domain tells you nothing about adjacent domains—organizations must map actual capability ruthlessly"
  - "Abundance mindset (review top 200 customers) creates different strategic options than efficiency mindset (review top 50 cheaper)"
  - "Knowledge collapse occurs when AI shortcuts bypass apprenticeship and tacit knowledge development, destroying organizational resilience"
topics:
  - "ai-engineering"
  - "career"
  - "entrepreneurship"
  - "best-practices"
---

## Key Concepts

### The Jagged Frontier
- AI capability is fundamentally uneven—brilliant in some domains, terrible in adjacent ones
- You cannot predict competence from surface similarity of tasks
- This creates critical organizational design problem: can't plug AI into existing structures
- Must map actual capability ruthlessly, then restructure work around it

### Engelbart vs Minsky Tension
- Engelbart: augment human intelligence (technology as amplifier)
- Minsky: replace human intelligence (technology as substitute)
- This tension was theoretical; now it's urgent business strategy
- Most organizations haven't chosen consciously—they're drifting toward replacement

### Three Ingredients for AI Adoption
1. **Domain expertise**: Someone who knows what good actually looks like
2. **Prompt craft skill**: Learnable, teachable, becoming foundational
3. **Workflow integration**: AI must be woven into how work gets done

### Abundance vs Efficiency Mindset
**Critical Choice**: Most companies optimize past (do same work cheaper) when they should invent future (do new work)
- "Review top 50 customers" vs "Review top 200 customers"—different strategic options entirely
- Efficiency is the easy, wrong path; abundance is harder, more valuable

### Knowledge Collapse Risk
- AI shortcuts bypass apprenticeship and tacit knowledge development
- Organizations lose deep expertise when everyone prompts instead of learns
- If AI breaks in novel situation, no fallback of human expertise
- Intentionally reserve some work for human development, even when AI could do it

## Paradigm-Level Insights

1. **Jagged frontier demands organizational redesign** - Traditional org charts assume predictable human capability profiles; AI breaks this assumption

2. **Prompting is foundational skill** - Not clever tricks but structured communication of intent; teachable and essential

3. **Abundance requires courage** - The gravitational pull is toward efficiency; transformation requires resisting it

4. **Apprenticeship is infrastructure** - Knowledge development isn't inefficiency—it's organizational resilience investment

5. **KPIs are counterproductive in R&D phase** - Measuring too early optimizes wrong things

## Integration Notes

- Jagged frontier concept essential for EFFICACY implementation guidance
- Three ingredients framework directly applicable to BIZ_BACKBONE adoption patterns
- Knowledge collapse warning critical for KNOWLEDGE chain preservation strategies
- Abundance mindset connects to broader SYNCRESCENDENCE growth philosophy


## Transcript

You can either fire most of your staff
and make more money per barrel of ale or
you can be Guinness and hire 100,000
people and expand worldwide. And I
really worry about too many people
taking the small path and not the big
one.
[Music]
I would love to start from the very
beginning when you were back at MIT with
you know the OJ and Marvin Minsky and
and so on. Uh what were sort of the
ideas at at that stage? So so this is a
little bit of like stolen technical
glory because I was not the coder with
Marvin. I was the person from the MBA
program who was trying to help the AI
people explain what AI was to everybody
else. So I worked with Marvin and a few
other people at the the media lab quite
a bit on this and what was really
interesting was a lot you know this was
sort of during one of the various AI
winters right so it was no one was
paying much attention to AI and it was
all about sort of elaborate schemes for
how we can create intelligence and so
there was projects to observe everything
a baby did and maybe that would somehow
let us make AI there was Marv Minsky's
society of mind of all these kind of
complex interlocking pieces and um I I
think about how kind of ironic it was
that the actual solution solution turned
out to be just shove a lot of language
into a learning system and you end up
with with LLMs. It's it's interesting
because a lot of the the technical ideas
turned out um to to be incorrect. Uh but
there was a lot of the core philosophies
there that I think are are are back in
fashion now. You had Minsky and
Angelbart. Uh Engelbart had this
philosophy of augmenting human
intelligence and Minsky was a lot about
replacing human intelligence and trying
to make machines conscious. Um what what
were some of those sort of foundational
ideas of how AI could be applied then
that you think can be relevant now?
Well, I mean, I think that's what we're
all kind of struggling with right now is
now that we have these things in sight
and you know, we're back to what is
sensient and what are the I mean, I
think it was two weeks ago a new paper
came out showing that the actual
original touring test, right, gets
passed the three-party touring test that
GBD 4.5 is capable of passing it, right?
And in fact, 70% of the time, um, people
will pick the AI as the human in the
room. Uh, which I don't know what that
means, but it's better than chance, but
that that's interesting. Um and so I
think we're faced with all these exact
set of issues that a few thinkers are
worrying about for a long time. So does
this replace humans and what do we use
that for? Right? And for augmentation,
what does augmentation look like becomes
the big question, right? Is it you know
and we we that that debate I think never
got as far as as it could be partially
because this was still kind of
fictional, right? So what do we do with
these very intelligent also very limited
machines and then where do humans fit
into the equation? And I don't think
that was ever answered and now it's
suddenly very very important. And the
Turing test was um it was a beautiful
idea back back then. But if we were to
design a new test now, a mollik test,
what would be your mollik test for for
AGI? So I uh I'm struggle with AGI as
this concept all the time, right? Which
is it's badly defined. I mean the reason
why the touring test is interesting just
like all the other tests is they were
great when we didn't have anything to
test, right? Like the touring test was
great when computers obviously failed
it. And similarly, we have the issues
where like the AI is acing all the
creativity tests we have, but those were
designed and they were always mediocre
for humans and now we're expecting AI to
do them. The way we figure out whether
someone has empathy in social science is
the best test is something called the
reading the mind in the eyes test. We
show a bunch of eyes and ask people what
emotion they have they have. Like none
of these things were designed for AI
stuff. So I think about this a lot and I
tend to be very practically oriented on
this, right? So first of all, everyone
kind of has their own AGIish test. Um,
you know, I'm a business school
professor. some some of the easiest is
can this agent go out in the world and
make money and and do things as a useful
useful test. Can we discover new
knowledge and actually test and come
with results? But I mean I think what
we're starting to realize is AGI is
going to be this sort of phase we're in
rather than a moment in in time, right?
There's not going to be a you know
fireworks going off. Tyler Cohen just
said O3 is AGI and when asked why he
says it's like pornography I know when I
see it. Um, and so we don't know what
the answer to these questions are. And I
think it's kind of realizing the
meaninglessness of it because it turns
out also like as you guys have learned,
if you connect AI to systems in the
right way and you connect with company
processes, suddenly you have something
that's much better than the sum of its
parts versus something you're prompting.
If you're just doing conversation, that
feels very different than can we do
strategic decision-m for example. And
frequently when these models are
released, it's always on the most
hardcore math problems and science
problems. It's it's very rarely they
take more business applications. If you
were to design a benchmark that was more
focused on the applications that you see
in in companies, what what would a
benchmark uh for that look like? So I
think that is one of the most critical
problems we're facing right now because
all of the people in the labs are math
and science people and they view the
only good thing you could do with your
life as coding, right? And then add to
that the fact that they want to use AI
to make better AI and coding and math
becomes like the important things
followed by biology because they all
want to live forever. So like that
becomes the angle that that that this
goes down and there are very few
benchmarks other things. So we know the
AI companies build towards benchmarks.
They build sketchy ways right of like
optimizing for benchmarks but also in
more broad ways of they use this for
testing and so the fact the lack of good
business benchmarks is a real problem.
So I actually one thing I've been
pushing is companies should be doing
this themselves to some extent right
like and some of this can be direct
number based like how often does it mess
up in being asked to do an accounting
process but some of this is vibes based
as as they say would you actually could
have outside experts and we've done this
for some of our experiments judge the
quality of answers and is this as good
as a human or not have your own touring
tests for various important parts of
your job right is the analysis report
good enough what's the error rate on it
you know if we use this to give us to
give us strategy advice how good is it
how good is it at a selection decision.
And those are questions that are not
that hard to measure, right? They're not
that technical, but they do require a
little bit of effort. I think that's
that's one of the areas where products
have been largely lacking, too,
especially when you deploy agents. The
ability to test these agents and see
what knowledge they have and what
knowledge they're lacking and correct
them and run these test sets has has
been really really limit limited. Um so
as we think about designing an an an AI
first uh org um so you basically get a
thousand person company and you get to
redesign the org to be completely AI
native. How do you structure it? So the
first thing you say is redesign to be AI
native is hard because it wasn't AI
native right so we are we are in this
really interesting spot where we've had
basically hundreds of years of
organizational development that that is
paralleled you know industrial
revolution the communications revolution
I mean the first org chart came out in
1855 for the New York and eerie railroad
and it solved a problem that never
existed before which is how do we
coordinate vast amounts of you know
traffic on train lines in real time
using a telegraph and they came up
McKenzie the guy who came up with this
came up with the org chart as a solution
and we still use them today. 1910s huge
breakthroughs in organizing work. Henry
Ford's production lines time clocks
still use those today. Early 2000s agile
development right all of these things
broke because they all depended on there
being only one form of intelligence
available which is human comes in
humansized packages can only be deployed
with a span of control of five or seven
people. Uh the you know two pizza
problem and now we're in a world where
that isn't the case. So things have to
be rebuilt from the ground up and I I
worry a little bit that um modern
western companies have given up on
organizational innovation as something
that they do. It used to be that the way
Dowo Chemical would win or the way that
IBM would win would they come up with
new approaches to sales or new
approaches to working with organizations
and now we've outsourced that. So
enterprise software companies will tell
you how to build your company because
Salesforce sells you a Salesforce
product that tells you how you use sales
or a large scale consulting company will
come in and tell you how how your
organization should run. And now is a
time where leaders actually need to
innovate. So if I to return to your sort
of core question, it has to be building
from both the idea that we're heading in
a in a trend line where humans are less
necessary in the product and then where
do you get you have to pick whether you
augmentation or replacement and then you
have to start building the systems from
that fewer people um doing more
impressive work or more people doing
ever more work and trying to take over
the world together. Does does this mean
that we have sort of fewer 100x
employees or do we sort of boost double
the productivity of everyone? Do do we
create this sort of small clusters of
folks that are uh overseeing the
orchestration of of of of agents and are
you know orders of magnitude more more
more productive or is it sort of more
deployed horizontally across the
organization where a few people get more
more so I think those are key choices. I
mean one of the things I really worry
about is when I look at early
implementations what people view this is
as an efficiency technology and I bear a
little blame for that our earliest work
focused on productivity gains from AI
and I still focus on that because it
matters but I I worry a lot that at the
edge of industrial revolution what we're
seeing happen or some sort of new
revolution what we're seeing happen
right now is that companies are viewing
this like a normal technology so they
get a 25% cost savings from you know or
efficiency gain in in customer service
let's cut 25% of people, right? Like I
hear that all the time and there's a
whole bunch of dangers with that. One of
them is nobody knows how to deploy AI in
your organization other than you, right?
You can build tools and the techniques
that are really useful, but ultimately
it has to be people in the company that
figure out is this good or bad. They're
the ones with the experience, the
evidence to do it. If they're terrified
of doing that because they'll get fired
or punished for using AI or they'll be
replaced if there's an efficiency gain,
they'll never show you an efficiency
gain. Right? And then the second set of
problems around that is if we're really
in a world where we're about to see an
explosion of performance and
productivity. The idea that you should
be as small and lean as possible going
into that. Like it's like if you imagine
the early industrial revolution and you
are you're a you know a local brewer in
the early 1800s, you got steam power.
You can either fire most of your staff
and make more money per barrel of ale or
you can be Guinness and hire 100,000
people and expand worldwide. And I
really worry about too many people
taking the small path and not the big
one. and and you've generally advocated
more for human augmentation and the idea
that you know the back in the days we
used to talk about bicycles for the mind
and now we might be getting you know
airplanes uh for for the minds to to to
to some extent. Um in in what ways do
you think this will be augmenting uh
human uh human intelligence? What what
because it's it's been quite
counterintuitive. What what we thought
historically was it would start with the
mundane repetitive tasks and then it
would move on to knowledge work and
coding and then the very last thing it
would take would be the creative tasks
but it's almost been like the exact
opposite um in the sense that you know
the creative tasks the knowledge work
but the mundane repetitive has been
really tricky to automate. So in what
what what ways do you think we'll
actually be be implementing this? I mean
it it is fascinating how much like
everyone you know the image of of AI
would be that if you talk if you tried
to explain the concept love it would
explode right does not compute instead
we have like these weird systems that
are super emotional and have to be
convinced to do things right like we've
actually found in prompt engineering
sometimes what you have to do is
actually just justify to the AI why it
should do a step rather than tell you to
do something it's like no this is why
it's important and you should do it
which is super weird um and so the thing
to think about with augmentation though
is that Our jobs are that we do are
bundles of many different tasks, right?
Nobody would have designed any job the
way we have as a professor, right? What
am I supposed to do? I'm supposed to be
a good teacher and come up with good
ideas and be able to have a conversation
with you and do research and run an
academic department and you know like no
one would be a counselor, right? No one
would want all of these jobs and a lot
of them are sort of hot AIS jobs. I
don't mind giving away grading to an AI,
right? If that helps. Like I wouldn't
mind providing more counseling support
through AI if that helps. even though
these are very human kinds of things. So
I don't think that augmentation
necessarily mean like just because it
does creative engaging sort of human
knowledge work tasks at least at the
current levels we're at it's definitely
below the expert level in these kind of
cases whatever you're best at you're
probably better than AI. So the question
become augmentation level one is just
hand off stuff you're less good at as
part of your job bundle and the second
level is how do you use it to boost what
you're doing right now and we're
starting to get some good evidence for
that too. And what happens when the
systems become uh more proactive than
than than reactive? We're so reliant on
giving these systems input to what they
should give back to us and um prompting
them and and and and so on. At some
point, we should be getting systems that
are better than us at asking those
questions too and can sort of
proactively serve this to us. Is that
something you're seeing? If if you take
your domain as an example, it could go
out and do all of the research for you
and then come and say then sort of this
this matches your research test. Here
are five papers I wrote. Um, pick the
best one. Um, have you started seeing
any applications like that? Yeah, I mean
I there's a couple things you said there
that are really important. One of them
is actually the more minor point, which
is the idea that it gives me 10 papers
to pick from, right? This idea of of,
you know, it's a hot word now, but
abundance. But we're not used to a
situation where you can just get a lot
of something and curate, right? So, one
of the things that actually matters a
lot is taste and curation that I want to
be able to pick out of a subset of
options and that still matters a lot.
That kind of taste piece or what to
pursue and start it starts to look like
management, which is not the end of the
world, right? Like management is what
most of us aspire to anyway, right? Or
at least a lot of people aspire to. And
it starts to be giving your direction
and taste where it goes. But like I
think at the end of this, we don't know
how good these systems get. And that
ultimately every question becomes
downstream of how good you think AI
gets, right? If it's good enough that it
does all of our work at the high level
of what the work you do and your
organization does and the work that I do
as a professor, then, you know, we're
sort of in uncharted territory overall
and I don't know what the answer is. I
think that real organizations um are,
you know, work much more in much more
complex ways than we think about.
They're not always aimed for efficiency
and AI remains very jagged in its
frontier of capability. So it can't
quite do the whole paper because parts
of it'll fail. But I I if I have
experience, I'll know where those fail
and can intervene and shape in those
places just like I would with a PhD
student. So I think we're going to be in
a longer world of limited autonomy than
people think where like direction
guidance, you know, is still going to be
important. I think the jagged frontier
is is probably one of the areas that's
most bottlenecking or organizations now.
It's so incredibly confusing talking to
a system that sometimes is genius and
sometimes is completely stupid and it
also makes it very difficult to uh
deploy it uh uned in organizations and a
bit similarly we've had with
self-driving cars where the deployment
took a very long time because it was
both sort of superhuman in some
applications and other uh in other
situations get quite tripped up. What
what do you think we'll we'll we'll see
in uned agents and how how they will be
deployed? Will we'll be you know
bottlenecked for another decade by the
jagged frontier or will we start
trusting these systems and to end quite
soon? I mean I think we're already in a
place where narrow agents are very good
right so the the best example of those
are the deep research agents that now
have been rolled out by you know Google
openai and uh x right um and the
perplexity as well they're all very good
right and they do the narrow task of
finding information being you know
giving you answers very well and that is
a highly renumerated task right and
they're not quite there yet because they
don't have access to the kind of private
data that people need to be able to use
these systems fully but you know they're
starting to get very good at legal
research accounting and market research
and finance research and like so I think
that there'll that kind of delegation to
a fairly complex task to narrow agents
feels very doable. I think there are
clever ways to do generalized agents
with other agents watching them that no
one's really pushing yet. Like we we're
so new into this that the that you kind
of have to make two bets, right? One is
the whole view of Frontier when when I
came with the idea of Jagged Frontier is
like the frontier is constantly pushing
out. So it's jagged, right? Some of
those jaggedness will stick around for a
while. some it doesn't matter if it's
still bad at it because it as the AIS
get better overall it still beats humans
right and so I think part of this is the
question do you wait for the frontier to
move out and then solve the problems or
do you build around them today and I
think part of the key is doing both
right of like how do we but if you
invest too much on trying to solve the
jaggedness today as long as models keep
getting better you end up stuck with a
legacy system built around a jagged
frontier that no longer exists makes a
makes a lot of sense and and one thing
that's organizations uh is find it quite
tricky is discovering the the the AI use
cases um and they have some bottoms up
strategies where effectively most parts
of the organizations is already using
these AI tools to some extent but just
not telling their their their leadership
and then they have some top down
initiatives where they're like let's
build some AI SDRs or whatever that that
that might be. How would you approach
you know discovering these use cases
internally? What are some tactics there?
So I tend to say you need three things
to make AI work in an organization. You
need leadership, lab and crowd. So we
can talk more about leadership later but
that's the idea that like this the
organization needs to start grappling
with the questions at the CEO level
seuite level of the kinds of things
you've been talking about here. What
does our organization do? How do we want
it to look? What experiments do we want
to do in organizational form? Like those
are fundamental questions. And by the
way, if those aren't answered, then the
incentives aren't set correctly for
people in the organization. And everyone
in the company wants to know what the
vision like you can't say people work
sideby-side with agents without giving
people an articulation of what that
actually looks like their day-to-day
job. So that has to have come from the
leadership level. And one of the
bottlenecks by the way has been that sea
level people have not used these systems
enough. And you can see where they do
because transformation happens much more
quickly. Um, you know, uh, Mary Erdo
said JP Morgan, for example, is, you
know, been very public about using AI
and that's trickled down and part of why
JP Morgan does quite well on AI stuff.
And so there's this leadership piece and
then there's the crowd that you're
talking about. Everybody gets access to
these tools in some way or another. Um,
and then how do you create the incentive
so they share what they're doing, right?
Because there's at least like seven or
eight reasons why people use AI and
don't tell you. Like everyone thinks
they're a genius. They don't want to
seem like a genius right now. They know
that efficiency gains get translated
into people being fired. They don't want
that to happen. they're working a lot
less and why would they ever return the
extra value to the company itself? They
they have come up with brilliant ideas
that that they don't want to share
without you know taking a risk for it.
Like there's lots of reasons people
don't share this stuff. So you have to
align that organization to do it. And
then the issue is like this is done
through individual prompting. So to turn
those into products, to turn those into
agents, to test whether they work or
not, you need to then extract some of
those and start doing some actual real
R&D work, which doesn't mean necessarily
coding, right? Tool bases like the kinds
you build are really important for what
you're doing here, but it's also just
how do we start experimenting? How do we
take what was a basic prompt and turn
into an agent agentic system? How do we
benchmark that system? So you need all
three of those pieces at the same time.
And what use cases have have you found
uh over the last year? you've done a lot
of research both as AI as a collaborator
in a in a team, AI as sort of assisting
BCG consultants and and so on. Um what
type of of of use cases do you think are
inside of the the frontier now where
it's delivering meaningful value? So I
think it's really clear at this I mean
so there's stuff that I think is that
like CSR people still struggle with
right and I think that those are in some
ways riskier things are external facing
human replacement the augmentation angle
the results are really clear right
individuals working with AI and
especially if you have way people
sharing that information ideation it's
absolutely useful to have you generate
better ideas working with AI in this
right there's some methods that work
better than others but that kind of
approach for supplementing work of all
kinds right translation
not just you know translation up and
down levels of abstraction not just
translation directly summarization um
but where you start to see the really
interesting stuff is trying to
accelerate cycles so I'm seeing a lot
more of like rapid prototyping and
development so going from like let's
take an idea then let's have the AI um
let's have the AI generate 25 ideas
let's have it create a rubric and test
those ideas then let's put simulated
people through those ideas and get their
reactions to it refine the ideas further
then let's go and and create a um you
know a prototype working prototype and
interview me about how to make it better
and then build a vibe coded first
version that is literally 25 minutes of
work at this point right with just a
command line in 03 so like we're in a
very weird spot where it's like but then
the organization ends up tripping that
up right because what do you do with the
fact that now we have 45 great
prototypes where's the manufacturing
capability build it where's the output
so that augmentation piece is pretty
good at the beginning and then research
agents are looking really interesting um
and then knowledge management agents
also seem to have a lot of value, right?
Which is like actually this is something
you forgot or thought about. Where I'm
starting to see really interesting stuff
happen is advisory. Like the idea that
we're going to give you advice that's
timely or un is is also really
interesting. What do you think happens
to uh the economy when we have I mean
it's effectively a renaissance where we
just have an abundance of everyone can
code, everyone can do science, everyone
can go deep into so many different uh
disciplines. uh if we um you know get
another sort of 10x uh the output from
the medical community as a as an example
will we still be bottlenecked by by by
the FDA or do you think the the system
will will will adapt and both right
systems take a lot longer to change um I
mean we've been talking to some of the
deep mind people and they are saying
that there's getting real drug
development results in a year that look
really good right um so there'll be
pressure to adapt to those kind of
things. And I think part of the question
like part of the issue with the
uncertainty in the regulatory
environment whether for different
reasons in Europe versus the US for
example um is is that it makes it hard
to figure out where to invest to make
these kind of changes happen because we
are going there's going to be societal
bottlenecks all over the place and
there's also you know the AI only has
limited ability to act in the physical
world at this point right robotics lags
this organizational structure lags this
so how do we start thinking about that
becomes a really big deal I think part
of why people find agents so appealing
is in part the idea that they solve some
of this problem by just doing stuff so I
don't have to worry about it. But at
some point they're going to hit the real
world, right? And and at those friction
points, that is where things slow down.
On the other hand, if you can get up to
that friction point and deliver here's
seven really good-looking, you know,
like um compounds that might make a
difference, that is a huge gain anyway.
So I think that the gain will be more
spread out. Um but we just don't know. I
mean part of this also is how autonomous
these systems get, right? Which roles do
you think will uh will end up being more
useful in organizations as a function of
this? Oh, that's a tough one and based a
lot on organizational choice, right? But
I think I think management roles does
like um roles that are sort of thinking
about systems are tend to be very
valuable because there's systems are
problematic. I think experts anywhere
become valuable, right? Uh it turns out
expertise actually is really good. None
of these systems are as good as an
actual expert at the top of their
fields. we tend to measure against the
average in a field and like the AI does
really well but if you're in the top 2%
of something you're going to be beating
the AI in that field and so expertise
actually matters a lot in this space so
either deep subject matter expertise
broad expertise across many areas um as
a system leader uh or really good taste
tend to be the three things that help
you one one thing that that I've been
thinking a lot about is um you know on
on one end you could be hiring more
senior developers as an example where
you say you know, we just hire the top
2%. Those are the only folks that are
going to be, you know, make a big
difference to us. Another argument could
be actually you could hire much more
junior developers nowadays because the
junior developers will be able to
execute at the quality of much more
senior uh developers. Um, what do you
think there? Should does does the
democratization of expertise actually
enable you to maybe staff your team with
more junior junior talent and maybe
folks that are slightly more senior will
actually not benefit as much from from
from this technology. So there's
actually a few effects happening at once
and I think it's worth unpacking them.
Like our our our Boston Consulting Group
study was the first one to document in
the real world the idea that like there
was this performance gain for the lower
performers got the highest gain. Uh but
people don't talk as much about the why
we found out that happened which is we
measure something called retainment
which is how much of the AI's answers
the consultants only turn ultimately
turn as their own and for sort of 80% of
consulting tasks the only way to screw
up was to added your own thoughts or
ideas into the AI's answer right as long
as you were just turning in the AI's
answer you did great as soon as you're
adding your own thoughts or ideas so
it's basically work at the eighth
percentile so when you say you're hiring
a junior developer and the AI makes them
better I think it's worth specifying is
it just that the the human is
substituting for the things we can't do
agentically yet which is like I'll paste
in the requirement and I'll attend the
meeting and the AI is actually doing the
work right is or is it actually bringing
people up to that level and at the same
time at this sort of really good person
level we're seeing effects where if
you're very good and you use AI the
right way you can get 10 or 100 times
performance improvement so I think you
need to think about both things right
there is this sort of substitution
effect and my view has been that a lot
of the benefit comes from having
expertise and then using AI to
supplement the areas that you're not
you're bad at, right? Like I think about
founders all the time. I was an
entrepreneur. I teach entrepreneurship.
Like entrepreneurship is all about you
being very bad at many things but really
really really good at one thing. And
your whole task as an entrepreneur and
the reason why I teach entrepreneurship
is to have those, you know, the 95% of
stuff you're bad at not trip you up,
right? Like the fact that you didn't
know you needed a business plan or that
you didn't know how to do a pitch like
because your idea is brilliant and you
know how to execute it in this market.
And so the fact that AI could bring you
to 80% in all of that is a really good
thing, right? And that is replacing your
work. But in the area where you're at
the 90 9.9th percentile, you get a 100
times multiplier. And I think that's the
same kind of angle. And I think the
danger is is that if you're hiring
junior people and expect them to use AI
the whole time, how will they ever
become senior? Becomes a real challenge.
What what do you think the answer to
that is? like a lot of the law firms I
speak to for example there's a a core
part of the training is the you know
basic work you do and then as you become
more more senior but you do more complex
legal analysis but when you look at
actually what the juniors are are are
doing I think most of that work is not
actually adding up to what the more
senior role will will be doing it's very
simple repetitive work and so on do you
think that will be an issue where people
don't grow um you know through the
hierarchy to the same extent and as a
function of that we don't have as many
folks that can step into this more
senior roles or will you just go into
the senior roles more quickly? No, I'm
I'm really worried about that, right?
Because like any other university I at
Wharton, you know, I teach really smart
people and but I teach to be
generalists, right? I don't teach them
to be, you know, I teach them about how
to do analysis. I don't teach them how
to be a Goldman Sachs analyst, right?
But then they go to Goldman Sachs or
they go to a law firm or whatever it is
and they learn the same way we've been
teaching any white collar knowledge work
for 4,000 years which is apprenticeship
right and it you're right they're asked
to do repetitive work over and over
again the repetitive work doing it over
and over again that's how you learn
expertise right you get yelled at by
your senior manager you're you know at
the wrong kind of firm or else treated
nicely. Um but you're basically given
correction over and over again till you
write a deal memo. But it's not just
that you're learning to write a deal
memo it's that you're also learning why
this approach didn't work. you're
absorbing a whole bunch of stuff from
your mentor about what the goal of this
is. So, we let like it just happens,
right? Apprenticeship, if you have a
good uh mentor, apprenticeship is a
thing that happens. We don't spend a lot
of time training people for. We just
sort of it's magic and some people pick
it up and then other people get fired,
right? And they might get fired because
they're bad, but they might get fired
because they got unlucky and got a men
good bad mentor or didn't learn the
right things. That mentorship just
snapped this summer. That chain that's
kept going for a few thousand years.
Because what happens now is if you're a
junior person, you go to a company, you
don't want to show people you don't know
something. It's because you want a
senior job. So you're going to use AI to
do everything. So you've turned off your
brain because the AI is better than you.
And every middle manager has realized
that rather than going to an intern who
sometimes like take messes up or cries,
um you could just have the AI do the
work because it's better than an intern.
And I really worry about that pipeline
being snapped. And the problem is is
that we've viewed this as an implicit
thing. Like there's very little work in
law firms to teach you how to be good at
teaching a lawyer, right? To someone to
be a good lawyer. Instead, you hope that
you had a good mentor yourself and you
replicate what they did, right? It's why
bankers will often, you know, like 120
hour weeks is part of your job. Why?
Because that's always been part of your
job and somehow that teaches you
something. And so I think we have to
move much more formally to how do we
want to teach people expertise and work
on it. that ironically the one place we
do this really well is actually in
sports because like that's an area where
we've learned how to build expertise
right practice with a coach and yeah
we're gonna have to do the same kind of
thing in other forms of learning as
well. So how would you think about it if
you started a new university now uh for
for the intelligence era. So assuming
you know models keep getting better over
the next few decades how would you
design a university around that? So
there's a few things happening, right?
One is what should we teach and the
other is how should we teach it. I'm
more concerned about two than one. I
think there's a big thing of like we
need to teach people AI skills and I
think as somebody who's worked with
these systems a lot, you know, like
there's not that like the skills are
first of all there's like five classes
worth of skills to learn, right? Unless
you want to build an LLM, which you
shouldn't do. Um it's really like five
or six skills classes and then there's a
lot of experience. Um, and so I think
the qu it's less about teaching people
to use AI and in fact I think a lot of
the discipline stuff that we teach are
really important. We want people to
still learn to be good writers. We want
that broad knowledge, right? As well as
deep knowledge. I think universities are
well suited to that. Where we break down
is how we teach, right? And so
everybody's cheating, right? And AI
detectors don't work. And they're
already cheating, by the way, but now
everyone's really cheating. There's a
great study that shows that from the
beginning of um from like when the
internet era and social media really
kicked in in like 2007 or 2006 students
at Ruters um who did their homework
almost all of them did better on tests
and by the time you reach 2020 almost
none of them like 20% were getting
better in test because everyone else was
just cheating right so like you have to
do the kind of hard work so AI doesn't
let us skip the hard work but it will
let us with AI tutors on a onetoone
basis you can actually teach people at
their level we can help excel accelerate
the learning process in real ways. And
so I'm much more interested in how you
ch and I already did this with my
classes. How do you transform how we
teach with AI becomes a really
interesting question. I don't know if
the subject matter changes and I think
we can increase scale also teach more
people but I think that some of the core
subjects stay the same and you've done
some really cool things and were
probably one of the first to actually
ask your students to to shoot. What are
some other things in in which you've uh
deployed this and how how you teach how
everything my class are 100% AI based I
mean so I teach entrepreneurship so the
easiest version is it used to be at the
end of a class right and you know people
have raised hundreds of millions of
dollars from my class and the ones
taught by my colleagues the same class
number but um you know you would you
basically have a business plan and a
powerpoint now at the end of a week I
have people have working products right
like literally when I first introduced
chatbt to my entrepreneurship class um
the Tuesday after it came out um you
know the one student was really
distracted came to me afterwards said I
just built my entire our product while
we were talking, right? And that seemed
entirely novel at the time that you
could it would write code was like
shocking, right? And now we're in a very
different world for where that is. Um,
but I think that um so I I have my
students now have AI simulations they
play. They have to teach the AI
something. We have a purpose naive AI
student. There's AI mentors for all the
class material. Uh they have to build
cases with AI. There's AI um watching
what they do in a in team settings and
giving feedback or acting as devil's
advocate. So there's lots of cool stuff
you can do to supplement it, but that's
all in service of having a classroom
experience that's active and engaged.
And so I think that classrooms don't go
away, right? But but what we do in them
kind of transforms. So one thing we've
been discussing is is the organizational
uh design and what it should be
structured like. Should companies hire a
shift AI officer who sort of oversees
all of the internal deployments? should
have a model where they deploy someone
in each team to figure out the the use
cases. What do you think like how how do
you structure your your AI or so I worry
a little bit sometimes on the chief AI
officer thing for the same problem that
everybody is having which is everybody
wants answers and like I talk to all the
AI labs on a regular basis I know you
guys do too you've been doing this for
much longer than most people in the
space and you know the horrible
realization you have fairly quite
quickly is that nobody knows anything
right it's not like the labs have an
instruction manual out there that they
haven't handed to you it's not like that
there's like more data than what I'm
sharing with you guys about this or that
I share online like there's no secrets
right there. Like there isn't everyone's
like desperate to copy somebody else and
there isn't. So like when you say hire a
chief AI officer, how are they going to
have any more experience in the last two
years than anyone else did? No one
thought LLMs would be this good. Like
you guys were there before almost anyone
else that like that gave you a year of
head start, right? Like this is a weird
place we're in. So there isn't someone
you can hire who's like an expert and
they often I mean one of the major
problems of AI in organizations is that
AI meant something very different from
2010 to 2022 that is still important by
the way. large data, you know, going
ahead and actually boosting everything
like still worth doing, right? But like
that's a very different beast. So a
chief AI officer is kind of a hard hire.
I really feel strongly that
organizations have the expertise they
need to succeed internally because the
only people who know how to use AI will
be the people who are experts. It's very
easy for someone who's done a job a
thousand times to, you know, run a model
and figure out whether it works or not.
And in fact, in our BCG study, we have a
second paper that shows that junior
people are much worse at using AI than
senior people, which is not something
people think about usually. They're
like, "We need the digital generation to
come in." Turns out not to be true
because junior people produce a memo and
they show that memo and you're like,
"It's a memo. It's great." And you're
like, "Well, I've looked at this for 20
I've done this for 20 years. Here's
seven things the memo doesn't do well,
right? So expertise and knowledge
matter. So I think it's less about
embedding people in teams." And then we
don't even know what makes someone good
at AI. So what I tend to do is suggest
the crowd and lab need to be linked
together. So what the crowd does is
you're not just surfacing you know AI
use cases. It basically by the way in
almost every organization you max out at
20 30% of people using your AI model
internally and everyone else is either
not using it or they're cheating and
using someone else some of their AI
because they don't want to show you what
they're doing. But you get like 20 30%
of your of your organization using it.
And then you'll find like one or two% of
your organization is just brilliant at
this stuff. They're amazing at it. Those
are the people who will be able to lead
you in your AI development effort. I
don't know who they're going to be at
first, right? And you won't know either,
but they will emerge. And then the
danger is they're making so much profit
for you on the line that you don't want
to pull them off the line. But those
become the people that become the center
of your lab and figure out how to use
it. So I really think building internal
effort is the right way. And it's very
hard for me to recommend hiring a bunch
of people for AI when we don't know what
makes someone good or bad at this. And
your organizational context actually
matters here. And what how do you think
we set up the incentives? So if you have
the experts in each domain and uh you
really hand it to them to figure out how
to deploy AI and effectively automate
away their own role. How do you create
the right incentives for them to do
that? And that's why the leadership leg
matters so much, right? So there's a few
things you need to do. One is this is
easier for companies with good culture,
right? If the CEO says and in growth
mode, right? If the CEO can, if you
trust the CEO or the founder and they
say things like, "Listen, we're not
going to fire anyone because of AI.
We're going to expand what we can do.
We're going to make this work for
everybody and people are incentivized to
do it, you're in a much easier spot than
if you're a large mature organization
that has a tendency to use it funds to
cut people, right? People will know the
difference. So, you have to acknowledge
this to start off with, right? Like, if
this is going to be a threat to people's
jobs, people want to know that and you
have to start thinking through what you
want to say. And then incentives can
often be pretty crazy in these
situations. I've talked to one company
that gave out $10,000 cash prizes at the
end of every week to whoever did the
best job automating their job. Um, and
you save money versus a typical IT
deployment just shoving over a suitcase
full of cash. I've talked to another
company that um before you hired anyone,
you needed to show um you need to spend
two hours as a team trying to do the job
with AI and then rewrite the job
description around the fact that AI
would be used or that you had to spend a
few when you proposed a project, you had
to try using AI to do it and then
resubmit the project proposal as a
result. So like you can incentivize
people in lots of different ways but
that clarity of vision matters so much
right if you say your job in four years
will be working with AI to do something
people are going to be like well what
does that mean like am I sitting at home
you know giving instructions to an agent
am I in a room doing things are there
less of us so that vision actually
matters and I find way too many
executives just want to kick that down
the road and say AI will do great stuff
why would I ever want to share my
productivity benefits with the
organization without being compensated
and so starting with that kind of piece
is really important
So another research you you did was when
when AI is embedded and collaborating
like more like a colleague and you
studied folks that were working
individually, folks that were working in
teams, folks that were working
individually with AI, folks that were
working in teams with w with with with
with AI. What what did that sort of
teach us about how this might be
embedded into teams? So we did this big
study with my colleagues at MIT and
Harvard University of Warwick uh of 776
people at Proctor and Gamble the big
consumer products company and um like
you said they were either teams of two
cross functional teams or individuals
working alone and then working with AI
and teams are alone. First off we found
individuals and this is all real job
tasks right not just like innovation
tasks. We found that individuals working
alone with AI performed as well as teams
um and um which was a pretty impressive
kind of boost and were actually happier
too as a results of working with it.
Like they got some of the social
benefits of of working with these
systems and produced high quality
results. Um and we also found that but
the teams that work with AI were much
more likely to come through come up with
really breakthrough ideas. We also found
that expertise tended to even out. So if
you sort of mapped how technical a
solution was and you had technical
people in room they'd produce highly
technical solutions you prod marketing
people produce highly marketing
solutions as soon as you added AI the
solutions were across the board so they
were much more even so it really turned
out like this was a good supplement to
kind of human work um you know and again
this was pretty naive like we gave them
a bunch of prompts to work with but a
lot of it was them just kind of playing
with these systems back and forth so you
know this leaves the same problem that
we've had before which is you need to
make some decisions like the the typical
company that sort of sits back and waits
for someone else to provide a solution
to them is going to be less well off
than if you start experimenting now and
figure out what works and what doesn't.
And what do you think will be the
interface for for collaboration? will
they just be embedded in natively into
our Google Docs and our Slack and we'll
just communicate with them just the way
we communicate with with all of our
colleagues or do you think there will be
something that's more an agent native
interface where we collaborate with
them? I I mean I think an agent native
interface makes a lot more sense you
know that born built around teams rather
than having each document have a
co-pilot on them. I want something to
maintain state across the various tasks.
I mean, we're close, right? Like, I've
got my phone here and I can, you know,
turn on if if we want to, I can even do
it. Uh, we can turn on, you know, chat
GBD's agent. I can look around us and
give feedback on what we're doing in the
world. And I think that that like that's
a promising way forward. And again, it's
about that redesigning work. I think
agentic systems are more less
interesting almost because they automate
work than that they can bring together
many threads of work. And you mentioned
one example uh a while a while ago which
I think it was ship like hallucinated a
quote from you and you actually thought
that was your your your own your own
quote. When do you think we'll have the
systems crit you know sort of ethanolic
level research and what's required for
that? Is it just sort of feeding them
more of of of your of your context? Do
you think we'll get there quite soon?
And and what will that mean? Would that
mean that you're basically just using
your test to select among the best
papers that it's generating? I mean, I
think a lot of this is already possible
with the levels of models we have. I
mean, there's a paper that shows 01
preview, which is not even a cutting
edge model at this point. You know, the
hallucination rate on the New England
Journal of Medicine case studies went
from like 25% in previous models like
0.25%. Like the hallucination problem
starts to drop when you connected data
sources, when you have smarter models. I
mean it's still there but like you
mentioned uh at one point that you know
I used AI in classrooms and my first
classroom policy was you could use AI in
class and that was great for three
months right when chatb 3.5 came out my
students are smarter than chat GPT and
it produced much more obvious errors and
I let them use AI for anything they
wanted because if they don't add their
own thinking they would get like a B
right like AI was not capable of doing
that GBT4 came out does as well as my
students you know who aren't putting a
huge amount of effort in so I think
we're in the same kind of boat here
which is these systems are very good and
as people who build agentic systems. I
think you're probably realizing what you
know I've long realized what I think we
know which is they're capable of a lot
more when you start thinking about them
agentically and you know Google's been
doing some stuff of building AI labs.
There's been work out of Carnegie Melon
doing the same sort of stuff. I actually
think it's more willpower than anything
else to build a research system that
does interesting work. And it's like so
many other areas in AI where I'm like
wow we've already shown that this can
work really well as a tutor. Where are
the thousand tutor, you know, that are
actually well done as opposed to just
prompting the AI to be a tutor? Where
are the thousand science applications?
Where's the internal training systems?
These are capable right now. Like, it's
really just doing it. What What has been
some of the most surprising uh things
you've gotten to work recently? What
have you seen in the latest generation
of of of the models? Things that didn't
work previously that are now starting to
work really well. I mean, so at with the
latest versions of say Gemini, the
hardest thing you have to do as an
academic is writing what's called a
tenure statement. So you do this
hopefully once in your life and you have
to write a statement where you go up for
tenure. And what you have to do is take
all of the academic work you've done,
which is often 15 years of work, very
complicated, and boil it down to a few
themes and write an essay sort of about
why your research has these themes. I
was able to recently with the new Gemini
models dump in all of my academic papers
I wrote because the context one is huge
and have it develop those themes and it
found two of the three themes I ended up
took me two months to write on my own at
a fairly high analytical level right
like um you know or on the more fun
version I can now throw in any academic
paper I've ever written and say turn
this into a video game and get a good
video working video game out of it um
you know I vibe coded some 3D games
recently which was like I can't code um
and you know building pretty good
working systems so I mean I like
threshold after threshold kind of keeps
falling uh and I'm sh surprised on a
regular basis like can't believe how
much these systems can do and how how
should we be thinking about this in
companies is this the equivalent to like
deploying more IQ into the system is it
deploying more labor into the system or
how should I view this as a company so
there's a tactical and then there's a
philosophic view on the philosophic view
we don't really know right like
certainly in intelligence but like
you've you know intelligence labor are
just sort of like two very simple
inputs, right? But also what does it
mean for better adi to get better
advice? What's it mean to get better
mentoring? What's better to have a
second opinion, right? Um the and on the
tactical side, I think that the thing to
aim is to be maximalist. I think too few
organizations are maximalist. Just push
the system to do everything. If it
doesn't do it, great. You now have a
benchmark for future systems to test and
it might actually just do all the stuff.
If it does all the stuff, you've learned
something valuable. So I really worry
about the incrementalist sort of like
let's summarize our documents like
that's fine but it could do that a long
time ago. Why why are you having that
document summarized? Let's just have it
do the thing as opposed to the
intermediate step. I I I think that's a
really interesting uh point because a
lot of companies now are like let's
start with a small proof of concept and
then we scale up and then it's sort of
six months in and they get stuck in that
proof of concept and they never quite
never quite scale. Whereas you see
others take the approach of let's
actually deploy it everywhere, get
everyone access to this and then double
down on the use cases that work really
well. But even that isn't maximalist
enough, right? Like you're absolutely
right because the problem with the use
cases that work well is they worked well
given the limits of the system and given
what people were able to do at that
point. And the building apps is often
the worst kind of angle because you end
up with now a semisuccessful product
that you have like that you built around
the limitations of llama 22 or whatever
it is because that was I mean we can
talk about the problem one of the
problems IT teams have with being the
nexus of AI deployment is it is very
interested in low latency and low cost
right and it turns out that low latency
and low cost are the exact opposite of
high intelligence in these models. So
there are times where you want to be low
latency, low cost, but there's also
times where it's like I'm willing to pay
15 cents for a really smart decision or
new chemical, right? Like that's a
reasonable amount to pay. Um, and so you
have to like that balancing act can be
really hard because people tend to build
off of cheap small models and then they
get stuck later on, right? Which is why
being agnostic is so important, but also
updating. So even when people do this,
they often don't find the maximalist
approach. So that's where the lab comes
in. You really need people building
impossible things. And what what's the
difference between using it as a centaur
versus a cyborg and and what do you
recommend there? So the the centaur
definition is you know Gary Kasparov
used that term at first. This idea that
I kind of took from that was the half
person half horse, right? the idea that
like you you're you're basically
dividing up the work with AI and I know
you know you know Kasparov's definition
was was vague around that right but like
that's how we view this and this is sort
of the beginning thing like I hate
writing emails I'm good analysis I'll do
the analysis you do the emails cyborg
work is more blended right so my book is
a cyborg task and you know the system's
gotten much better since then but at
that time it was very bad at writing I'm
a very good writer I think or at least
I'm proud of my writing so the I did
almost no writing but writing books is
terrible and so all the things that made
writing books terrible it helped me I
got stuck on a sentence. Give me 30 ways
to end the sentence and pick one. Read
this chapter and make sure that I'm, you
know, like my Substack. I have the AIS
read my Substack all the time, two or
three of them, and give me feedback. I
rarely, you know, I use it for core
writing, but I absolutely get feedback
all the time from it and make changes as
a result. Read these academic papers and
make sure I'm citing them properly. Like
those sorts of use cases are where the
power really comes in. And there was
this other uh study where um the folks
that that got advice from the AI
ultimately ended up being more
productive, but it it was largely
benefiting the the more more senior
folks and and and and not as as much the
lower performers that they couldn't
quite sort of internalize the the the
advice. What does this mean for advice?
If everyone is sort of getting you know
your advice on how to deploy AI in their
or organizations, what what will that
mean for for the society? So, I mean, I
think that part of the thing is it's not
always the same advice, right? Like the
AI is good at context. I think the study
you're talking about is the Kenya study
of entrepreneurs, which was this great
controlled study that you only got
advice from GPT4. They couldn't get it
to make products for them or anything
else. And what they found was that for
um that high performers got I forget
what it was eight or 13% improvement
profitability, which is by the way
insane for advice. Like if I could do
that with my students and just give them
advice and get a 13% profitability
boost, that's amazing. And again,
remember people are jagged too. So like
even if you like you're going to need
different advice than someone else. So
even if you're getting advice from the
AI, it's going to be about the thing
you're weakest at, not the thing you're
strongest at. And the low performers did
worse because their business were
already struggling. So they couldn't
implement the ideas. So I think it's
very much true that the advisory role,
the second opinion role, there's some
danger that does shape us all in the
same direction, right? We find this in
ideation too. The AI has a bunch of
themes. If you've worked with these
models, you know that for example like
GPD40 loves to generate ideas that have
to do with crypto. It loves to generate
ideas that have to do with AR and VR and
it loves environmentally friendly ideas,
right? Like just from the way it's post
training worked, I assume and just
churns these out. And but we found in
some of our other work that if you
prompt it better, you can get as diverse
ideas as a group of people. So part of
this is about like what does the adviser
do for you? Maybe you wanted four or
five advisors. you don't want Ethan
Mollik to be the adviser or you want me
but you also want Adam Grant and you
also want Garrett Kasparov and that can
be valuable too and um if I if I take
the case of abundance uh here and and
prompt you to give 30 examples of uh of
good things companies are doing
deploying AI can you list as many as
possible uh in now what how should you
you mentioned the example of you know
handing out cash for the folks that are
uh deploying it the best. What are some
of these crazy ideas you've seen and
certainly work really well? I've been
see I mean so there's tons of them. I
can't give you 30 and I can't even talk
about all of them unfortunately because
I'm not allowed to but um you know
certainly right the easy stuff is all
your coders use these you know and but
then you know change the reward systems
around doing that. Um so every ideation
session you stop in the middle of
meetings and you ask AI how's it going
so far or whether or not you should
continue the meeting at all um and then
drop out otherwise. uh if if the meeting
if the AI think the meeting is done. Uh
even in physical meetings just stopping
and having an AI conversation with the
AI and thinking about what they're doing
at that stage. Um I I have uh the I have
seen cases where people are using
everyone gets an AI consultant or
adviser that they kind of ask about
strategy decision-m uh on every point.
Um there is some really interesting
stuff being done on training, right? So
ask the AI to simulate a training
environment or play through that one way
or another. Turns out to be really cool.
I don't know. I'm not going to be able
to hit 30 here in the room with you. But
I think that the Ethan probably could.
Abs. Absolutely. And that's how you know
I'm real is that I'm not doing a very
good job. And I'm I'm kind of worried
now. You're not even responding to my
prompts. You have enough footage of me
that I'm desperately worried that this
that you're going to get much better
answers. Yeah, we'll definitely try what
what the what the AI version will will
will do. And um and and what do you
think is the best case scenar scenario?
So assuming everything get gets right,
this gets deployed in into society. What
do you think is this the best case
scenario a decade from now? I mean so I
do think that the idea of sort of a
let's let's leave aside an ASI kind of
world where there we're all watched over
by machines of love and grace, right?
And let's just focus on on sort of I
think what happens is that you know I
mean I the problem is a best case link
also requires policy decisions because
there is clearly going to be employment
impacts from this. We don't know what
form they're going to take. It's very
possible that everyone gets more jobs,
but we need retraining. I don't know
what the future holds in that case. So,
there has to be some policy piece. It's
kind of missing on that right now. But I
think that there is a place where your
jobs get more satisfying because you
have to do less grunt work, where we
have a world where productivity is now
flowing in in fun ways rather than just
like productivity office is like are you
typing enough stuff? But like if you're
architecting a system of agents that's
building stuff for you, suddenly this is
feels like a very different kind of
world you're in. It's much more
satisfying, right? um where you work
less and more stuff comes out and you
add your humanness at the key elements
that you know the people who still have
a sense of style or approach or
perspective produce very different work
than somebody else so you have
differentiation variation I mean that
kind of looks like a world where AI gets
five to 10 times better than it does
right now but it doesn't get beyond that
you know which is sort of a weird thing
to root for in some ways but that's the
easiest way to imagine a you know a kind
of outcome that feels like the world of
today if these systems get a lot smarter
then it's like well why do you come into
work when it's like we could sit here
and you've we've autogenerated this
video here. I feel like 5 years to come
back like recreate the people, make it
3D, put us in a volcano um and have us
talk individually to everybody in their
language and voice, right? We're close
to that. So that starts to change jobs
much more dramatically. And what are
some beliefs u that are in the field
currently that that you really disagree
with? So I think that there is a a huge
focus and I understand the safety focus
but I think there's a huge focus that we
and there's a paper that just proves it
that we need to either focus on
existential risks or not. And I I think
that there's a lot on existential risks
and it's worth thinking about but that
worries me a lot less than agency over
the decisions we're making right now.
And I worry that people are by treating
AI as this technological thing which
we're even having this discussion here
where it's like a steamroller. That's
not actually how this is, right? like we
have to figure out how this technology
is used and shaped and that's important
and everybody who's at this you know at
this event gets to make decisions right
about how AI is used and shaped and
those will in turn shape where AI goes
so I really worry about this lack of
agency kind of approach which is like
the AI will do things to us we get to
make choices and we can make those
choices that defend what we think is
important to be human what our customers
need what society needs and so I that
concerns me is avoiding that kind of
conversation um I also think that a lot
of people in the technical field of AI
don't understand how actual
organizations work and that they're
messier and that you know even super
smart agents won't necessarily change
how companies work overnight right which
is why I always struggle five or 10
years we don't know when the change
happens and it will happen in bursts but
you know you know there's a naive
sometimes sort of like I have I have a
sister who who's a Hollywood producer
and every time I hear that AI will
replace Hollywood I'm like you don't
understand how much work goes into a
Hollywood film and some of that will
disappear in fact they're using AI
actually to accelerate performance is
one fun example. So, she is um she's
made a movie with Michelle Fefeifer and
every time uh and when they have to do
test audio dubs and they now have a fake
Michelle Feifer voice that they could
test the audio dubs with, but they never
can use that for actual theater crowds
because there's good union protections
around the actor. So, it's a test bed to
do experiments, but Michelle Fefeifer
still has to come in and record in her
human voice with what she wants to do or
not. So, I think we can build a world
where we defend that humanness, but we
have to make choices to do it. And if if
you had to prompt a a model and to
basically make all of your decisions
from from now on, what what would you
prompt it? Okay, so I'd probably do
something of, you know, I so first of
all, I'd like to give it a lot of
context, right? Something you guys know
a lot about um about me and my choices.
So paste in a couple couple million
characters of stuff. But I would
probably say, you know, the good thing I
have this advantage disadvantage, which
is I've written enough that the AIs care
about that they have opinions about me.
Um, and um, so I I get a pretty good act
like Ethan Mollik. I get pretty good
answers. It tends to be a little
overenthusiastic and it likes hashtags
for some reason uh, that I don't
recommend and really loves emojis and
I'm not really an emoji person. So I
think it thinks I'm more millennial than
I am. But aside from that, um, if I was
asking for it, I'd be like, okay, so you
know, taking on the person realizing
that you're working for Ethan Mllock to
help make decisions and knowing that,
you know, here are four or five things
that he values that are very important.
Before making a decision, I want you to
go and uh pick four or five possible
options that we might follow in the
decision. At least a couple of them
should be very radical. Then u I want
you to compare those decisions versus
each other and give for each one give
two or three simulated outcomes. Then I
want you to create a a um a expedient
version of Ethan and a thoughtful
version of Ethan. Have them argue over
which path is best. Then I want you to
give me a set of pros and cons for each
of them and then select the best of
those. So a little chain of thought,
little perspective taking. It's a very
good very good one. We should we should
should try it. One thing I actually I I
I did a couple of years back is I
trimmed one on everything that Steve
Jobs had ever said because it was very
interesting to get one that was founded
in in his principle. So during COVID for
example, I asked it you know should
should we go remote? Should we become a
remote first company? And uh Steve
replied to me no 95% of all
communication problems are solved by
putting people in the same room. Always
colllocate teams. And it's quite
interesting if you ground it in a in a
person's writing and so on. It gets a
specific point of view that's not like
the average of of of the internet. Yes.
And that's what's so important when you
going back to that idea of where you get
advice from like and that's why
companies are important like your
founder can have an influence on this.
Your principles if you give the AI a
manual if this is what we believe that
will get very different results than
someone who isn't. I think the idea of
viewing this is this you know universal
mind that is always giving you the right
answer. It's giving you opinions and
points of view and that is a shapable
thing. And if you believe your
principles about the world are right,
giving those principles to the AI to
have it help you execute those
principles is a lot better than just
letting it tell you stuff. One thing I
find quite quite interesting is that the
systems are are yet to be optimized for
for engagement. So we basically just
train them to predict the next the next
token. But um if we know anything about
the sort of consumer services, they'll
very soon um start um evolving to engage
us deep in deeper conversations. You can
can imagine a a bot deployed in our
organization and we want to maximize the
engagement with it and it starts
enticing people and asking them
interesting questions and so on. What do
you think will happen when this once
these systems get optimized for for
engagement, which hasn't really been the
case yet? Yeah, I'm nervous about that.
Um, I think that that is uh is starting
to play with fire and the bigger labs
are starting to realize they can do
that, right? I think if you kind of look
at the trend of OpenAI's stuff, it's
become more casual, more chatty. Um,
there's a a fun incident where the new
Llama 4 model was just released and it
was top of the leaderboards. Uh, and
then it was revealed that the version
that he had that was the top of the
leaderboards was not the same model as
the model that was released to
everybody. And if you look at the
transcript from the leaderboard one,
it's full of emojis. It tells you how
great you are. it like makes little
jokes that are kind of semi funny and
that's not the model they released,
right? There's there's an optimized for
engagement thing that throws out a lot
more tokens trying to flatter you and so
I do worry about that, right? We have
some early evidence that it makes things
more sticky and that you know that that
optimizing for engagement is what made
social media such a risky place to be
and I really do worry about that kind of
outcome. Um and I think it's inevitable
though. Uh and so this is kind of you
know what we do with that becomes a
really big question and and and what um
one thing that I get asked a lot is you
know how should we measure the outcome
of of this? So you made a business
leader and they they want to measure one
thing which showed that we deployed this
and it um improved uh productivity. Um
what do you think we should be
measuring? So, I'm going to this is one
of my uh opinions I feel most strongly
about, which is in the early R&D phase,
the worst thing you do is have a bunch
of KPIs, right? We just talked about
maximizing for engagement. You if you
maximize for something, you'll get the
thing you maximized for and probably not
the other stuff. We don't know what
these systems do. You're spending R&D
cash on this. Like, we know you get
performance improvements because we'll
see those. But if you're optimizing for
performance, is that how many word
documents are produced every day? Is
that how fast people turn around their
reports? Like, is that what you want?
Like part of the problem is
organizations aren't built for the KPIs
that you need to have. Like people are
like it it used to be valuable to
produce as many words as possible. Like
if you can write a good report or four
PowerPoint presentations or cover six
companies now do you want people
covering 25 companies 300 PowerPoints a
week like what what are we maximizing
the number of lines of code that people
are writing? I mean you can imagine some
cases how quickly clear the backlog is
important but is that what we want to
have people do? So I I really worry
about KPIs, measurable KPIs being doom,
especially because they end up always
end up falling to cost savings and
they're always 30% cost savings and
they're always let's fire people which
undermines everything you're doing. So I
think people do need to adopt an an R&D
mindset like the productivity gaines are
pretty clear and will happen pretty
quickly and fine throw them into coding
because like coding there's clear
productivity gains but I really worry
about people who's like productivity
gains for document writing feels like a
risky thing to do because what are you
optimizing for?
[Music]
