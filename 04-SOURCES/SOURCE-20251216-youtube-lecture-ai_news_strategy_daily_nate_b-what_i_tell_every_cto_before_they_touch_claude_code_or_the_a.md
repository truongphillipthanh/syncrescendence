---
id: SOURCE-20251216-818
platform: youtube
format: lecture
cadence: evergreen
value_modality: audio_primary
signal_tier: strategic
status: raw
chain: null
topics:
  - "tell"
  - "cto"
  - "before"
  - "touch"
  - "claude"
creator: "AI News & Strategy Daily | Nate B Jones"
guest: null
title: "What I Tell Every CTO Before They Touch Claude Code or the Anthropic API"
url: "https://www.youtube.com/watch?v=mnWMTzkjWmk"
date_published: 2025-12-16
date_processed: 2026-02-22
date_integrated: null
processing_function: transcribe_youtube
integrated_into: []
duration: "20m 5s"
has_transcript: no
synopsis: "What I Tell Every CTO Before They Touch Claude Code or the Anthropic API by AI News & Strategy Daily | Nate B Jones. A lecture covering tell, cto, before."
key_insights: []
visual_notes: null
teleology: implement
notebooklm_category: claude-code
aliases:
  - "What I Tell Every"
  - "What I Tell Every CTO Before"
---

# What I Tell Every CTO Before They Touch Claude Code or the Anthropic API

**Channel**: AI News & Strategy Daily | Nate B Jones
**Published**: 2025-12-16
**Duration**: 20m 5s
**URL**: https://www.youtube.com/watch?v=mnWMTzkjWmk

## Description (no transcript available)

My site: https://natebjones.com
Full Story: https://natesnewsletter.substack.com/p/ive-reviewed-20-enterprise-ai-builds?r=1z4sm5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true
My substack: https://natesnewsletter.substack.com/
_______________________
What's really happening when AI projects fail to deliver value? The common story is that the model is dumb — but the reality is that nobody defined what correct even means.

In this video, I share the inside scoop on why correctness is upstream of every AI architecture decision:

 • Why undefined quality makes every downstream choice meaningless
 • How humans move goalposts and blame the system for unreliability
 • What reward hacking teaches us about hallucinations and evals
 • Where Microsoft Copilot adoption problems actually come from

Chapters: 
0:00 Correctness is upstream of everything
0:30 Why we change definitions midstream and blame the system
01:24 Normal software vs probabilistic AI systems
02:18 First order decision: What does correct mean?
04:08 When structured and unstructured data collide
05:24 The hidden failure mode: Humans move goalposts
06:56 Example: Sales pipeline probability estimates
08:18 Why hallucinations are a correctness problem, not a model problem
09:18 Goodhart's Law and measurement distortion
10:35 Why models fail at multi-turn conversations
11:35 Reward hacking and emotional AI relationships
13:10 Building a culture of correctness that resists gaming
13:58 Why humans like to stay vague about quality
15:30 Microsoft Copilot's adoption problem explained
17:47 Correctness as claims, evidence, and penalties
19:18 Why this matters for your prompting right now
20:08 The fractal insight: Do you know what good looks like?

Builders who define correctness as claims, evidence, and failure penalties will ship AI that works — those who stay conveniently vague will keep wondering why adoption stalls.

Subscribe for daily AI strategy and news.
For deeper playbooks and analysis: https://natesnewsletter.substack.com/
