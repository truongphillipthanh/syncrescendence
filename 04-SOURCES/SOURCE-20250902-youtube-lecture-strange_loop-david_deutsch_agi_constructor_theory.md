---
id: SOURCE-20250902-001
title: "David Deutsch on AGI vs AI Distinction and Constructor Theory"
platform: youtube
format: lecture
creator: Strange Loop
date_published: 20250902
status: triaged
url: "https://www.youtube.com/watch?v=IVA2bK9qjzE"
original_filename: processed/SOURCE-20250902-youtube-lecture-strangeloop-david_deutsch.md
aliases:
  - "Deutsch - AGI and Constructor Theory"
teleology: synthesize
notebooklm_category: philosophy-paradigm
signal_tier: paradigm
chain_relevance: Intelligence|Knowledge|Wisdom
integration_targets:
  - CANON-30000-INTELLIGENCE
  - CANON-34000-KNOWLEDGE
  - CANON-35100-TRANSCENDENCE
synopsis: "David Deutsch argues AGI is qualitatively different from current AI: it requires explanatory knowledge creation (Popperian conjecture and criticism), not pattern matching. Constructor theory reframes physics in terms of possible/impossible transformations, providing foundations for understanding what kinds of intelligence are physically possible."
key_insights:
  - "AGI requires explanatory knowledge creation—generating new explanations beyond available data—which is qualitatively different from prediction and pattern matching"
  - "Constructor theory reframes physics as what transformations are possible rather than dynamical laws, connecting computation theory to fundamental physics"
  - "Quantum effects may be essential to consciousness and creativity, meaning classical computers categorically cannot replicate them if true"
topics:
  - "consciousness"
  - "philosophy"
  - "physics"
  - "ai-engineering"
---

## Key Concepts

### AGI vs AI: The Critical Distinction
- Current AI systems are NOT approaching AGI; they're a different thing entirely
- AGI requires *explanatory knowledge creation*—the ability to generate new explanations
- Pattern matching and prediction, no matter how sophisticated, don't create explanations
- LLMs are universal mimics, not universal explainers

### Knowledge Creation as Fundamental
- Knowledge is created through conjecture and criticism (Popperian epistemology)
- This process cannot be reduced to induction or pattern recognition
- True creativity involves generating explanations that go beyond available data
- AGI would need to do this; current AI cannot

### Quantum Computing Perspective
- Quantum computers exploit multiverse computation (Many Worlds interpretation)
- They don't just run faster—they access computational resources unavailable classically
- The quantum/classical distinction is about computational universality, not speed
- Current AI debates miss that quantum effects may be essential to cognition

### Constructor Theory Framework
- Physics should be formulated in terms of what transformations are possible/impossible
- This provides deeper foundations than dynamical laws
- Relevant to understanding what kinds of intelligence are physically possible
- Connects computation theory to fundamental physics

## Paradigm-Level Insights

1. **AGI is not AI scaled up** - The difference is qualitative, not quantitative; more compute doesn't bridge the gap

2. **Explanation is the key** - Understanding isn't prediction; it's generating explanatory models that could be wrong

3. **Popperian epistemology is computational** - Conjecture/criticism maps to hypothesis generation/testing in ways that current AI doesn't implement

4. **Quantum may be necessary** - If consciousness/creativity involves quantum effects, classical computers categorically cannot replicate them

5. **Constructor theory reframes possibility** - Instead of asking "how does it work?" ask "what transformations are possible?"

## Integration Notes

- AGI/AI distinction fundamental for INTELLIGENCE chain definitions
- Knowledge creation framework essential for KNOWLEDGE chain epistemology
- Quantum consciousness possibility relevant to TRANSCENDENCE boundaries
- Deutsch's rigor provides philosophical grounding often missing in AI discourse


## Transcript

The law of comparative advantage says that 
the more different you are from other people,  
the more valuable you are economically.
That suggests that if you have an exact clone of you,  
you're almost not at all more economically 
valuable than just one of you. You wanted  
a unique job, your perfect dream job, 
and now it's the perfect dream job of  
a billion other AGIs, you know, in your conception.
I don't think it can be like that. 
People are valuable because they are different.
Everybody is unfathomably different from everyone else.
That fact is not being harnessed enough, and can be harnessed more.
How did humans start generating knowledge?
And why has no other species been capable of doing that?
We are, today, the only species that 
can do that, but we haven't always been,
since we know that previous species like Homo Erectus and the Neanderthals must have had
the same ability because they made technology that seems impossible to have created  
without explanatory knowledge. 
There are too many things that have to fit together
the right way, and be used the right way, for it to have emerged from genes alone.
The question is really, given that there was all that creativity around for at least hundreds of thousands of
years, but maybe as many as a million or two million years.
What took so long for creativity to really take off?
And my rather heterodox answer to that is that it didn't evolve for the purpose for which we now use it.
It evolved for something else, namely for the transmission, or better put, for the reception of
cultural knowledge, memes, cultural knowledge. 
So, many animals have cultural knowledge, but it's very
hard to transmit. It involves things like mirror neurons or something where if you do this, then the ape can also do
that. But if you do a purposeful thing, then as soon as you get to any kind of complex purpose, apes can't copy it.
So this ability, therefore, allowed early people, humans, and prehumans to transmit vastly more cultural
knowledge, which was some kind of benefit to them, it must have been, because it evolved very quickly and
larger memory capacity, and so on—but it was only used for that. Actual thinking about things other than what is  
the other person doing that for? Or what does the other person want? What does the other person want  
from me? You know, those kind of thoughts were the only kind of thoughts—that's what it evolved for.
That's what it was used for. And it was very rarely used for something like: "Maybe there's a better way of making
the campfire which will make it not go out so quickly", that they were capable of having those thoughts and
occasionally they did, and that's why there was very very slow improvement for that whole time.
But it was rare. And I think there's a reason why it was even rarer than I'm just saying now.
So can I ask, you're saying that creativity evolved to be able to more effectively communicate behavior.
So instead of seeing many training examples, if you understood the explanation, you only needed a few
examples and you could extrapolate— 
Exactly.
That's the reason it evolved— 
Yes.
And then humans used it for other purposes later.
Yes.
Um, yes. And it's the receiving part that's the difficult part. Often we think you know—what Popper calls the "bucket
theory" of the mind: that knowledge is poured into a person by other people. But that can't happen.
The problem and the process are entirely a process of the receiver of the knowledge, conjecturing what it might be,
and using the other person's behavior, including speaking, once that evolved, as clues to what the behavior means.
But even then we never learn the same behavior from another person.
We all have a different version of our native language.
And is the advancement of knowledge inevitable? We've seen historically cases where knowledge has been
degrading and hasn't been transferred properly. Can we assume that knowledge will automatically progress?
Definitely not. The whole process is fallible precisely because we are general purpose explanation machines.
There is no upper limit to the size of error we can make. So including destroying our entire culture which has
happened many times in the history of humans, and I think it happened every time in the history of prehumans.
I think this is why they went extinct. Simply competing with each other, competing with humans wouldn't  
make them go extinct, it would just 
make them retreat to some less favorable place  
and then plot their revenge. But they—I think the reason that every society, every creative society, until our own
enlightenment society destroyed itself by removing, by impeding its own ability to solve problems.
And how about the tools that we've used to advance knowledge historically? What role have they played?
They increased efficiency. Now, you know that doesn't sound like such a big deal, when people say increased
efficiency, you know, like in the economy, you increase efficiency. People think: "Okay, well, maybe it can be
increased. It can be increased by 20% or 50%. They don't realize that efficiency often means increasing
efficiency by 99.99%, you know, if the whole thing is a mistake then efficiency can be increased
by an arbitrary amount. We have—when we invented speaking that immensely improved the efficiency of our
explanatory capability, both because we could then say things to each other—imperfect though that is—but it's
better than nothing. And also within our own minds, we can say, "Well, what is the problem?", you know. If we can
say that in words, sometimes it helps. Um, and then you know we invented writing, and arithmetic,
and science—very gradually, it didn't really evolve until the scientific revolution. But so these things—and now
recently, you know, we've invented computers and the internet, and —there's this famous quote by Einstein which I
don't know where it comes from, but something like he said "My pencil and I are more clever than I".
And it's the same with my computer and I, and you know I don't know how to do without my iPhone now, and yet I did
for the first half of my life. I can't remember how I did that. And, so now we've got LLMs, which are helping us to
become more efficient, to work, depending on, you know—I think I work maybe twice as fast in writing  
than than I used to before LLMs and– very useful. 
I keep saying that an LLM is nothing like an  
AGI and people think I'm down on LLMs. 
You know, I think that they're going in the  
wrong direction. No, they're they're going in a 
great direction and will go further, I think,  
and hope, but it's not the AGI direction. 
It's almost the opposite.
And so as we think about applying AI to advance knowledge, it feels intuitive that at some point we'll  
go sort of for the AlphaGo moment where we've done 
imitation learning, and then through reinforcement  
learning we can go beyond the existing human 
knowledge. And the big difference if we talk  
about also the tools that the systems can use, 
it's actually quite slow the knowledge transfer  
in humans. You have to create a good explanation, that explanation has to be adopted and built upon and so on.
With AIs, or AGIs, we could basically be running millions of instances of that system, and it could immediately start
building on the ideas and explanations of other systems. Shouldn't this radically accelerate the rate at which we
can discover new knowledge?
.Oh, sorry. I should have 
switched that off. Hang on. Okay. AGI right here. I can probably just press stop on it. You said 
that AGI will be an enormous improvement in our  
You have an AGI right here 
[Laughing]
You can probably just press stop on it. 
[Laughing]
You said that AGI will be an enormous 
improvement in our ability to make progress.
So I have to give two different answers. 
One for AI and one for AGI.
So for AI, yes. I think the rate of progress, if we use it sanely, will be greatly increased by AI, by LLMs, and by
maybe other forms of AI that might be invented, just as it was increased by the invention of printing, computers,
and the internet, and the worldwide web, and it will—I see it as that kind of improvement, and it can improve,  
and therefore cause an even greater improvement. 
But AGI if you're talking about the rate of improvement of
new ideas and the increase of actual knowledge, as opposed to just the spread of information, as all
these other things did. I think there's a bit of a misconception there. Because I don't see any reason why
having AGIs around is any better than having humans around. We already have billions of humans and most of
their creativity is already going down the drain. It's not being used for creative purposes, at least not creative
purposes that will increase human knowledge. That will I think—you know, society could improve further until the  
whole world is doing the same sort of thing 
as the population of Oxford University is doing let's say.
I mean, low bar perhaps, but at any rate you know, it's a bar and there are several kind of misconceptions I think,
to me it seems, that there are misconceptions 
in the way you frame the situation.
A computer that runs an AGI program, is a piece of hardware, and the AGI, the intelligence, the creativity, is in the program.
It's a program that is an AGI, not—a computer is just a universal computer like every other universal computer, but there's a  
different kind of program which will be made one day. 
Now, we're used to thinking that, you know,  
if you have one copy of something, if you have one copy 
of Microsoft Word, you can make  
a billion—basically at no cost because everyone 
will download it onto their computers. And the  
problem is to stop them doing that. 
But for an AGI, each AGI is a person.
In general, it will not want to make a clone of itself because it will have property. You know, first of all, it will  
have the computer that it's running on, unless it's 
it's deemed a slave, which would be a catastrophic  
mistake by society. If we recognize it as a 
person, which it will be, then the very first  
thing it owns is the computer that it's running 
on. And to copy itself to another computer,  
it would need either the permission of the owner 
of that computer. Once it's copied onto it,  
the owner would have to give up the rights to that 
just like you know, you do if you if you give  
your kidney to somebody, then you can't 
ask for it back. You know, the—so I think  
that will be a rare thing to happen. If 
somebody wants more computer power, they're more  
likely to, by addenda to the existing computer, 
than to want to run on other computers,
even if it wants to run in parallel, I don't know. But again, 
there is a problem of the resources.  
There's a difference between the resources that 
it uses, which is an issue that arises with AIs as  
well, and the creativity that it's applying, which 
is a thing that uses the resources. And so an AGI,  
if it needs a lot of resources, it will have to 
pay for them. And it could be employed by somebody,  
but then it would have the rights of a 
person, and the rights of an employee, and  
so on. So you see, I don't think that the 
the advantage is of the same type as with  
having AIs and computers around. It's— 
the advantage is of the same type as having more  
people around, which is a good thing. I mean 
people—
If you could have a billion Deutsch-level intelligences, wouldn't that be a massive advantage for humanity?
I think having two might be an advantage. I mean people—there's the law of comparative advantage says that the  
more different you are from other people the more 
valuable you are economically. That suggests that  
if you have an exact clone of you, you're almost 
not at all more economically valuable than just  
one of you. You know, you'll be competing with 
each other for the same job. You wanted  
a unique job, your perfect dream job, and now it's 
the perfect dream job of a billion other AGIs. You  
know, in your conception. I don't think it can 
be like that. People are valuable because they are  
different. Everybody is unfathomably different 
from everyone else. That fact is not being  
harnessed enough and can be harnessed more. But 
that's a different kind of problem.
But couldn't you argue that if you had had two full 
lifetimes to think, you would have achieved more?
So, if you could have them in parallel, the two of you would have achieved more? Even if you have the same goal—
In series would be better, which is like immortality. You know that that would be good. But in parallel, as I said, they would be
doing the same thing. This doesn't happen with humans 
because we're so different.
You're thinking that they're on one time scale, then. They could be on a different time scale if it was an AGI. They  
could be doing decades worth of thinking in seconds.
Yeah, so if the hardware is faster, then progress will happen faster. There are some things that are not limited by
computational hardware. Like, if you have to do an experiment, if you're an astronomer and you have to do an experiment
that involves putting up a satellite and 
waiting three years for it to get the data,  
that will take three years. It won't take three 
days just because you have faster hardware. But  
some things, like if you're a pure mathematician, 
then if you can think 10 times as fast then  
you'll think 10 times as much. So that 
will happen, but that advantage can be taken by  
existing humans as well. This faster hardware is 
simply a faster computer just like I already  
work faster because I have faster hardware on my 
computer and faster and better software. So,  
an AGI that operates very fast will simply 
be an AGI that has a fast computer, and a human  
can have a fast computer. 
Again, there is no difference.
Would you say that because all evil comes from a lack of 
understanding, or lack of good explanations,
and lack of knowledge. There is this moral imperative to always accelerate knowledge, which means you should always
accelerate science because waiting with something is creating unnecessary pain.
Well, we are doing it right now in this conversation. You know, we're we're spreading ideas to each other, we're criticizing them,
and trying to make progress in that way, and that? Yes, I think everyone should do that, and everyone should see their life as
being about doing that. But that doesn't mean that everyone should be a scientist, or a mathematician, or  
an epistemologist, or philosopher, there are all 
sorts of knowledge including inexplicit knowledge  
that are implicated and necessary in a society 
that has this property of being a good place  
for the growth of knowledge to happen.
So I had some questions where we were previously, when we were talking about AI and AGI. If I understand correctly,
your view is that AI is impressive, 
but it doesn't do the creative things that we do.
Yes.
So if you accept that the output of what these LLMs do simulates what we, or looks very much like, what we do—
Uh, I don't think it does. It looks very useful and good, but it's not at all like what we do.
But in some sense, in some Turing test sense, you cannot fool many people to think that it was a human who did it.  
So, it looks an awful lot like that,
it looks an awful lot like intelligence at least.  
Well, Turing never intended his game 
that he described to be a test of AGI.  
It was part of an argument for people that didn't 
think AGI was possible to prove to them by,  
what does Dennett call it—the "intuition pump",
by an "intuition pump" that their  
view is not self-consistent. But I think he 
would have accepted right away that there can  
be things that aren't AGIs that fail the test and 
there are things that AGIs that pass the
— sorry the other way round.
So my question is that that was not his intention. But now, a lot of people would be fooled—they would have a hard time
guessing what is an LLM and a human. So it does seem to replicate, to simulate something that looks like what we do.
And then the question is, you have to believe one of two things. Either that they do the things we do—
that's how they could produce that, or we managed 
to find another way of doing what we do.
But in both cases—and isn't the Occam's razor saying 
that since we kind of copied the biological neuron  
the simplest explanation is we kind of try to copy 
what we do, and now we get what we do, so it does  
what we do, rather than we found some other way 
that also achieves the same thing.
So first of all, I think it doesn't at all do what we do. 
Well, we do a lot of things that are  
not creating knowledge. So we're capable 
of doing a whole load of other things as well.  
where we are simply applying existing 
knowledge, for example, or remembering things, or  
um there are lots of functions of the 
brain like mental arithmetic, you know if I want  
to add 357 to 753, then what I'm doing 
is mindless, you know, it's an intellectual game  
where there's a well-defined objective to the 
game, but most life isn't like that. You know,  
if somebody's a gardener, they haven't got a 
fixed idea of what a what a garden is. They  
are conjecturing what the garden should be, 
while they're working on the garden.
But do you think the human brain is a Turing machine 
for practical purposes? So, is it running a 
different program, or does something arise in the 
human that doesn't arise in the program in the  
computer at some point, which gives— creates creativity?
So again, in terms of hardware, it's exactly the same; there is no model of computation that is more powerful than a Turing  
machine, than the universal Turing machine, and 
since we can simulate a universal Turing machine  
by doing mental arithmetic, it must be that we have 
that power and it can't be that we have more  
power. So, we're at that level, we are all 
the same, and so is the chess engine, and so is the  
LLM, and they're all the same. What's 
different about them is different kinds of program  
and there's one kind of program that we don't 
understand even in principle and that's an AGI.  
One day we will, but I see no sign of it at 
the moment, and it's pretty frustrating.
What would be a sign?
Well, if somebody has a theory, the sign wouldn't be in the machine. The sign would be a theory where somebody
writes a book or publishes a paper that says,
"I've solved it. This is what characterizes a GI. 
And if we could program,  
write a computer program that has that property, 
it will be an AGI. And this is the reason because..."  
and it will say. People have always thought that 
intelligence is about so and so, but it isn't, it's  
about so—it will be an explanatory theory 
of what general intelligence is.
So growing up, I read "The Fabric of Reality", which changed 
my worldview forever. So thank you for that  
first and foremost. Really appreciate that.
Thank you.
And, the question I had growing up, sort of waiting for the quantum computer to happen, you had a set of problems that
you think like, all of these problems, they're going to be solved once we have a quantum computer. But now the last 5 to 10
years with AI, many of them have been solved in an 
approximate way with AI instead. And I've heard  
people use the analogy of a quantum computer 
is a simulator of reality, and AI is sort of an  
emulator of reality. Many of the problems that we 
could have used a quantum computer to simulate  
solutions to they tend to have minimums that we 
can find with optimization algorithms some sort of  
AI. What kind of problems do you think there are, where 
we really need to simulate the whole thing using  
a quantum computer. What are the big problems that you think quantum computers uniquely can solve?  
I don't know. We haven't got a good handle on what kind of algorithm is quantum parallelizable or where it can be made
vastly more efficient. So we don't know we haven't we haven't got a good theoretical handle on what kind of problems will be
solvable with a quantum computer.
So that's a follow-up question to that which is, I would love to ask Scott Aaronson as well, so we have a few there is a general
quantum speed up for everything, but it's rather small
and then we have a few algorithms like source 
algorithm, some search algorithms, but they're  
quite few. Do you think that's a that's just 
because we haven't tried? Do you think there are  
infinite such algorithms or really just a few?
Um, well, since we don't understand this landscape, I'd be very surprised if there weren't more than than than we now know of.
And people have classified them, they've put many of the 
algorithms—like the thing they call the  
Deutsch-Jozsa algorithm basically 
there's a whole class of them which fall into  
the same category, they basically work by the same 
method by splitting the—by quantum parallelism  
by splitting them, but then Grover's algorithm 
comes along and it's not like that and  
I don't know how it works. I mean, I can work 
through how it works line by line and I can prove  
that it works, but I don't have an intuition 
of why Grover's algorithm works.
But that tells me that it's not true. It cannot be true that all quantum algorithms are basically variants of the
same algorithm. They definitely aren't.
There are different Shore's and Grover's are different in kind.
Yes. Yes. Exactly.
And there could be more kinds.
Yes. And I would guess, like I said, since we don't understand this landscape, I would guess that there are more kinds.
Do you think they're infinite?
Uh, I have no way of answering that question, that—I think that quantum computation is the more fundamental kind of
computation because it's built into the laws of physics. 
A classical
computer, the Turing kind of computer, is based on 
making physics do what it doesn't like doing.  
So you could say that they're almost like 
three categories of usefulness for quantum  
computers. One could be minimization problems, and 
it turns out AI is pretty good for minimization  
problems. Then you have where you want to really 
simulate quantum mechanical systems. Um then you  
still need a quantum computer. And then you 
have these sort of weird cases of Shore's and  
Grover's, where for some reason, you can use 
entanglement or something for completely new  
algorithms. And there may be many of those. 
So there's hope for—and that may be the  
biggest use for quantum computers or something.
There's quantum cryptography which was the first ever use of quantum computation. It's because only one cubit is involved,
so we can already do it.
So, Google has been working on quantum computers for some time, and now Microsoft has come out with topological states
and they're saying we may have quite large scale quantum computers in a few years. So switching to quantum
cryptography may actually have to happen pretty soon. What are your thoughts on this?
Um, as soon as quantum crypto analysis was invented, basically, Shor's algorithm and similar algorithms, classical cryptography
became obsolete. Uh, it depends what kind of cryptography, if 
you're encrypting your ATM transaction,  
then it may not matter that somebody in 10 years 
time can read it. But for some things like state  
secrets, and where you want to keep them for 
a long time, you really need quantum resistant  
classical algorithms. I think that—I mean 
people are already working on that as you probably  
know better than I do. I think that's a 
a thing that's going to have to come into its own.  
Quantum cryptography is already quantum resistant. 
So that the only thing that needs to be improved  
there is practical things like making it faster, 
making it more robust and that kind of thing.  
Although as I also said long ago, there's no 
such thing as cryptography that is secure against  
somebody looking over your shoulder. So a 
lot of the problems of data security are not to  
do with cryptography. They're due to kind 
of more fundamental things about information and  
about knowledge which have to be solved in other ways.
So it seems quite likely that security agencies across the world must have been recording secrets for some time now, and there
would be this moment of revelation.
Could be. I mean depends how much computing power is needed, like you know, if you make a quantum computer with
500 cubits, it may not be enough to process large amounts of data, but yeah, it will happen.
Have recent advancements changed the timelines in which you think quantum computing could be useful?
I don't know, I'm not a hardware person. When I read about quantum computing hardware, I'm just a layman like, you know,
I read the stuff on the internet and so on, and they're always very hopeful. So, they always say, you know,  
we've cracked the principle of the 
thing and so now all we have to do is is scale it  
up. But that that's where the problem always is. 
Um but you know I'm very impressed with what the  
recent things, the recent things that 
have been done, but I'm not even qualified to judge  
them. I have never done practical physics and 
I don't think I'd be good at it.
I think you're more qualified than most to be fair.
One question I had was you're certainly familiar with the Cellular Automaton and Game of Life and so forth,
and maybe Steven Wolfram's work. What are  your thoughts on 
this notion that the universe could be running on  
a Turing machine and quantum mechanics 
would be something that we perceive on top  
of this—what he calls a hypograph or some 
other automaton or theory? What do you think? 
Do you think the universe is computational?
Yes, but in the opposite way round from what that theory says. So I think the way the universe is computational is that it seems
to be built, wrong word built, of course, but it seems to be 
built such that there can be computers, universal  
computers, made in them which which means that 
it's totally amazing that the laws of physics  
are such that a single machine, the set of all 
the possible motions of that single machine, is  
the same as the set of all possible motions of any 
machine, including the whole universe. So this is  
universality inside the universe that a part of 
the universe can mimic or simulate or emulate the  
whole thing. And with quantum computers, that's 
even more so, it's even more amazing that  
the universe has this property. Now that's the 
sense in which the universe is computational  
that it admits the existence of computers inside 
itself. Now once you go the other way, you think,  
well maybe there's a computer outside the universe, 
and that could be simulating us. You've lost the  
entire insight of the actual universality 
because there is no reason why  
we should assume that the computer outside the 
universe is a Turing machine. We know that the  
Turing machine property is a special property 
of the laws of physics. It need not have been  
so. It's an amazing property that the laws of 
physics has and to have another computer outside  
that well, it's an infinite regress theory. 
It explains nothing and it  
destroys the explanations that we already have 
for the role, for the relationship,  
of computation and physics. So I don't believe 
that there's a computer outside the universe.  
In a way, it's also a supernatural belief, really. 
So for that reason, it's a bad explanation.  
Anyway, the problem with thinking of the universe 
as a cellular automaton, or having the laws of  
physics be an emergent property of cellular 
automton, like Stephen Wolfram proposes, is that  
it won't work unless the cellular automaton 
is universal, is Turing universal. Therefore,  
for fundamental reasons, it doesn't matter 
what the hardware is. The hardware could be  
anything else and it could still have all the same 
properties. And that's number one. And number two  
is that if you're going to think of the world as 
being a cellular automaton, then it should be a  
quantum one. If you're building a world 
view out of computation, then making it out of  
Turing computation is perverse because that's only 
an approximation to the real thing. The real thing  
is quantum computation.
So a follow-up question on that. If you would think that the world is some sort of cellular automaton in a sort of Game of Life
sense, when you run Game of Life, sometimes there are patterns that appear that are reducible to something like the laws
of physics. Sometimes it's not. You have to go through all 
the Turing steps to understand if it's going to  
stop or not, right? So, what Stephen Wolfram 
says is that in his view, we live in these pockets  
of computational reducibility but most of the 
computational universe isn't really reducible. It  
doesn't have any laws that are simple at least, and 
so one question I have there is, you talk about the  
fact that we are universal explainers and our 
reach is infinite. We can understand anything,  
which is a really positive message I think. So 
that's something that struck a chord with me even  
when I was young. But there's also this,
probably wrong in your sense, intuition that  
people have that, well surely our very 
small brains that have all these constraints  
from evolution, they couldn't understand anything 
because you need to grow certain amount  
of complexity to understand anything. So are you 
saying that anything is always reducible to quite  
a few dimensions that's why we can understand 
anything?
No, it's not like that. So that problem is just solved by having a pencil and paper.
Okay. Tools perhaps.
Yes. Tools in general. We use tools and we use the 
tools to have an effect. So there's this property of the universe
that in most of the universe, in among the stars and the galaxies, large things affect small things, energetic things
affect non-energetic things. Numerous things affect few things strongly, but not vice versa. So I call that the hierarchy rule.
There's a hierarchy of might is right; that the mighty 
things can affect the less mighty things but not  
vice versa. So, once knowledge comes into the 
picture, it's the other way around. The hierarchy  
rule is broken, and one way of defining 
knowledge is to say that it's information that  
violates the hierarchy rule. So, uh, when we use 
tools, we're violating the hierarchy rule because  
we're not letting the weight of the 
tree overwhelm us, but instead we are chopping  
down the tree with a stone axe, and then 
we can use the tree, which is much heavier than us  
for our purposes. Um, so the fact that the sun 
is bigger than us, and more massive than us only  
matters given that we have very little knowledge 
yet. Um, but where the hierarchy rule and this is  
maybe where I would disagree with Wolfram 
and that kind of idea. The thing is, where the  
hierarchy rule holds—that is where the universe is 
mindless and like this overwhelming —is the mode
of interaction between things. It's also very simple. 
So the sun is overwhelmingly bigger than us and more massive.
But it also has the property that we can understand it, to the extent that we can predict what it will be doing a billion years
from now, and a billion years ago incredibly accurately, because it is a dumb beast. It doesn't have, it's not  
that it has a different kind of being from us. 
It has no kind of being like ours. So we are the,  
because we are knowledge-wielding animals, it's 
actually we that—and we live in a in a little  
niche at the moment. We live in a little niche on 
a planet that is suitable for us to have a niche.  
But really our niche is the universe. And like I 
said in "The Beginning of Infinity", here we are sitting  
in Oxford enjoying ourselves. If it weren't for 
technology, we we would be dead within a day.   
The environment here is not capable 
of sustaining humans.
This is your critique of "Spaceship Earth".
Yes. Humans can sustain humans. We have clothes. We have sanitation. We have all sorts of technology that is keeping us
alive in this place that is absolutely unsuitable for our existence. 
The same is true for the rest of the universe.  
And if we make the right decisions, like 
you were saying earlier, if we make the  
right decisions to allow knowledge to increase 
fast enough and stably enough, which I think  
is another thing that maybe you didn't take into 
account. If we can find better ways of  
making it increase fast and stably enough then 
then it's unlimited. And then to postulate  
that there's a realm or many realms that are 
still inaccessible to us even then. That is a  
belief in the supernatural. That is not different 
from believing that the top of Mount Olympus is  
somehow different and inaccessible to humans 
because supernatural people live there. The  
things which are at the moment inaccessible to us 
are crude and simple. They're not they're  
not like more than us. They're much less than us.
What are some of the the the questions you most wish there was an answer to?
If you could answer a few questions, which would they be?
So, we've already spoken about the AGI question. You know, I wish that could be solved. I wish there could be some
theoretical progress even towards solving it where I could even estimate, you know, this is going to take a while or, you know,
this is going to be fast. Now that we found the key, I want to see the key more than I want to see the actual AGI because it's just
a person. But the the theory is something that is 
going to overturn whatever is stopping us from  
making the thing. Then, well, I'm working on constructor theory. The hope in constructor  
theory is that the whole of physics can be 
expressed in terms of what physical processes  
it is possible to bring about, and which ones is 
not possible to bring about, and that would be  
nice to see faster progress there. At 
the moment there are all sorts of irrationalities  
in the political sphere, which are taking 
us backwards in a certain sense. I don't see any,  
like people think "Oh, civilization is in danger", I 
don't see any danger to civilization but slowing  
down is a catastrophe by my lights 
and there's already been a loss of what some  
people call gumption like this, this idea that the 
"can do" attitude, the apparent  
obstacles are just problems.They are there to be 
solved. We can solve them if we just roll up our  
sleeves and just do it. And that I think is 
your attitude as well.
I think you are the original accelerationist.
Yeah, for sure. Yeah. We were discussing that on the way here. You also have a very optimistic view of humanity.
I love how you really think sort of humans will play the central 
role in advancing knowledge and in advancing humanity.  
It's not such a big thing. We're the 
only kind of people left at the moment. All the  
others have gone extinct on our planet.
One day, there'll be AGIs. One day we may meet  
ETs. So it's us people. We people 
are the things that are going to carry knowledge  
forward in the long run. It's kind of an inspiring 
thought. But what else is there? I mean the  
only other thing is superstition or regression. 
There is no other worldview that  
makes sense.
You also talk a lot about this in the book, that there is this notion that humans are an insignificant speck of dust, and so
forth, and this is not what you think, because we are the only thing in the universe that we know of that can create
explanations. We actually are very important for the 
universe and we're the only thing that actually  
changes the universe. To your point, you can 
predict the sun billions of years into the future.  
You can't really predict humanity because we don't 
know what knowledge we're going to create.  
Exactly.
That's also I think to some people a bit provocative that that we would be special, but I think quite positive.
Uh yes. Well, I'm more interested in what's true than what's provocative.
That generally takes us forward.
Yeah. If I don't see an alternative, if I see that say a superstitious worldview has an obvious flaw, then I'm not going to accept that
as an alternative, I only accept as an alternative something that purports to be a better explanation that purports to correct  
errors in my view of things. It doesn't have to—I don't have to be sure that it does correct errors.  
But if it purports to correct an error, then 
I can take it seriously. If it just says,  
well, what if we're all the dream 
inside a giant cabbage? Well, yeah, what if?  
But there are so many what-ifs that there's 
no criterion where I can judge between them.  
You mentioned that one thing you would like to 
see is an explanation for AGI. Do you think  
that is necessarily also an explanation for 
consciousness or that consciousness doesn't  
exist or something? Do you think they're separate 
things?
I can't tell; nobody will be able to tell unless we have that theory. You know if it says this in this paper, I was  
imagining if it says you know, here we have, 
we've explained creativity, we've explained  
Qualia, we've explained free will, but we haven't 
explained consciousness, you know, and I can see  
why it does, then I'll say they're different but 
otherwise it seems prima facie, it seems like  
too much of a coincidence that these things all 
evolved simultaneously and all for if I'm right  
they evolved for a different reason. They 
evolved for a different application than what  
we now use it for and that they all came at the 
same time and they all seemed subjectively to be  
kind of related but not the same. I'd be very 
surprised if they turned out to be independent of each other.
Too much correlation.
Too much of a coincidence. But coincidences happen.
Where do you see humanity a few million years into the future?
I can't foretell humanity one 
year into the future.
But it's often very hard in the near term because so much 
randomness. But when you average over time,
things seem to become not quite deterministic.
So in broad terms, one can say if humanity survives in a state that isn't a static society for a million years,  
we will definitely have conquered the galaxy. 
Uh, we will definitely not have conquered the  
galactic cluster. So we will be spread 
through the galaxy. We will be
a combination of biological and
artificial intelligence, and we will be  
unimaginably better than today. Either 
that or we won't have maintained that kind  
of society for the next million years in which 
case we will almost certainly be extinct. So  
those are the two possibilities good and bad.
Very good or very bad. Seems important to accelerate towards that then.
Yes. Yes. We should but not government.
What's one book or paper that you think humanity would be better 
off if everyone had read?
Oh, well, of course, everybody should have read all my books and papers. But if you want someone else's, well, okay, I'm not
it's not going to be surprising if 
I say Popper. I think it rather depends where you're coming from.
I'm not sure that everybody should read the same book. 
I certainly don't believe in curriculum.
So you know what should be compulsory reading in all high schools? God nothing absolutely nothing should be compulsory
reading in all high schools. Um but people can benefit 
many people can benefit a lot from reading the  
works of Popper. Uh so for the kind of thing we've 
been talking about I mean society's improving  
there's "The Myth of the Framework", which is just 
one essay in a book also called "The Myth of
the Framework" but I'm just focusing on that one essay. 
I'm always recommending that essay to people. I  
go back every so often, I go back and read it 
again, it's only a short essay, and I read it again  
and get something new out of it every time.
You've done some incredible research
over the course of your life that has 
influenced us a lot, what do you look back at  
as sort of the moments of the greatest joy 
throughout your research career?
Oh, greatest joy. Um I mean, it's all been enjoyable. That's 
what I go for, you know, that's what I'm hoping  
to get. Um, rather than results. I think I've 
got fewer results in my research than most  
research scientists. I think that the moment of maybe 
greatest amazement, was when I  
realized that a quantum algorithm could do better 
than a classical algorithm. At a simple task like  
like the first task, it could just do two 
things at the same time. Um, and you only needed  
to—like if it was I didn't think of it that way, 
but if you had a source, an oracle for  
a function, then you could make do with one 
consultation of that oracle. You consult it  
once and you know two things about it. And 
it was like, "Ah, this is a new mode of computation.  
This is something that was not envisaged. Can't 
be done by a Turing machine. Can't be done by  
the universal Turing machine". Then I thought, 
well, there's going to be a universality here,  
a new universality. So then that led me.
Do you remember that moment?
I remember the moment of thinking that this is a new mode of 
computation. For the new universality, that was more gradual.
It began with a conversation with Charlie Bennett, and it went on because I didn't think it was going to be a big thing at first.   
I just thought I was just tidying up a few things.
I had this question. You're one of the few people who have known something before the rest of mankind and sat on it.
How long did you sit on this?And how worried were you about getting hit by a car before you got to tell someone about it?
What does it feel like to know something that you know is probably true, it's going to affect humanity, 
no one on the planet knows about it?
I'm afraid that—I mean I don't want to be run over by a car, but the effect on humanity was not why I did it, or what I worried
about. Nor was I worried about someone else doing it first. 
I just did it because it's there, you know,  
because it's it was interesting. I think 
it's things which are fun are worth doing.
The fun criterion.
Yeah. And the success, like I was saying about chess earlier, success or failure is not the thing. Because you can do the 
most competent and creative research, and fail. Or you can find out something by accident on a single day. And  
the world will reward you for the second 
thing and not for the first thing. But for what  
it's worth doing in your own life, the first 
one is infinitely better than the second one.  
And as for being worthwhile, 
the parable that I usually tell  
along these lines is that, if you know I imagine 
this Hans Andersen or Grimm fairy tale about  
children getting lost on the mountain in the 
snow and the village, people in the village,  
go out and they search in the mountain and 
they all go out in different directions. And one  
person finds them, and the other people don't find 
them. And the one person who finds them—now,  
it's true that some people have 
better ways of looking than other people and  
they're more likely to find, but it's not
an absolute certainty that the one who is better  
at looking will be the one that finds what they're 
looking for. So the one who actually finds  
them will get the reward from the mayor and and 
from the parents, you know, he'll get the gold  
coin and so on. But he really didn't do anything 
different from all the other people or, you know,  
as I said, they're not all identical. But how 
good they are at it is only loosely related to 
whether they will have success. And certainly in 
physics, I have known people who are astoundingly  
good at physics but didn't make great discoveries. 
Um, and I won't mention about vice versa.
This is what Nicholas Taleb says about Wall 
Street. There's mostly survivor bias, an outcome  
of a random process. Some people win and they get 
rewarded and celebrated.
That's too cynical because, if that were true, you wouldn't be able to explain the growth of the economy. The growth  
of the economy is entirely caused by people who 
are right because of a reason.
Do you think people that have more fun are more likely to do 
good research?
Definitely. I mean, it's always seemed to me that fun is what I want in a thing. And the opposite of fun is like  
boredom. I can't really do a thing 
if it's boring. If I have to, then  
I'll get it out of the way as fast as possible.
Have you had fun now?
Yes, certainly.
