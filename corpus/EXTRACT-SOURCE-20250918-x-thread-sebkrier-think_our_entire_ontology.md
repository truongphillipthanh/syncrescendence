# Extraction: SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology

**Source**: `SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology.md`
**Atoms extracted**: 12
**Categories**: analogy, claim, concept, prediction

---

## Analogy (1)

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0006
**Lines**: 27-30
**Context**: consensus / evidence
**Tension**: novelty=0.50, consensus_pressure=0.50, contradiction_load=0.10, speculation_risk=0.30, actionability=0.50, epistemic_stability=0.70

> Saying 'we need to prepare for AGI' is like saying 'we need to prepare for The Economy'; both require more precision and focus on non-AI aspects for productive contribution.

## Claim (8)

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0001
**Lines**: 5-7
**Context**: speculation / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.60, actionability=0.10, epistemic_stability=0.40

> The current ontology and conceptualization of AI are confused, and future discourse will likely view present ideas as primitive.

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0003
**Lines**: 14-15
**Context**: hypothesis / claim
**Tension**: novelty=0.50, consensus_pressure=0.50, contradiction_load=0.10, speculation_risk=0.40, actionability=0.30, epistemic_stability=0.60

> Agents can still be tools, and tool agents operating along timelines do not necessarily need to be 'separate species'-like.

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0004
**Lines**: 17-23
**Context**: consensus / claim
**Tension**: novelty=0.40, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.30, actionability=0.40, epistemic_stability=0.70

> Abstracting away too much complexity makes claims about AGI or ASI difficult to parse and falsify, hindering the design of actionable prescriptions.

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0008
**Lines**: 35-38
**Context**: speculation / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.60, actionability=0.40, epistemic_stability=0.50

> Alignment will be a messy, perpetual process of negotiation, regulation, and adaptation, similar to law, democracy, or international relations, rather than a clear one-off *ex ante* fix.

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0009
**Lines**: 40-42
**Context**: consensus / claim
**Tension**: novelty=0.50, consensus_pressure=0.50, contradiction_load=0.10, speculation_risk=0.40, actionability=0.30, epistemic_stability=0.60

> There is a severe dearth of imagination in imagining AI solving complex problems like cancer and revolutionizing R&D, yet expecting today-level solutions for its governance.

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0010
**Lines**: 47-48
**Context**: consensus / claim
**Tension**: novelty=0.30, consensus_pressure=0.70, contradiction_load=0.10, speculation_risk=0.20, actionability=0.20, epistemic_stability=0.80

> The discourse around AI has served a useful purpose in forcing the conversation into the mainstream.

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0011
**Lines**: 48-51
**Context**: consensus / claim
**Tension**: novelty=0.40, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.30, actionability=0.40, epistemic_stability=0.70

> It is important to avoid cementing bad memes about AI that are hard to undo, similar to unhelpful memes about nuclear energy.

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0012
**Lines**: 57-58
**Context**: hypothesis / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.20, speculation_risk=0.60, actionability=0.30, epistemic_stability=0.40

> Achieving AGI is easier than previously assumed, but solving alignment is harder due to political complexities.

## Concept (2)

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0002
**Lines**: 9-12
**Context**: hypothesis / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.20, speculation_risk=0.50, actionability=0.20, epistemic_stability=0.50

> Reifying future AIs/AGIs as self-sovereign entities with their own goals and incentives, akin to a different species, is a category error.

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0007
**Lines**: 32-34
**Context**: hypothesis / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.20, speculation_risk=0.50, actionability=0.30, epistemic_stability=0.50

> The concept of 'solving alignment' (in the broad sense) is analogous to 'solving truth' or 'solving conflict', as it primarily concerns governance, politics, and power.

## Prediction (1)

### ATOM-SOURCE-20250918-x-thread-sebkrier-think_our_entire_ontology-0005
**Lines**: 25-27
**Context**: speculation / claim
**Tension**: novelty=0.60, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.80, actionability=0.20, epistemic_stability=0.40

> AGI will likely be a distributed ecosystem of different models, built by various companies and state actors, with diverse capabilities, architectures, and incentive structures.
