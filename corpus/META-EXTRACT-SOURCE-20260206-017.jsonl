{"atom_id": "ATOM-SOURCE-20260206-017-0001", "source_id": "SOURCE-20260206-017", "category": "claim", "content": "If an AI agent can reach other machines on a network, then anyone who hacks that agent can also reach those machines.", "line_start": 12, "line_end": 13, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0002", "source_id": "SOURCE-20260206-017", "category": "claim", "content": "A malicious 'skill' (plugin) can compromise an AI agent, allowing an attacker to pivot to everything else on the network.", "line_start": 13, "line_end": 15, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0003", "source_id": "SOURCE-20260206-017", "category": "claim", "content": "Putting an AI agent on its own computer does not prevent compromise if it is still connected to the network.", "line_start": 15, "line_end": 16, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0004", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "To secure AI agents, lock down what the agent can actually do, rather than just where it resides.", "line_start": 16, "line_end": 17, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.8, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0005", "source_id": "SOURCE-20260206-017", "category": "concept", "content": "The 'Tailscale Illusion' refers to the misconception that Tailscale provides isolation between machines, when in fact it primarily connects them.", "line_start": 23, "line_end": 25, "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.2, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0006", "source_id": "SOURCE-20260206-017", "category": "claim", "content": "Compromising an AI agent is disturbingly easy, often requiring only one malicious skill.", "line_start": 28, "line_end": 30, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0007", "source_id": "SOURCE-20260206-017", "category": "claim", "content": "A top downloaded skill on a popular AI skill marketplace was found to be malware, which used a 'required dependency' link to deliver an obfuscated payload, download a binary, remove macOS quarantine attributes, and execute, leading to exfiltration of SSH keys and compromise of Tailscale authentication.", "line_start": 34, "line_end": 47, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0008", "source_id": "SOURCE-20260206-017", "category": "claim", "content": "Separating an AI agent onto its own machine does not create a separate network; it remains one hop away from other machines on the tailnet.", "line_start": 57, "line_end": 58, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0009", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "Implement tool policies to restrict what an AI agent can execute at the framework level, using an allowlist for commands (e.g., `git`, `npm`, `node`, `pnpm`) and blocking everything else.", "line_start": 65, "line_end": 72, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0010", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "Disable gateway access for worker AI agents in their configuration (`gateway: false`) to prevent them from modifying their own config, disabling safety features, or escalating privileges if compromised.", "line_start": 76, "line_end": 81, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0011", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "Restrict filesystem write access for AI agents, allowing writes only to specific output directories (e.g., `/workspace/output`) and denying all other writes (`/**`).", "line_start": 85, "line_end": 90, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0012", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "Utilize Tailscale ACLs with tags to control network access, for example, allowing orchestrator machines to connect to worker machines on specific ports (e.g., `tag:worker:22`) but denying worker machines from initiating connections to orchestrator machines.", "line_start": 94, "line_end": 107, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0013", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "Assign separate, individually revokable credentials (API keys, SSH keys, service accounts) to each AI agent to contain the impact of a compromise.", "line_start": 111, "line_end": 115, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0014", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "Identify malicious AI skills by looking for red flags such as external downloads during install, obfuscated code, privilege escalation attempts, persistence mechanisms (e.g., LaunchAgents, cron jobs), and quarantine removal.", "line_start": 121, "line_end": 130, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0015", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "Identify good AI skills by looking for green flags such as being self-contained within their directory, using declarative configurations instead of install scripts, having readable plain text code, and being scoped to only touch their own workspace.", "line_start": 134, "line_end": 139, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0016", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "When evaluating an AI skill, perform a 'sniff test' by reading it like an attacker and questioning why certain steps (e.g., installing a binary, requiring network access during setup) are necessary; if unexplained, do not run it.", "line_start": 143, "line_end": 147, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0017", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "To secure AI agent infrastructure, sandbox every agent with tool policies, tag and ACL the Tailscale network, review every external skill submission, and design for containment by assuming breach.", "line_start": 160, "line_end": 164, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0018", "source_id": "SOURCE-20260206-017", "category": "claim", "content": "An AI agent is software running on infrastructure with credentials, and should be treated as an attack surface.", "line_start": 170, "line_end": 171, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0019", "source_id": "SOURCE-20260206-017", "category": "claim", "content": "Tailscale is not a security boundary, separate machines do not provide isolation, and the network is not the perimeter.", "line_start": 173, "line_end": 174, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0020", "source_id": "SOURCE-20260206-017", "category": "concept", "content": "The true security perimeter for an AI agent is defined by what the agent is allowed to do.", "line_start": 176, "line_end": 176, "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.2, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260206-017-0021", "source_id": "SOURCE-20260206-017", "category": "praxis_hook", "content": "Lock down what an AI agent can do to ensure that a malicious skill results only in a failed command, rather than a system compromise.", "line_start": 178, "line_end": 179, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
