# Extraction: SOURCE-20251216-818

**Source**: `SOURCE-20251216-youtube-lecture-ai_news_strategy_daily_nate_b-what_i_tell_every_cto_before_they_touch_claude_code_or_the_a.md`
**Atoms extracted**: 7
**Categories**: claim, praxis_hook

---

## Claim (5)

### ATOM-SOURCE-20251216-818-0001
**Lines**: 16-17
**Context**: consensus / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.20, actionability=0.60, epistemic_stability=0.70

> The primary reason AI projects fail to deliver value is not due to the model's intelligence, but because 'correctness' is undefined.

### ATOM-SOURCE-20251216-818-0003
**Lines**: 21-21
**Context**: consensus / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.20, actionability=0.70, epistemic_stability=0.70

> Undefined quality in AI projects renders all subsequent architectural choices meaningless.

### ATOM-SOURCE-20251216-818-0004
**Lines**: 22-22
**Context**: anecdote / evidence
**Tension**: novelty=0.50, consensus_pressure=0.50, contradiction_load=0.20, speculation_risk=0.30, actionability=0.50, epistemic_stability=0.60

> Humans often shift goalposts and attribute unreliability to the AI system rather than their own undefined expectations.

### ATOM-SOURCE-20251216-818-0005
**Lines**: 33-33
**Context**: hypothesis / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.20, speculation_risk=0.40, actionability=0.60, epistemic_stability=0.50

> Hallucinations in AI are fundamentally a correctness problem, not solely a model problem.

### ATOM-SOURCE-20251216-818-0007
**Lines**: 43-43
**Context**: consensus / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.60, actionability=0.70, epistemic_stability=0.70

> AI builders who remain vague about quality will experience stalled adoption of their AI solutions.

## Praxis Hook (2)

### ATOM-SOURCE-20251216-818-0002
**Lines**: 19-19
**Context**: method / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.20, actionability=0.90, epistemic_stability=0.80

> Before making any AI architecture decisions, define what 'correct' means for the project.

### ATOM-SOURCE-20251216-818-0006
**Lines**: 42-43
**Context**: method / claim
**Tension**: novelty=0.80, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.20, actionability=0.90, epistemic_stability=0.80

> To build effective AI, define correctness in terms of claims, evidence, and failure penalties.
