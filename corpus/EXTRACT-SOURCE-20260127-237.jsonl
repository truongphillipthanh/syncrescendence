{"atom_id": "ATOM-SOURCE-20260127-237-0001", "source_id": "SOURCE-20260127-237", "category": "claim", "content": "Artificial superintelligence must be stopped, as argued by Eliezer Yudkowsky and Nate Soares in their book \"If Anyone Builds It, Everyone Dies.\"", "line_start": 10, "line_end": 11, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.2, 0.7, 0.2, 0.4], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260127-237-0002", "source_id": "SOURCE-20260127-237", "category": "claim", "content": "The core argument against artificial superintelligence is that \"If Anyone Builds It, Everyone Dies,\" implying catastrophic risks.", "line_start": 12, "line_end": 13, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.4, 0.7, 0.3, 0.8, 0.1, 0.3], "opposes_atom_ids": []}, "extensions": {}}
