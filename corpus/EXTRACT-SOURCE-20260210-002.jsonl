{"atom_id": "ATOM-SOURCE-20260210-002-0001", "source_id": "SOURCE-20260210-002", "category": "concept", "content": "Observational memory is a text-based memory system for agentic AI systems that compresses context into discrete observations, designed to mimic human cognitive distillation.", "line_start": 6, "line_end": 6, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.8, 0.2, 0.1, 0.1, 0.7, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0002", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "Observational memory is state-of-the-art (SoTA) on benchmarks like LongMemEval and is compatible with prompt caching mechanisms from providers like Anthropic and OpenAI.", "line_start": 8, "line_end": 8, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.2, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0003", "source_id": "SOURCE-20260210-002", "category": "praxis_hook", "content": "Mastra's implementation of observational memory is open-source.", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.6, 0.2, 0.1, 0.1, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0004", "source_id": "SOURCE-20260210-002", "category": "analogy", "content": "The human brain processes millions of pixels but distills them into one or two observations, similar to how observational memory compresses context.", "line_start": 15, "line_end": 16, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.4, 0.5, 0.1, 0.1, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0005", "source_id": "SOURCE-20260210-002", "category": "framework", "content": "Observational memory uses a log-based message format characterized by formatted text (not structured objects), a three-date model for temporal reasoning (observation, referenced, and relative dates), and emoji-based prioritization (ðŸ”´ for important, ðŸŸ¡ for maybe important, ðŸŸ¢ for info only).", "line_start": 28, "line_end": 37, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.1, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0006", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "Text is a universal interface, easier to use, optimized for LLMs, and simpler to debug compared to structured objects like knowledge graphs.", "line_start": 30, "line_end": 30, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.5, 0.4, 0.1, 0.1, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0007", "source_id": "SOURCE-20260210-002", "category": "framework", "content": "In observational memory, the context window is divided into two blocks: a list of observations and raw messages not yet compressed. New messages append to the raw message block.", "line_start": 41, "line_end": 43, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.1, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0008", "source_id": "SOURCE-20260210-002", "category": "praxis_hook", "content": "When the raw message block hits a default threshold of 30k tokens, a separate 'observer agent' compresses these messages into new observations, which are appended to the observation block.", "line_start": 45, "line_end": 46, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.1, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0009", "source_id": "SOURCE-20260210-002", "category": "praxis_hook", "content": "When the observation block hits a default threshold of 40k tokens, a separate 'reflector agent' garbage collects irrelevant observations.", "line_start": 48, "line_end": 49, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.1, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0010", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "Observational memory's structure enables consistent prompt caching: full cache hits occur until the raw message threshold is met, and partial cache hits occur during observation runs because the observation prefix remains consistent.", "line_start": 54, "line_end": 57, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.6, 0.3, 0.1, 0.1, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0011", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "Mastra's Observational Memory achieved 94.87% on LongMemEval with gpt-5-mini, exceeding previous scores by over 3 points, and 84.23% with gpt-4o, beating the gpt-4o oracle by 2 points and Supermemory's gpt-4o SOTA by 2.6 points.", "line_start": 61, "line_end": 63, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.8, 0.2, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0012", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "These benchmark scores were achieved with a completely stable, predictable, reproducible, and fully prompt-cacheable context window.", "line_start": 65, "line_end": 65, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.6, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0013", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "EmergenceMem's reported 86.00% score for an 'Internal' configuration is not publicly reproducible, and both EmergenceMem and Hindsight use multi-stage retrieval and neural reranking, unlike OM's single-pass stable context window.", "line_start": 78, "line_end": 79, "chaperone": {"context_type": "rebuttal", "argument_role": "limitation", "tension_vector": [0.5, 0.4, 0.3, 0.2, 0.4, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0014", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "Mastra previously shipped working memory and semantic recall systems in March and April, before 'context engineering' became a recognized concept.", "line_start": 84, "line_end": 84, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.5, 0.1, 0.1, 0.3, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0015", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "Working memory provided moderate, cacheable benchmark improvements, while semantic recall offered larger but non-cacheable improvements.", "line_start": 86, "line_end": 86, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.5, 0.1, 0.1, 0.3, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0016", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "The need for aggressive caching to reduce token costs and the explosion of context windows from tool call results and parallelizable agents (e.g., browser, coding, research agents) highlighted the problem observational memory addresses.", "line_start": 88, "line_end": 94, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.5, 0.4, 0.1, 0.1, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0017", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "Observational memory is considered the new primary Mastra memory system, combining aspects of working memory and semantic recall, and users are encouraged to migrate to it.", "line_start": 96, "line_end": 97, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.1, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0018", "source_id": "SOURCE-20260210-002", "category": "claim", "content": "A current limitation of observational memory is that observation runs synchronously, blocking the conversation when the token threshold is hit.", "line_start": 100, "line_end": 100, "chaperone": {"context_type": "consensus", "argument_role": "limitation", "tension_vector": [0.5, 0.4, 0.1, 0.1, 0.4, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0019", "source_id": "SOURCE-20260210-002", "category": "prediction", "content": "Mastra is shipping an async background buffering mode this week that will run observation outside the conversation loop, solving the synchronous blocking issue.", "line_start": 101, "line_end": 101, "chaperone": {"context_type": "speculation", "argument_role": "claim", "tension_vector": [0.6, 0.3, 0.1, 0.8, 0.7, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-002-0020", "source_id": "SOURCE-20260210-002", "category": "praxis_hook", "content": "To use Mastra's observational memory, import `Memory` and `Agent` from `@mastra/memory` and `@mastra/core/agent` respectively, then instantiate `Agent` with `observationalMemory: true` in the memory options.", "line_start": 105, "line_end": 109, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.6, 0.2, 0.1, 0.1, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
