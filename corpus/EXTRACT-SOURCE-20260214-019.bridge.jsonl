{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "6bf77dec-5fca-5367-a8cb-dcf5d94f02b7", "timestamp": "2026-02-24T00:39:12.366702+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-019-0001", "source_id": "SOURCE-20260214-019", "category": "claim", "content": "Stanford and Caltech researchers have published the first comprehensive taxonomy of how Large Language Models (LLMs) fail at reasoning.", "line_start": 2, "line_end": 3, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.8, 0.5, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-019", "entity_type": "Claim", "name": "Stanford and Caltech researchers have published the first comprehensive taxonomy", "content": "Stanford and Caltech researchers have published the first comprehensive taxonomy of how Large Language Models (LLMs) fail at reasoning.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-019", "line_start": 2, "line_end": 3, "atom_id": "ATOM-SOURCE-20260214-019-0001"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.8, 0.5, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "5676f0ef-4ed6-55f5-8f18-6dc82fe9a577", "timestamp": "2026-02-24T00:39:12.366702+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-019-0002", "source_id": "SOURCE-20260214-019", "category": "framework", "content": "A novel categorization framework for LLM reasoning failures distinguishes reasoning into embodied and non-embodied types, with non-embodied further subdivided into informal (intuitive) and formal (logical) reasoning. Failures are classified along a complementary axis into three types: fundamental failures (intrinsic to LLM architectures), application-specific limitations, and robustness issues (inconsistent performance across minor variations).", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.9, 0.4, 0.1, 0.1, 0.6, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-019", "entity_type": "Framework", "name": "A novel categorization framework for LLM reasoning failures distinguishes reason", "content": "A novel categorization framework for LLM reasoning failures distinguishes reasoning into embodied and non-embodied types, with non-embodied further subdivided into informal (intuitive) and formal (logical) reasoning. Failures are classified along a complementary axis into three types: fundamental failures (intrinsic to LLM architectures), application-specific limitations, and robustness issues (inconsistent performance across minor variations).", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-019", "line_start": 10, "line_end": 10, "atom_id": "ATOM-SOURCE-20260214-019-0002"}, "metadata": {"category": "framework", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.9, 0.4, 0.1, 0.1, 0.6, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "ddd5b3ab-4afb-5a4a-9744-7fd00442d5e7", "timestamp": "2026-02-24T00:39:12.366702+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-019-0003", "source_id": "SOURCE-20260214-019", "category": "claim", "content": "The research provides a structured perspective on systemic weaknesses in LLM reasoning, offering insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities.", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.7, 0.6, 0.1, 0.2, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-019", "entity_type": "Claim", "name": "The research provides a structured perspective on systemic weaknesses in LLM rea", "content": "The research provides a structured perspective on systemic weaknesses in LLM reasoning, offering insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-019", "line_start": 10, "line_end": 10, "atom_id": "ATOM-SOURCE-20260214-019-0003"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.7, 0.6, 0.1, 0.2, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "848bb9f7-7e9e-51d8-bcde-644d058d74cf", "timestamp": "2026-02-24T00:39:12.366702+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-019-0004", "source_id": "SOURCE-20260214-019", "category": "praxis_hook", "content": "A comprehensive collection of research works on LLM reasoning failures is available as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures.", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.0, 0.0, 0.9, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-019", "entity_type": "PraxisHook", "name": "A comprehensive collection of research works on LLM reasoning failures is availa", "content": "A comprehensive collection of research works on LLM reasoning failures is available as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-019", "line_start": 10, "line_end": 10, "atom_id": "ATOM-SOURCE-20260214-019-0004"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.0, 0.0, 0.9, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
