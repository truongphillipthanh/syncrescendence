{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "8ddf1009-fc99-550f-8df6-010ef54943bb", "timestamp": "2026-02-24T00:44:53.325504+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-698-0001", "source_id": "SOURCE-20251229-698", "category": "concept", "content": "In-context Learning (ICL) refers to the phenomenon where large language models learn new tasks or adapt to new data within the context of the input prompt, without explicit weight updates or fine-tuning.", "line_start": 7, "line_end": 7, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-698", "entity_type": "Concept", "name": "In-context Learning (ICL) refers to the phenomenon where large language models l", "content": "In-context Learning (ICL) refers to the phenomenon where large language models learn new tasks or adapt to new data within the context of the input prompt, without explicit weight updates or fine-tuning.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-698", "line_start": 7, "line_end": 7, "atom_id": "ATOM-SOURCE-20251229-698-0001"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "1cb4bc24-cdd8-521f-8824-3a6fcc0e74dc", "timestamp": "2026-02-24T00:44:53.325504+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-698-0002", "source_id": "SOURCE-20251229-698", "category": "claim", "content": "The exact learning algorithms within transformer layers that activate in-context learning are not fully understood.", "line_start": 7, "line_end": 8, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.2, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-698", "entity_type": "Claim", "name": "The exact learning algorithms within transformer layers that activate in-context", "content": "The exact learning algorithms within transformer layers that activate in-context learning are not fully understood.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-698", "line_start": 7, "line_end": 8, "atom_id": "ATOM-SOURCE-20251229-698-0002"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.2, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "a8f44c7d-bb97-5ec2-a7f0-c796384e9843", "timestamp": "2026-02-24T00:44:53.325504+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-698-0003", "source_id": "SOURCE-20251229-698", "category": "claim", "content": "In-context learning might be a form of fine-tuning, specifically a Rank-1 update function in the weight tensor space.", "line_start": 12, "line_end": 12, "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.6, 0.3, 0.2, 0.7, 0.2, 0.4], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-698", "entity_type": "Claim", "name": "In-context learning might be a form of fine-tuning, specifically a Rank-1 update", "content": "In-context learning might be a form of fine-tuning, specifically a Rank-1 update function in the weight tensor space.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-698", "line_start": 12, "line_end": 12, "atom_id": "ATOM-SOURCE-20251229-698-0003"}, "metadata": {"category": "claim", "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.6, 0.3, 0.2, 0.7, 0.2, 0.4], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "7f9989a6-0f8f-5d67-982c-0f8bdb5c9a48", "timestamp": "2026-02-24T00:44:53.325504+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-698-0004", "source_id": "SOURCE-20251229-698", "category": "claim", "content": "The transformer architecture may possess open degrees of freedom that enable in-context learning, which are only now being discovered.", "line_start": 13, "line_end": 14, "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.7, 0.2, 0.1, 0.8, 0.1, 0.3], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-698", "entity_type": "Claim", "name": "The transformer architecture may possess open degrees of freedom that enable in-", "content": "The transformer architecture may possess open degrees of freedom that enable in-context learning, which are only now being discovered.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-698", "line_start": 13, "line_end": 14, "atom_id": "ATOM-SOURCE-20251229-698-0004"}, "metadata": {"category": "claim", "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.7, 0.2, 0.1, 0.8, 0.1, 0.3], "opposes_atom_ids": []}, "extensions": {}}}
