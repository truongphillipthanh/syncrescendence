# Extraction: SOURCE-20251226-001

**Source**: `SOURCE-20251226-youtube-video-david_shapiro-david_shapiro_the_scaling_paradox_why_capabilit.md`
**Atoms extracted**: 8
**Categories**: claim, concept, framework

---

## Claim (5)

### ATOM-SOURCE-20251226-001-0002
**Lines**: 7-9
**Context**: consensus / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.10, actionability=0.20, epistemic_stability=0.70

> Diminishing returns on vanilla pre-training (one scaling vector) does not imply diminishing returns on the overall AI capability frontier.

### ATOM-SOURCE-20251226-001-0004
**Lines**: 20-22
**Context**: consensus / evidence
**Tension**: novelty=0.40, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.10, actionability=0.10, epistemic_stability=0.80

> The length of tasks that generalist AI agents can complete has been doubling approximately every seven months for the past six years, accelerating to every four months recently.

### ATOM-SOURCE-20251226-001-0005
**Lines**: 25-27
**Context**: anecdote / evidence
**Tension**: novelty=0.40, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.10, actionability=0.10, epistemic_stability=0.80

> The ARC-AGI benchmark showed rapid progress, taking four years to go from 0% to 5% performance, then only months to reach near saturation, necessitating the release of ARC-AGI 2 and 3.

### ATOM-SOURCE-20251226-001-0007
**Lines**: 38-39
**Context**: anecdote / evidence
**Tension**: novelty=0.30, consensus_pressure=0.70, contradiction_load=0.10, speculation_risk=0.10, actionability=0.10, epistemic_stability=0.80

> Sam Altman stated that GPT-4's capabilities were due to hundreds of small improvements, not a single 'secret sauce'.

### ATOM-SOURCE-20251226-001-0008
**Lines**: 42-43
**Context**: consensus / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.10, actionability=0.20, epistemic_stability=0.70

> The flattening of vanilla pre-training returns is often mistaken for a slowdown in overall AI progress, but the capability frontier is advanced by multiple simultaneous research programs.

## Concept (1)

### ATOM-SOURCE-20251226-001-0001
**Lines**: 6-7
**Context**: hypothesis / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.20, speculation_risk=0.20, actionability=0.10, epistemic_stability=0.60

> The 'Scaling Paradox' describes the apparent contradiction between claims that AI scaling laws are hitting a wall and the observed acceleration of AI capabilities.

## Framework (2)

### ATOM-SOURCE-20251226-001-0003
**Lines**: 9-11
**Context**: method / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.10, actionability=0.50, epistemic_stability=0.70

> AI progress proceeds through multiple simultaneous vectors: test-time compute, architectural innovations, agent scaffolding, post-training improvements, and better recipes.

### ATOM-SOURCE-20251226-001-0006
**Lines**: 30-35
**Context**: method / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.10, actionability=0.50, epistemic_stability=0.70

> Multiple simultaneous research programs drive AI progress, including test-time compute (chain-of-thought, search, tool use), architectural innovations (MoE, state space models), agent scaffolding and tool integration, post-training (RLHF, DPO, synthetic data, self-play), and better training recipes.
