# Extraction: SOURCE-20260218-019

**Source**: `SOURCE-20260218-x-thread-altimor-some_notes_about_the_slew_of_chinese_models_coming_out_kimi_k2_5_minimax_glm_5_etc.md`
**Atoms extracted**: 3
**Categories**: claim

---

## Claim (3)

### ATOM-SOURCE-20260218-019-0001
**Lines**: 3-10
**Context**: anecdote / claim
**Tension**: novelty=0.30, consensus_pressure=0.40, contradiction_load=0.20, speculation_risk=0.20, actionability=0.60, epistemic_stability=0.70

> Chinese AI models like Kimi K2.5, MiniMax, and GLM-5, despite claims of matching models like Sonnet/Opus/God in evaluations at a tenth of the price, consistently fall short in real-life performance for agentic behavior and non-coding use cases.

### ATOM-SOURCE-20260218-019-0002
**Lines**: 12-16
**Context**: consensus / claim
**Tension**: novelty=0.40, consensus_pressure=0.60, contradiction_load=0.30, speculation_risk=0.50, actionability=0.20, epistemic_stability=0.60

> The industry consensus suggests that Chinese AI labs are distilling frontier models, leading to 'shallow' intelligence, training specifically for evaluations, and potentially stealing weights (with a belief that at least GPT-4o's weights were exfiltrated).

### ATOM-SOURCE-20260218-019-0003
**Lines**: 17-18
**Context**: consensus / claim
**Tension**: novelty=0.30, consensus_pressure=0.50, contradiction_load=0.20, speculation_risk=0.30, actionability=0.40, epistemic_stability=0.70

> Chinese AI models are currently at least one generation behind models like Sonnet and Opus, despite claims of being at their level.
