# Claude Code + Ollama: Local & Cloud Models from Your Terminal (Step‑by‑Step Guide)

**Channel**: CodeCrafters
**Published**: 2026-01-21
**Duration**: 4m 44s
**URL**: https://www.youtube.com/watch?v=RujtpfFzVtQ

## Description (no transcript available)

#ClaudeCode #Ollama #LocalLLM #AICoding #Anthropic #CodingTutorial


Run Claude Code 100% locally with Ollama and stop worrying about API keys or subscriptions. In this step‑by‑step tutorial, you’ll connect Claude Code to Ollama, use powerful local models, and even try Ollama cloud models – all from your terminal.
​

In this video you’ll learn:
​
How to install and set up Ollama on your machine
How to install Claude Code (CLI) on macOS, Windows, or Linux
How to set ANTHROPIC_AUTH_TOKEN and ANTHROPIC_BASE_URL to point Claude Code to your local Ollama server
How to run Claude Code with local models like gpt-oss:20b or qwen-coder
How to connect to Ollama cloud models and generate real code (Hello World, prime checker, etc.)
​
​
By the end, you’ll have a fully working Claude Code + Ollama setup so you can code with local and cloud models without paying for Anthropic tokens.
​

Timestamps (adjust to your cuts):
​
0:00 Intro – Claude Code + Ollama update
0:35 Install Ollama & pull models
1:30 Install Claude Code CLI
2:30 Set environment variables for Anthropic‑compatible API
3:30 Run Claude Code with a local model
4:30 Test Ollama cloud model from Claude Code
5:30 Final thoughts & next steps

Like, share, and subscribe for more AI + coding tutorials (Claude, Ollama, local LLMs, and automation).
