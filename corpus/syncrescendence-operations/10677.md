# What is Prompt Caching? Optimize LLM Latency with AI Transformers

**Channel**: IBM Technology
**Published**: 2026-02-07
**Duration**: 9m 6s
**URL**: https://www.youtube.com/watch?v=u57EnkQaUTY

## Description (no transcript available)

Ready to become a certified watsonx Generative AI Engineer? Register now and use code IBMTechYT20 for 20% off of your exam â†’ https://ibm.biz/BdbNiK

Learn more about Prompt Caching here â†’ https://ibm.biz/BdbNia

Can AI models run faster? ðŸš€ Martin Keen explains prompt caching, a technique that reduces LLM latency and costs by storing key-value pairs and optimizing transformer-based systems. Discover how it improves AI efficiency for applications like chatbots, summarization, and more.

AI news moves fast. Sign up for a monthly newsletter for AI updates from IBM â†’ https://ibm.biz/BdpcLh

#llm #ai #transformers #latencyreduction
