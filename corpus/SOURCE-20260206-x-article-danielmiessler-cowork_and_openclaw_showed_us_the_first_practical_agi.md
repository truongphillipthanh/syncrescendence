# Cowork and OpenClaw Showed Us The First Practical AGI

(Description: Hero image displaying two distinct visual concepts side by side. Left panel features a figure in business attire sitting above a chess board with mathematical and technical diagrams in the background, labeled "NARROW vs GENERAL" with "Two definitions of intelligence" subtitle on the right. The right panel shows abstract neural network-like organic structures in red and purple tones with various annotations and connection diagrams representing neural complexity.)

I think we just saw the first glimpse of AGI, and most are missing it.

But what does that even mean.

Well, if you're an academic in an AI or related field you probably jump to some theoretical level of synthetic intelligence that meets or exceeds human performance on any cognitive task. Which many believe is impossible using current LLMs.

I think about generality in everyday terms, and I capture it as a question:

> What tasks could technology NOT do up through the year 2022 that an average, not-exceptional, just-barely-clocking-in-and-out knowledge worker COULD do?

In other words, we've been paying knowledge workers for decades even though we had programming and automation? So why is that? What does a knowledge worker do that tech couldn't?

The answer that "G" in AGI. Generality.

## Examples of Generality

Kind of hard to talk about without examples. So here are some everyday instances you'll probably recognize.

1. The boss says we're abandoning the plan the whole team spent 3 months on and pivoting to a new thing due to new directives from corporate
2. You are told to use the "proper tone" in the report you're writing for the board, given the "kinds of stuff they care about".
3. While you're at work as the office manager, mold is detected in the conference room you had booked for a big customer meeting today, so now you have 2.5 hours to find a conference room that seats 30 people and has X, Y, Z features.
4. On Monday IT says you're switching from Slack and Monday to ClickUp and it's one of your busiest days and you need to remap all your workflows to completely new software that doesn't have exact analogs on the new system.
5. You're told to play nice with Chris on the engineering team, and reframe the dashboard you made to highlight their team accomplishments without downplaying ours.
6. Meanwhile the printer broke. And the place you normally get the boss's favorite lunch went out of business. And he freaks out if his sandwich isn't this one particular way.
7. New email: Find an offsite location that will suit our team's personality and vibe, without being extravagant or boring.
8. Corporate says we're switching from OKRs to KPIs, with a focus on first principles thinking, so we need to redo our whole performance measurement system we built over the last 6 months.

This could be 8 employees or 1 employee doing all 8. You could come in one day and the whole world is chaos. You have a new boss. New coworkers. You're being asked to do completely new stuff. With different goals.

And it could change again in 3 weeks when the company gets bought by private equity.

The average day for an average knowledge worker involves dozens or thousands of these tiny, highly-varied micro-adjustments to even enable them to do their regular jobâ€”whatever that is.

## So what did Moltbot and Claude Cowork do?

(Description: Stylized illustration of interconnected systems and data flows. The image shows stock charts and financial graphs with downward trending lines labeled "SOFTWARE STOCKS CRASHING" in the top right. Below are various business domains including "LEGAL," "SALES," "FINANCE," and "MARKETING" with interconnected nodes. At the bottom center is a large brain-like structure in gray and purple tones with the text "CLAUDE COWORK" beneath it, suggesting the neural processing capabilities of the AI system.)

The reason OpenClaw and Claude Cowork are so significant is because they are approaching this generality we're talking about here. It's not the real thing, of course, but there's some point where this distinction doesn't have a difference. At least for replacing workers.

For OpenClaw the best example is the creator's "Oh crap" moment. He tried to get his agent to book a reservation, and it struggled for a bit and eventually got it to work.

Turned out the reservation system online was broken, so his agent taught itself how to speak using Eleven Labs and just called the restaurant itself and booked it. ðŸ¤¯

And with Claude Cowork there was a recent stock crash around a bunch of companies that provide legal software and services. All because Cowork got skills related to legal work.

(Description: Digital dashboard interface with futuristic blue neon styling showing AI impact on Indian IT stocks. Header reads "Anthropic's Claude AI CRASHING INDIAN IT STOCKS" with stock ticker cards displaying three major companies: TCS showing -5.2% decline, INFY showing -4.8% decline, and WIPRO showing -3.9% decline. Background includes stock market candlestick charts and technical analysis indicators with holographic visual effects.)

People are figuring out how much this will impact existing companies and services.

In the OpenClaw example we're seeing obstacle navigation, just like we deal with on a constant bases. Even a few months ago it'd be insane to imagine an AI teaching itself how to speak so it could complete a task.

And in the Cowork case it's a matter of accruing Skills. Skills are literally just text files in a folder, combined with some tooling, that lets you capture expertise in a way that can be replicated.

And now Skills are coming out for doing legal work, writing high-end consulting reports, doing security assessments, and hundreds of other professions and tasks. These are industries that made hundreds of millions of dollars by being specialized and hard to execute.

Now people can download a folder and press go and produce what they used to be able to charge thousands or millions of dollars for.

## The Real Game is Worker Replacement

I think that's a good time to remind ourselves of one of the main goals with all this AI. Yes, companies are investing billions upon billions, but there's a reason for that.

Companies wish they could do all their own work. Or, put another way, the ideal number of human employees in most companies is zero.

Global compensation of knowledge workers is around $40 trillion dollars annually.

That's how much companies could save if they brought all their work internal through AI/automation. But forget 100%, just imagine 25% to 80% in the next 2-10 years (nobody knows how fast it'll go).

We're talking about insane amounts of disruption.

And it all hinges off of whether or not a product can become general enough to replace an average knowledge worker. Which happens to be my definition of AGI.

## Think About the Thing We Care About

The point isn't some technical definition of AGI or generality. That's nerd shit. No hate. I'm a nerd too. I'm just saying we have to stop taking our eye off the ball.

The point of all of this is humans and their ability to survive and thrive.

Nobody would argue that a knowledge worker has general intelligence because they're doing work that requires it.

I am simply saying that an AI product or model that can seamlessly pivot between enough of these general capabilities or skills will hit a point that's close enough. It will be doing work that an average (and then beyond) knowledge worker can do. And at that point it'll be "generally" intelligent. Practically. In the way that matters. Because humans will be laid off and not replaced as a result.

I don't need a better definition or metric than that.

Oh, and meanwhile, they're also massively cheaper. And ultra-deep experts in thousands of fields. While not ever complaining or getting tired.

## Summary

- There's all this debate about AGI
- There are a million definitions and few people agree on them
- I think the cleanest is to start with human generality
- If a human knowledge worker's job requires general intelligence, and an AI product/service can do said job, it has general intelligence
- This is precisely what things like OpenClaw and Cowork (and all their competitors) are pushing towards

I've been saying since 2023 that we'd get an AI system that can replace a human knowledge worker before 2028.

Based on what I've seen in just the last few months, I think it's coming a whole lot faster than I thought.

I'm now expecting later this year or 2027 at the latest.