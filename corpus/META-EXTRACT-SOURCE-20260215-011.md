# Extraction: SOURCE-20260215-011

**Source**: `SOURCE-20260215-x-article-saboo_shubham_-what_30_days_of_running_24_7_ai_agent_team_taught_me.md`
**Atoms extracted**: 10
**Categories**: claim, concept, framework, praxis_hook

---

## Claim (2)

### ATOM-SOURCE-20260215-011-0001
**Lines**: 9-12
**Context**: anecdote / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.20, actionability=0.60, epistemic_stability=0.70

> After 30 days, an AI agent team using the same model and prompts can produce high-quality drafts with minimal edits, due to accumulated context stored in files that agents read daily.

### ATOM-SOURCE-20260215-011-0007
**Lines**: 49-50
**Context**: consensus / claim
**Tension**: novelty=0.30, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.20, actionability=0.60, epistemic_stability=0.70

> Skill files for AI agents compound faster than memory because they are prescriptive.

## Concept (1)

### ATOM-SOURCE-20260215-011-0006
**Lines**: 48-49
**Context**: consensus / claim
**Tension**: novelty=0.20, consensus_pressure=0.70, contradiction_load=0.10, speculation_risk=0.10, actionability=0.70, epistemic_stability=0.80

> Memory in the context of AI agents refers to what the agent has learned about working with a specific user or context, while skill refers to how to correctly perform a specific task.

## Framework (1)

### ATOM-SOURCE-20260215-011-0009
**Lines**: 63-79
**Context**: method / claim
**Tension**: novelty=0.50, consensus_pressure=0.40, contradiction_load=0.20, speculation_risk=0.20, actionability=0.60, epistemic_stability=0.60

> AI agent teams typically progress through three phases: Phase 1 (Mediocre everything, days 1-7) with generic output and high correction overhead; Phase 2 (Specific competence, days 8-21) with feedback accumulation and output improvements; and Phase 3 (Compounding returns, day 22+) with rich context and minimal changes needed.

## Praxis Hook (6)

### ATOM-SOURCE-20260215-011-0002
**Lines**: 23-26
**Context**: method / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.20, actionability=0.80, epistemic_stability=0.70

> Corrective prompt-engineering involves starting with a rough sketch of an agent's personality, observing its behavior, and course-correcting over time, similar to managing human team members.

### ATOM-SOURCE-20260215-011-0003
**Lines**: 30-30
**Context**: method / claim
**Tension**: novelty=0.50, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.20, actionability=0.90, epistemic_stability=0.70

> To improve AI agent output, provide specific feedback that can be codified into rules, rather than general instructions like 'make this better.'

### ATOM-SOURCE-20260215-011-0004
**Lines**: 34-39
**Context**: method / evidence
**Tension**: novelty=0.60, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.20, actionability=0.90, epistemic_stability=0.70

> Maintain an agent's memory file with specific examples of rejected patterns and good tone/style, allowing the agent to self-correct and reverse-engineer desired output.

### ATOM-SOURCE-20260215-011-0005
**Lines**: 46-49
**Context**: method / claim
**Tension**: novelty=0.60, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.20, actionability=0.90, epistemic_stability=0.70

> Implement 'ruthless rules' for research agents, such as filtering information based on whether the target reader can immediately act on it, to reduce noise and increase signal.

### ATOM-SOURCE-20260215-011-0008
**Lines**: 51-58
**Context**: method / claim
**Tension**: novelty=0.40, consensus_pressure=0.50, contradiction_load=0.10, speculation_risk=0.20, actionability=0.90, epistemic_stability=0.60

> To ensure AI agent improvement, feedback must be stored persistently in a file (memory or skill file) rather than remaining in conversational chat, so the agent can load the lesson in subsequent sessions.

### ATOM-SOURCE-20260215-011-0010
**Lines**: 79-79
**Context**: method / claim
**Tension**: novelty=0.30, consensus_pressure=0.50, contradiction_load=0.10, speculation_risk=0.20, actionability=0.90, epistemic_stability=0.60

> To transition an AI agent from Phase 1 to Phase 2, provide specific feedback that explains 'why' something is wrong and how it relates to the audience or goals, rather than vague corrections.
