# Extraction: SOURCE-20251223-002

**Source**: `SOURCE-20251223-youtube-video-dwarkesh_patel-dwarkesh_patel_what_are_we_scaling_the_continu.md`
**Atoms extracted**: 10
**Categories**: claim, framework, prediction

---

## Claim (8)

### ATOM-SOURCE-20251223-002-0001
**Lines**: 4-6
**Context**: hypothesis / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.20, speculation_risk=0.40, actionability=0.20, epistemic_stability=0.50

> Current reinforcement learning (RL) scaling approaches are fundamentally misguided if humanity is close to developing human-like learners.

### ATOM-SOURCE-20251223-002-0002
**Lines**: 6-8
**Context**: consensus / evidence
**Tension**: novelty=0.20, consensus_pressure=0.80, contradiction_load=0.10, speculation_risk=0.10, actionability=0.60, epistemic_stability=0.90

> Human workers are valuable because they do not require bespoke training loops for every small part of their job.

### ATOM-SOURCE-20251223-002-0003
**Lines**: 8-9
**Context**: consensus / claim
**Tension**: novelty=0.40, consensus_pressure=0.60, contradiction_load=0.20, speculation_risk=0.30, actionability=0.50, epistemic_stability=0.70

> Models currently lack on-the-job learning capabilities, which explains why their economic impact lags behind their capability benchmarks.

### ATOM-SOURCE-20251223-002-0004
**Lines**: 9-11
**Context**: rebuttal / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.30, speculation_risk=0.50, actionability=0.30, epistemic_stability=0.40

> The argument that economic diffusion lag is 'cope' implies that if current AI models were true AGI, they would integrate faster than human hires.

### ATOM-SOURCE-20251223-002-0006
**Lines**: 20-23
**Context**: anecdote / evidence
**Tension**: novelty=0.40, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.20, actionability=0.70, epistemic_stability=0.80

> It is not economically productive to build custom training pipelines for every specific microtask, such as identifying macrophages in lab-specific slides.

### ATOM-SOURCE-20251223-002-0007
**Lines**: 25-28
**Context**: hypothesis / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.20, speculation_risk=0.50, actionability=0.40, epistemic_stability=0.50

> If AI models were truly human-like, they would diffuse incredibly quickly by integrating with existing digital systems like Slack and Drive to distill skills from other AI employees.

### ATOM-SOURCE-20251223-002-0008
**Lines**: 30-33
**Context**: consensus / claim
**Tension**: novelty=0.50, consensus_pressure=0.50, contradiction_load=0.30, speculation_risk=0.40, actionability=0.30, epistemic_stability=0.60

> The goalposts for AGI are continually shifting because advancements that were once thought sufficient for AGI (e.g., Gemini 2.0 in 2020) have not yet led to its realization.

### ATOM-SOURCE-20251223-002-0010
**Lines**: 43-43
**Context**: speculation / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.40, speculation_risk=0.60, actionability=0.30, epistemic_stability=0.40

> Models are becoming more impressive at the rate predicted by 'short timelines' proponents, but more useful at the rate predicted by 'long timelines' proponents.

## Framework (1)

### ATOM-SOURCE-20251223-002-0009
**Lines**: 35-38
**Context**: speculation / claim
**Tension**: novelty=0.80, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.90, actionability=0.30, epistemic_stability=0.20

> A future 'broadly deployed intelligence explosion' might involve continual learning agents performing diverse jobs, generating value, and contributing their learnings to a 'hive mind model' for batch distillation.

## Prediction (1)

### ATOM-SOURCE-20251223-002-0005
**Lines**: 11-12
**Context**: speculation / claim
**Tension**: novelty=0.50, consensus_pressure=0.30, contradiction_load=0.20, speculation_risk=0.70, actionability=0.40, epistemic_stability=0.30

> AI labs will make progress on continual learning by 2030 but will not fully automate all knowledge work.
