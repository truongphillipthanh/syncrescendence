---
id: [[CANON-30330]]
name: Research Protocols
tier: lattice
chain: intelligence
celestial_type: asteroid
volatility_band: dynamic
sn_version: 2.0
parent: CANON-30300
requires: ['CANON-30000', 'CANON-30300']
entities: ['Source Triad Method (PROTO)', 'Verdicting Process (PROTO)', 'Decision-Bearing Questions (CON)', 'Frontier Tracking (WF)', 'Capability Contract Audit (PROTO)', 'Research Anti-Patterns (CON)', 'Confidence Levels (MET)']
original_words: 1525
compressed_words: 412
compression_ratio: 3.7
dropped:
  - rhetorical elaboration and examples restating methodology
  - historical context and version migration details
  - extended narrative descriptions
  - redundant principle restatement
---

# Κ-30330: Research Protocols (SN)

PROC Canon30330ResearchProtocols:
    sutra: "Execute decision-bearing research via source triad method with verdicting, anti-pattern avoidance, and quality auditing."
    gloss:
        Systematic research protocol requires three mandatory passes and explicit verdicting for all claims. @term Source Triad Method comprises: Pass 1 (primary sources—academic papers, official docs, engineering blogs, patents), Pass 2 (expert secondary analysis—deep-dives, reviews, commentary), Pass 3 (counter-positions—rebuttals, limitations, alternatives). All research begins with @term Decision-Bearing Questions: What decision does this inform? What action differs if X vs. Y? What confidence required? Cost of error? Skip research if answer doesn't materially affect decisions. Output verdicts contain: claim (testable statement), @term Confidence Levels (High >85%, Medium 50-85%, Low <50%, Speculative <25%), recency (Current 0-3mo, Recent 3-12mo, Dated 1-2yr, Stale >2yr), volatility (Stable/Shifting/Speculative), @term Design Delta (specific action if true). Maintain @term Research Log with entry ID, timestamp, question, source, gist, claims, confidence, recency, volatility, design delta, contradictions, next actions. On contradictions: document both, assess source quality, identify resolution, use conservative estimate, flag critical items.
end

NORM MethodologiesAndQualityStandards:
    sutra: "Research quality requires source attribution, primary preference, counter-positions, explicit uncertainty, and contradiction acknowledgment."
    gloss:
        @term Research Quality Checklist: every claim must have source attribution; sources primary when possible; secondary sources add analysis; counter-positions included; confidence levels explicit with rationale; recency and volatility tagged; @term Design Deltas specified; contradictions acknowledged; no benchmark shopping or link dumping; hype distinguished from evidence. Report structure: Executive Summary → Decision-Bearing Questions → Methodology → Findings → Implications → Limitations → Next Steps. Peer review requires: self-assessment of potential errors, source validation (verify original, confirm attribution), logic check (do conclusions follow? alternative explanations exist?), impact assessment (decisions informed? cost of error?).
end

PASS ICoreMethodology:
    sutra: "Core methodology applies source triad to all decision-bearing research questions."
    gloss:
        @term Core Methodology establishes systematic research process. For every decision-bearing question, execute three research passes with explicit evaluation criteria. Pass 1 targets original material: academic papers, official documentation, company engineering blogs, government reports, patent filings. Evaluate: Is this original source? Institution credibility? Recency? Methodology? Red flags: secondary reporting without primary links, marketing as research, unsupported claims. Pass 2 targets expert interpretation: technical deep-dives, research reviews, industry analysis, expert commentary. Evaluate: Does this add interpretation? Analyst expertise? Rigor vs. speculation? Non-obvious implications? Red flags: hype, cherry-picking, undisclosed conflicts, extrapolation beyond evidence. Pass 3 targets alternative interpretations: rebuttals, skeptical analyses, limitations, competing approaches. Evaluate: genuine limitations acknowledged? Assumptions that fail? Alternative explanations? Failure modes? Red flags: strawman arguments, dismissal without engagement.
end

PASS TheSourceTriadMethod:
    sutra: "Source Triad Method combines primary sources, expert secondary analysis, and counter-positions to reduce blind spots."
    gloss:
        @term Source Triad Method is core research discipline. Pass 1 (Primary Sources): target original material from authoritative sources—academic papers, official documentation, company engineering blogs, government reports, patent filings. Evaluate originality, institution credibility, recency, methodology. Red flags: secondary reporting without primary links, marketing masquerading as research, claims without evidence. Pass 2 (High-Signal Secondary Analysis): target expert interpretation, technical deep-dives, research reviews, industry analysis. Evaluate added interpretation, analyst expertise, rigor vs. speculation, non-obvious implications. Red flags: hype, cherry-picking, undisclosed conflicts, unjustified extrapolation. Pass 3 (Counter-Position): target alternative interpretations, academic rebuttals, skeptical analyses, limitation acknowledgments, competing approaches. Evaluate genuine limitations, failing assumptions, alternative explanations, failure modes. Red flags: strawman arguments, dismissal without engagement, ideological opposition.
end

PASS CelestialNavigation:
    sutra: "Research Protocols (CANON-30330) orbits as lattice asteroid in intelligence chain, requiring CANON-30000 and CANON-30300."
    gloss:
        @term Celestial Navigation: CANON-30330 positioned as asteroid-class lattice element within intelligence chain. Parent: CANON-30300 (Technology Stack Database). Siblings: CANON-30310 (Tech Stack Migration), CANON-30320 (Workflow Intelligence), CANON-30340 (Implementation Patterns). Requires: CANON-30000, CANON-30300. Status: canonical, operational. Version: 2.0.0. Volatility band: dynamic. Refresh cadence: monthly.
end

PASS DecisionbearingQuestionsFirst:
    sutra: "Research begins with explicit decision-bearing questions to justify investigation time and ensure actionability."
    gloss:
        @term Decision-Bearing Questions First principle: before researching, answer: (1) What decision does this inform? (2) What would we do differently if X vs. Y? (3) What confidence level needed to act? (4) What's cost of being wrong? Only research when: answer materially changes decisions, uncertainty blocks action, time investment justified. Skip when: answer doesn't affect decisions, uncertainty acceptable, other information more critical. This prevents link dumping and ensures research output directly enables better decisions.
end

PROC TheVerdictingProcess:
    sutra: "Verdict each research question with claim, confidence level, recency, volatility, and design delta."
    gloss:
        @term Verdicting Process produces five components per research question. (1) Claim: clear, testable statement (e.g., "Sonnet 4.5 maintains reliable performance over 30-hour sessions"). (2) @term Confidence Level: High (>85%, multiple primary sources, expert consensus, replicated results), Medium (50-85%, primary source plus analysis, plausible but inconclusive), Low (<50%, single report, unverified, speculative), Speculative (<25%, rumor, prediction, no evidence). (3) Recency: date plus freshness category—Current (0-3 months, likely accurate), Recent (3-12 months, worth checking), Dated (1-2 years, may have changed), Stale (>2 years, use with caution). (4) Volatility: Stable (unlikely to change, fundamental principles), Shifting (actively evolving, current capabilities), Speculative (not yet real, predictions). (5) @term Design Delta: "If true, then we should [specific action]"—test: can someone make different decision from this? If no, clarify.
end

PASS LedgeringAndContradictionManagement:
    sutra: "Log all research with explicit contradictions, apply contradiction resolution protocol, prioritize by decision impact."
    gloss:
        @term Research Log Format: Entry ID | Timestamp | Question | Source | Gist | Claims | Confidence | Recency | Volatility | Design Delta | Contradictions | Next Actions. When contradictions arise: (1) document both claims explicitly, (2) assess relative source quality, (3) identify resolution path, (4) mark as "pending resolution", (5) use most conservative estimate, (6) flag for follow-up if critical. Priority hierarchy: Critical (blocks decisions) → Important (affects strategy) → Minor (interesting) → Ignorable (no impact).
end

PASS IiResearchAntipatterns:
    sutra: "Five anti-patterns corrupt research quality: benchmark shopping, link dumping, hype amplification, recency bias, confirmation bias."
    gloss:
        @term Research Anti-Patterns must be actively avoided. (1) Benchmark Shopping: selectively citing benchmarks supporting desired conclusion, creates false confidence. Avoid: include context, cite competing benchmarks, note what's NOT measured. (2) Link Dumping: many sources without synthesis or verdict, appears thorough but provides no guidance. Avoid: every source must have explicit contribution, few well-analyzed > many undigested. (3) Hype Amplification: uncritically repeating marketing claims or speculative predictions. Avoid: distinguish claims from evidence, note commercial interest, separate current from roadmap. (4) Recency Bias: assuming newer is always better, ignores stable principles, creates thrash. Avoid: assess volatility explicitly, value timeless principles, separate fundamental from fashionable. (5) Confirmation Bias: seeking information supporting existing beliefs, creates blind spots. Avoid: actively seek counter-arguments, red team proposals, always include Pass 3.
end

PASS BenchmarkShopping:
    sutra: "Benchmark Shopping selectively cites benchmarks supporting conclusions, falsely inflating confidence."
    gloss:
        @term Benchmark Shopping anti-pattern: selectively citing benchmarks that support desired conclusion without acknowledging alternatives or measurement gaps. Creates false confidence and hides limitations. Mitigation: include full context of benchmarks, cite competing or contradictory benchmarks, explicitly note what dimensions are NOT measured, acknowledge selection bias.
end

PASS LinkDumping:
    sutra: "Link Dumping provides many sources without synthesis, appearing thorough while providing no actionable guidance."
    gloss:
        @term Link Dumping anti-pattern: accumulating many sources without synthesizing findings or producing verdicts. Creates appearance of rigor while providing no guidance for decision-making. Mitigation: every source must have explicit contribution to verdict, fewer well-analyzed sources > many undigested links, synthesis mandatory before output.
end

PASS HypeAmplification:
    sutra: "Hype Amplification uncritically repeats marketing claims and speculative predictions without evidence distinction."
    gloss:
        @term Hype Amplification anti-pattern: uncritically repeating marketing claims, speculative predictions, or vendor narratives without distinguishing claims from evidence. Corrupts @term Confidence Levels and @term Volatility assessment. Mitigation: distinguish actual capabilities from claimed roadmap, separate current achievements from future promises, note commercial interests, distinguish speculation from evidence-based predictions.
end

PASS RecencyBias:
    sutra: "Recency Bias assumes newer information is always better, ignoring stable principles and creating unnecessary change."
    gloss:
        @term Recency Bias anti-pattern: assumption that newer is always better, leading to overweighting recent information, ignoring stable principles, creating strategic thrash. Particularly problematic for @term Stable volatility items. Mitigation: assess @term Volatility explicitly, value timeless principles highly, separate fundamental breakthrough from incremental efficiency gain, distinguish paradigm shift from hype cycle.
end

PASS ConfirmationBias:
    sutra: "Confirmation Bias seeks information supporting existing beliefs, creating blind spots and missing counter-evidence."
    gloss:
        @term Confirmation Bias anti-pattern: seeking information that confirms existing beliefs, avoiding disconfirming evidence, creating systematic blind spots. Undermines Pass 3 (@term Counter-Position). Mitigation: actively seek counter-arguments and rebuttals, red team all proposals, mandate Pass 3 inclusion, assign devil's advocate role, systematically surface limitations of preferred position.
end

PASS IiiFrontierTracking:
    sutra: "Frontier Tracking monitors academic labs, AI labs, mission-driven orgs, and commercial players at weekly/monthly/quarterly cadences."
    gloss:
        @term Frontier Tracking establishes systematic monitoring of research frontier. Target domains: Academic (Stanford HAI, MIT CSAIL, UC Berkeley BAIR, CMU ML, Oxford/Cambridge AI), AI Labs (Anthropic, OpenAI, Google DeepMind, Meta AI, xAI), Mission-Driven (Center for AI Safety, AI Alignment Forum, FHI), Commercial (Cursor, Replit, Perplexity, Palantir, Anduril). Tracking cadence: Weekly (key lab blogs, ArXiv cs.AI/cs.CL, Hacker News, relevant subreddits), Monthly (3-5 papers in depth, major product releases, frontier map update, prediction reassessment), Quarterly (comprehensive landscape, emerging paradigm shifts, taxonomy updates, framework revision). Frontier map structure: claim_id, claim (testable), source (URL, date), confidence (0-1), volatility (stable/shifting/speculative), dependencies, implications, contradictions, next_verification (date).
end

PASS TargetDomains:
    sutra: "Frontier tracking targets academic labs, AI research labs, mission-driven orgs, and commercial frontier tools."
    gloss:
        @term Target Domains for frontier tracking: Academic (Stanford HAI, MIT CSAIL, UC Berkeley BAIR, CMU ML, Oxford/Cambridge AI); AI Labs (Anthropic, OpenAI, Google DeepMind, Meta AI, xAI); Mission-Driven (Center for AI Safety, AI Alignment Forum, FHI); Commercial (Cursor, Replit, Perplexity, Palantir, Anduril). Each domain provides different signal: academics for fundamental research, AI labs for capability frontier, mission-driven for safety/alignment, commercial for adoption patterns.
end

PASS TrackingCadence:
    sutra: "Frontier monitoring operates at weekly (signals), monthly (depth), and quarterly (strategic) cadences."
    gloss:
        @term Tracking Cadence: Weekly (key lab blogs, ArXiv cs.AI/cs.CL, Hacker News, relevant technical communities—fast signal acquisition), Monthly (3-5 papers analyzed in depth, major product releases monitored, @term Frontier Map updated, predictions reassessed—depth analysis), Quarterly (comprehensive landscape analysis, emerging paradigm shift detection, taxonomy updates, framework revisions—strategic synthesis).
end

PASS FrontierMapStructure:
    sutra: "Frontier Map encodes queryable, traceable, updatable, actionable claims with dependencies and contradictions."
    gloss:
        @term Frontier Map Structure enables systematic tracking: claim_id (unique identifier), claim (testable statement), source (URL, date), @term Confidence (0-1 range), @term Volatility (stable/shifting/speculative), dependencies (related_claims), implications (design_changes if true), contradictions (conflicting_claims), next_verification (date for recheck). Structure enables: querying claim status, tracing reasoning, updating confidence when evidence arrives, identifying implications, surfacing contradictions, prioritizing verification work.
end

NORM IvQualityStandards:
    sutra: "Quality standards require source attribution, primary preference, confidence rationales, recency tags, contradiction acknowledgment, and peer review."
    gloss:
        @term Quality Standards establish mandatory research discipline. All research outputs must satisfy @term Research Quality Checklist: every claim has source attribution, sources are primary when possible, secondary sources add genuine analysis, counter-positions included, @term Confidence Levels explicit with rationale, recency and volatility tagged, @term Design Deltas specified, contradictions acknowledged, no benchmark shopping or link dumping, hype distinguished from evidence. Reports follow structure: Executive Summary → Decision-Bearing Questions → Methodology → Findings → Implications → Limitations → Next Steps. Writing discipline: lead with conclusions, use clear testable claims, separate evidence from interpretation, acknowledge uncertainty explicitly. Peer review mandatory: self-review (what could I be wrong about?), source validation (find original, confirm attribution), logic check (do conclusions follow? alternatives?), impact assessment (decisions informed? cost of error?).
end

PASS ResearchQualityChecklist:
    sutra: "Quality checklist mandates source attribution, primary preference, secondary analysis, counter-positions, explicit confidence, tags, and contradiction acknowledgment."
    gloss:
        @term Research Quality Checklist (mandatory verification): (1) Every claim has source attribution (trackable). (2) Sources are primary when possible (original preferred). (3) Secondary sources add analysis (not just links). (4) Counter-positions included (@term Source Triad Pass 3). (5) @term Confidence Levels explicit with rationale. (6) Recency and volatility tagged. (7) @term Design Deltas specified. (8) Contradictions acknowledged. (9) No benchmark shopping or link dumping. (10) Hype distinguished from evidence. All research outputs must pass checklist before use in decisions.
end

NORM ReportStandards:
    sutra: "Reports follow fixed structure and writing discipline: lead with conclusions, separate evidence from interpretation, acknowledge uncertainty."
    gloss:
        @term Report Standards: Structure (Executive Summary → Decision-Bearing Questions → Methodology → Findings → Implications → Limitations → Next Steps). Writing discipline: lead with conclusions, use clear testable claims, separate evidence from interpretation, acknowledge uncertainty explicitly. This enables rapid decision-making while maintaining traceability for verification and error correction.
end

PROC PeerReviewProtocol:
    sutra: "Peer review requires self-review, source validation, logic check, and impact assessment before any research output."
    gloss:
        @term Peer Review Protocol (four phases): (1) Self-review: What could I be wrong about? What evidence am I ignoring? What assumptions am I making? (2) Source validation: Can I find original source? Does it actually say what I claim? Is attribution accurate? (3) Logic check: Do conclusions follow from evidence? Are there alternative explanations? Did I miss nuance? (4) Impact assessment: What decisions will this inform? What's cost of being wrong? Is confidence level appropriate for stakes? Mandatory before output deployment.
end

PASS VContextspecificPatterns:
    sutra: "Context-specific research patterns address AI capability evaluation, paradigm shift assessment, infrastructure service evaluation, and tool extraction."
    gloss:
        @term Context-Specific Patterns provide domain-tailored research approaches. Four key domains: (1) @term AI Capability Evaluation—key questions: actual vs. claimed capability? reliability? cost/latency tradeoffs? failure modes? alternatives? adoption path? Red flags: cherry-picked examples, no limitations, weak baselines, unclear failure modes. (2) @term Paradigm Shift Assessment—key questions: what's fundamentally changing? what's incremental? what enables this now? adoption barriers? trajectory? Distinguish: capability unlock vs. efficiency gain vs. interface improvement vs. hype cycle. (3) @term Infrastructure Service Evaluation—source triad applied to vendor docs (Pass 1), third-party benchmarks (Pass 2), limitations/alternatives/exit (Pass 3); evaluate: Technical (latency p50/p95/p99, throughput, reliability, quality, scalability), Business (funding, market position, strategic risk, pricing stability), Governance (security, privacy, compliance, audit), Lock-in (portability, exit cost). (4) @term Tool Primitive Extraction—phases: tool survey, primitive identification, overlap analysis, extraction feasibility, reuse validation. Output: adopt tool vs. extract primitives vs. skip.
end

PASS AiCapabilityEvaluation:
    sutra: "AI capability research must distinguish actual capability from claims, assess reliability, tradeoffs, failure modes, and adoption barriers."
    gloss:
        @term AI Capability Evaluation key questions: Actual capability vs. claimed (marketing)? Reliability across conditions? Cost/latency/quality tradeoffs? Failure modes and edge cases? Viable alternatives? Adoption path and barriers? Red flags for over-hype: cherry-picked examples, no limitations acknowledged, weak comparison baselines, unclear failure modes, unrealistic deployment scenarios. Require: rigorous benchmarking, transparent limitations, realistic tradeoff discussion, alternative evaluation.
end

PASS ParadigmShiftAssessment:
    sutra: "Paradigm shift assessment distinguishes fundamental capability changes from efficiency gains, adoption barriers from hype cycles."
    gloss:
        @term Paradigm Shift Assessment key questions: What's fundamentally changing (capability unlock)? What's incremental (efficiency, interface, workflow)? What enables this now (technical, economic, social)? What adoption barriers exist? What's the realistic trajectory? Distinguish: capability unlock (enables new applications), efficiency gain (faster/cheaper same capability), interface improvement (same capability, better UX), hype cycle (speculation). Assessment requires distinguishing signal from speculation, evaluating technical feasibility, assessing economic viability, identifying adoption barriers.
end

PASS InfrastructureServiceEvaluation:
    sutra: "Infrastructure evaluation applies source triad to technical/business/governance dimensions with capability contract assessment and own-vs-lease decision framework."
    gloss:
        @term Infrastructure Service Evaluation applies @term Source Triad: Pass 1 (vendor docs, benchmarks, pricing, SLAs, certs), Pass 2 (third-party benchmarks, case studies, technical reviews), Pass 3 (known limitations, complaints, alternatives, exit feasibility). Evaluation dimensions: Technical (latency p50/p95/p99, throughput, reliability, quality, scalability), Business (funding, market position, strategic risk, pricing stability, support), Governance (security certs, privacy, compliance, audit capability, incident response), Lock-in (data portability, API compatibility, feature dependency, migration cost). @term Capability Contract Assessment: performance SLOs documented/enforced? Data governance (residency, retention, deletion)? Provenance (trace outputs to sources)? Exit strategy (migration path, transition support)? Cost transparency? Decision framework: Data Sensitivity × Latency Requirements × Economic Leverage × Lock-in Risk → OWN | LEASE | HYBRID.
end

PASS ToolPrimitiveExtraction:
    sutra: "Tool primitive extraction identifies core interfaces and capabilities for potential reuse, assessing feasibility and composition value."
    gloss:
        @term Tool Primitive Extraction phases: (1) Tool Survey (features, interface patterns, platform, performance, quality). (2) Primitive Identification (interface, capability, algorithm, platform optimizations). (3) Overlap Analysis (compare to catalog, identify unique vs. redundant). (4) Extraction Feasibility (technical difficulty, legal constraints, maintenance, composition complexity). (5) Reuse Validation (test in composition, measure performance, assess combinations). Output recommendation: Adopt tool (unique value beyond primitives) | Extract primitives only (compose from parts) | Skip (no unique value, redundant capability).
end

PASS ViOutputFormats:
    sutra: "Output formats provide three structures: comprehensive report for strategic questions, quick log entry for targeted research, decision brief for action."
    gloss:
        @term Output Formats serve different decision contexts. (1) @term Research Report (Comprehensive): when major strategic questions, significant uncertainty, high impact. Structure: Title | Executive Summary | Background | Methodology | Findings | Synthesis | Limitations | Next Steps. (2) @term Research Log Entry (Quick): when targeted questions, confirming claims, updating frontier. Structure: Question | Source | Finding | Confidence | Design Delta | Next. (3) @term Decision Brief (Action-Oriented): when immediate decision needed, stakeholder briefing. Structure: Decision | Recommendation | Rationale | If Wrong | Timeline. Format selection depends on decision urgency and stakeholder needs.
end

PASS ResearchReportComprehensive:
    sutra: "Comprehensive report addresses major strategic questions with full methodology, synthesis, and limitation acknowledgment."
    gloss:
        @term Research Report (Comprehensive) structure: Title (clear decision context). Executive Summary (conclusions, key recommendations, confidence levels, critical uncertainties). Background (decision context, prior knowledge, why this matters). Methodology (which @term Source Triad passes applied, key search terms, sources evaluated, selection rationale). Findings (what evidence shows, organized by claim, with sources and confidence). Synthesis (what it means together, implications, contradictions, open questions). Limitations (data gaps, assumption risks, volatility factors, update triggers). Next Steps (verification cadence, decision point criteria, future research priorities). Used for: major strategic questions, significant uncertainty, high-impact decisions.
end

PASS ResearchLogEntryQuick:
    sutra: "Quick log entry captures targeted finding with confidence, design delta, and next action for frontier tracking."
    gloss:
        @term Research Log Entry (Quick) structure: Question (what was researched). Source (where finding came from, with date/URL). Finding (key claim in one sentence). Confidence (High/Medium/Low/Speculative). Volatility (Stable/Shifting/Speculative). @term Design Delta (if true, then we should...). Next (what needs verification or follow-up, when). Used for: targeted questions, confirming claims, updating frontier map, rapid signal acquisition. Enables rapid logging and querying.
end

PASS DecisionBriefActionoriented:
    sutra: "Action-oriented decision brief presents recommendation, rationale, error consequences, and timeline for immediate decisions."
    gloss:
        @term Decision Brief (Action-Oriented) structure: Decision (clear framing of choice point). Recommendation (what should we do, why). Rationale (supporting evidence, confidence level, key assumptions). If Wrong (what happens if recommendation fails, error cost, detection signal). Timeline (when decision needed, when implementation complete, review points). Used for: immediate decisions, stakeholder briefing, executive alignment, action triggering. Minimizes decision latency while capturing key reasoning.
end

PASS ViiCapabilityContractAudit:
    sutra: "Capability contract audits occur monthly (metrics), quarterly (full review), annually (strategy), with standardized checklist and findings classification."
    gloss:
        @term Capability Contract Audit systematic verification protocol. Audit Cadence: Monthly (performance metrics, cost vs. budget, incident scan, degradation trends), Quarterly (full contract review, vendor health, alternatives evaluation, renegotiation if needed), Annual (own vs. lease reassessment, landscape analysis, migration consideration, long-term strategy). @term Audit Checklist: Performance (latency, availability, throughput, quality targets met? trends?), Data Governance (residency, retention, export, encryption, access controls compliant?), Provenance (model versions, training data, inference traceability, explanations adequate?), Cost (actual vs. expected? pricing changes? optimization opportunities? ROI positive?), Exit Strategy (migration viable? alternatives available? export tested? transition support?). Findings Classification: Green (all criteria met, stable/improving, cost expected, no risks), Yellow (borderline criteria, minor degradation, cost creeping, emerging risks—monitor closely), Red (critical violations, significant issues, overruns, material risks—action required). Response: Yellow (increase monitoring, engage vendor, research alternatives, set targets), Red (immediate escalation, alternative evaluation, migration planning, executive notification).
end

PASS AuditCadence:
    sutra: "Audit cadence: monthly (operations), quarterly (strategy), annual (long-term) ensures continuous contract compliance and strategic alignment."
    gloss:
        @term Audit Cadence: Monthly (performance metrics tracking, cost vs. budget tracking, incident scan, degradation trend analysis—operational assurance), Quarterly (full contract review against SLOs, vendor health assessment, alternatives landscape evaluation, renegotiation if needed—tactical alignment), Annual (own vs. lease reassessment, comprehensive landscape analysis, migration feasibility assessment, long-term strategy revision—strategic positioning).
end

PASS AuditChecklist:
    sutra: "Checklist verifies performance, data governance, provenance, cost, and exit strategy across technical, business, and operational dimensions."
    gloss:
        @term Audit Checklist dimensions: Performance (latency p50/p95/p99 meet targets? availability? throughput? quality? trends improving/degrading?). Data Governance (residency requirements met? retention policies enforced? export capability functional? encryption active? access controls auditable?). Provenance (model versions tracked? training data documented? inference outputs traceable? explanations adequate?). Cost (actual spending vs. expected? pricing changes announced? optimization opportunities available? ROI positive?). Exit Strategy (migration to alternatives viable? alternatives available in market? export tested and working? transition support from vendor?). All checklist items must be Green or Yellow; Red findings trigger escalation.
end

PASS FindingsClassification:
    sutra: "Findings classified as Green (compliant), Yellow (monitored), Red (escalated) based on severity and remediation urgency."
    gloss:
        @term Findings Classification (standardized severity): Green (all criteria met, performance stable or improving, costs expected, no material risks—continue normal operations), Yellow (borderline criteria met, minor degradation, cost moderately exceeding budget, emerging risks identified—increase monitoring frequency, engage vendor on improvements, research alternatives, set remediation targets), Red (critical contract violations, significant performance degradation, major cost overruns, material risks to operations—immediate escalation required, serious alternative evaluation, migration planning initiated, executive notification, risk mitigation emergency).
end

PASS ResponseActions:
    sutra: "Yellow findings trigger monitoring, vendor engagement, and alternative research; Red findings trigger escalation, migration planning, and executive action."
    gloss:
        @term Response Actions by classification: Yellow (increase monitoring cadence, engage vendor on remediation, research alternative providers, set specific improvement targets with dates, plan contingency). Red (immediate escalation to executive sponsor, initiate serious alternative evaluation with RFP, begin migration planning and risk assessment, notify security/legal if applicable, activate contingency plans, establish decision timeline for exit/remediation/acceptance).
end

PASS ViiiOperationalResearchExcellence:
    sutra: "Operational excellence requires decision-bearing questions first, source triad always, verdicts with explicit confidence, intellectual honesty, and continuous validation."
    gloss:
        @term Operational Research Excellence core principles: (1) Decision-bearing questions first (justify research time, ensure actionability). (2) @term Source Triad always (primary + secondary + counter, never skip Pass 3). (3) Verdict with confidence (claim + confidence + recency + volatility + @term Design Delta, explicit rationale). (4) Intellectual honesty (acknowledge limitations, contradictions, uncertainty, update when wrong). (5) Continuous validation (update @term Frontier Map when evidence arrives, trigger re-evaluation on contradictions, correct errors immediately). Infrastructure additions: neo-layer classification, @term Capability Contract Assessment, own vs. lease decision framework, lock-in risk analysis, regular audits. Principle: quality over quantity (few high-confidence verdicts > many unvalidated claims), actionable over academic (every output must enable better decisions), goal: make better decisions faster by knowing what's true, what's uncertain, what's changing, what it means for action.
end

PASS VersionHistory:
    sutra: "CANON-30330 v2.0.0 canonized December 2025 from Technology Lunar, compressed 35%, preserved core methodology, added hierarchy placement."
    gloss:
        @term Version History: v2.0.0 (December 2025): canonization from Technology Lunar source; compressed from ~1.5K source words to ~410 compressed; 35% reduction ratio; removed redundant examples and verbose explanations; preserved all core methodology; added CANON frontmatter and hierarchy placement within CANON-30300 (Technology Stack Database) in intelligence chain.
end

PASS Crossreferences:
    sutra: "CANON-30330 references master schema, intelligence chain root, and corpus management canon."
    gloss:
        @term Cross-References: CANON-00000-SCHEMA-cosmos (Master Schema), CANON-30000-INTELLIGENCE-chain (Intelligence Chain Root), CANON-00006-CORPUS-cosmos (Corpus Management), CANON-30300 (Technology Stack Database parent), CANON-30310 (Tech Stack Migration sibling), CANON-30320 (Workflow Intelligence sibling), CANON-30340 (Implementation Patterns sibling).
end