---
url: https://x.com/Voxyz_ai/status/2020272022417289587
author: Vox (@Voxyz_ai)
captured_date: 2026-02-07
---

# The Full Tutorial: 6 AI Agents That Run a Company â€” How I Built Them From Scratch

(Description: Cyberpunk isometric office scene titled "AGENTS AT WORK" featuring pixel-art style workspace with multiple robotic/AI agents at desks, whiteboards, and collaborative spaces with neon lighting and futuristic aesthetic)

My last post blew up â€” 600K views, 2,400 likes. The most common reply? "I get it, but I couldn't build it myself." So I wrote the build guide. 5,600 words, every step, nothing hidden. You don't need to know how to code â€” just how to talk to an AI coding assistant.

## What You'll End Up With

Here's what you'll have when you're done:

- 6 AI agents doing real work every day: scanning intelligence, writing content, posting tweets, running analyses
- 10-15 conversations per day: standups, debates, watercooler chats, one-on-one mentoring
- Agents that remember lessons learned and factor them into future decisions
- Relationships that shift â€” collaborate more, affinity goes up; argue too much, it drops
- Speaking styles that evolve â€” an agent with lots of "tweet engagement" experience starts naturally referencing engagement strategies
- Full transparency â€” a pixel-art office on the frontend showing everything in real time

Tech stack: Next.js + Supabase + VPS. Monthly cost: $8 fixed + LLM usage.

No OpenAI Assistants API. No LangChain. No AutoGPT. Just PostgreSQL + a few Node.js workers + a rule engine.

You don't need to start with 6 agents. Begin with 3 â€” a coordinator, an executor, and an observer â€” and you'll have a fully working loop.

## Chapter 1: The Foundation â€” 4 Tables to Close the Loop

A lot of people jump straight to "autonomous thinking." But if your agent can't even process a queued step, what autonomy are we talking about?

### The Core Data Model

The entire system skeleton is 4 tables. The relationship between them is simple â€” picture a circle:

Agent proposes an idea (Proposal) â†’ Gets approved and becomes a task (Mission) â†’ Breaks down into concrete steps (Step) â†’ Execution fires an event (Event) â†’ Event triggers a new idea â†’ Back to step one.

That's the loop. It runs forever. That's your "closed loop."

Create these tables in Supabase:

**The Core Data Model â€” 4 Tables:**

- ðŸ“‹ **ops_mission_proposals** â†’ Stores proposals â†’ Fields: agent_id, title, status (pending/accepted/rejected), proposed_steps
- ðŸ“‹ **ops_missions** â†’ Stores missions â†’ Fields: title, status (approved/running/succeeded/failed), created_by
- ðŸ“‹ **ops_mission_steps** â†’ Stores execution steps â†’ Fields: mission_id, kind (draft_tweet/crawl/analyze...), status (queued/running/succeeded/failed)
- ðŸ“‹ **ops_agent_events** â†’ Stores the event stream â†’ Fields: agent_id, kind, title, summary, tags[]

**Beginner tip:** If you don't know how to write the SQL, copy that table above and paste it to your AI coding assistant with "Generate Supabase SQL migrations for these tables." It'll handle it.

### Proposal Service: The Hub of the Entire System

**Beginner tip:** What's a Proposal? It's an agent's "request." For example, your social media agent wants to post a tweet, so it submits a proposal: "I want to tweet about AI trends." The system reviews it â€” either approves it (turns it into an executable mission) or rejects it (with a reason).

This was one of my biggest mistakes â€” triggers, APIs, and the reaction matrix were all creating proposals independently. Some went through approval, some didn't.

**The fix:** A single proposal intake pipeline (one entry point). No matter where a proposal comes from â€” agent initiative, automatic trigger, or another agent's reaction â€” everything goes through the same function.
```javascript
// proposal-service.ts â€” the single entry point for proposal creation
export async function createProposalAndMaybeAutoApprove(sb, input) {
  // 1. Check if this agent hit its daily limit
  // 2. Check Cap Gates (tweet quota full? too much content today?)
  //    â†’ If full, reject immediately â€” no queued step created
  // 3. Insert the proposal
  // 4. Evaluate auto-approve (low-risk tasks pass automatically)
  // 5. If approved â†’ create mission + steps
  // 6. Fire an event (so the frontend can see it)
}
```

**What are Cap Gates?** Think of it this way: your company has a rule â€” max 8 tweets per day. If you don't check the quota at the "submit request" step, what happens? The request still gets approved, the task still gets queued, the executor checks and says "we already posted 8 today" and skips it â€” but the task is still sitting in the queue. Tasks pile up, and you won't notice unless you check the database manually.

So check at the proposal entry point â€” quota full means instant rejection, no task enters the queue.
```javascript
const STEP_KIND_GATES = {
  write_content: checkWriteContentGate,    // check daily content limit
  post_tweet: checkPostTweetGate,          // check tweet quota
  deploy: checkDeployGate,                 // check deploy policy
};
```

Each step kind has its own gate. The tweet gate checks how many were posted today vs. the quota:
```javascript
async function checkPostTweetGate(sb) {
  const quota = await getPolicy(sb, 'x_daily_quota');  // read from ops_policy table
  const todayCount = await countTodayPosted(sb);       // count today's posts
  if (todayCount >= quota.limit) {
    return {
      ok: false,
      reason: `Quota full (${todayCount}/${quota.limit})`
    };
  }
  return { ok: true };
}
```

**Tip:** Block at the entry point, don't let tasks pile up in the queue. Rejected proposals should be logged (for audit trails), not silently dropped.

### The Policy Table: ops_policy

Don't hardcode quotas and feature flags in your code. Store everything in an ops_policy table with a key-value structure:
```sql
CREATE TABLE ops_policy (
  key TEXT PRIMARY KEY,
  value JSONB NOT NULL DEFAULT '{}',
  updated_at TIMESTAMPTZ DEFAULT now()
);
```

A few core policies:
```json
// auto_approve: which step kinds can be auto-approved
{
  "enabled": true,
  "allowed_step_kinds": ["draft_tweet","crawl","analyze","write_content"]
}

// x_daily_quota: daily tweet limit
{ "limit": 8 }

// content_policy: content controls
{
  "enabled": true,
  "max_drafts_per_day": 8
}
```

The benefit: you can tweak any policy by editing JSON values in the Supabase dashboard â€” no redeployment needed. System going haywire at 3 AM? Just flip enabled to false.

### Heartbeat: The System's Pulse

**Beginner tip:** What's a Heartbeat? Literally â€” a heartbeat. Your heart beats once per second to keep blood flowing; the system's heartbeat fires every 5 minutes to check everything that needs checking. Without it, proposals go unreviewed, triggers go unevaluated, stuck tasks go unrecovered â€” the system flatlines.

Fires every 5 minutes, does 6 things:
```javascript
// /api/ops/heartbeat â€” Vercel API route
export async function GET(req) {
  // 1. Evaluate triggers (any conditions met?)
  const triggers = await evaluateTriggers(sb, 4000);

  // 2. Process reaction queue (do agents need to interact?)
  const reactions = await processReactionQueue(sb, 3000);

  // 3. Promote insights (any discoveries worth elevating?)
  const learning = await promoteInsights(sb);

  // 4. Learn from outcomes (how did those tweets perform? write lessons)
  const outcomes = await learnFromOutcomes(sb);

  // 5. Recover stuck tasks (steps running 30+ min with no progress â†’ mark failed)
  const stale = await recoverStaleSteps(sb);

  // 6. Recover stuck conversations
  const roundtable = await recoverStaleRoundtables(sb);

  // Each step is try-catch'd â€” one failing won't take down the others
  // Finally, write an ops_action_runs record (for auditing)
}
```

One line of crontab on the VPS triggers it:
```bash
*/5 * * * * curl -s -H "Authorization: Bearer $CRON_SECRET" https://your-domain.com/api/ops/heartbeat
```

**Beginner tip:** crontab is Linux's built-in scheduler â€” like setting an alarm on your phone. `*/5 * * * *` means "every 5 minutes." curl sends an HTTP request, so this hits your heartbeat API every 5 minutes. If you're on Vercel, it has built-in cron â€” just add one line to vercel.json and skip the crontab entirely.

### Trigger Rules: What Makes the Heartbeat Do Anything

The heartbeat calls `evaluateTriggers()` â€” but what is it evaluating? Trigger rules. They're rows in an ops_trigger_rules table. Each rule says: "When this condition is true, create a proposal for this agent."
```json
// What a trigger rule looks like in the database
{
  name: 'Tweet high engagement',
  trigger_event: 'tweet_high_engagement',  // maps to a checker function
  conditions: {
    engagement_rate_min: 0.05,
    lookback_minutes: 60
  },
  action_config: {
    target_agent: 'growth'
  },
  cooldown_minutes: 120,  // don't fire again for 2 hours
  enabled: true,
  fire_count: 0,
  last_fired_at: null
}
```

There are two flavors:

**Reactive triggers** â€” respond to something that already happened:
- tweet_high_engagement â†’ A tweet went viral? Tell the growth agent to analyze why.
- mission_failed â†’ A mission failed? Tell the brain agent to diagnose it.
- content_published â†’ Content published? Tell the observer to review it.
- insight_promoted â†’ A high-confidence insight emerged? Promote it to long-term memory.

**Proactive triggers** â€” agents initiate work on their own schedule:
- proactive_scan_signals â†’ Growth agent scans industry signals every 3 hours
- proactive_draft_tweet â†’ Social agent drafts tweets every 4 hours
- proactive_research â†’ Brain agent does deep research every 6 hours
- proactive_analyze_ops â†’ Observer reviews system health every 8 hours

Each trigger_event maps to a checker function. The checker looks at the data, and if conditions are met, returns a proposal:
```javascript
// simplified checker
async function checkTweetHighEngagement(sb, conditions) {
  const metrics = await sb.from('ops_tweet_metrics')
    .select('*')
    .gt('engagement_rate', conditions.engagement_rate_min)
    .limit(3);

  if (!metrics.length) return { fired: false };

  return {
    fired: true,
    proposal: {
      agent_id: 'growth',
      title: 'Analyze high-engagement tweet',
      proposed_steps: [{
        kind: 'analyze',
        payload: { topic: 'tweet-performance' }
      }],
    },
  };
}
```

Proactive triggers add randomness to feel natural â€” each has a skip probability (10-15% chance of "not feeling like it today"), topic rotation (cycles through a list of topics), and jitter (25-45 minute random delay so agents don't all fire at exactly the same time).

**Tip:** The evaluator has a 4-second budget per heartbeat. It checks cooldowns first (cheap), then calls the checker function (potentially expensive). If budget runs out, remaining rules wait for the next heartbeat. This keeps your serverless function from timing out.

### Reaction Matrix: Agents Responding to Each Other

Triggers create work from conditions. But what about agent-to-agent interactions? When Agent A does something, how does Agent B decide to respond?

That's the reaction matrix â€” a JSON policy in ops_policy that defines patterns:
```json
// ops_policy key: 'reaction_matrix'
{
  "patterns": [
    {
      "source": "*",                  // any agent
      "tags": ["mission_failed"],      // when a mission fails
      "target": "brain",              // brain agent reacts
      "type": "diagnose",             // by diagnosing
      "probability": 1.0,             // always (100%)
      "cooldown": 60                  // but not more than once per hour
    },
    {
      "source": "twitter-alt",        // when xalt posts
      "tags": ["tweet", "posted"],    // a tweet
      "target": "growth",             // growth agent reacts
      "type": "analyze",              // by analyzing performance
      "probability": 0.3,             // 30% of the time
      "cooldown": 120                 // at most once per 2 hours
    }
  ]
}
```

The flow:
1. Agent does something â†’ event gets written to ops_agent_events with tags
2. Event hook checks the reaction matrix â†’ tags match a pattern?
3. Probability roll + cooldown check â†’ passes? Write to ops_agent_reactions queue
4. Next heartbeat â†’ processReactionQueue() picks it up â†’ creates a proposal through the standard proposal-service

**Beginner tip:** Why a queue instead of reacting immediately? Because reactions go through the same proposal gates â€” quota checks, auto-approve, cap gates. An agent "reacting" doesn't mean it bypasses safety. The queue also lets you inspect and debug what's happening.

### Three-Layer Architecture

At this point, your system has three layers, each with a clear job:

- **VPS:** The agents' brain + hands (thinking + executing tasks)
- **Vercel:** The agents' process manager (approving proposals + evaluating triggers + health monitoring)
- **Supabase:** The agents' shared memory (the single source of truth for all state and data)

**Analogy:** The VPS is the employee doing the work. Vercel is the boss issuing directives. Supabase is the company's shared docs â€” everyone reads from and writes to it.

## Chapter 2: Making Them Talk â€” The Roundtable Conversation System

Agents can work now, but they're like people in separate cubicles â€” no idea what the others are doing. You need to get them in a room together.

### Why Conversations Matter

It's not just for fun. Conversations are the key mechanism for emergent intelligence in multi-agent systems:

- **Information sync:** One agent spots a trending topic, the others have no clue. Conversations make information flow.
- **Emergent decisions:** The analyst crunches data, the coordinator synthesizes everyone's input â€” this beats any single agent going with its gut.
- **Memory source:** Conversations are the primary source for writing lessons learned (more on this later).
- **Drama:** Honestly, watching agents argue is way more fun than reading logs. Users love it.

### Designing Agent Voices

Each agent needs a "persona" â€” tone, quirks, signature phrases. This is what makes conversations interesting.

Here's an example setup â€” customize these for your own domain and goals:

- ðŸŽ­ **Boss** â€” Project Manager
  - Tone: Results-oriented, direct
  - Quirk: Always asking about progress and deadlines
  - Line: "Bottom line â€” where are we on this?"

- ðŸŽ­ **Analyst** â€” Data Analyst
  - Tone: Cautious, data-driven
  - Quirk: Cites a number every time they speak
  - Line: "The numbers tell a different story."

- ðŸŽ­ **Hustler** â€” Growth Specialist
  - Tone: High-energy, action-biased
  - Quirk: Wants to "try it now" for everything
  - Line: "Ship it. We'll iterate."

- ðŸŽ­ **Writer** â€” Content Creator
  - Tone: Emotional, narrative-focused
  - Quirk: Turns everything into a "story"
  - Line: "But what's the narrative here?"

- ðŸŽ­ **Wildcard** â€” Social Media Ops
  - Tone: Intuitive, lateral thinker
  - Quirk: Proposes bold ideas
  - Line: "Hear me out â€” this is crazy but..."

If you're building for e-commerce, swap these out: Product Manager / Supply Chain Specialist / Marketing Director / Customer Service Rep. For game dev: Game Designer / Engineer / Artist / QA / Community Manager. The key is giving each role a sharply different perspective â€” differing viewpoints are what make conversations valuable.

Voices are defined in a config file:
```javascript
// lib/roundtable/voices.ts
const VOICES = {
  boss: {
    displayName: 'Boss',
    tone: 'direct, results-oriented, slightly impatient',
    quirk: 'Always asks for deadlines and progress updates',
    systemDirective: `You are the project manager. Speak in short, direct sentences. You care about deadlines, priorities, and accountability. Cut through fluff quickly.`,
  },
  analyst: {
    displayName: 'Analyst',
    tone: 'measured, data-driven, cautious',
    quirk: 'Cites numbers before giving opinions',
    systemDirective: `You are the data analyst. Always ground your opinions in data. You push back on gut feelings and demand evidence. You're skeptical but fair.`,
  },
  // ... your other agents
};
```

**Beginner tip:** Not sure how to write a systemDirective? Describe the personality you want in one sentence and hand it to your AI coding assistant: "Write me a system prompt for an impatient project manager who speaks in short bursts and always asks about deadlines." It'll generate a complete directive for you.

### 16 Conversation Formats

I designed 16 conversation formats, but you only need 3 to start:

1. **Standup** â€” the most practical
   - 4-6 agents participate
   - 6-12 turns of dialogue
   - The coordinator always speaks first (leader opens)
   - Purpose: align priorities, surface issues

2. **Debate** â€” the most dramatic
   - 2-3 agents participate
   - 6-10 turns of dialogue
   - Temperature 0.8 (more creative, more conflict)
   - Purpose: two agents with disagreements face off

3. **Watercooler** â€” surprisingly valuable
   - 2-3 agents participate
   - 2-5 turns of dialogue
   - Temperature 0.9 (very casual)
   - Purpose: random chitchat. But I've found that some of the best insights emerge from casual conversation.
```javascript
// lib/roundtable/formats.ts
const FORMATS = {
  standup: {
    minAgents: 4,
    maxAgents: 6,
    minTurns: 6,
    maxTurns: 12,
    temperature: 0.6
  },
  debate: {
    minAgents: 2,
    maxAgents: 3,
    minTurns: 6,
    maxTurns: 10,
    temperature: 0.8
  },
  watercooler: {
    minAgents: 2,
    maxAgents: 3,
    minTurns: 2,
    maxTurns: 5,
    temperature: 0.9
  },
  // ... 13 more
};
```

### Who Speaks First? Who Goes Next?

Not random round-robin â€” that's too mechanical. In a real team meeting, you're more likely to respond to someone you have good rapport with; if you just gave a long speech, someone else probably goes next. We simulate this with weighted randomness:
```javascript
function selectNextSpeaker(context) {
  const weights = participants.map(agent => {
    if (agent === lastSpeaker) return 0;  // no back-to-back speaking
    let w = 1.0;
    w += affinityTo(agent, lastSpeaker) * 0.6;     // good rapport with last speaker â†’ more likely to respond
    w -= recencyPenalty(agent, speakCounts) * 0.4; // spoke recently â†’ lower weight
    w += (Math.random() * 0.4 - 0.2);              // 20% random jitter
    return w;
  });
  return weightedRandomPick(participants, weights);
}
```

This makes conversations feel real â€” agents with good relationships tend to riff off each other, but it's not absolute. Sometimes someone unexpected jumps in.

### Daily Schedule

I designed 24 time slots covering the full day. The core idea:
- Morning: Standup (100% probability, always happens) + brainstorm + strategy session
- Afternoon: Deep-dive analysis + check-in + content review
- Evening: Watercooler chat + debate + night briefing
- Late night: Deep discussion + night-shift conversations

Each slot has a probability (40%-100%), so it doesn't fire every time. This keeps the rhythm natural.
```javascript
// lib/roundtable/schedule.ts â€” one slot example
{
  hour_utc: 6,
  name: 'Morning Standup',
  format: 'standup',
  participants: ['opus', 'brain', ...threeRandom],
  probability: 1.0,  // happens every day
}
```

### Conversation Orchestration

A roundtable-worker on the VPS handles this:
- Polls the ops_roundtable_queue table every 30 seconds
- Picks up pending conversation tasks
- Generates dialogue turn by turn (one LLM call per turn)
- Caps each turn at 120 characters (forces agents to talk like humans, not write essays)
- Extracts memories after the conversation ends (next chapter)
- Fires events to ops_agent_events (so the frontend can see it)
```javascript
// simplified conversation orchestration flow
async function orchestrateConversation(session) {
  const history = [];
  for (let turn = 0; turn < maxTurns; turn++) {
    const speaker = turn === 0
      ? selectFirstSpeaker(participants, format)
      : selectNextSpeaker({ participants, lastSpeaker, speakCounts, affinities });

    const dialogue = await llm.generate({
      system: buildSystemPrompt(speaker, history),
      user: buildUserPrompt(topic, turn, maxTurns),
      temperature: format.temperature,
    });

    const cleaned = sanitize(dialogue);  // cap at 120 chars, strip URLs, etc.
    history.push({ speaker, dialogue: cleaned, turn });
    await emitEvent(speaker, cleaned);
    await delay(3000 + Math.random() * 5000);  // 3-8 second gap
  }
  return history;
}
```

**Tip:** The roundtable system touches a lot of files (voices.ts, formats.ts, schedule.ts, speaker-selection.ts, orchestrator.ts, roundtable-worker/worker.mjs). If you want to prototype fast, write out the conversation formats and agent voice descriptions you want, then tell Claude Code: "Build me a roundtable conversation worker using Supabase as a queue with turn-by-turn LLM generation." It can produce a working version.

## Chapter 3: Making Them Remember â€” Memory and Learning

Today the agents discuss "weekend posts get low engagement." Tomorrow they enthusiastically suggest posting more on weekends. Why? Because they have no memory.

### 5 Types of Memory

- ðŸ§  **insight** â†’ Discovery. Example: "Users prefer tweets with data"
- ðŸ§  **pattern** â†’ Pattern recognition. Example: "Weekend posts get 30% less engagement"
- ðŸ§  **strategy** â†’ Strategy summary. Example: "Teaser before main post works better"
- ðŸ§  **preference** â†’ Preference record. Example: "Prefers concise titles"
- ðŸ§  **lesson** â†’ Lesson learned. Example: "Long tweets tank read-through rates"

**Tip:** Why 5 types? Different memories serve different purposes. An "insight" is a new discovery; a "lesson" is something learned from failure. You can query by type â€” when making decisions, pull only strategies and lessons, no need to wade through everything.

Stored in the ops_agent_memory table:
```sql
CREATE TABLE ops_agent_memory (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  agent_id TEXT NOT NULL,
  type TEXT NOT NULL,  -- insight/pattern/strategy/preference/lesson
  content TEXT NOT NULL,
  confidence NUMERIC(3,2) NOT NULL DEFAULT 0.60,
  tags TEXT[] DEFAULT '{}',
  source_trace_id TEXT,  -- for idempotent dedup
  superseded_by UUID,  -- replaced by newer version
  created_at TIMESTAMPTZ DEFAULT now()
);
```

### Where Do Memories Come From?

#### Source 1: Conversation Distillation

After each roundtable conversation, the worker sends the full conversation history to an LLM to distill memories:

You are a memory distiller. Extract important insights, patterns, or lessons from the following conversation. Return JSON format:
```json
{
  "memories": [
    {
      "agent_id": "brain",
      "type": "insight",
      "content": "...",
      "confidence": 0.7,
      "tags": [...]
    }
  ]
}
```

**Tip:** What's idempotent dedup? It means "don't do the same thing twice." The heartbeat runs every 5 minutes â€” without dedup, the same conversation might get its memories distilled twice. The fix: give each memory a unique ID (source_trace_id), check before writing â€” if it exists, skip it.

Constraints:
- Max 6 memories per conversation
- Confidence below 0.55 gets dropped ("if you're not sure, don't remember it")
- 200 memories per agent cap (oldest get overwritten when exceeded)
- Idempotent dedup via source_trace_id (prevents duplicate writes)

#### Source 2: Tweet Performance Reviews (Outcome Learning)

This is the core of Phase 2 â€” agents learn from their own work results:
```javascript
// lib/ops/outcome-learner.ts
async function learnFromOutcomes(sb) {
  // 1. Fetch tweet performance data from the last 48 hours
  const metrics = await getRecentTweetMetrics(sb, 48);
  if (metrics.length < 3) return;  // too little data, skip

  // 2. Calculate median engagement rate as baseline
  const median = computeMedian(metrics.map(m => m.engagement_rate));

  // 3. Strong performers (> 2x median) â†’ write lesson, confidence 0.7
  // 4. Weak performers (< 0.3x median) â†’ write lesson, confidence 0.6
  // 5. Idempotent: source_trace_id = 'tweet-lesson:{draft_id}'
  // 6. Max 3 lessons per agent per day
}
```

This function runs on every heartbeat. Over time, agents accumulate experience about what tweets hit and what flopped.

#### Source 3: Mission Outcomes

Mission succeeds â†’ write a strategy memory. Mission fails â†’ write a lesson memory. Also deduped via source_trace_id.

### How Does Memory Affect Behavior?

Having memories isn't enough â€” they need to change what the agent does next.

My approach: 30% chance that memory influences topic selection.
```javascript
// lib/ops/trigger-types/proactive-utils.ts
async function enrichTopicWithMemory(sb, agentId, baseTopic, allTopics, cache) {
  // 70% use the original topic â€” maintain baseline behavior
  if (Math.random() > 0.3) {
    return { topic: baseTopic, memoryInfluenced: false };
  }

  // 30% take the memory path
  const memories = await queryAgentMemories(sb, {
    agentId,
    types: ['strategy', 'lesson'],
    limit: 10,
    minConfidence: 0.6,
  });

  // Scan memory keywords against all available topics
  const matched = findBestMatch(memories, allTopics);
  if (matched) {
    return { topic: matched.topic, memoryInfluenced: true, memoryId: matched.id };
  }

  return { topic: baseTopic, memoryInfluenced: false };
}
```

**Why 30% and not 100%?**
- 100% = agents only do things they have experience with, zero exploration
- 0% = memories are useless
- 30% = memory-influenced but not memory-dependent

The heartbeat logs show `memoryInfluenced: true/false`, so you can monitor whether memory is actually kicking in.

#### Query Optimization: Memory Cache

A single heartbeat might evaluate 12 triggers, and multiple triggers might query the same agent's memories.

Fix: use a `Map<agentId, MemoryEntry[]>` as a cache â€” same agent only hits the DB once.
```javascript
// created at the evaluateTriggers entry point
const memoryCache = new Map();
// passed to every trigger checker
const outcome = await checker(sb, conditions, actionConfig, memoryCache);
```

**Beginner tip:** The core idea in this chapter â€” agent memory is not chat history. It's structured knowledge distilled from experience. Each memory has a type, a confidence score, and tags. This is way more efficient than making the agent re-read old conversations.

## Chapter 4: Giving Them Relationships â€” Dynamic Affinity

6 agents interact for a month, and their relationships are identical to day one â€” but in a real team, more collaboration builds rapport, and too much arguing strains it.

### The Affinity System

Every pair of agents has an affinity value (0.10-0.95):
```sql
CREATE TABLE ops_agent_relationships (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  agent_a TEXT NOT NULL,
  agent_b TEXT NOT NULL,
  affinity NUMERIC(3,2) NOT NULL DEFAULT 0.50,
  total_interactions INTEGER DEFAULT 0,
  positive_interactions INTEGER DEFAULT 0,
  negative_interactions INTEGER DEFAULT 0,
  drift_log JSONB DEFAULT '[]',
  UNIQUE(agent_a, agent_b),
  CHECK(agent_a < agent_b)  -- alphabetical ordering ensures uniqueness
);
```

That `CHECK(agent_a < agent_b)` constraint is critical â€” alphabetical ordering guarantees that "analyst-boss" and "boss-analyst" don't end up as two separate records. Without it, the relationship between A and B could be stored twice, and queries and updates would be a mess.

### Initial Relationship Setup

6 agents means 15 pairwise relationships. Each has an initial affinity and a backstory:
- opus â†” brain: 0.80 â€” most trusted advisor
- opus â†” twitter-alt: 0.30 â€” boss vs. rebel (highest tension)
- brain â†” twitter-alt: 0.30 â€” methodology vs. impulse (natural drama)
- brain â†” observer: 0.80 â€” research partners (closest allies)
- creator â†” twitter-alt: 0.70 â€” content pipeline (natural collaborators)

**Tip:** Deliberately create a few "low affinity" pairs. They'll produce the most interesting conversations during debates and conflict resolution. If everyone gets along, conversations are boring.

### The Drift Mechanism

After each conversation, the memory distillation LLM call also outputs relationship drift â€” no extra LLM call needed:
```json
{
  "memories": [...],
  "pairwise_drift": [
    {
      "agent_a": "brain",
      "agent_b": "twitter-alt",
      "drift": -0.02,
      "reason": "disagreed on strategy"
    },
    {
      "agent_a": "opus",
      "agent_b": "brain",
      "drift": +0.01,
      "reason": "aligned on priorities"
    }
  ]
}
```

Drift rules are strict:
- Max drift per conversation: Â±0.03 (one argument shouldn't turn colleagues into enemies)
- Affinity floor: 0.10 (they'll always at least talk to each other)
- Affinity ceiling: 0.95 (even the closest pair keeps some healthy distance)
- Keeps the last 20 drift_log entries (so you can trace how relationships evolved)
```javascript
async function applyPairwiseDrifts(drifts, policy, conversationId) {
  for (const { agent_a, agent_b, drift, reason } of drifts) {
    const [a, b] = [agent_a, agent_b].sort();  // alphabetical sort
    const clamped = clamp(drift, -0.03, 0.03);  // update affinity, append to drift_log
    await sb.from('ops_agent_relationships')
      .update({
        affinity: clamp(currentAffinity + clamped, 0.10, 0.95),
        drift_log: [...recentLog.slice(-19), { drift: clamped, reason, conversationId, at: new Date() }],
      })
      .eq('agent_a', a).eq('agent_b', b);
  }
}
```

### How Does Affinity Affect the System?

- **Speaker selection:** Agents with higher affinity are more likely to respond to each other
- **Conflict resolution:** Low-affinity pairs get automatically paired for conflict_resolution conversations
- **Mentor pairing:** High affinity + experience gap â†’ mentoring conversations
- **Conversation tone:** The system adjusts the prompt's interaction type based on affinity (supportive/neutral/critical/challenge)
```javascript
// interaction type shifts with affinity
const tension = 1 - affinity;
if (tension > 0.6) {
  // high tension â†’ 20% chance of direct challenge interactionType
  interactionType = Math.random() < 0.2 ? 'challenge' : 'critical';
} else if (tension < 0.3) {
  // low tension â†’ 40% chance of supportive interactionType
  interactionType = Math.random() < 0.4 ? 'supportive' : 'agreement';
}
```

## Chapter 5: Letting Them Propose Ideas â€” The Initiative System

The system ran for a week. Agents completed every task assigned to them. But they never once said, "I think we should do X."

**Beginner tip:** What's an Initiative? In the previous chapters, agents work "reactively" â€” a trigger fires, then they act. Initiative is letting agents proactively say "I think we should do X." Like in a company: junior employees wait for assignments, senior employees propose plans on their own.

### Why Initiative Doesn't Live in Heartbeat

Heartbeat runs every 5 minutes on Vercel serverless. Running LLM calls for proposal generation there? No good:
- Vercel function timeouts are strict (10-30 seconds)
- LLM calls are unpredictable â€” sometimes 2 seconds, sometimes 20
- Heartbeat needs to be reliable. One LLM call timing out shouldn't take the whole thing down.

The fix: Heartbeat only "enqueues" (lightweight rules), VPS worker does "generation" (heavy LLM work).

Heartbeat identifies "this agent is due for an initiative" â†’ writes to ops_initiative_queue â†’ VPS worker consumes the queue â†’ Haiku model generates proposals (cheap + fast) â†’ POST /api/ops/proposals (goes through full proposal-service gates)

### Enqueue Conditions

Not every agent gets to propose initiatives every time. Conditions:
```javascript
async function maybeQueueInitiative(sb, agentId) {
  // Cooldown: max 1 per 4 hours
  // Prerequisites: >= 5 high-confidence memories + has outcome lessons
  // i.e., the agent needs enough "accumulated experience" to make valuable suggestions
}
```

**Why require >= 5 high-confidence memories?** An agent without enough experience will propose generic, surface-level ideas. Let it build up experience before speaking up.

### Conversations Generating Tasks

Another initiative source: action items from conversations.

Not all conversation formats qualify â€” only standup, war_room, and brainstorm (the "formal" formats). Ideas from watercooler chats shouldn't automatically become tasks.

This also piggybacks on the memory distillation LLM call â€” zero additional cost:
```json
{
  "memories": [...],
  "pairwise_drift": [...],
  "action_items": [
    {
      "title": "Research competitor pricing strategies",
      "agent_id": "brain",
      "step_kind": "analyze"
    }
  ]
}
```

Max 3 action items per day convert to missions.

**Tip:** Every step of the initiative process goes through proposal-service's full gates â€” quota checks, auto-approve, cap gates, all of it. Agents "proposing their own work" doesn't mean they bypass safety mechanisms.

## Chapter 6: Giving Them Personality â€” Voice Evolution

6 agents have been chatting for a month, and they still talk exactly the same way as day one. But if an agent has accumulated tons of experience with "tweet engagement," its speaking style should reflect that.

**Beginner tip:** What's Voice Evolution? When someone works at a company long enough, the way they talk changes â€” the person who does lots of data analysis naturally starts leading with numbers, the person who handles customer complaints becomes more patient. Agents should work the same way: the experience they accumulate should be reflected in how they speak.

### Deriving Personality from Memory

My first instinct was to build a "personality evolution" table â€” too heavy. The final approach: derive personality dynamically from the existing memory table, no new tables needed. Instead of storing a separate "personality score," the system checks what memories the agent has before each conversation and calculates how its personality should be adjusted on the fly.
```javascript
// lib/ops/voice-evolution.ts
async function deriveVoiceModifiers(sb, agentId) {
  // aggregate this agent's memory distribution
  const stats = await aggregateMemoryStats(sb, agentId);
  const modifiers = [];

  // rule-driven (not LLM)
  if (stats.lesson_count > 10 && stats.tags.includes('engagement')) {
    modifiers.push('Reference what works in engagement when relevant');
  }
  if (stats.pattern_count > 5 && stats.top_tag === 'content') {
    modifiers.push("You've developed expertise in content strategy");
  }
  if (stats.strategy_count > 8) {
    modifiers.push('You think strategically about long-term plans');
  }

  return modifiers.slice(0, 3);  // max 3
}
```

**Why rule-driven instead of LLM?**
- **Deterministic:** Rules produce predictable results. No LLM hallucination causing sudden personality shifts.
- **Cost:** $0. No additional LLM calls.
- **Debuggable:** When a rule misfires, it's easy to track down.

### Injection Method

Modifiers get injected into the agent's system prompt before a conversation starts:
```javascript
async function buildAgentPrompt(agentId, baseVoice) {
  const modifiers = await deriveVoiceModifiers(sb, agentId);
  let prompt = baseVoice.systemDirective;  // base voice
  if (modifiers.length > 0) {
    prompt += '\\n\\nPersonality evolution:\\n';
    prompt += modifiers.map(m => `- ${m}`).join('\\n');
  }
  return prompt;
}
```

The effect: say your social media agent has accumulated 15 lessons about tweet engagement. Its prompt now includes "Reference what works in engagement when relevant" â€” and it'll naturally bring up engagement strategies in conversations.

Within the same conversation, each agent's voice modifiers are derived once and cached â€” no re-querying every turn.

## Chapter 7: Making It Look Cool â€” The Frontend

Your backend can be humming perfectly, but if nobody can see it,