---
id: SOURCE-20260101-657
platform: youtube
format: lecture
cadence: evergreen
value_modality: audio_primary
signal_tier: tactical
status: raw
chain: null
topics:
  - "limits"
  - "llms"
creator: "Caleb Writes Code"
guest: null
title: "Limits of LLMs"
url: "https://www.youtube.com/watch?v=_pY-JXtkW6g"
date_published: 2026-01-01
date_processed: 2026-02-22
date_integrated: null
processing_function: transcribe_youtube
integrated_into: []
duration: "8m 15s"
has_transcript: no
synopsis: "Limits of LLMs by Caleb Writes Code. A lecture covering limits, llms."
key_insights: []
visual_notes: null
teleology: contextualize
notebooklm_category: ai-engineering
aliases:
  - "Limits of LLMs"
---

# Limits of LLMs

**Channel**: Caleb Writes Code
**Published**: 2026-01-01
**Duration**: 8m 15s
**URL**: https://www.youtube.com/watch?v=_pY-JXtkW6g

## Description (no transcript available)

Do LLMs have inherent flaws that actually limit us from performing specific tasks? Should we try to invest in other AI architectures than LLMs like world models that actually try to understand physics and truths in a different way? Are we investing too much into LLMs and are they over-invested in comparison to other methods that we should really be considering?

#ai #llm #investing #technology #artificialintelligence 

Chapters
00:00 Intro
00:52 Brief History
01:37 Perceptron vs LLM
02:31 Flaws
04:37 Intelligence
05:05 Benchmarks
06:08 Implications
07:45 Conclusion
