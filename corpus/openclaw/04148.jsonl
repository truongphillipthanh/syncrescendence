{"atom_id": "ATOM-SOURCE-20260216-010-0001", "source_id": "SOURCE-20260216-010", "category": "claim", "content": "Running OpenClaw with a single frontier model for all tasks is the primary reason users incur high costs.", "line_start": 17, "line_end": 18, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0002", "source_id": "SOURCE-20260216-010", "category": "claim", "content": "OpenClaw's default behavior of sending all tasks, including heartbeats, sub-agent tasks, simple calendar lookups, and web searches, to the primary model leads to excessive costs.", "line_start": 20, "line_end": 25, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0003", "source_id": "SOURCE-20260216-010", "category": "claim", "content": "OpenClaw's architecture compounds costs through context accumulation, system prompt re-injection, tool output storage, heartbeat overhead, and cron job overhead.", "line_start": 28, "line_end": 40, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0004", "source_id": "SOURCE-20260216-010", "category": "prediction", "content": "A well-configured multi-model setup can reduce monthly API costs by up to 90% while maintaining or improving output quality.", "line_start": 41, "line_end": 42, "chaperone": {"context_type": "speculation", "argument_role": "claim", "tension_vector": [0.3, 0.7, 0.1, 0.6, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0005", "source_id": "SOURCE-20260216-010", "category": "claim", "content": "Using a Claude Max Plan with OpenClaw is against Anthropic's terms of service, and users have reported being banned for it.", "line_start": 45, "line_end": 47, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0006", "source_id": "SOURCE-20260216-010", "category": "praxis_hook", "content": "To implement custom model routing in OpenClaw, define a `RouterSkill` with a `rules` dictionary mapping regex patterns in prompts to specific LLM models, then enable the skill.", "line_start": 52, "line_end": 69, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.2, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0007", "source_id": "SOURCE-20260216-010", "category": "praxis_hook", "content": "To optimize model routing in OpenClaw, match model capability to task complexity using OpenClaw's per-function model assignment.", "line_start": 63, "line_end": 64, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.7, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0008", "source_id": "SOURCE-20260216-010", "category": "framework", "content": "A simple framework for optimizing OpenClaw costs involves categorizing tasks into four tiers: Heartbeat/Status (e.g., Gemini Flash-Lite), Simple Queries (e.g., Haiku 4.5), Standard Work (e.g., Sonnet 4.5), and Complex Reasoning (e.g., Opus 4.6), and assigning models based on their cost profile and capability.", "line_start": 66, "line_end": 69, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.4, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0009", "source_id": "SOURCE-20260216-010", "category": "framework", "content": "OpenRouter's built-in routing uses prompt analysis to automatically select models based on auto-detected tasks (e.g., 'routine', 'coding'), allowing configuration of specific models and maximum costs per task, with a fallback model.", "line_start": 75, "line_end": 89, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.3, 0.1, 0.1, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0010", "source_id": "SOURCE-20260216-010", "category": "concept", "content": "ClawRouter is an OpenClaw-native intermediary layer that analyzes incoming queries locally using a lightweight classifier based on weighted scoring dimensions (e.g., query length, code presence, reasoning markers, multi-step intent, tool usage signals) to classify requests into complexity tiers.", "line_start": 93, "line_end": 103, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.1, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0011", "source_id": "SOURCE-20260216-010", "category": "framework", "content": "ClawRouter classifies requests into complexity tiers: 'Simple' for cheap/fast models, 'Medium' for mid-tier, 'Complex' for strong models, and 'Heavy reasoning/agentic/multi-step' for frontier models, routing to the cheapest capable model for each level.", "line_start": 103, "line_end": 109, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.1, 0.1, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0012", "source_id": "SOURCE-20260216-010", "category": "praxis_hook", "content": "ClawRouter supports multiple profiles: Auto (balanced), Eco (max savings, up to 95â€“100% on simple queries), Premium (best quality), and Free (zero-cost models).", "line_start": 110, "line_end": 112, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.5, 0.4, 0.1, 0.1, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0013", "source_id": "SOURCE-20260216-010", "category": "concept", "content": "Prompt caching allows LLM providers to remember static parts of a prompt (like OpenClaw's SOUL.md, AGENTS.md, MEMORY.md, and system instructions) and reuse them across calls, significantly reducing costs for subsequent requests within the cache's Time-To-Live (TTL).", "line_start": 116, "line_end": 124, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260216-010-0014", "source_id": "SOURCE-20260216-010", "category": "praxis_hook", "content": "To enable prompt caching for an LLM in OpenClaw, configure `cacheRetention` (e.g., 'long'), `cacheSystemPrompts: true`, and `cacheThresholdTokens` (e.g., 2048) in the model's configuration.", "line_start": 125, "line_end": 131, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.2, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}
