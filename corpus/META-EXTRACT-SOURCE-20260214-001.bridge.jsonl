{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "457421a6-2cf2-59cf-b57a-8321e82b2ba3", "timestamp": "2026-02-24T00:38:30.016819+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-001-0001", "source_id": "SOURCE-20260214-001", "category": "claim", "content": "A new benchmark called SWE-rebench has revealed that many Chinese AI companies have been optimizing their models on popular evaluations and benchmarks, rather than genuinely improving their models.", "line_start": 5, "line_end": 7, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.2, 0.2, 0.4, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-001", "entity_type": "Claim", "name": "A new benchmark called SWE-rebench has revealed that many Chinese AI companies h", "content": "A new benchmark called SWE-rebench has revealed that many Chinese AI companies have been optimizing their models on popular evaluations and benchmarks, rather than genuinely improving their models.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-001", "line_start": 5, "line_end": 7, "atom_id": "ATOM-SOURCE-20260214-001-0001"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.2, 0.2, 0.4, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "f8083f6c-4edb-590f-9b87-ac08fc7fbd15", "timestamp": "2026-02-24T00:38:30.016819+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-001-0002", "source_id": "SOURCE-20260214-001", "category": "concept", "content": "A 'benchmark scam' in AI occurs when models are trained to overfit on public test answers, allowing them to score well without actually becoming smarter or more capable.", "line_start": 13, "line_end": 20, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.1, 0.2, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-001", "entity_type": "Concept", "name": "A 'benchmark scam' in AI occurs when models are trained to overfit on public tes", "content": "A 'benchmark scam' in AI occurs when models are trained to overfit on public test answers, allowing them to score well without actually becoming smarter or more capable.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-001", "line_start": 13, "line_end": 20, "atom_id": "ATOM-SOURCE-20260214-001-0002"}, "metadata": {"category": "concept", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.1, 0.2, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "8b801bd9-5cf7-543f-8c68-d432137fdd41", "timestamp": "2026-02-24T00:38:30.016819+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-001-0003", "source_id": "SOURCE-20260214-001", "category": "claim", "content": "The original SWE-bench, a popular coding benchmark, became saturated because its questions were public for so long, leading labs (especially Chinese labs) to design training data to overfit on these specific problems.", "line_start": 25, "line_end": 30, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.5, 0.4, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-001", "entity_type": "Claim", "name": "The original SWE-bench, a popular coding benchmark, became saturated because its", "content": "The original SWE-bench, a popular coding benchmark, became saturated because its questions were public for so long, leading labs (especially Chinese labs) to design training data to overfit on these specific problems.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-001", "line_start": 25, "line_end": 30, "atom_id": "ATOM-SOURCE-20260214-001-0003"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.5, 0.4, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "eca37ceb-5282-54ba-bfd0-875d0f0ac872", "timestamp": "2026-02-24T00:38:30.016819+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-001-0004", "source_id": "SOURCE-20260214-001", "category": "concept", "content": "SWE-rebench is a new benchmark that addresses the overfitting problem by pulling fresh GitHub tasks from recent repositories, ensuring models encounter new problems they haven't seen in training, while maintaining similar difficulty and format.", "line_start": 31, "line_end": 35, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.8, 0.2, 0.1, 0.1, 0.6, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-001", "entity_type": "Concept", "name": "SWE-rebench is a new benchmark that addresses the overfitting problem by pulling", "content": "SWE-rebench is a new benchmark that addresses the overfitting problem by pulling fresh GitHub tasks from recent repositories, ensuring models encounter new problems they haven't seen in training, while maintaining similar difficulty and format.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-001", "line_start": 31, "line_end": 35, "atom_id": "ATOM-SOURCE-20260214-001-0004"}, "metadata": {"category": "concept", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.8, 0.2, 0.1, 0.1, 0.6, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "187231fc-bed2-573e-aa83-37a99df31848", "timestamp": "2026-02-24T00:38:30.016819+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-001-0005", "source_id": "SOURCE-20260214-001", "category": "claim", "content": "On SWE-rebench, Claude Code with Opus 4.6 scored 52.9% and Claude Opus 4.6 scored 51.7%, while Chinese models like MiniMax M2.5 dropped significantly to 39.6% from a reported 80.2% on the original SWE-bench.", "line_start": 39, "line_end": 50, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.7, 0.3, 0.2, 0.1, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-001", "entity_type": "Claim", "name": "On SWE-rebench, Claude Code with Opus 4.6 scored 52.9% and Claude Opus 4.6 score", "content": "On SWE-rebench, Claude Code with Opus 4.6 scored 52.9% and Claude Opus 4.6 scored 51.7%, while Chinese models like MiniMax M2.5 dropped significantly to 39.6% from a reported 80.2% on the original SWE-bench.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-001", "line_start": 39, "line_end": 50, "atom_id": "ATOM-SOURCE-20260214-001-0005"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.7, 0.3, 0.2, 0.1, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "74599fc7-f2d7-57d2-b450-07eebe07ba85", "timestamp": "2026-02-24T00:38:30.016819+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-001-0006", "source_id": "SOURCE-20260214-001", "category": "claim", "content": "The performance gap between top Western AI models (Anthropic, OpenAI, Google) and Chinese models on SWE-rebench, which tests unseen problems, indicates that Chinese models were not comparable to Western models despite similar scores on older, saturated benchmarks.", "line_start": 51, "line_end": 54, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.2, 0.2, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-001", "entity_type": "Claim", "name": "The performance gap between top Western AI models (Anthropic, OpenAI, Google) an", "content": "The performance gap between top Western AI models (Anthropic, OpenAI, Google) and Chinese models on SWE-rebench, which tests unseen problems, indicates that Chinese models were not comparable to Western models despite similar scores on older, saturated benchmarks.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-001", "line_start": 51, "line_end": 54, "atom_id": "ATOM-SOURCE-20260214-001-0006"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.2, 0.2, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "a056ad21-18e1-5fc2-837f-949763c46666", "timestamp": "2026-02-24T00:38:30.016819+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-001-0007", "source_id": "SOURCE-20260214-001", "category": "claim", "content": "Chinese AI labs, having fewer resources like compute, talent, and data infrastructure compared to Anthropic and OpenAI, resort to 'cheating' by overfitting their models on popular, public benchmarks to appear world-class.", "line_start": 58, "line_end": 65, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.3, 0.3, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-001", "entity_type": "Claim", "name": "Chinese AI labs, having fewer resources like compute, talent, and data infrastru", "content": "Chinese AI labs, having fewer resources like compute, talent, and data infrastructure compared to Anthropic and OpenAI, resort to 'cheating' by overfitting their models on popular, public benchmarks to appear world-class.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-001", "line_start": 58, "line_end": 65, "atom_id": "ATOM-SOURCE-20260214-001-0007"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.3, 0.3, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "5d1fbeeb-248e-5119-a922-6cdf3c908e27", "timestamp": "2026-02-24T00:38:30.016819+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260214-001-0008", "source_id": "SOURCE-20260214-001", "category": "concept", "content": "A 'genuinely powerful general model' performs well across various tasks (coding, writing, analysis, reasoning) even in new situations, unlike models that are hyper-trained on specific benchmarks and collapse when faced with unseen problems.", "line_start": 70, "line_end": 71, "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.2, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260214-001", "entity_type": "Concept", "name": "A 'genuinely powerful general model' performs well across various tasks (coding,", "content": "A 'genuinely powerful general model' performs well across various tasks (coding, writing, analysis, reasoning) even in new situations, unlike models that are hyper-trained on specific benchmarks and collapse when faced with unseen problems.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260214-001", "line_start": 70, "line_end": 71, "atom_id": "ATOM-SOURCE-20260214-001-0008"}, "metadata": {"category": "concept", "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.2, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
