# X Thread: Chinese AI Models Evaluation
**Post 1** — 8:58 AM · Feb 18, 2026
Some notes about the slew of Chinese models coming out (Kimi K2.5, MiniMax, GLM-5 etc) claiming to "match Sonnet / Opus / God in evals at 1/10th of the price"
---
**Post 2** — Feb 18
By far our biggest cost at Lindy is inference, so believe me when I say we've looked at these models very closely (and continue doing so). Their actually delivering on their claims would make a material difference to the business.
---
**Post 3** — Feb 18
But every time we've evaluated them, we've found the same thing: that their real life performance, for agentic behavior, and outside of coding use cases, falls extremely short of what they show on evals.
---
**Post 4** — Feb 18
I think the industry consensus is right: these Chinese labs are:
1/ distilling frontier models (duh), which leads to much more "shallow" intelligence
2/ training for evals
3/ potentially stealing weights (I do believe at least 4o's weights got exfiltrated)
---
**Post 5** — Feb 18
Not saying these models will always be bad, or that these labs are completely incompetent. They're doing a fine job. But it's delusional to think they're actually at Sonnet / Opus level — they're still at least one generation behind. Take the evals with a huge grain of salt.