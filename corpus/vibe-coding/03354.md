# Extraction: SOURCE-20260201-008

**Source**: `SOURCE-20260201-x-thread-nbaschez-single_biggest_improvement_to_your.md`
**Atoms extracted**: 4
**Categories**: framework, praxis_hook

---

## Framework (2)

### ATOM-SOURCE-20260201-008-0002
**Lines**: 29-44
**Context**: method / claim
**Tension**: novelty=0.80, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.20, actionability=0.90, epistemic_stability=0.80

> A 'Bug Fixes: Prove It Pattern' for AI agents involves: 1. Spawning a subagent to write a test that reproduces the issue, confirming reproduction. 2. Implementing the fix. 3. Confirming the test now passes, proving the fix works. If a test is not feasible, document the reason.

### ATOM-SOURCE-20260201-008-0003
**Lines**: 33-38
**Context**: method / claim
**Tension**: novelty=0.60, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.20, actionability=0.80, epistemic_stability=0.70

> The 'Bug Fixes: Prove It Pattern' suggests a test level hierarchy for bug reproduction: 1. Unit test for pure logic/isolated functions. 2. Integration test for component interactions/API boundaries. 3. UX spec test for full user flows/browser-dependent behavior.

## Praxis Hook (2)

### ATOM-SOURCE-20260201-008-0001
**Lines**: 5-8
**Context**: method / claim
**Tension**: novelty=0.70, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.20, actionability=0.90, epistemic_stability=0.70

> When an AI agent reports a bug, instruct it to first write a test that reproduces the bug, and only then attempt to fix the bug, proving the fix with a passing test.

### ATOM-SOURCE-20260201-008-0004
**Lines**: 50-51
**Context**: method / claim
**Tension**: novelty=0.50, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.20, actionability=0.70, epistemic_stability=0.60

> For AI agents, it is beneficial to include detailed instructions for using a skill within the skill itself, in addition to mentioning the skill in a higher-level documentation like CLAUDE.md.
