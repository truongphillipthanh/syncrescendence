---
id: SOURCE-20251124-1013
platform: youtube
format: lecture
cadence: evergreen
value_modality: audio_primary
signal_tier: tactical
status: raw
chain: null
topics:
  - "graphic"
  - "cards"
creator: "Caleb Writes Code"
guest: null
title: "Graphic Cards for AI"
url: "https://www.youtube.com/watch?v=z83-C4iwwEk"
date_published: 2025-11-24
date_processed: 2026-02-22
date_integrated: null
processing_function: transcribe_youtube
integrated_into: []
duration: "11m 48s"
has_transcript: no
synopsis: "Graphic Cards for AI by Caleb Writes Code. A lecture covering graphic, cards."
key_insights: []
visual_notes: null
teleology: strategize
notebooklm_category: ai-engineering
aliases:
  - "Graphic Cards for AI"
---

# Graphic Cards for AI

**Channel**: Caleb Writes Code
**Published**: 2025-11-24
**Duration**: 11m 48s
**URL**: https://www.youtube.com/watch?v=z83-C4iwwEk

## Description (no transcript available)

Nvidia and AMD graphic cards are becoming a commodity where pricing has been a huge barrier for even consumers to look into running AI locally at home.
What are some metrics I should consider in purchasing graphic cards and what graphic cards is good? Do I only look at VRAM? or do the software stack matter as well like ROCm and CUDA?

Open models like Qwen, QwQ, GLM, Llama, Phi and more are all becoming a strong candidate to run these models at home.

#ai #graphiccard #technology 

Chapters
00:00 Intro
00:29 AI on CPU
02:23 AI on GPU
03:08 RTX 3060
04:10 RTX 3090
06:28 Rent GPU
07:20 RTX 4090
09:39 RTX 5090
10:13 AMD
11:18 Conclusion
