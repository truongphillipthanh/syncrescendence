# GPT-5.3 Codex and Opus 4.6: An Unexpected Breakthrough -- Everything Important Here

## Release Intervals are Accelerating

Release intervals are getting shorter. While Opus 4.5 was released on November 24th, the GPT-5.2 codex was released on December 18th, 2025. It's now early February, so release intervals are shrinking to 2-3 months. And within those 2 months, we're seeing *significant* performance improvements.

(Description: Comprehensive performance comparison table showing metrics across Opus 4.6, Opus 4.5, Sonnet 4.5, Gemini 3 Pro, and GPT-5.2 (all models). The table compares agentic terminal coding, agentic coding, agentic computer use, agentic tool use, scaled tool use, agentic search, multidisciplinary reasoning, agentic financial analysis, office tasks, novel problem-solving, graduate-level reasoning, visual reasoning, and multilingual Q&A. The Opus 4.6 column is highlighted in light pink/salmon color, indicating highest performance across multiple benchmarks.)

That alone would be significant enough, considering the evaluations, which will not be discussed in detail here.

## Anthropic's Feature Enhancements

Much more significant is what comes with the additional features in the releases. Anthropic, for example, increased the context window of Opus 4.6 to 1m, "sustains agentic tasks for longer, operates reliably in massive codebases, and catches its own mistakes." In addition, Anthropic extends the use case of its models to PowerPoint and improves their use in Excel - almost as an afterthought.

(ARG-AGI-2 is not even 12 months old)

(Description: Scatter plot titled "ARC-AGI-2" showing performance distribution across multiple AI models. Chart displays multiple data points for Opus 4.6, Opus 4.5, GPT-5.2 (medium and high), GPT-5.2 (high), Gemini 3, Gemini 3 Flash (high), and Gemini 3 Pro. The axes show "Score (%)" on the Y-axis (ranging from 20% to 80%) and "Cost per Task ($)" on the X-axis (ranging from $0.1 to $10k). Opus 4.6 variants are highlighted in red/salmon color at the top of the chart, indicating superior cost-performance frontier. Figure caption reads: "[Figure 2.9] ARC-AGI-1 (upper) and ARC-AGI-2 (lower) performance across a variety of effort levels. For ARC-AGI-1, Claude Opus 4.6 surpassed previous state of the art for medium, high, and max effort, with a best performance of 94% for high effort. For ARC-AGI-2, Claude Opus 4.6 achieved a new cost/performance frontier across a variety of effort levels, reaching a new SOTA of 69.17% at high effort.")

## The OpenAI Developments

Even more exciting, however, are the developments at OpenAI. The rivalry between Anthropic and OpenAI has famously intensified recently, and Anthropic's release of its anti-ad YouTube videos was a clear dig at OpenAI. Therefore, it was certainly no coincidence that both major AI companies released their latest models almost simultaneously on the same day.

But OpenAI also has a few tricks up its sleeve.

### OpenAI's Token Efficiency Breakthrough

OpenAI researcher Noam Brown sums it up perfectly:

> "GPT-5.3-Codex's much better token efficiency *AND* faster inference is the biggest story of this release."

(Description: Graph titled "SWE-Bench Pro (Public)" displaying accuracy performance curves for GPT-5.3-Codex, GPT-5.2-Codex, and GPT-5.2 against output tokens (X-axis ranging from 0 to 100,000). GPT-5.3-Codex (dark blue line) shows superior performance trajectory, reaching approximately 60% accuracy. GPT-5.2-Codex (light blue line) plateaus around 56% accuracy. The curve demonstrates the advantage of GPT-5.3-Codex's token efficiency.)

Reference: https://x.com/polynoamial/status/2019476535044948419

### The Self-Improving AI Model

But the real magic can only be found in OpenAI's blog post about the GPT-5.3 codex, presented in a completely unpretentious way:

> GPT-5.3-Codex is our first model that was instrumental in creating itself. The Codex team used early versions to debug its own training, manage its own deployment, and diagnose test results and evaluationsâ€”our team was blown away by how much Codex was able to accelerate its own development.

We recently heard a similar announcement from Anthropic, namely that Opus 4.5 delivered all the code for Claude Code, but OpenAI is going a step further.

## The Era of Self-Improving Models

It's certainly not presumptuous to say that we are entering the era of self-improving models and can expect that the intervals between releases will either decrease even further or - if the intervals remain the same - that even better models will be released within just a few months.

From here on out, it will only get faster - and better.