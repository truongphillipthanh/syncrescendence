---
id: SOURCE-20260112-470
platform: youtube
format: lecture
cadence: evergreen
value_modality: audio_primary
signal_tier: strategic
status: raw
chain: null
topics:
  - "nvidia"
  - "tidar"
  - "think"
  - "diffusion"
  - "talk"
creator: "Emergent Behaviors"
guest: null
title: "NVIDIA: TiDAR: Think in Diffusion, Talk in Autoregression"
url: "https://www.youtube.com/watch?v=Y-u-V5v9RiE"
date_published: 2026-01-12
date_processed: 2026-02-22
date_integrated: null
processing_function: transcribe_youtube
integrated_into: []
duration: "3m 55s"
has_transcript: no
synopsis: "NVIDIA: TiDAR: Think in Diffusion, Talk in Autoregression by Emergent Behaviors. A lecture covering nvidia, tidar, think."
key_insights: []
visual_notes: null
teleology: implement
notebooklm_category: ai-engineering
aliases:
  - "NVIDIA: TiDAR: Think in"
  - "NVIDIA: TiDAR: Think in Diffusion, Talk"
---

# NVIDIA: TiDAR: Think in Diffusion, Talk in Autoregression

**Channel**: Emergent Behaviors
**Published**: 2026-01-12
**Duration**: 3m 55s
**URL**: https://www.youtube.com/watch?v=Y-u-V5v9RiE

## Description (no transcript available)

https://www.emergent-behaviors.com/tidar-think-in-diffusion-talk-in-autoregression/

The trade-off between speed and quality has always been the "holy grail" problem in Large Language Models (LLMs). While Standard Autoregression (AR) gives us high quality, itâ€™s slow. Pure Diffusion is fast, but often lacks coherence. Enter TiDAR (Think in Diffusion, Talk in Autoregression).

In this video, we break down how NVIDIAâ€™s latest hybrid approach utilizes the best of both worlds to maximize GPU efficiency on the H100 hardware. Weâ€™ll explore:

    ğŸï¸ Why LLM decoding is currently "memory bound."

    ğŸ§  How TiDAR drafts in parallel and verifies sequentially.

    ğŸ“ˆ Why TiDAR models (1.5B & 8B) are crushing the speed-to-quality curve.

Whether you're an AI researcher, a developer, or just a tech enthusiast, TiDAR is a game-changer you need to know about! ğŸ’¡

ğŸ“Œ Timestamps:
0:00 - Introduction to TiDAR ğŸŒŸ
0:19 - The Problem: LLMs are Memory Bound ğŸ§ 
0:38 - Efficient Token Processing & Latency ğŸ¢ 
0:55 - The Failed Contenders (AR vs. Diffusion) ğŸ¢ğŸ‡ 
1:13 - Enter TiDAR: The Hybrid Approach ğŸ§¬ 
1:30 - The Single Forward Pass Machine âš™ï¸ 
1:44 - Under the Hood: Hybrid Attention Mask ğŸ­ 
1:58 - The Full Masking Strategy ğŸ‘º 
2:13 - Self-Speculative Generation Cycle ğŸ”„ 
2:27 - Efficiency: No Wasted Compute â™»ï¸ 
2:42 - Speed Results: 1.5B vs 8B Models âš¡
2:57 - Quality Results: Can you tell the difference? ğŸ§
3:14 - Breaking the Pareto Frontier ğŸ“‰ 
3:28 - Why This Matters for Deployment ğŸš€ 
3:41 - Final Conclusion ğŸ¬

TiDAR: Think in Diffusion, Talk in Autoregression
https://arxiv.org/pdf/2511.08923
Jingyu Liu 1, Xin Dong 1, Zhifan Ye 2, Rishabh Mehta, Yonggan Fu, Vartika Singh, Jan Kautz, Ce Zhang1, Pavlo Molchanov
NVIDIA

#NVIDIA #TiDAR #LLM #ArtificialIntelligence #MachineLearning #GPU #H100 #DeepLearning #TechExplained #airesearch
