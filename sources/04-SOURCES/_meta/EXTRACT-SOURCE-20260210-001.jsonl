{"atom_id": "ATOM-SOURCE-20260210-001-0001", "source_id": "SOURCE-20260210-001", "category": "claim", "content": "A system prompt is a critical part of an AI agent's harness, providing baseline instructions that are present in every call to the model, defining core behaviors, strategies, and tone.", "line_start": 9, "line_end": 12, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0002", "source_id": "SOURCE-20260210-001", "category": "claim", "content": "System prompts matter significantly more than commonly assumed in determining an AI agent's effectiveness.", "line_start": 19, "line_end": 20, "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.3, 0.4, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0003", "source_id": "SOURCE-20260210-001", "category": "claim", "content": "While the AI model sets the theoretical performance ceiling for an agent, the system prompt dictates whether that peak performance is achieved.", "line_start": 20, "line_end": 21, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.6, 0.1, 0.2, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0004", "source_id": "SOURCE-20260210-001", "category": "praxis_hook", "content": "To analyze system prompt effectiveness, one can obtain and analyze system prompts from various agents, semantically cluster them, compare their instructions, swap prompts between agents, and observe behavioral changes.", "line_start": 22, "line_end": 25, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.5, 0.1, 0.2, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0005", "source_id": "SOURCE-20260210-001", "category": "claim", "content": "System prompts for CLI coding agents like Claude Code, Cursor, Gemini CLI, Codex CLI, OpenHands, and Kimi CLI, despite performing similar basic functions, exhibit significant variety in their structure and content.", "line_start": 29, "line_end": 33, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0006", "source_id": "SOURCE-20260210-001", "category": "claim", "content": "System prompts for AI coding agents consistently instruct models to add comments sparingly and avoid conversational comments, reflecting a common problem in existing codebases.", "line_start": 36, "line_end": 44, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0007", "source_id": "SOURCE-20260210-001", "category": "claim", "content": "AI models like Opus 4.5 have been observed to reason within code comments if their 'thinking' process is disabled, indicating an inherent tendency that system prompts must counteract.", "line_start": 44, "line_end": 46, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.3, 0.5, 0.2, 0.3, 0.1, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0008", "source_id": "SOURCE-20260210-001", "category": "claim", "content": "System prompts for AI coding agents frequently emphasize the use of parallel tool calls, overriding the models' likely serial reasoning patterns developed during training, to improve user experience speed.", "line_start": 47, "line_end": 57, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0009", "source_id": "SOURCE-20260210-001", "category": "concept", "content": "The concept of 'fighting the weights' describes how system prompts are used to mitigate inherent quirks or biases of AI models, acquired during training, to enhance user experience in agentic applications.", "line_start": 58, "line_end": 61, "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.1, 0.2, 0.3, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0010", "source_id": "SOURCE-20260210-001", "category": "claim", "content": "While many system prompt specifications are shared across different AI coding agents, their differences significantly impact application behavior.", "line_start": 62, "line_end": 64, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.6, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0011", "source_id": "SOURCE-20260210-001", "category": "praxis_hook", "content": "OpenCode allows users to customize system prompts, enabling experimentation with different instructions (e.g., from Kimi, Gemini, Codex) to measure their impact on agent behavior.", "line_start": 68, "line_end": 70, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.5, 0.1, 0.1, 0.8, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0012", "source_id": "SOURCE-20260210-001", "category": "claim", "content": "Testing two OpenCode agents running Opus 4.5 with different system prompts (Claude Code vs. Codex) on SWE-Bench Pro questions revealed immediate and significant divergence in their workflows.", "line_start": 71, "line_end": 73, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.4, 0.1, 0.2, 0.3, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260210-001-0013", "source_id": "SOURCE-20260210-001", "category": "claim", "content": "The Codex prompt led to a methodical, documentation-first approach, while the Claude prompt resulted in an iterative, test-driven strategy for solving SWE-Bench problems.", "line_start": 80, "line_end": 82, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.4, 0.1, 0.2, 0.7, 0.6], "opposes_atom_ids": []}, "extensions": {}}
