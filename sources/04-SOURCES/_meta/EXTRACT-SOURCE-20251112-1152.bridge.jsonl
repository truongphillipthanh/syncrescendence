{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "ed6f3cf5-01b9-5a3e-987a-acf58652faf0", "timestamp": "2026-02-24T00:50:01.385431+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251112-1152-0001", "source_id": "SOURCE-20251112-1152", "category": "claim", "content": "Inclusion AI developed and released trillion-parameter models in eight months.", "line_start": 12, "line_end": 13, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251112-1152", "entity_type": "Claim", "name": "Inclusion AI developed and released trillion-parameter models in eight months.", "content": "Inclusion AI developed and released trillion-parameter models in eight months.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251112-1152", "line_start": 12, "line_end": 13, "atom_id": "ATOM-SOURCE-20251112-1152-0001"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "5c51779b-bc59-5d82-a2ea-52e7ac2b95a5", "timestamp": "2026-02-24T00:50:01.385431+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251112-1152-0002", "source_id": "SOURCE-20251112-1152", "category": "concept", "content": "Inclusion AI views AGI as a shared milestone for humanity.", "line_start": 13, "line_end": 14, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251112-1152", "entity_type": "Concept", "name": "Inclusion AI views AGI as a shared milestone for humanity.", "content": "Inclusion AI views AGI as a shared milestone for humanity.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251112-1152", "line_start": 13, "line_end": 14, "atom_id": "ATOM-SOURCE-20251112-1152-0002"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "1a671bb3-0b6f-586b-8f7c-9456c28264ff", "timestamp": "2026-02-24T00:50:01.385431+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251112-1152-0003", "source_id": "SOURCE-20251112-1152", "category": "claim", "content": "The 'DeepSeek moment' catalyzed a wave of research labs across China.", "line_start": 15, "line_end": 16, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.4, 0.6, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251112-1152", "entity_type": "Claim", "name": "The 'DeepSeek moment' catalyzed a wave of research labs across China.", "content": "The 'DeepSeek moment' catalyzed a wave of research labs across China.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251112-1152", "line_start": 15, "line_end": 16, "atom_id": "ATOM-SOURCE-20251112-1152-0003"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.4, 0.6, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "a7078667-afaf-5ac3-acd9-5594b9268ceb", "timestamp": "2026-02-24T00:50:01.385431+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251112-1152-0004", "source_id": "SOURCE-20251112-1152", "category": "praxis_hook", "content": "Inclusion AI utilized FP8 training optimizations, MoE architecture decisions, QK-norm for gradient stability, and innovative pipeline parallelism strategies in building their Ling, Ring, and Ming models.", "line_start": 18, "line_end": 20, "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.5, 0.5, 0.1, 0.2, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251112-1152", "entity_type": "PraxisHook", "name": "Inclusion AI utilized FP8 training optimizations, MoE architecture decisions, QK", "content": "Inclusion AI utilized FP8 training optimizations, MoE architecture decisions, QK-norm for gradient stability, and innovative pipeline parallelism strategies in building their Ling, Ring, and Ming models.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251112-1152", "line_start": 18, "line_end": 20, "atom_id": "ATOM-SOURCE-20251112-1152-0004"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.5, 0.5, 0.1, 0.2, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "e9b1a932-4daf-5dda-bd41-187b1dcde74a", "timestamp": "2026-02-24T00:50:01.385431+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251112-1152-0005", "source_id": "SOURCE-20251112-1152", "category": "concept", "content": "Richard Bian's vision for 'model as product' thinking treats foundation models like operating system kernels that enable entirely new categories of applications.", "line_start": 22, "line_end": 24, "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.2, 0.6, 0.7, 0.5], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251112-1152", "entity_type": "Concept", "name": "Richard Bian's vision for 'model as product' thinking treats foundation models l", "content": "Richard Bian's vision for 'model as product' thinking treats foundation models like operating system kernels that enable entirely new categories of applications.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251112-1152", "line_start": 22, "line_end": 24, "atom_id": "ATOM-SOURCE-20251112-1152-0005"}, "metadata": {"category": "concept", "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.2, 0.6, 0.7, 0.5], "opposes_atom_ids": []}, "extensions": {}}}
