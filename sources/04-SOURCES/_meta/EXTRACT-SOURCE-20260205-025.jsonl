{"atom_id": "ATOM-SOURCE-20260205-025-0001", "source_id": "SOURCE-20260205-025", "category": "framework", "content": "Agentic reasoning for LLMs can be organized into foundational agentic reasoning (planning, tool use, search), self-evolving agents (feedback, memory, adaptation), and multi-agent systems (coordination, knowledge sharing).", "line_start": 18, "line_end": 21, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0002", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "Multi-agent LLM systems fail 41-86.7% of the time in production, not due to edge cases or adversarial attacks, but in standard deployment across 7 state-of-the-art frameworks.", "line_start": 32, "line_end": 34, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.3, 0.2, 0.2, 0.7, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0003", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "Berkeley researchers analyzed 1,642 execution traces of multi-agent LLM systems and identified 14 unique failure modes, with most failures stemming from system design and coordination issues.", "line_start": 36, "line_end": 39, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.3, 0.2, 0.2, 0.7, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0004", "source_id": "SOURCE-20260205-025", "category": "framework", "content": "The survey distinguishes two approaches to agentic reasoning: in-context reasoning (scales test-time interaction without changing weights) and post-training (optimizes via reinforcement learning).", "line_start": 43, "line_end": 46, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0005", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "Agents achieving 60% pass@1 on benchmarks may only exhibit 25% consistency across multiple trials, indicating that benchmark performance does not equate to production reliability.", "line_start": 48, "line_end": 51, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.5, 0.3, 0.2, 0.3, 0.6, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0006", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "If a benchmark reports 90% accuracy for LLM agents, expect 70-80% in production when accounting for consistency and faults.", "line_start": 58, "line_end": 59, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.4, 0.4, 0.2, 0.3, 0.6, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0007", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "Simpler LLM agent architectures often outperform complex ones under realistic conditions because additional complexity introduces failure modes that outweigh the benefits.", "line_start": 61, "line_end": 63, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.5, 0.1, 0.2, 0.5, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0008", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "The 14 failure modes identified by Berkeley researchers for multi-agent LLM systems cluster into three categories: system design issues (~44% of failures), inter-agent misalignment (~32% of failures), and task verification failures (~24% of failures).", "line_start": 72, "line_end": 76, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.3, 0.2, 0.2, 0.7, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0009", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "Most failures in multi-agent LLM systems are not due to model limitations but rather coordination issues.", "line_start": 78, "line_end": 78, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.4, 0.4, 0.2, 0.2, 0.6, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0010", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "The 'open challenges' listed in the survey, such as personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for deployment, are not future problems but current, fundamental gaps.", "line_start": 82, "line_end": 86, "chaperone": {"context_type": "rebuttal", "argument_role": "claim", "tension_vector": [0.6, 0.3, 0.4, 0.3, 0.5, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0011", "source_id": "SOURCE-20260205-025", "category": "concept", "content": "'Long-horizon interaction' is a euphemism for agents losing coherence after a few steps.", "line_start": 88, "line_end": 88, "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.4, 0.3, 0.4, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0012", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "Techniques for agentic reasoning in LLMs that work on benchmarks often fail 41-86% of the time in production due to fundamental gaps in reliability and coordination.", "line_start": 91, "line_end": 93, "chaperone": {"context_type": "rebuttal", "argument_role": "claim", "tension_vector": [0.6, 0.3, 0.4, 0.3, 0.7, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0013", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "The gap between benchmark accuracy (60-90%) and production consistency (25-70%) or multi-agent failure rates (41-86.7%) for LLM agents is fundamental, not incremental.", "line_start": 102, "line_end": 105, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.5, 0.4, 0.2, 0.3, 0.6, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0014", "source_id": "SOURCE-20260205-025", "category": "claim", "content": "LLM agents are currently research projects, not production infrastructure.", "line_start": 107, "line_end": 107, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.5, 0.4, 0.2, 0.3, 0.6, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260205-025-0015", "source_id": "SOURCE-20260205-025", "category": "analogy", "content": "The survey provides a 'map' of agentic reasoning, while failure data provides the 'territory,' and they are not the same.", "line_start": 109, "line_end": 111, "chaperone": {"context_type": "anecdote", "argument_role": "claim", "tension_vector": [0.4, 0.5, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}
