{"atom_id": "ATOM-SOURCE-20260213-011-0001", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "The quality of search context feeding LLMs is a more fundamental bottleneck for AI agents than the choice of LLM itself.", "line_start": 15, "line_end": 16, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0002", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "An agent with poor search capabilities cannot reason effectively, regardless of the underlying LLM's power, leading to hallucinations, looping, and poor user outcomes.", "line_start": 18, "line_end": 21, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0003", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Brave's research indicates that a weaker open-weight model (Qwen3) combined with high-quality search context can outperform stronger models like ChatGPT, Perplexity, and Google AI Mode.", "line_start": 30, "line_end": 33, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.5, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0004", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "In a 1,500-query evaluation, Ask Brave (Qwen3 + LLM Context API) scored 4.66/5, surpassing ChatGPT (4.32), Google AI Mode (4.39), and Perplexity (4.01), with only Grok (4.71) scoring higher.", "line_start": 35, "line_end": 36, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.5, 0.1, 0.2, 0.3, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0005", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Context quality is a more significant differentiator than model capability for AI agents.", "line_start": 37, "line_end": 37, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0006", "source_id": "SOURCE-20260213-011", "category": "framework", "content": "Five search APIs (Brave, Tavily, Exa, Perplexity, Firecrawl) offer different philosophies for integrating web content into LLMs: Brave (own index, LLM-optimized chunks, privacy-first), Tavily (agent-native, search + extract + crawl), Exa (neural semantic search), Perplexity (Sonar API, answers with citations), Firecrawl (extract-first, scrape anything, structured).", "line_start": 41, "line_end": 43, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.5, 0.4, 0.2, 0.2, 0.6, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0007", "source_id": "SOURCE-20260213-011", "category": "concept", "content": "Brave's LLM Context API performs a full search on its independent index of 35+ billion pages, then extracts 'smart chunks' (clean text, structured data, JSON-LD, tables, code, forum discussions, YouTube captions) from pages in real-time, ranking them by relevance and compiling them into a token-efficient format for LLM consumption.", "line_start": 47, "line_end": 51, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.4, 0.3, 0.4, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0008", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Brave's LLM Context API has a p90 latency under 600ms for the full pipeline (search + extraction + ranking) with less than 130ms overhead on top of normal search.", "line_start": 53, "line_end": 53, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0009", "source_id": "SOURCE-20260213-011", "category": "concept", "content": "Tavily is a search API purpose-built for AI agents, offering four endpoints: Search (web discovery), Extract (content from URLs), Map (site structure), and Crawl (navigate and extract from entire sites).", "line_start": 65, "line_end": 67, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.4, 0.3, 0.4, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0010", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Perplexity's citation tokens are now free on standard Sonar and Sonar Pro, which reduces per-query costs.", "line_start": 70, "line_end": 72, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0011", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Perplexity's total cost per query is difficult to predict due to its dependence on model choice, context size, and token volume, unlike flat-rate APIs.", "line_start": 72, "line_end": 74, "chaperone": {"context_type": "consensus", "argument_role": "limitation", "tension_vector": [0.1, 0.7, 0.1, 0.6, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0012", "source_id": "SOURCE-20260213-011", "category": "concept", "content": "Exa (formerly Metaphor) uses neural embeddings trained on its own index of tens of billions of pages to enable semantic search by meaning rather than keywords.", "line_start": 77, "line_end": 78, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.4, 0.3, 0.4, 0.5], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0013", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Exa 2.0 offers three tiers: Exa Fast (sub-350ms p50), Exa Auto (balances latency and quality), and Exa Deep (agentic multi-search, ~3.5s p50, highest quality), with Exa Instant pushing latency below 200ms for real-time applications.", "line_start": 79, "line_end": 80, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0014", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Firecrawl specializes in extraction, starting with a URL to retrieve all content from a page, including handling JavaScript rendering, pagination, authentication, CAPTCHAs, and multi-page workflows automatically.", "line_start": 84, "line_end": 88, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.8, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0015", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Firecrawl is open source and self-hostable.", "line_start": 88, "line_end": 88, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0016", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Firecrawl reported 77.2% coverage and 0.638 F1 quality in its own comparison testing, compared to Exa's 69.2% coverage and 0.508 F1.", "line_start": 88, "line_end": 91, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.2, 0.5, 0.1, 0.3, 0.2, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0017", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Firecrawl's reported benchmarks are vendor-published and not independently audited.", "line_start": 91, "line_end": 92, "chaperone": {"context_type": "consensus", "argument_role": "limitation", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0018", "source_id": "SOURCE-20260213-011", "category": "concept", "content": "Firecrawl is an extraction tool used after a search engine finds relevant URLs, rather than a search engine itself.", "line_start": 94, "line_end": 95, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.6, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0019", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Firecrawl's pricing is flat and predictable at one credit per page, without depth multipliers or variable consumption.", "line_start": 95, "line_end": 97, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.6, 0.3, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0020", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "At 100K pages monthly, Firecrawl costs $83, which is significantly less than Tavily's $500-800 for equivalent extraction.", "line_start": 97, "line_end": 98, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.7, 0.1, 0.1, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0021", "source_id": "SOURCE-20260213-011", "category": "praxis_hook", "content": "The optimal strategy for agent builders is to route different query types to different APIs based on their strengths, rather than picking a single API.", "line_start": 119, "line_end": 121, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0022", "source_id": "SOURCE-20260213-011", "category": "concept", "content": "Brave's LLM Context API is not a wrapper around other search engines, not bundled with a forced LLM, and not just a search endpoint.", "line_start": 139, "line_end": 142, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0023", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Brave's blog argues that scrapers are legally risky (citing Google v. SerpAPI), can be arbitrarily shut off, and cannot offer true Zero Data Retention because queries pass through a third party.", "line_start": 139, "line_end": 141, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.3, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0024", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Brave is positioning itself as the safe infrastructure choice for enterprise AI due to being the only western independent search index at scale outside Big Tech.", "line_start": 141, "line_end": 142, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.3, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0025", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Brave's LLM Context API is a purpose-built pipeline that searches an independent 35B-page index, extracts smart chunks from results, ranks them for relevance, and delivers them in a token-budget-controlled format for any LLM, all under 600ms.", "line_start": 144, "line_end": 148, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0026", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "The search layer is becoming the primary differentiator for agent builders because LLMs are commoditizing and their performance is converging.", "line_start": 147, "line_end": 150, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.1, 0.3, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0027", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "An open-weight model with good search context outperformed frontier models with worse context in Brave's evaluation.", "line_start": 148, "line_end": 149, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.3, 0.1, 0.2, 0.1, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0028", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "The search API choice is now a product decision, not just an infrastructure one, due to its impact on LLM performance differentiation.", "line_start": 150, "line_end": 151, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.5, 0.4, 0.1, 0.3, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0029", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "The most effective agent stacks utilize a hybrid approach, routing different query types to different APIs.", "line_start": 154, "line_end": 155, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.6, 0.5, 0.1, 0.3, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0030", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Using a multi-API approach can reduce costs by 40-60% compared to using a single provider for all agent search needs.", "line_start": 158, "line_end": 159, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.5, 0.4, 0.1, 0.3, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0031", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Privacy and independence are now critical factors for enterprise AI teams, especially in regulated industries with sensitive query content.", "line_start": 162, "line_end": 164, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.6, 0.5, 0.1, 0.3, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0032", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Brave's ZDR policy and SOC 2 attestation are competitive advantages for enterprise AI in regulated industries.", "line_start": 164, "line_end": 165, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.5, 0.4, 0.1, 0.2, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0033", "source_id": "SOURCE-20260213-011", "category": "claim", "content": "Firecrawl's open-source self-hosting option provides full data sovereignty for teams requiring it.", "line_start": 165, "line_end": 166, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.4, 0.3, 0.1, 0.2, 0.5, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0034", "source_id": "SOURCE-20260213-011", "category": "praxis_hook", "content": "For agents running on OpenClaw, the Brave LLM Context API is the easiest path to high-quality web grounding, especially since Brave Search MCP server is already integrated.", "line_start": 169, "line_end": 171, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.4, 0.5, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260213-011-0035", "source_id": "SOURCE-20260213-011", "category": "praxis_hook", "content": "Combine Brave LLM Context API with Exa for research tasks and Firecrawl for deep extraction to create a more powerful agent stack.", "line_start": 171, "line_end": 171, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.6, 0.5, 0.1, 0.3, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
