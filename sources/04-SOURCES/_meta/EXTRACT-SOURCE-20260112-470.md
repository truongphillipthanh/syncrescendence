# Extraction: SOURCE-20260112-470

**Source**: `SOURCE-20260112-youtube-lecture-emergent_behaviors-nvidia_tidar_think_in_diffusion_talk_in_autoregression.md`
**Atoms extracted**: 7
**Categories**: claim, concept, praxis_hook

---

## Claim (4)

### ATOM-SOURCE-20260112-470-0002
**Lines**: 10-10
**Context**: consensus / claim
**Tension**: novelty=0.10, consensus_pressure=0.80, contradiction_load=0.10, speculation_risk=0.10, actionability=0.10, epistemic_stability=0.80

> Standard Autoregression (AR) in LLMs provides high quality but is slow.

### ATOM-SOURCE-20260112-470-0003
**Lines**: 10-10
**Context**: consensus / claim
**Tension**: novelty=0.10, consensus_pressure=0.80, contradiction_load=0.10, speculation_risk=0.10, actionability=0.10, epistemic_stability=0.80

> Pure Diffusion models are fast but often lack coherence.

### ATOM-SOURCE-20260112-470-0005
**Lines**: 17-17
**Context**: consensus / claim
**Tension**: novelty=0.20, consensus_pressure=0.70, contradiction_load=0.10, speculation_risk=0.10, actionability=0.20, epistemic_stability=0.80

> LLM decoding is currently "memory bound."

### ATOM-SOURCE-20260112-470-0007
**Lines**: 21-21
**Context**: consensus / claim
**Tension**: novelty=0.80, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.20, actionability=0.70, epistemic_stability=0.70

> TiDAR models (1.5B & 8B) are significantly improving the speed-to-quality curve for LLMs.

## Concept (2)

### ATOM-SOURCE-20260112-470-0001
**Lines**: 10-10
**Context**: consensus / claim
**Tension**: novelty=0.10, consensus_pressure=0.80, contradiction_load=0.10, speculation_risk=0.10, actionability=0.10, epistemic_stability=0.80

> The "holy grail" problem in Large Language Models (LLMs) is the trade-off between speed and quality.

### ATOM-SOURCE-20260112-470-0004
**Lines**: 10-10
**Context**: method / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.20, actionability=0.60, epistemic_stability=0.70

> TiDAR (Think in Diffusion, Talk in Autoregression) is a hybrid approach that combines the strengths of Diffusion and Autoregression to maximize GPU efficiency.

## Praxis Hook (1)

### ATOM-SOURCE-20260112-470-0006
**Lines**: 19-19
**Context**: method / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.20, actionability=0.70, epistemic_stability=0.70

> TiDAR drafts in parallel and verifies sequentially.
