---
url: https://x.com/ChrisPainterYup/status/2019534216405606623
author: "Chris Painter (@ChrisPainterYup)"
captured_date: 2026-02-05
id: SOURCE-20260205-019
original_filename: 20260205-x_thread-my_bio_says_i_work_on-chrispainteryup.md
status: triaged
platform: x
format: thread
creator: chrispainteryup
signal_tier: strategic
topics:
  - extended-thinking
  - api
  - product-development
  - benchmarks
  - cost-optimization
teleology: contextualize
notebooklm_category: philosophy-paradigm
aliases:
  - "20260205 x thread chrispainteryup my bio says i work on"
synopsis: "AGI Preparedness Thread My bio says I work on AGI preparedness, so I want to clarify: **We are not prepared.** Over the last year, dangerous capability evaluations have moved into a state where it's difficult to find any Q&A benchmark that models don't saturate."
key_insights:
  - "Broadly, it's becoming a stretch to rule out any threat model using Q&A benchmarks as a proxy."
  - "And what happens if we concede that it's difficult to "rule out" these risks?"
  - "Does society wait to take action until we can "rule them in" by showing they are end-to-end clearly realizable?"
---
## AGI Preparedness Thread

My bio says I work on AGI preparedness, so I want to clarify: **We are not prepared.**

Over the last year, dangerous capability evaluations have moved into a state where it's difficult to find any Q&A benchmark that models don't saturate. Work has had to shift toward measures that are either much more finger-to-the-wind (quick surveys of researchers about real-world use) or much more capital- and time-intensive (randomized controlled "uplift studies").

Broadly, it's becoming a stretch to rule out any threat model using Q&A benchmarks as a proxy. Everyone is experimenting with new methods for detecting when meaningful capability thresholds are crossed, but the water might boil before we can get the thermometer in. The situation is similar for agent benchmarks: our ability to measure capability is rapidly falling behind the pace of capability itself (look at the confidence intervals on METR's time-horizon measurements), although these haven't yet saturated.

And what happens if we concede that it's difficult to "rule out" these risks? Does society wait to take action until we can "rule them in" by showing they are end-to-end clearly realizable?

Furthermore, what would "taking action" even mean if we decide the risk is imminent and real? Every American developer faces the problem that if it unilaterally halts development, or even simply implements costly mitigations, it has reason to believe that a less-cautious competitor will not take the same actions and instead benefit. From a private company's perspective, it isn't clear that taking drastic action to mitigate risk unilaterally (like fully halting development of more advanced models) accomplishes anything productive unless there's a decent chance the government steps in or the action is near-universal. And even if the US government helps solve the collective action problem (if indeed it *is* a collective action problem) in the US, what about Chinese companies?

At minimum, I think developers need to keep collecting evidence about risky and destabilizing model properties (chem-bio, cyber, recursive self-improvement, sycophancy) and reporting this information publicly, so the rest of society can see what world we're heading into and can decide how it wants to react. The rest of society, and companies themselves, should also spend more effort thinking creatively about how to use technology to harden society against the risks AI might pose.

This is hard, and I don't know the right answers. My impression is that the companies developing AI don't know the right answers either. While it's possible for an individual, or a species, to not understand how an experience will affect them and yet "be prepared" for the experience in the sense of having built the tools and experience to ensure they'll respond effectively, I'm not sure that's the position we're in. I hope we land on better answers soon.

---

### Follow-up (Feb 6, 2026)

I think a well-designed international treaty on transformative AI development, if credibly verified and enforced, could be very good!