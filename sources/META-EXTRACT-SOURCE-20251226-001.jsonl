{"atom_id": "ATOM-SOURCE-20251226-001-0001", "source_id": "SOURCE-20251226-001", "category": "concept", "content": "The 'Scaling Paradox' describes the apparent contradiction between claims that AI scaling laws are hitting a wall and the observed acceleration of AI capabilities.", "line_start": 6, "line_end": 7, "chaperone": {"context_type": "hypothesis", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.2, 0.2, 0.1, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20251226-001-0002", "source_id": "SOURCE-20251226-001", "category": "claim", "content": "Diminishing returns on vanilla pre-training (one scaling vector) does not imply diminishing returns on the overall AI capability frontier.", "line_start": 7, "line_end": 9, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20251226-001-0003", "source_id": "SOURCE-20251226-001", "category": "framework", "content": "AI progress proceeds through multiple simultaneous vectors: test-time compute, architectural innovations, agent scaffolding, post-training improvements, and better recipes.", "line_start": 9, "line_end": 11, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.1, 0.1, 0.5, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20251226-001-0004", "source_id": "SOURCE-20251226-001", "category": "claim", "content": "The length of tasks that generalist AI agents can complete has been doubling approximately every seven months for the past six years, accelerating to every four months recently.", "line_start": 20, "line_end": 22, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.4, 0.6, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20251226-001-0005", "source_id": "SOURCE-20251226-001", "category": "claim", "content": "The ARC-AGI benchmark showed rapid progress, taking four years to go from 0% to 5% performance, then only months to reach near saturation, necessitating the release of ARC-AGI 2 and 3.", "line_start": 25, "line_end": 27, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.4, 0.6, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20251226-001-0006", "source_id": "SOURCE-20251226-001", "category": "framework", "content": "Multiple simultaneous research programs drive AI progress, including test-time compute (chain-of-thought, search, tool use), architectural innovations (MoE, state space models), agent scaffolding and tool integration, post-training (RLHF, DPO, synthetic data, self-play), and better training recipes.", "line_start": 30, "line_end": 35, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.1, 0.1, 0.5, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20251226-001-0007", "source_id": "SOURCE-20251226-001", "category": "claim", "content": "Sam Altman stated that GPT-4's capabilities were due to hundreds of small improvements, not a single 'secret sauce'.", "line_start": 38, "line_end": 39, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.3, 0.7, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20251226-001-0008", "source_id": "SOURCE-20251226-001", "category": "claim", "content": "The flattening of vanilla pre-training returns is often mistaken for a slowdown in overall AI progress, but the capability frontier is advanced by multiple simultaneous research programs.", "line_start": 42, "line_end": 43, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.6, 0.4, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}
