# Extraction: SOURCE-20251231-680

**Source**: `SOURCE-20251231-youtube-lecture-machine_learning_street_talk-autograd_changed_everything_not_transformers_dr_jeff_beck.md`
**Atoms extracted**: 7
**Categories**: claim, concept, praxis_hook, prediction

---

## Claim (4)

### ATOM-SOURCE-20251231-680-0001
**Lines**: 19-19
**Context**: speculation / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.20, speculation_risk=0.80, actionability=0.30, epistemic_stability=0.30

> The future of AI might look less like ChatGPT and more like the human brain.

### ATOM-SOURCE-20251231-680-0002
**Lines**: 23-26
**Context**: speculation / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.30, speculation_risk=0.70, actionability=0.20, epistemic_stability=0.40

> The human brain does not function like a giant prediction engine; instead, it operates like a scientist, continuously testing hypotheses about a world composed of objects interacting through forces, rather than pixels and tokens.

### ATOM-SOURCE-20251231-680-0004
**Lines**: 34-36
**Context**: hypothesis / claim
**Tension**: novelty=0.50, consensus_pressure=0.40, contradiction_load=0.30, speculation_risk=0.60, actionability=0.20, epistemic_stability=0.40

> Automatic differentiation was the true catalyst for the AI boom, transforming AI from a mathematical challenge into an engineering problem, but this shift may have obscured fundamental aspects of intelligence.

### ATOM-SOURCE-20251231-680-0006
**Lines**: 45-48
**Context**: hypothesis / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.40, speculation_risk=0.60, actionability=0.20, epistemic_stability=0.40

> Grounding AI in language, as is common with Large Language Models (LLMs), is fundamentally misguided because self-report is an unreliable data source in psychology; AI should instead be grounded in physics.

## Concept (1)

### ATOM-SOURCE-20251231-680-0003
**Lines**: 28-32
**Context**: consensus / claim
**Tension**: novelty=0.30, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.20, actionability=0.70, epistemic_stability=0.70

> The 'Bayesian Brain' concept posits that the brain essentially runs the scientific method on autopilot, performing optimal Bayesian inference when combining sensory information, which is supported by decades of behavioral experiments showing human efficiency in handling uncertainty.

## Praxis Hook (1)

### ATOM-SOURCE-20251231-680-0005
**Lines**: 38-43
**Context**: method / claim
**Tension**: novelty=0.80, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.70, actionability=0.60, epistemic_stability=0.30

> To address the 'Cat in the Warehouse Problem' (where current AI fails to generalize to unseen objects), AI models should be designed to 'know what they don't know,' be able to dynamically acquire new object models, and learn continuously, allowing them to question unfamiliar inputs rather than confidently making errors.

## Prediction (1)

### ATOM-SOURCE-20251231-680-0007
**Lines**: 50-53
**Context**: speculation / claim
**Tension**: novelty=0.80, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.90, actionability=0.40, epistemic_stability=0.30

> Future AI systems will likely be structured like video game engines, comprising thousands of small, modular object models that can be combined, swapped, and updated independently, offering greater efficiency, flexibility, and a closer resemblance to human thought processes, rather than relying on one massive neural network.
