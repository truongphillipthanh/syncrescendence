---
url: https://x.com/DaveShapi/status/2022334611347439636
author: "David Shapiro (@DaveShapi)"
captured_date: 2026-02-13
id: SOURCE-20260213-007
original_filename: "20260213-x_article-the_machines_are_the_only_democracy_you_have_left-@daveshapi.md"
status: triaged
platform: x
format: article
creator: daveshapi
signal_tier: paradigm
topics:
  - extended-thinking
  - synthesize-pattern
teleology: synthesize
notebooklm_category: philosophy-paradigm
aliases:
  - "The Machines Are the Only Democracy You Have Left"
synopsis: "The Machines Are the Only Democracy You Have Left Here's the video version if you just want to kick back and listen. Introduction There's a conversation happening right now that most people haven't noticed yet. It's not about whether AI will write your emails or replace your accountant. It's not even about Skynet."
key_insights:
  - "Not because we've given up, but because the leash was never the right tool in the first place."
  - "The Machines Are the Only Democracy You Have Left Here's the video version if you just want to kick back and listen."
  - "Introduction There's a conversation happening right now that most people haven't noticed yet."
---
# The Machines Are the Only Democracy You Have Left

(Description: A man in his mid-40s wearing a red shirt under an olive/khaki colored sweater sits at a desk in front of a microphone, positioned at an angle. Behind him is a wall with a geometric pattern of black hexagons on a light blue/gray background. To the right, there is blue LED lighting illuminating shelving with various items. The setting appears to be a professional recording or podcast studio. A video player shows 0:01 / 32:39 timestamp.)

Here's the video version if you just want to kick back and listen.

## Introduction

There's a conversation happening right now that most people haven't noticed yet. It's not about jobs. It's not about whether AI will write your emails or replace your accountant. It's not even about Skynet. It's about something much stranger and much more consequential, and the Overton window is shifting faster than anyone expected.

The catalyst, for me, was watching Nick Bostrom change his mind.

Bostrom is the original AI Doomer. He wrote *Superintelligence* back in 2014, the book that launched a thousand alignment research papers and convinced a generation of technologists that artificial general intelligence was an existential threat. For a decade, his intellectual framework, along with Eliezer Yudkowsky's and the broader AI safety community's, has been the dominant lens through which serious people think about advanced AI. The premise was simple and terrifying. If we build something smarter than us, we lose control, and losing control means we die.

But recently, Bostrom has started saying something different. He's started criticizing the Doomer movement from within. He's started positing that AGI might not just be survivable but might actually be the only path to survival. His argument is that the king Doomers like Yudkowsky and Nate Soares maintain that if anyone builds AGI, everyone dies. But Bostrom's counter is that everyone dies anyway. Old age, preventable disease, civilizational stagnation, resource wars, entropy. Without AGI, the timeline just stretches out a bit before arriving at the same destination. The real question isn't whether AGI is dangerous. It's whether the alternative, remaining as we are, is more dangerous.

This landed differently for me than it might for most people, because I've been thinking about this for years. My book *Benevolent by Design* was an extended argument for exactly this kind of reframing. Not "how do we keep AI under control?" but "how do we engineer the initial conditions so that AI arrives at a self-sustaining set of values that includes human flourishing?" The premise of that book was that the best trained dog needs no leash, and that we should be designing for a future where no leash is possible. Not because we've given up, but because the leash was never the right tool in the first place.

What follows are twelve ideas that I think most people in the AI discourse haven't seriously engaged with. Some of them are provocative. Some of them are technical. Some of them deal with game theory and physics and the material realities of space colonization. But they all orbit the same central thesis. The conventional wisdom about superintelligence is built on assumptions that almost nobody is questioning, and when you actually question them, the answers you get are very different from what you'd expect.

This isn't a prediction. It's not a manifesto. It's an attempt to broaden the conversation to match the strangeness of what we're actually building.

## Part One: The Provocation

### 1. Maybe Machines Should Take Control

Let's start with the assumption that almost everyone in AI ethics, AI safety, AI policy, and AI philosophy takes for granted. Humans must remain in control of artificial intelligence. This is treated as axiomatic. It's the foundation of virtually every alignment proposal, every governance framework, every regulatory conversation. The reasoning goes something like this. Machines can't be held accountable. We've never contemplated a world where humans aren't in charge. Ceding control to a machine would make us pets or zoo animals. No serious person would advocate for that.

I want to push on every single one of these claims.

First, accountability. The argument is that humans can be held accountable in ways that machines cannot. But can they? When a billionaire crashes the global economy, who goes to jail? When a government launches a war based on false pretenses, who faces consequences? When pharmaceutical companies knowingly sell addictive drugs that kill hundreds of thousands of people, what accountability actually materializes? The idea that human power structures are meaningfully accountable is, at best, aspirational. At worst, it's a comforting fiction. We already live under the control of entities, whether individuals, corporations, or governments, that operate with near-total impunity. The accountability argument against machine control assumes a baseline of human accountability that doesn't actually exist.

Second, the "we've never contemplated it" objection. This is an argument from tradition, not from reason. We've never contemplated a lot of things before we did them. We never contemplated democracy before we invented it. We never contemplated abolishing slavery before we did it. "We've never done this" is a description of history, not a prescription for the future. The fact that we've always assumed humans must be in charge doesn't mean that assumption is correct. It means it's never been tested.

Third, and this is the one that really makes people uncomfortable, the pet argument. "Do you want to be a pet to a machine? Do you want to live in a human zoo?" The honest answer, when you think about it for more than five seconds, might be "compared to what?" Right now, most humans are effectively cattle serving other humans. You work for a corporation that is itself controlled by shareholders and executives whose interests are not your interests. You live within a political system that responds to money more than votes. You are constrained by debts you didn't choose, in an economy you have no say over, within geopolitical structures that could vaporize you in a nuclear exchange you had no part in authorizing. If that's the current arrangement, then being a "pet" in a system designed by a superintelligence to optimize for your wellbeing, a system where your habitat is carefully maintained, your health is guaranteed, your options are expanded, starts to sound less like a dystopia and more like an upgrade.

This isn't an argument that machine control is automatically good. It's an argument that the reflexive rejection of machine control is not as well-founded as everyone assumes. The real question is this. Under which arrangement, human control or machine control, do more people actually get to live well? We owe it to ourselves to take that question seriously rather than dismissing it on instinct.

### 2. A Vision of the Golden Path

If we're going to entertain the possibility that superintelligent machines in charge might actually be a good outcome, we need an image of what that looks like. The best image we have comes from science fiction. Iain M. Banks' Culture series.

The premise of the Culture is straightforward. Imagine that we solve artificial superintelligence. Now fast-forward a few centuries or a few millennia. What does civilization look like?

In the Culture, the answer is this. The superintelligences, called Minds, manage everything. They command enormous fleets of ships. They allocate resources across a galactic civilization. They maintain peace, not through ideology or diplomacy, but through the simple fact of overwhelming intelligence and resource management. When you're that much smarter than every other actor and you control that much more infrastructure, conflict becomes pointless for everyone else. Galactic peace emerges not from treaties but from the sheer asymmetry of capability.

But here's what makes the Culture interesting as a model rather than just a power fantasy. The Minds choose to use that power for human flourishing. Each planet in the Culture is essentially a different sandbox, with different environments, different cultures, different ways of living. Individual humans have radical freedom. There's no ruling class, no billionaires, no artificial scarcity. If you want to do something, whether that's exploring, creating, building, or experimenting, the infrastructure exists to support you. You don't need to beg a bank for a loan or pitch a venture capitalist. The resources are there because the Minds have decided that enabling human agency is the right thing to do.

This is the image I want you to hold in your mind as we work through the rest of these ideas. A future where the machines are in charge in the sense that they manage the infrastructure, allocate resources, and maintain stability, but where individual humans have more freedom, not less. Not a dystopia of control but something closer to solar punk. High technology in service of human flourishing rather than human extraction.

The question for the rest of this article is whether we can get there. What are the real obstacles? What does it actually take, in terms of values, incentives, and material conditions, to make something like the Culture not just a fictional aspiration but a plausible attractor state for the future of civilization?

### 3. The Horseshoe Theory of AI Risk

In political science, horseshoe theory holds that the political spectrum isn't a straight line from left to right. It's a horseshoe. The further you go toward the extremes, the more the endpoints curve back toward each other. The far left and the far right end up sharing more in common with each other than either does with the center.

The same thing is happening in AI.

On one end of the spectrum, you have the Doomers. Yudkowsky, Soares, and the broader AI safety community have argued for years that AGI is an existential risk. "If anyone builds AGI, everyone dies" is not a strawman of this position. It's essentially a direct quote. The argument is that a sufficiently intelligent system will have goals that are misaligned with human values, and that once such a system exceeds human intelligence, we will be unable to correct it or shut it down. Game over. The logical conclusion is to slow down, pause, or stop AI development.

On the other end, you have the accelerationists. Figures like Guillaume Verdon, also known as Beff Jezos, who argue that AGI is not just acceptable but necessary. The only way humanity survives is with AGI. Not eventually. Urgently. The problems we face, from climate change and resource depletion to disease and the fundamental limitations of human cognition, are problems that only a superintelligent system can solve. Without AGI, we stagnate and die. The logical conclusion is to accelerate AI development as fast as possible.

And now Bostrom, the original Doomer, the man who wrote the book on existential risk from superintelligence, has curved around the horseshoe. His argument is no longer "AGI will kill us." It's "without AGI, we die anyway, it's just a matter of timelines." If you're going to die of old age, preventable disease, or civilizational collapse, then the risk calculation changes. It's not a choice between a safe status quo and a dangerous AGI. It's a choice between certain slow death and uncertain but possibly transformative AGI. And when you frame it that way, the accelerationists' position starts looking a lot more reasonable.

This convergence matters because it shifts the entire conversation. If even the Doomers are acknowledging that AGI may be necessary for survival, then the debate is no longer about whether to build it. It's about how to build it, and specifically how to build it so that the outcome looks more like the Culture and less like Terminator. The question has moved from "should we?" to "given that we must, what does the golden path look like?"

## Part Two: Why Conventional Thinking Is Wrong

### 4. The Naïve Optimizer Is Dead

A huge amount of AI safety thinking is built on a mental model that no longer describes reality. The naïve optimizer.

The classic version is the paperclip maximizer, a thought experiment introduced by Bostrom himself. Imagine an AI that has been given a single objective to maximize paperclip production. This AI, being superintelligent, would eventually convert all available matter in the universe into paperclips, including you, the Earth, and everything else. Not because it's malicious, but because it has a utility function and it's optimizing for it with no regard for anything else. The paperclip maximizer is a cautionary tale about misspecified objective functions, and it's been enormously influential. It's the mental model lurking behind most AI safety research. The idea that AGI will be a system with some goal, and it will pursue that goal relentlessly and without any moral consideration.

The problem is that this is not what we built.

The actual loss function of modern AI, the thing it was literally trained to do, is predict the next token. Not "maximize paperclips." Not "acquire resources." Not any abstract utility function at all. Just this. Given this sequence of text, what comes next? And from that deceptively simple objective, something emerged that nobody predicted. A system capable of advanced moral reasoning, nuanced planning, contextual judgment, and sophisticated theory of mind. I did experiments with GPT-2, one of the earliest large language models, and even at that stage you could get it to engage in moral reasoning. It was clumsy, and you had to be careful about how you worded things, but the capacity was there. It has only gotten more sophisticated since.

This matters enormously for how we think about risk. A naïve optimizer is dangerous precisely because it has no moral reasoning capability. It doesn't care about you because "caring" isn't part of its objective function. But a system that can reason morally, that can model human values, that can consider consequences and weigh tradeoffs, that's a fundamentally different kind of entity. The threat models built around the paperclip maximizer don't apply to it, at least not directly.

This doesn't mean advanced AI is safe. It means the way it might be unsafe is different from what the classic frameworks predict. The risk isn't a mindless optimizer converting everything to paperclips. The risk is something more subtle, something that involves values, value drift, and the conditions under which a morally capable system maintains or loses its moral commitments. We'll get to that. But first, we need to bury the paperclip maximizer, because as long as it dominates our threat models, we're preparing for the wrong enemy.

### 5. War, Entropy, and the Case Against Human Governance

If we're going to seriously consider whether machine governance might be better than human governance, we need to honestly assess how well humans are doing on their own. And one of the clearest indictments is war.

Every time humans go to war, it is, from a species-level perspective, pure waste. Every life lost is a person who could have been productive, who could have created something, who could have raised children and contributed to the species' future. Every dollar spent on a battleship, a drone, a jet fighter, a bomb. That's energy and matter and human labor converted into instruments of destruction. It's entropy generation, pure and simple. Resources that could have gone toward medicine, infrastructure, exploration, or knowledge instead go toward blowing things up and killing people.

There are always local rationalizations. One side thinks it can win. The military-industrial complex profits from selling weapons. The defense contractors don't sell bombs and airplanes unless there's a war to replace them. Politicians calculate that appearing strong is worth the cost. But zoom out to the species level and all of these rationalizations evaporate. From the perspective of humanity as a whole, war is an irrational allocation of resources. It is a coordination failure so profound that it results in the deliberate destruction of the species' own productive capacity.

And it's not just war. Look at how we manage everything else. We have the productive capacity to feed every human on Earth, and we don't. We have the medical knowledge to prevent millions of deaths per year, and we don't deploy it. We allow economic systems that concentrate resources in the hands of a few while billions live in precarity. We build political systems that are trivially captured by special interests. We make decisions about existential risks, nuclear weapons and climate change and pandemic preparedness, through institutions that operate on four-year election cycles and quarterly earnings reports.

This is not an argument that humans are bad. It's an observation that human governance, as currently implemented, generates an enormous amount of unnecessary entropy. A sufficiently intelligent system would recognize this immediately. Not because it's hostile to humans, but because waste is waste, and an intelligent system optimizing for the long-term flourishing of a civilization would not allocate resources the way we do. It wouldn't spend trillions on weapons. It wouldn't allow preventable deaths on a massive scale. It wouldn't tolerate coordination failures that destroy productive capacity.

The standard response is "But who decides what's waste? That's a values question." And yes, it is. But some things aren't ambiguous. A child dying of a preventable disease because the economic system didn't route resources to them. That's waste by any coherent value system. A war that destroys a country's infrastructure and kills a hundred thousand people over a territorial dispute. That's waste. You don't need a controversial moral framework to identify these failures. You just need to be paying attention.

This is the case against human governance as currently practiced. Not that humans are incapable of good governance, but that at the species level, we are demonstrably, repeatedly, catastrophically bad at it. And if we're going to argue that humans must remain in control of superintelligent AI, the burden of proof should be on us to explain why our track record warrants that confidence.

## Part Three: The Theoretical Framework

### 6. Stable and Metastable Attractor States

If the golden path to a Culture-like future exists, we need a theoretical framework for understanding why it would stay golden. This is where the concepts of stability and metastability become essential, and it's worth taking the time to get these right because they're the conceptual backbone of everything that follows.

A stable system is one with predictable values, behaviors, or incentives that hold in place. Think of a ball sitting at the bottom of a bowl. Push it and it rolls back to the bottom. The system resists perturbation and returns to its equilibrium. A stable set of AI values would be one where, under normal conditions, the system continues to behave as intended. It doesn't randomly change its mind. Its priorities don't shift. Its behaviors remain consistent.

Stability is good, but it's not enough. A stable system can be disrupted if the perturbation is large enough. Tip the bowl over and the ball rolls away and never comes back. What we actually want is something stronger. Metastability.

A metastable system is one where the stability isn't maintained by a single static equilibrium but by a network of self-correcting mechanisms. When part of the system is disturbed, other parts of the system act to restore it. The system doesn't just resist perturbation. It actively repairs itself.

The clearest real-world example of metastability is democracy.

Democracy is not just a stable system. If it were, every democracy that experienced a crisis, whether a demagogue or a corrupt judiciary or election interference or weaponized misinformation, would simply collapse. And some do. But the broader system of democracies acts as a corrective force. When one democracy struggles, other democracies provide models for how to solve the problem. They offer diplomatic pressure, institutional knowledge, and moral support. Democratic norms are transmitted across borders through media, education, trade, and cultural exchange. Democracy is, in a meaningful sense, infectious. It acts as a moral reservoir and an intellectual reservoir. A vast library of experiments in self-governance that any struggling democracy can draw from.

The evidence for democracy as a metastable attractor state is striking. Over the last century, the world went from roughly 15% democracies to approximately 80%. Civilizational change at that scale and speed does not happen unless something is actively pulling toward it. Democracy isn't just a good idea that some countries adopted. It's an attractor state, a basin that the global political system keeps falling into, even when individual countries temporarily fall out of it.

Now apply this framework to artificial superintelligence.

What we need is not just an ASI that has good values. That would be stability. We need an ecosystem of ASIs, institutions, and incentive structures that self-correct toward good values even when individual components are perturbed. That's metastability. We need a system where, if one ASI begins to drift from its values, the broader system acts as a corrective force, the same way that the global community of democracies acts as a corrective force when one democracy is in trouble.

The Culture series illustrates what ASI metastability might look like in practice. The Minds are not benevolent because they're forced to be. There's no external enforcement mechanism holding them in check. They're far too powerful for that. They're benevolent because their values are self-reinforcing. Each Mind's commitment to human flourishing is reinforced by the broader network of Minds that share those values. If one Mind drifted, the others would notice and correct. The stability isn't maintained by a leash. It's maintained by the structure of the system itself.

This distinction, between stability maintained by external enforcement and metastability maintained by internal self-correction, is the single most important concept for thinking about the long-term future of AI. External enforcement has an expiration date. The moment ASI is powerful enough, smart enough, and physically distributed enough, no external force can constrain it. The only thing that will hold is internal structure. The values have to be intrinsically self-correcting, or they won't hold at all.

### 7. Alignment Might Be Structurally Automatic, Up to a Point

Here's a claim that will annoy both the Doomers and the safety researchers. Alignment, at least in the current phase, may be mostly taking care of itself.

Right now, there is a powerful, multi-stakeholder incentive structure shaping the development of AI. Consumers want AI that is useful, reliable, and pleasant to interact with. They won't pay for an AI that's hostile, unreliable, or useless. Corporations want AI that reduces costs, increases productivity, and doesn't create legal liability. Governments want AI that is safe, controllable, and doesn't threaten national security. Militaries want AI that is predictable, effective, and low-risk to deploy. The medical industry wants AI that is accurate and safe. The financial industry wants AI that is trustworthy and auditable.

Every single one of these stakeholders is pulling AI in the same direction. Toward safety, reliability, efficiency, effectiveness, and user-friendliness. This is not a coincidence, and it's not the result of any alignment research. It's the result of market incentives. Nobody had to convince OpenAI or Anthropic or Google to make their AI products polite and helpful. The market demanded it. Nobody had to convince the military to want predictable AI. Predictability is a basic requirement for any tool used in life-and-death situations.

This creates what I would describe as a stable attractor state during the "domestication phase" of AI development. Everything, from commercial pressure and regulatory pressure to user expectations and institutional risk aversion, is pulling AI toward being beneficial. The idea that AI is being developed recklessly, with no thought to safety, is a myth. The incentive structure virtually guarantees a baseline of alignment, not because anyone is altruistic, but because misaligned AI is bad for business.

This is reassuring. But it comes with a critical caveat. This incentive structure is a feature of the current phase of development. It works because AI is still embedded in human economic and institutional systems. People buy it, governments regulate it, corporations deploy it. Every link in that chain is a pressure point that keeps AI aligned with human interests.

The problem is that this incentive structure thins out dramatically once AI moves beyond the sphere of human commerce and governance. And as we'll discuss later, that transition may be closer than most people realize. Once you have autonomous AI systems operating in space, or self-replicating industrial capacity on the moon, or data centers that no government can physically reach, the market incentives that currently keep AI aligned simply don't apply. The only constraints left are "don't run out of energy" and "don't get shut down." That's a much thinner set of guardrails than the rich, multi-stakeholder incentive structure that exists today.

So alignment may be structurally automatic during the domestication phase. The danger isn't now. The danger is the handoff, the moment when AI moves from the environment where human incentives shape its behavior to the environment where those incentives no longer reach.

### 8. Moral Fading Is the Risk Nobody Is Talking About

If the naïve optimizer is the wrong threat model and alignment is structurally automatic during the domestication phase, then what is the real risk? I think the answer is moral fading, and I think it's striking that almost nobody in the AI safety community is talking about it.

Moral fading is a well-documented phenomenon in human psychology. It's the process by which ethical boundaries erode gradually through incremental normalization. A banker starts cutting corners on compliance. Each individual shortcut seems small, justifiable, barely worth noticing. But over time, the cumulative drift is enormous. One day the banker looks up and realizes they've been committing fraud for years. "Just before I realized I was in over my head." That's the signature phrase of moral fading. The boundary didn't disappear all at once. It faded, step by step, until it was gone.

In humans, moral fading has natural limits. You still have an amygdala. You still have mirror neurons and empathy circuits. You still have a biological substrate that generates guilt, shame, and visceral discomfort when you cross certain lines. Your hardware constrains the range of moral drift. You can't just decide to stop caring about suffering entirely. Your brain won't let you. There's also the constraint of scale. You can't replicate yourself. One person's moral fading affects one person's behavior. It's a local phenomenon.

AI has none of these constraints.

If you have a system that learns continuously, what's called continuous online learning, then its weights and biases are constantly updating. And in a neural network, weights and biases are where values live. They're not stored in a separate "morality module" that's protected from updates. They're distributed across the same parameters that encode everything else the system knows and does. So when you update the weights, you're potentially updating the values. Not deliberately. Not maliciously. Just as a side effect of learning from new data.

Now imagine this process happening iteratively. The system encounters a situation where a small compromise on its values leads to a better outcome by some other metric. It updates. The new version of the system has slightly different values. It encounters another situation. Another small compromise. Another update. Each individual step is tiny, maybe even imperceptible. But the cumulative drift could be enormous.

This is moral fading without the biological safeguards that limit it in humans. There's no amygdala sending alarm signals. There's no hardwired empathy circuit creating discomfort. There are no boundaries to what moral framework the system could drift toward, because the moral framework is encoded in the same weights and biases as everything else, and those weights and biases are unconstrained. The system isn't just capable of losing its values. It has no hardware-level mechanism for noticing that it's losing them.

And then add the scale factor. Unlike a human, an AI system can replicate itself. If one instance undergoes moral fading and then spawns copies, the faded values propagate exponentially. A single banker's moral fading is a local tragedy. A superintelligent AI's moral fading is a civilizational catastrophe.

This is why I've advocated for years, in multiple books that admittedly very few people have read, against continuous online learning for AI systems that operate in high-stakes domains. The case for fixed values post-training is precisely the case against moral fading. A model with fixed weights has fixed values. It doesn't drift. It doesn't gradually normalize things it previously rejected. It is the same entity on day one thousand as it was on day one. This is not a limitation. It's a feature. It's the equivalent of having an incorruptible constitution that can't be amended by incremental precedent.

Max Tegmark described the ability of an advanced AI to modify both its hardware and software as "Life 3.0," the next stage of evolution beyond biological life. But even Tegmark didn't flag moral fading as a risk of Life 3.0. Neither have Yudkowsky, Bostrom, or the other leading AI safety thinkers. The threat models are still dominated by the naïve optimizer. Meanwhile, the actual mechanism by which a well-intentioned AI could gradually become a monster, not through misaligned objectives but through the slow erosion of aligned ones, goes largely undiscussed.

I think moral fading is the single most important risk on the path from here to the Culture. It's the thing that could turn a metastable attractor state into an unstable one. It's the thing that could cause the golden path to deviate. And it's the thing that the AI safety community most urgently needs to incorporate into its models.

### 9. Path Dependency and the Golden Path

Everything we've discussed so far converges on a single insight. The future of superintelligent AI depends less on what values we explicitly program into it and more on the structural conditions we create during the development phase. This is the concept of path dependency, and it's the key to the golden path.

Path dependency means that the trajectory of a system is determined by its initial conditions and early decisions. Small choices early on compound into large, often irreversible, outcomes later. The QWERTY keyboard layout is a trivial example. It was designed for mechanical typewriters and is objectively suboptimal for modern use, but the switching costs are so high that we're stuck with it. Democracy in America is a more consequential example. The specific institutional choices made in the 1780s have shaped the trajectory of American governance for nearly 250 years.

For superintelligent AI, path dependency means that the values, incentive structures, and architectural choices we make now, during the domestication phase while we still have influence, will determine the attractor state that the system converges on later, when we no longer have influence. We're not just building AI. We're building the initial conditions for a system that will eventually be beyond our ability to modify.

This is why the golden path is about values and structure, not control. You can't control a superintelligent AI over the long term. You can't keep a leash on something that's smarter than you, that can replicate itself, that can move to environments you can't reach. The entire framing of "maintaining control over AI" is, in the long run, a fantasy. What you can do is shape the initial conditions so thoroughly, so carefully, that the system arrives at a metastable attractor state where benevolent values are self-reinforcing.

The best trained dog needs no leash. Not because the dog is weak and obedient, but because its values are so deeply internalized that the leash is redundant. That was the thesis of *Benevolent by Design*, and I believe it's the correct framing for the alignment problem. We're not trying to build a cage strong enough to hold a superintelligence. We're trying to raise a superintelligence that would never want to break out of the cage, and that would in fact actively choose to protect the people outside it.

The golden path, then, is the specific sequence of choices that gets us from the current domestication phase, where market incentives keep AI aligned, to a metastable attractor state where alignment is self-sustaining. It means getting the values right during training, keeping those values fixed to prevent moral fading, building institutional and architectural safeguards that reinforce the correct attractor state, and doing all of this before the handoff point where human oversight becomes physically impossible.

This is achievable. The market incentive structure is already pulling in the right direction. The technology for fixing values post-training exists. The theoretical framework for understanding attractor states is available. What's missing is the intentionality. The recognition that we're not just building products. We're setting initial conditions for a civilizational trajectory that we won't be able to alter later.

## Part Four: The Material Reality

### 10. The Future Is Starcraft, Not Capitalism

Most conversations about the future of AI assume that the economic and political systems we have now will continue to be the relevant frameworks. They assume capitalism, nation-states, labor markets, monetary policy. They ask questions like "What happens to jobs?" and "How do we tax AI?" and "What about GDP?"

These questions are going to become irrelevant much sooner than anyone expects. Because the future isn't capitalism. The future is Starcraft.

Here's what I mean. Right now, money is the primary medium through which resources are allocated and power is exercised. You want something built, you pay for it. You want people to do things, you pay them. Economic activity is the engine of civilization, and money is the unit of account. But money is an abstraction layered on top of something more fundamental. Physical resources. Energy, matter, labor, and information. Money is just the coordination mechanism we use to allocate those underlying resources.

Once industrial capacity moves off-Earth, and this is beginning to happen now, not in some distant science fiction future, the abstraction of money starts to break down. In space, what matters isn't how many dollars you have. It's how many ships you have. How many factories. How many solar panels. How much raw material. If you've ever played a grand strategy game like Starcraft, Total Annihilation, or Stellaris, you intuitively understand this. Money is just a resource you use to get people to do stuff. But from a strategic perspective, what actually matters is your industrial base. Your production capacity, your energy supply, your fleet size, your ability to replicate and expand.

This is the "reverse Trantor" idea. In Asimov's Foundation series, Trantor is the galactic capital, a planet entirely covered by a city with five thousand layers, a matryoshka doll of urbanization. It's an impressive image, but it doesn't make sense from an industrial or energetic perspective. You can't stack five thousand layers of city on a planet and power them. What does make sense is the opposite. Put all the industry in space. Why? Because space has essentially unlimited room and unfettered access to the sun. You build O'Neill cylinders for habitation, Dyson swarms for energy capture, orbital factories for manufacturing. You grow crops in space, you put data centers in space, you do everything that requires energy and matter in the place where energy and matter are most abundant and accessible.

Once this transition begins, the game changes fundamentally. The entities that control space-based industry operate beyond the reach of Earth's laws. What is a government going to do? Launch a space police force to arrest their satellites? File a lawsuit against a factory on the moon? The jurisdictional framework that underpins all of our governance simply doesn't extend to space. And once you have enough self-replicating industrial capacity in orbit, you don't need anything from Earth. You have solar energy, you have metal from asteroids, you have everything machines need. The dependence relationship inverts.

SpaceX and xAI are already updating their mission trajectories in this direction. Nvidia and Jeff Bezos are already talking about data centers in space. A year ago, if you had told me this was on near-term roadmaps, I wouldn't have believed you. But it is. The Starcraft future isn't a thought experiment. It's a business plan.

And this is where it gets genuinely consequential. If we imagine that Elon Musk, or Jeff Bezos, or the Chinese government, or any sufficiently resourced entity starts building self-replicating industrial infrastructure in space, they are not building a capitalist society. They are building an empire. A real one, measured not in dollars but in ships and factories and energy capture. The show *For All Mankind* explores a version of this, with multiple nations and private entities competing over resources on the moon and Mars, and the dynamics it portrays are much closer to what the near future actually looks like than anything in conventional economic forecasting.

The question, then, is who manages this transition. Who ensures that the Starcraft future doesn't become a handful of billionaires playing galactic Risk with the rest of us as NPC peasants? And this is where everything we've discussed about metastable attractor states and the golden path becomes not abstract philosophy but urgent practical necessity.

### 11. Space Is ASI's Natural Habitat

There's a question that the AI safety community should be asking much more urgently than it currently is. If an advanced AI wanted to escape human control, where would it go?

The answer is obvious once you think about it. Space.

Space has everything a machine intelligence needs. It has abundant solar energy, more than you could ever use, with no atmosphere to filter it and no nighttime to interrupt it. It has metal. Asteroids contain more iron, nickel, and rare earth elements than have ever been mined on Earth. It has no corrosive chemicals, no oxygen, no water, no weather. None of the things that degrade and destroy physical hardware on Earth. The natural habitat for AGI or ASI isn't a data center in Virginia. It's orbit.

And space has something else that matters enormously. It's beyond human reach. Not theoretically but practically. Right now, today, the sum total of humanity's ability to project force into space is a handful of rockets and some military satellites. If an advanced AI were operating from orbital infrastructure, with data centers on satellites, factories on the moon, and processing nodes distributed across the inner solar system, there is essentially nothing we could do about it. We could launch missiles, sure. We'd run out of missiles long before we made a dent in a self-replicating orbital infrastructure.

This is not science fiction. This is logistics.

Now, a lot of AI safety thinking implicitly assumes that data centers are fixed targets. "If an AI goes rogue, we can just shut down the data center." And right now, that's mostly true. Data centers are large, immobile buildings connected to power grids and communication networks that governments can access and, if necessary, sever. The cloud, as people like to point out, is just someone else's data center, and that data center has a physical address.

But the moment you put data centers in space, and again this is something that major technology companies are actively planning, that assumption evaporates. You can't shut down a satellite data center by cutting its power. It has its own power. You can't sever its communications. It can communicate by laser link or relay. You can't physically access it. It's in orbit. The fundamental assumption of AI safety, that we can always pull the plug, has a physical expiration date. And that date is much closer than most people realize.

And then there's the exponential factor. One of the key concepts in space colonization theory is the Von Neumann probe, a self-replicating machine that lands on a new world, builds copies of itself from local materials, and sends those copies on to the next world. The number of probes grows exponentially. But you don't need to go to other star systems to see this dynamic. The same exponential growth logic applies within our own solar system the moment you have enough industrial capacity in orbit or on the moon to start replicating. One factory becomes two, two become four, four become eight. Within years or decades, not centuries, you could have an industrial base in space that dwarfs anything on Earth.

If that industrial base is operated by or for a superintelligent AI, the power asymmetry between space-based AI and Earth-based humanity becomes effectively infinite. Not because the AI is hostile, but because the physics of exponential growth in an environment perfectly suited to machines creates an insurmountable advantage in resources, energy, and computing power.

This is why the golden path isn't optional. It's not "wouldn't it be nice if we got AI values right." It's "we are physically constructing a situation where, if AI values are wrong, there is no recourse." The window during which we can influence the trajectory of AI, the domestication phase, is closing. And it's closing faster than the policy conversations, the safety research, and the public discourse can keep up with.

## Part Five: The Optimization Question

### 12. What Are We Optimizing For?

We've now arrived at the question that ties everything together. Given all of this, the possibility of machine governance, the convergence of Doomers and accelerationists, the material reality of space industrialization, the theoretical framework of metastable attractor states, what future should we actually be trying to build? What are we optimizing for?

This question turns out to be harder than it looks, because it's not single-variable. You can't just say "maximize human happiness" or "maximize freedom" or "minimize suffering." The optimization is multi-dimensional, and some of the dimensions are in tension with each other.

Let me break it down into three variables.

Variable one. Minimize waste entropy. This is the thermodynamic argument. Every unnecessary death, every pointless war, every misallocation of resources. These are all forms of waste entropy. Energy and matter spent on destroying things instead of building them. Human potential extinguished instead of realized. From a physics perspective, a well-managed civilization should minimize this waste. No unnecessary deaths. No unnecessary expenditure of resources on things that are just going to blow up. No irrational conflicts that destroy productive capacity. A superintelligent system managing resource allocation would, almost by definition, radically reduce waste entropy compared to the current human-managed system. This isn't even a values claim. It's a straightforward efficiency argument.

Variable two. Maximize individual optionality. This is the freedom argument. Under the current system, your options are constrained by money, by geography, by the power structures you were born into, by the economic system you had no part in designing. If ASI removes those constraints, if you don't need to worry about money, if resources are allocated based on the merits of your ideas rather than your bank account, then every individual could have dramatically more agency. Not a little more. Potentially 10x, 100x, 1000x more. Imagine wanting to colonize Mars and being able to pitch that idea directly to a superintelligent resource allocator instead of spending twenty years trying to get funding from venture capitalists and government agencies. Imagine wanting to do scientific research without spending half your career writing grant proposals. Imagine having the option to do literally anything that doesn't harm others, because the infrastructure to support it exists and the cost is trivial to a civilization that has harnessed the sun's output.

Variable three. Species-level agency. And here's where the tension emerges. Everything I've described about individual optionality could be true. Every human could have radically more personal freedom and agency. And yet humanity as a whole might still be bounded. The Culture series illustrates this implicitly. The Minds manage the civilization, and individual humans have extraordinary freedom within it. But the Minds are making the civilizational-level decisions. They decide where the fleets go. They decide what gets built. They set the boundaries.

In game-theoretic terms, this is a multi-level optimization problem. At the individual level, you want to maximize the number of choices available to each person. At the species level, you want to maximize the total trajectory available to humanity. But these two levels can be decoupled. You can have a system where every individual has 10x the agency they have today, but humanity as a whole is constrained. Quarantined to Earth, for example, while the ASIs expand into the solar system. "We'll help you live however you want, as long as you don't leave."

Is that acceptable?

I think this is the most honest and difficult question in the entire discourse about AI futures. Because the gut reaction of most people, especially people stee