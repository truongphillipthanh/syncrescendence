{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "eb745a26-515e-55d4-9e82-24a07c8bb5a2", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0001", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "The capabilities that make AI agents useful, such as autonomy, intelligence, and flexibility, also make them difficult to evaluate.", "line_start": 5, "line_end": 6, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "The capabilities that make AI agents useful, such as autonomy, intelligence, and", "content": "The capabilities that make AI agents useful, such as autonomy, intelligence, and flexibility, also make them difficult to evaluate.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 5, "line_end": 6, "atom_id": "ATOM-SOURCE-20260109-001-0001"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "513d5a4e-a896-512b-9d0d-e86cb9806b48", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0002", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "Good evaluations help teams ship AI agents more confidently by making problems and behavioral changes visible before they affect users, preventing reactive loops where issues are only caught in production.", "line_start": 12, "line_end": 15, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.7, 0.1, 0.1, 0.8, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "Good evaluations help teams ship AI agents more confidently by making problems a", "content": "Good evaluations help teams ship AI agents more confidently by making problems and behavioral changes visible before they affect users, preventing reactive loops where issues are only caught in production.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 12, "line_end": 15, "atom_id": "ATOM-SOURCE-20260109-001-0002"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.7, 0.1, 0.1, 0.8, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "58068bb2-dd47-55fc-9511-43f892a9a9e7", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0003", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "An 'evaluation' (eval) is a test for an AI system that involves providing an AI with an input and then applying grading logic to its output to measure success.", "line_start": 22, "line_end": 23, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "An 'evaluation' (eval) is a test for an AI system that involves providing an AI", "content": "An 'evaluation' (eval) is a test for an AI system that involves providing an AI with an input and then applying grading logic to its output to measure success.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 22, "line_end": 23, "atom_id": "ATOM-SOURCE-20260109-001-0003"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "adb7e416-df7c-5e71-a663-06d6da91bff1", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0004", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Automated evals are evaluations that can be run during development without real users.", "line_start": 23, "line_end": 24, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Automated evals are evaluations that can be run during development without real", "content": "Automated evals are evaluations that can be run during development without real users.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 23, "line_end": 24, "atom_id": "ATOM-SOURCE-20260109-001-0004"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "bf9165df-8939-5223-a957-187afbb9c73d", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0005", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Single-turn evaluations involve a prompt, a response, and grading logic, and were the primary method for earlier LLMs.", "line_start": 26, "line_end": 27, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Single-turn evaluations involve a prompt, a response, and grading logic, and wer", "content": "Single-turn evaluations involve a prompt, a response, and grading logic, and were the primary method for earlier LLMs.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 26, "line_end": 27, "atom_id": "ATOM-SOURCE-20260109-001-0005"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "8ed932c1-8c80-55af-a8e9-d9a17d3dbf9b", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0006", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Multi-turn evaluations have become increasingly common as AI capabilities have advanced.", "line_start": 27, "line_end": 28, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Multi-turn evaluations have become increasingly common as AI capabilities have a", "content": "Multi-turn evaluations have become increasingly common as AI capabilities have advanced.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 27, "line_end": 28, "atom_id": "ATOM-SOURCE-20260109-001-0006"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "50df4ceb-ce0b-5a23-a1af-90ce4d017eed", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0007", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Agent evaluations are complex because agents use tools across many turns, modifying state and adapting, which can lead to mistakes propagating and compounding.", "line_start": 33, "line_end": 34, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Agent evaluations are complex because agents use tools across many turns, modify", "content": "Agent evaluations are complex because agents use tools across many turns, modifying state and adapting, which can lead to mistakes propagating and compounding.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 33, "line_end": 34, "atom_id": "ATOM-SOURCE-20260109-001-0007"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "94cb4904-57ca-5436-a7fc-380c4b9b6605", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0008", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "A 'task' (also known as a problem or test case) is a single test with defined inputs and success criteria.", "line_start": 40, "line_end": 40, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "A 'task' (also known as a problem or test case) is a single test with defined in", "content": "A 'task' (also known as a problem or test case) is a single test with defined inputs and success criteria.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 40, "line_end": 40, "atom_id": "ATOM-SOURCE-20260109-001-0008"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "b746f263-ba7b-57bd-866a-83fdd3dc14e7", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0009", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "A 'trial' is each attempt at a task, with multiple trials run to produce more consistent results due to model output variability.", "line_start": 41, "line_end": 42, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "A 'trial' is each attempt at a task, with multiple trials run to produce more co", "content": "A 'trial' is each attempt at a task, with multiple trials run to produce more consistent results due to model output variability.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 41, "line_end": 42, "atom_id": "ATOM-SOURCE-20260109-001-0009"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "a39242e8-4ce1-53f3-8fb7-97c5598a7da8", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0010", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "A 'grader' is logic that scores an aspect of an agent's performance, with a task potentially having multiple graders, each containing multiple assertions (checks).", "line_start": 43, "line_end": 44, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "A 'grader' is logic that scores an aspect of an agent's performance, with a task", "content": "A 'grader' is logic that scores an aspect of an agent's performance, with a task potentially having multiple graders, each containing multiple assertions (checks).", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 43, "line_end": 44, "atom_id": "ATOM-SOURCE-20260109-001-0010"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "12f8fb29-4f94-5f03-bfab-fd93b245aa09", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0011", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "A 'transcript' (also called a trace or trajectory) is the complete record of a trial, including outputs, tool calls, reasoning, intermediate results, and any other interactions.", "line_start": 45, "line_end": 47, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "A 'transcript' (also called a trace or trajectory) is the complete record of a t", "content": "A 'transcript' (also called a trace or trajectory) is the complete record of a trial, including outputs, tool calls, reasoning, intermediate results, and any other interactions.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 45, "line_end": 47, "atom_id": "ATOM-SOURCE-20260109-001-0011"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "3cbba44f-0e81-58c1-adda-5457c3e13ffe", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0012", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "The 'outcome' is the final state in the environment at the end of a trial, which may differ from the agent's stated output.", "line_start": 48, "line_end": 50, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "The 'outcome' is the final state in the environment at the end of a trial, which", "content": "The 'outcome' is the final state in the environment at the end of a trial, which may differ from the agent's stated output.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 48, "line_end": 50, "atom_id": "ATOM-SOURCE-20260109-001-0012"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "c9b01030-fb3b-577d-99ad-5c3178358aea", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0013", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Teams with established evaluation systems (evals) can adopt new, more powerful AI models in days, while those without evals may take weeks due to extensive testing requirements.", "line_start": 50, "line_end": 54, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.2, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Teams with established evaluation systems (evals) can adopt new, more powerful A", "content": "Teams with established evaluation systems (evals) can adopt new, more powerful AI models in days, while those without evals may take weeks due to extensive testing requirements.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 50, "line_end": 54, "atom_id": "ATOM-SOURCE-20260109-001-0013"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.2, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "2f13436e-c5f5-58c8-948a-4e4b5f03cad3", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0014", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "An 'evaluation harness' is the infrastructure that runs evals end-to-end, providing instructions and tools, running tasks concurrently, recording steps, grading outputs, and aggregating results.", "line_start": 51, "line_end": 53, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "An 'evaluation harness' is the infrastructure that runs evals end-to-end, provid", "content": "An 'evaluation harness' is the infrastructure that runs evals end-to-end, providing instructions and tools, running tasks concurrently, recording steps, grading outputs, and aggregating results.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 51, "line_end": 53, "atom_id": "ATOM-SOURCE-20260109-001-0014"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "801e4f73-9115-5420-8985-646c623f5fad", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0015", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Evals provide baselines and regression tests for metrics like latency, token usage, cost per task, and error rates on a static bank of tasks.", "line_start": 54, "line_end": 57, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Evals provide baselines and regression tests for metrics like latency, token usa", "content": "Evals provide baselines and regression tests for metrics like latency, token usage, cost per task, and error rates on a static bank of tasks.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 54, "line_end": 57, "atom_id": "ATOM-SOURCE-20260109-001-0015"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "e368a2c2-f7a8-55d0-8077-1332db36c493", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0016", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Evals can serve as a high-bandwidth communication channel between product and research teams by defining metrics for researchers to optimize.", "line_start": 57, "line_end": 59, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.2, 0.5, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Evals can serve as a high-bandwidth communication channel between product and re", "content": "Evals can serve as a high-bandwidth communication channel between product and research teams by defining metrics for researchers to optimize.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 57, "line_end": 59, "atom_id": "ATOM-SOURCE-20260109-001-0016"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.2, 0.5, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "2ee833ba-c318-5fbb-96f3-59c1e43eac8d", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0017", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "AI agents can be categorized into types such as coding agents, research agents, computer use agents, and conversational agents, all of which can be evaluated using similar techniques.", "line_start": 65, "line_end": 69, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "AI agents can be categorized into types such as coding agents, research agents,", "content": "AI agents can be categorized into types such as coding agents, research agents, computer use agents, and conversational agents, all of which can be evaluated using similar techniques.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 65, "line_end": 69, "atom_id": "ATOM-SOURCE-20260109-001-0017"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "6447298c-ff5b-58ba-8f27-6aac4067fd11", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0018", "source_id": "SOURCE-20260109-001", "category": "framework", "content": "Agent evaluations typically combine three types of graders: code-based, model-based, and human, each evaluating portions of the transcript or outcome, with the choice of grader being essential for effective evaluation design.", "line_start": 73, "line_end": 76, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Framework", "name": "Agent evaluations typically combine three types of graders: code-based, model-ba", "content": "Agent evaluations typically combine three types of graders: code-based, model-based, and human, each evaluating portions of the transcript or outcome, with the choice of grader being essential for effective evaluation design.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 73, "line_end": 76, "atom_id": "ATOM-SOURCE-20260109-001-0018"}, "metadata": {"category": "framework", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "b98fb74a-29bb-5c63-9719-d04fc26e109e", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0019", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Code-based graders are fast, cheap, objective, reproducible, easy to debug, and verify specific conditions, but are brittle to valid variations and limited for subjective tasks.", "line_start": 81, "line_end": 87, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Code-based graders are fast, cheap, objective, reproducible, easy to debug, and", "content": "Code-based graders are fast, cheap, objective, reproducible, easy to debug, and verify specific conditions, but are brittle to valid variations and limited for subjective tasks.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 81, "line_end": 87, "atom_id": "ATOM-SOURCE-20260109-001-0019"}, "metadata": {"category": "concept", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "c5652c49-6d39-5170-9e3f-5bb0f891b8bd", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0020", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Model-based graders are flexible, scalable, capture nuance, and handle open-ended or freeform output, but are non-deterministic, more expensive than code-based graders, and require calibration with human graders for accuracy.", "line_start": 92, "line_end": 98, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.7, 0.1, 0.2, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Model-based graders are flexible, scalable, capture nuance, and handle open-ende", "content": "Model-based graders are flexible, scalable, capture nuance, and handle open-ended or freeform output, but are non-deterministic, more expensive than code-based graders, and require calibration with human graders for accuracy.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 92, "line_end": 98, "atom_id": "ATOM-SOURCE-20260109-001-0020"}, "metadata": {"category": "concept", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.7, 0.1, 0.2, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "e4c0e3d8-9391-5f1f-86ca-1eecbdca2b2f", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0021", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Human graders provide gold standard quality, match expert user judgment, and are used to calibrate model-based graders, but are expensive, slow, and often require access to human experts at scale.", "line_start": 103, "line_end": 109, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.4, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Human graders provide gold standard quality, match expert user judgment, and are", "content": "Human graders provide gold standard quality, match expert user judgment, and are used to calibrate model-based graders, but are expensive, slow, and often require access to human experts at scale.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 103, "line_end": 109, "atom_id": "ATOM-SOURCE-20260109-001-0021"}, "metadata": {"category": "concept", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.4, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "9f2f077e-f33a-514f-b6be-50f928363c7d", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0022", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Capability or \"quality\" evals assess what an agent can do well, starting with a low pass rate on challenging tasks to encourage improvement.", "line_start": 114, "line_end": 116, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Capability or \"quality\" evals assess what an agent can do well, starting with a", "content": "Capability or \"quality\" evals assess what an agent can do well, starting with a low pass rate on challenging tasks to encourage improvement.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 114, "line_end": 116, "atom_id": "ATOM-SOURCE-20260109-001-0022"}, "metadata": {"category": "concept", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "6cef6230-f09b-593e-a48a-826782282de0", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0023", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Regression evals ensure an agent maintains its past performance, aiming for a nearly 100% pass rate to detect any backsliding or issues caused by changes.", "line_start": 118, "line_end": 121, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Regression evals ensure an agent maintains its past performance, aiming for a ne", "content": "Regression evals ensure an agent maintains its past performance, aiming for a nearly 100% pass rate to detect any backsliding or issues caused by changes.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 118, "line_end": 121, "atom_id": "ATOM-SOURCE-20260109-001-0023"}, "metadata": {"category": "concept", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "96299d0d-5654-5c86-952c-eeb7546206bc", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0024", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "After an agent is launched and optimized, capability evals with high pass rates can be converted into a continuous regression suite to monitor for drift.", "line_start": 124, "line_end": 126, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.7, 0.1, 0.1, 0.8, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "After an agent is launched and optimized, capability evals with high pass rates", "content": "After an agent is launched and optimized, capability evals with high pass rates can be converted into a continuous regression suite to monitor for drift.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 124, "line_end": 126, "atom_id": "ATOM-SOURCE-20260109-001-0024"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.7, 0.1, 0.1, 0.8, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "99419f74-9684-5e0d-b6ba-738887ee60e5", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0025", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Coding agents write, test, and debug code, navigate codebases, and run commands, similar to human developers.", "line_start": 129, "line_end": 130, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.4, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Coding agents write, test, and debug code, navigate codebases, and run commands,", "content": "Coding agents write, test, and debug code, navigate codebases, and run commands, similar to human developers.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 129, "line_end": 130, "atom_id": "ATOM-SOURCE-20260109-001-0025"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.4, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "c2b506a4-1af7-5f6a-a194-ee466968e7f0", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0026", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Effective evaluations for modern coding agents typically use well-specified tasks, stable test environments, and thorough tests for the generated code.", "line_start": 130, "line_end": 132, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Effective evaluations for modern coding agents typically use well-specified task", "content": "Effective evaluations for modern coding agents typically use well-specified tasks, stable test environments, and thorough tests for the generated code.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 130, "line_end": 132, "atom_id": "ATOM-SOURCE-20260109-001-0026"}, "metadata": {"category": "claim", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "617dc757-4f0a-5e64-b4e8-c37ff5433ec3", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0027", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Deterministic graders are suitable for coding agents because software evaluation is generally straightforward, focusing on whether code runs and tests pass.", "line_start": 134, "line_end": 136, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Deterministic graders are suitable for coding agents because software evaluation", "content": "Deterministic graders are suitable for coding agents because software evaluation is generally straightforward, focusing on whether code runs and tests pass.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 134, "line_end": 136, "atom_id": "ATOM-SOURCE-20260109-001-0027"}, "metadata": {"category": "claim", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "3d49dd7e-ec5b-5d60-b39f-c09f8a7a3508", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0028", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "SWE-bench Verified evaluates coding agents by having them fix GitHub issues from Python repositories, passing only if failing tests are fixed without breaking existing ones.", "line_start": 137, "line_end": 141, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "SWE-bench Verified evaluates coding agents by having them fix GitHub issues from", "content": "SWE-bench Verified evaluates coding agents by having them fix GitHub issues from Python repositories, passing only if failing tests are fixed without breaking existing ones.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 137, "line_end": 141, "atom_id": "ATOM-SOURCE-20260109-001-0028"}, "metadata": {"category": "claim", "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "443f38a7-687f-5bf2-8540-3531a4a1cf93", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0029", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Terminal-Bench evaluates coding agents by testing end-to-end technical tasks, such as building a Linux kernel or training an ML model.", "line_start": 141, "line_end": 143, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Terminal-Bench evaluates coding agents by testing end-to-end technical tasks, su", "content": "Terminal-Bench evaluates coding agents by testing end-to-end technical tasks, such as building a Linux kernel or training an ML model.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 141, "line_end": 143, "atom_id": "ATOM-SOURCE-20260109-001-0029"}, "metadata": {"category": "claim", "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "c1dc9ea1-9379-5ff2-a8e7-bb605e1a5c41", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0030", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "Beyond pass-or-fail tests for coding task outcomes, it is useful to grade the transcript using heuristics-based code quality rules or model-based graders with rubrics to assess agent behaviors like tool calls or user interaction.", "line_start": 145, "line_end": 150, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.2, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "Beyond pass-or-fail tests for coding task outcomes, it is useful to grade the tr", "content": "Beyond pass-or-fail tests for coding task outcomes, it is useful to grade the transcript using heuristics-based code quality rules or model-based graders with rubrics to assess agent behaviors like tool calls or user interaction.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 145, "line_end": 150, "atom_id": "ATOM-SOURCE-20260109-001-0030"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.2, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "5bd3d7eb-3d61-5a5f-898b-5238b9d67539", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0031", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Research agents are AI systems designed to gather, synthesize, and analyze information to produce outputs such as answers or reports.", "line_start": 156, "line_end": 158, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Research agents are AI systems designed to gather, synthesize, and analyze infor", "content": "Research agents are AI systems designed to gather, synthesize, and analyze information to produce outputs such as answers or reports.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 156, "line_end": 158, "atom_id": "ATOM-SOURCE-20260109-001-0031"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "2e770274-411c-5101-9325-dfcced4d738b", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0032", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Unlike coding agents, where unit tests provide binary pass/fail signals, the quality of research agent output is judged relative to the specific task, as what constitutes 'comprehensive,' 'well-sourced,' or 'correct' varies by context (e.g., market scan vs. scientific report).", "line_start": 158, "line_end": 164, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Unlike coding agents, where unit tests provide binary pass/fail signals, the qua", "content": "Unlike coding agents, where unit tests provide binary pass/fail signals, the quality of research agent output is judged relative to the specific task, as what constitutes 'comprehensive,' 'well-sourced,' or 'correct' varies by context (e.g., market scan vs. scientific report).", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 158, "line_end": 164, "atom_id": "ATOM-SOURCE-20260109-001-0032"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "15bdc5e5-945f-5fd0-8aad-e4e3bc52789c", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0033", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Evaluating research agents presents unique challenges, including expert disagreement on synthesis comprehensiveness, constantly shifting ground truth due to changing reference content, and increased potential for errors in longer, open-ended outputs.", "line_start": 166, "line_end": 170, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.2, 0.2, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Evaluating research agents presents unique challenges, including expert disagree", "content": "Evaluating research agents presents unique challenges, including expert disagreement on synthesis comprehensiveness, constantly shifting ground truth due to changing reference content, and increased potential for errors in longer, open-ended outputs.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 166, "line_end": 170, "atom_id": "ATOM-SOURCE-20260109-001-0033"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.2, 0.2, 0.2, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "99b4befa-35e9-541f-9a14-4e3d5034aba4", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0034", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "A strategy for building research agent evaluations is to combine grader types, using groundedness checks for source support, coverage checks for essential facts, and source quality checks for authoritativeness.", "line_start": 173, "line_end": 177, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.4, 0.5, 0.1, 0.2, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "A strategy for building research agent evaluations is to combine grader types, u", "content": "A strategy for building research agent evaluations is to combine grader types, using groundedness checks for source support, coverage checks for essential facts, and source quality checks for authoritativeness.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 173, "line_end": 177, "atom_id": "ATOM-SOURCE-20260109-001-0034"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.4, 0.5, 0.1, 0.2, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "4dbfa19a-333f-5f6f-bd92-2523fcb8d1fd", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0035", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "For research tasks with objectively correct answers (e.g., Company X's Q3 revenue), exact match evaluation works, while LLMs can flag unsupported claims, identify coverage gaps, and verify open-ended synthesis for coherence and completeness.", "line_start": 177, "line_end": 180, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "For research tasks with objectively correct answers (e.g., Company X's Q3 revenu", "content": "For research tasks with objectively correct answers (e.g., Company X's Q3 revenue), exact match evaluation works, while LLMs can flag unsupported claims, identify coverage gaps, and verify open-ended synthesis for coherence and completeness.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 177, "line_end": 180, "atom_id": "ATOM-SOURCE-20260109-001-0035"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "c6810273-9929-5589-8e37-c4405d91a880", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0036", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "LLM-based rubrics for grading research agents should be frequently calibrated against expert human judgment due to the subjective nature of research quality.", "line_start": 182, "line_end": 184, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.4, 0.5, 0.1, 0.2, 0.7, 0.6], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "LLM-based rubrics for grading research agents should be frequently calibrated ag", "content": "LLM-based rubrics for grading research agents should be frequently calibrated against expert human judgment due to the subjective nature of research quality.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 182, "line_end": 184, "atom_id": "ATOM-SOURCE-20260109-001-0036"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.4, 0.5, 0.1, 0.2, 0.7, 0.6], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "04980043-e6c2-5eef-8702-66bdf08f3c7d", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0037", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Computer use agents are AI systems that interact with software through human-like interfaces (screenshots, mouse clicks, keyboard input, scrolling) rather than APIs or code, enabling them to use any application with a graphical user interface (GUI).", "line_start": 186, "line_end": 189, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Computer use agents are AI systems that interact with software through human-lik", "content": "Computer use agents are AI systems that interact with software through human-like interfaces (screenshots, mouse clicks, keyboard input, scrolling) rather than APIs or code, enabling them to use any application with a graphical user interface (GUI).", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 186, "line_end": 189, "atom_id": "ATOM-SOURCE-20260109-001-0037"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "ad6eab8a-11c8-5c1c-af4b-3a4d89940a0f", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0038", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Evaluating computer use agents requires running them in real or sandboxed environments to interact with software applications and verifying that they achieve the intended outcome.", "line_start": 189, "line_end": 192, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Evaluating computer use agents requires running them in real or sandboxed enviro", "content": "Evaluating computer use agents requires running them in real or sandboxed environments to interact with software applications and verifying that they achieve the intended outcome.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 189, "line_end": 192, "atom_id": "ATOM-SOURCE-20260109-001-0038"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.4, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "1971f814-ad5a-59bb-989e-0f46b36489fb", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0039", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Browser use agents require a balance between token efficiency and latency: DOM-based interactions are fast but token-intensive, while screenshot-based interactions are slower but more token-efficient.", "line_start": 198, "line_end": 201, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Browser use agents require a balance between token efficiency and latency: DOM-b", "content": "Browser use agents require a balance between token efficiency and latency: DOM-based interactions are fast but token-intensive, while screenshot-based interactions are slower but more token-efficient.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 198, "line_end": 201, "atom_id": "ATOM-SOURCE-20260109-001-0039"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "0d83136b-555c-531b-97b4-ee9afdcb3e4b", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0040", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Agent behavior varies between runs, making evaluation results harder to interpret due to non-determinism.", "line_start": 207, "line_end": 208, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Agent behavior varies between runs, making evaluation results harder to interpre", "content": "Agent behavior varies between runs, making evaluation results harder to interpret due to non-determinism.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 207, "line_end": 208, "atom_id": "ATOM-SOURCE-20260109-001-0040"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "7c9fbb8d-2eab-5a09-b3f4-b358213d9ccf", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0041", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Pass@k measures the likelihood that an agent achieves at least one correct solution within k attempts, with the score increasing as k rises.", "line_start": 213, "line_end": 214, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Pass@k measures the likelihood that an agent achieves at least one correct solut", "content": "Pass@k measures the likelihood that an agent achieves at least one correct solution within k attempts, with the score increasing as k rises.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 213, "line_end": 214, "atom_id": "ATOM-SOURCE-20260109-001-0041"}, "metadata": {"category": "concept", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "5599fff7-378d-5fca-8e1d-06bab996b62a", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0042", "source_id": "SOURCE-20260109-001", "category": "concept", "content": "Pass^k measures the probability that all k trials succeed, with the score decreasing as k increases due to the harder bar of consistent success.", "line_start": 218, "line_end": 220, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Pass^k measures the probability that all k trials succeed, with the score decrea", "content": "Pass^k measures the probability that all k trials succeed, with the score decreasing as k increases due to the harder bar of consistent success.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 218, "line_end": 220, "atom_id": "ATOM-SOURCE-20260109-001-0042"}, "metadata": {"category": "concept", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.5, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "90cf546c-d3eb-5c50-a245-42fd543e3f4b", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0043", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "To create effective test cases for AI agents, begin by converting existing manual checks, common user tasks, and reported bugs into automated tests, prioritizing based on user impact.", "line_start": 218, "line_end": 224, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "To create effective test cases for AI agents, begin by converting existing manua", "content": "To create effective test cases for AI agents, begin by converting existing manual checks, common user tasks, and reported bugs into automated tests, prioritizing based on user impact.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 218, "line_end": 224, "atom_id": "ATOM-SOURCE-20260109-001-0043"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "7f32c1ca-bd88-53ed-b510-5cdaa0a89c55", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0044", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Pass@k and Pass^k diverge as the number of trials increases; they are identical at k=1 but tell opposite stories by k=10, with Pass@k approaching 100% and Pass^k falling to 0%.", "line_start": 222, "line_end": 223, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Pass@k and Pass^k diverge as the number of trials increases; they are identical", "content": "Pass@k and Pass^k diverge as the number of trials increases; they are identical at k=1 but tell opposite stories by k=10, with Pass@k approaching 100% and Pass^k falling to 0%.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 222, "line_end": 223, "atom_id": "ATOM-SOURCE-20260109-001-0044"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.3, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "2b706aaf-ec3c-547a-960d-947f592a77fe", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0045", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "When designing tasks for AI agent evaluation, ensure they are unambiguous: two domain experts should independently reach the same pass/fail verdict, and the task should be solvable by an agent following instructions correctly.", "line_start": 226, "line_end": 230, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "When designing tasks for AI agent evaluation, ensure they are unambiguous: two d", "content": "When designing tasks for AI agent evaluation, ensure they are unambiguous: two domain experts should independently reach the same pass/fail verdict, and the task should be solvable by an agent following instructions correctly.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 226, "line_end": 230, "atom_id": "ATOM-SOURCE-20260109-001-0045"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "c64ecf4f-c451-51ba-b64a-0d6cc0227224", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0046", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "A 0% pass rate across many trials for frontier models often signals a broken task or ambiguous specification, rather than an incapable agent.", "line_start": 237, "line_end": 240, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "A 0% pass rate across many trials for frontier models often signals a broken tas", "content": "A 0% pass rate across many trials for frontier models often signals a broken task or ambiguous specification, rather than an incapable agent.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 237, "line_end": 240, "atom_id": "ATOM-SOURCE-20260109-001-0046"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "e40eae97-2e08-5c6b-bc2b-249db6ca2042", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0047", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "For each evaluation task, create a reference solution (a known-working output) to prove solvability and verify grader configuration.", "line_start": 242, "line_end": 244, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.7, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "For each evaluation task, create a reference solution (a known-working output) t", "content": "For each evaluation task, create a reference solution (a known-working output) to prove solvability and verify grader configuration.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 242, "line_end": 244, "atom_id": "ATOM-SOURCE-20260109-001-0047"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.7, 0.1, 0.1, 0.9, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "18257190-3f08-5ef3-bf9f-ea29123e7d18", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0048", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "Build balanced problem sets that test both when a behavior should occur and when it shouldn't, to avoid one-sided optimization and class-imbalanced evaluations.", "line_start": 246, "line_end": 249, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "Build balanced problem sets that test both when a behavior should occur and when", "content": "Build balanced problem sets that test both when a behavior should occur and when it shouldn't, to avoid one-sided optimization and class-imbalanced evaluations.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 246, "line_end": 249, "atom_id": "ATOM-SOURCE-20260109-001-0048"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "b21fa126-3ccd-5add-8225-526fd35d38ca", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0049", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "One-sided evaluations, such as only testing if an agent searches when it should, can lead to agents over-optimizing for that behavior (e.g., searching for almost everything).", "line_start": 247, "line_end": 249, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "One-sided evaluations, such as only testing if an agent searches when it should,", "content": "One-sided evaluations, such as only testing if an agent searches when it should, can lead to agents over-optimizing for that behavior (e.g., searching for almost everything).", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 247, "line_end": 249, "atom_id": "ATOM-SOURCE-20260109-001-0049"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "7ba4b212-53d2-521f-82b4-ca2f592e1f8a", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0050", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "Ensure the AI agent in the evaluation harness functions similarly to the production agent, and isolate each trial with a clean environment to prevent noise from shared state or infrastructure flakiness.", "line_start": 258, "line_end": 262, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "Ensure the AI agent in the evaluation harness functions similarly to the product", "content": "Ensure the AI agent in the evaluation harness functions similarly to the production agent, and isolate each trial with a clean environment to prevent noise from shared state or infrastructure flakiness.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 258, "line_end": 262, "atom_id": "ATOM-SOURCE-20260109-001-0050"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "e4adae79-b811-5fe1-9d57-47b239c0e0dc", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0051", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Unnecessary shared state between evaluation runs (e.g., leftover files, cached data) can cause correlated failures due to infrastructure flakiness rather than agent performance, or artificially inflate performance (e.g., by allowing agents to examine git history from previous trials).", "line_start": 260, "line_end": 265, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Unnecessary shared state between evaluation runs (e.g., leftover files, cached d", "content": "Unnecessary shared state between evaluation runs (e.g., leftover files, cached data) can cause correlated failures due to infrastructure flakiness rather than agent performance, or artificially inflate performance (e.g., by allowing agents to examine git history from previous trials).", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 260, "line_end": 265, "atom_id": "ATOM-SOURCE-20260109-001-0051"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "df6c1e72-480e-5929-8ebf-2c9f76e624a1", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0052", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "Do not take evaluation scores at face value until someone investigates the details of the evaluation and reviews transcripts, as issues like unfair grading or ambiguous tasks can invalidate results.", "line_start": 265, "line_end": 268, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.9, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "Do not take evaluation scores at face value until someone investigates the detai", "content": "Do not take evaluation scores at face value until someone investigates the details of the evaluation and reviews transcripts, as issues like unfair grading or ambiguous tasks can invalidate results.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 265, "line_end": 268, "atom_id": "ATOM-SOURCE-20260109-001-0052"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.9, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "9993ebcd-b903-50c5-8da0-7514bea330b5", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0053", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "Establish dedicated evaluation teams to own core infrastructure, while domain experts and product teams contribute most evaluation tasks and run the evaluations themselves to maintain a healthy evaluation suite long-term.", "line_start": 274, "line_end": 277, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.7, 0.1, 0.2, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "Establish dedicated evaluation teams to own core infrastructure, while domain ex", "content": "Establish dedicated evaluation teams to own core infrastructure, while domain experts and product teams contribute most evaluation tasks and run the evaluations themselves to maintain a healthy evaluation suite long-term.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 274, "line_end": 277, "atom_id": "ATOM-SOURCE-20260109-001-0053"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.3, 0.7, 0.1, 0.2, 0.8, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "fc5c2390-9055-530e-87fe-276a5f8db751", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0054", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "For AI product teams, owning and iterating on evaluations should be as routine as maintaining unit tests.", "line_start": 279, "line_end": 280, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "For AI product teams, owning and iterating on evaluations should be as routine a", "content": "For AI product teams, owning and iterating on evaluations should be as routine as maintaining unit tests.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 279, "line_end": 280, "atom_id": "ATOM-SOURCE-20260109-001-0054"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "b034c29c-5eb1-5eed-928f-df25768ac8fc", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0055", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "Practice eval-driven development: build evaluations to define planned capabilities before agents can fulfill them, then iterate until the agent performs well.", "line_start": 285, "line_end": 287, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.4, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "Practice eval-driven development: build evaluations to define planned capabiliti", "content": "Practice eval-driven development: build evaluations to define planned capabilities before agents can fulfill them, then iterate until the agent performs well.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 285, "line_end": 287, "atom_id": "ATOM-SOURCE-20260109-001-0055"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.4, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "9945c25d-2492-5ee8-bcbe-e8411ee47fb9", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0056", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Automated evaluations can be run against an agent in thousands of tasks without deploying to production or affecting real users, but they are only one method for understanding agent performance.", "line_start": 294, "line_end": 296, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Automated evaluations can be run against an agent in thousands of tasks without", "content": "Automated evaluations can be run against an agent in thousands of tasks without deploying to production or affecting real users, but they are only one method for understanding agent performance.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 294, "line_end": 296, "atom_id": "ATOM-SOURCE-20260109-001-0056"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.7, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "49c8b2cb-d713-5f0e-b673-501d59c02b26", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0057", "source_id": "SOURCE-20260109-001", "category": "framework", "content": "A complete picture of AI agent performance includes automated evals, production monitoring, user feedback, A/B testing, manual transcript review, and systematic human evaluation.", "line_start": 296, "line_end": 298, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Framework", "name": "A complete picture of AI agent performance includes automated evals, production", "content": "A complete picture of AI agent performance includes automated evals, production monitoring, user feedback, A/B testing, manual transcript review, and systematic human evaluation.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 296, "line_end": 298, "atom_id": "ATOM-SOURCE-20260109-001-0057"}, "metadata": {"category": "framework", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "9af22403-578d-59eb-88c9-cb4c5f58a9a7", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0058", "source_id": "SOURCE-20260109-001", "category": "analogy", "content": "Like the Swiss Cheese Model from safety engineering, no single evaluation method catches every issue; combining multiple methods ensures failures that slip through one layer are caught by another.", "line_start": 317, "line_end": 319, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.7, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Concept", "name": "Like the Swiss Cheese Model from safety engineering, no single evaluation method", "content": "Like the Swiss Cheese Model from safety engineering, no single evaluation method catches every issue; combining multiple methods ensures failures that slip through one layer are caught by another.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 317, "line_end": 319, "atom_id": "ATOM-SOURCE-20260109-001-0058"}, "metadata": {"category": "analogy", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.7, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "956e17a8-be12-5ad8-b428-841df327cfcf", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0059", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Promptfoo is a lightweight, flexible, and open-source framework that uses declarative YAML configuration for prompt testing, offering assertion types from string matching to LLM-as-judge rubrics.", "line_start": 318, "line_end": 319, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Promptfoo is a lightweight, flexible, and open-source framework that uses declar", "content": "Promptfoo is a lightweight, flexible, and open-source framework that uses declarative YAML configuration for prompt testing, offering assertion types from string matching to LLM-as-judge rubrics.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 318, "line_end": 319, "atom_id": "ATOM-SOURCE-20260109-001-0059"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "46b9d460-9aef-5b84-ab9e-b0343e5ae382", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0060", "source_id": "SOURCE-20260109-001", "category": "praxis_hook", "content": "The most effective teams combine automated evaluations for fast iteration, production monitoring for ground truth, and periodic human review for calibration.", "line_start": 321, "line_end": 322, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.9, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "PraxisHook", "name": "The most effective teams combine automated evaluations for fast iteration, produ", "content": "The most effective teams combine automated evaluations for fast iteration, production monitoring for ground truth, and periodic human review for calibration.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 321, "line_end": 322, "atom_id": "ATOM-SOURCE-20260109-001-0060"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.9, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "f5849da0-43bc-5c64-b1ed-9f85ba543e80", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0061", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Braintrust is a platform that integrates offline evaluation with production observability and experiment tracking, suitable for teams needing both development iteration and production quality monitoring.", "line_start": 321, "line_end": 322, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Braintrust is a platform that integrates offline evaluation with production obse", "content": "Braintrust is a platform that integrates offline evaluation with production observability and experiment tracking, suitable for teams needing both development iteration and production quality monitoring.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 321, "line_end": 322, "atom_id": "ATOM-SOURCE-20260109-001-0061"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "9d750df3-f8a7-5ae8-9b0e-9e9a3d59335e", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0062", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Braintrust's `autoevals` library includes pre-built scorers for common dimensions like factuality and relevance.", "line_start": 322, "line_end": 323, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Braintrust's `autoevals` library includes pre-built scorers for common dimension", "content": "Braintrust's `autoevals` library includes pre-built scorers for common dimensions like factuality and relevance.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 322, "line_end": 323, "atom_id": "ATOM-SOURCE-20260109-001-0062"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "58f3172b-9d57-5f05-aa06-254b6708f904", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0063", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "LangSmith provides tracing, offline and online evaluations, and dataset management, with strong integration into the LangChain ecosystem.", "line_start": 325, "line_end": 325, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "LangSmith provides tracing, offline and online evaluations, and dataset manageme", "content": "LangSmith provides tracing, offline and online evaluations, and dataset management, with strong integration into the LangChain ecosystem.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 325, "line_end": 325, "atom_id": "ATOM-SOURCE-20260109-001-0063"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "4b57d318-4898-568e-be49-37230fc29761", "timestamp": "2026-02-24T00:39:03.627739+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260109-001-0064", "source_id": "SOURCE-20260109-001", "category": "claim", "content": "Langfuse offers capabilities similar to LangSmith as a self-hosted open-source alternative for teams with data residency requirements.", "line_start": 327, "line_end": 327, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260109-001", "entity_type": "Claim", "name": "Langfuse offers capabilities similar to LangSmith as a self-hosted open-source a", "content": "Langfuse offers capabilities similar to LangSmith as a self-hosted open-source alternative for teams with data residency requirements.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260109-001", "line_start": 327, "line_end": 327, "atom_id": "ATOM-SOURCE-20260109-001-0064"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
