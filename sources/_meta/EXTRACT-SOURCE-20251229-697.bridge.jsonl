{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "526cfce7-8c34-54ab-a52f-ca0f0300c152", "timestamp": "2026-02-24T00:54:36.727138+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-697-0001", "source_id": "SOURCE-20251229-697", "category": "claim", "content": "Current Large Language Models (LLMs) struggle with niche, \"long-tail\" knowledge that falls outside their training data or within knowledge cutoffs.", "line_start": 19, "line_end": 21, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-697", "entity_type": "Claim", "name": "Current Large Language Models (LLMs) struggle with niche, \"long-tail\" knowledge", "content": "Current Large Language Models (LLMs) struggle with niche, \"long-tail\" knowledge that falls outside their training data or within knowledge cutoffs.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-697", "line_start": 19, "line_end": 21, "atom_id": "ATOM-SOURCE-20251229-697-0001"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "1ca511d7-501a-5631-a358-33a414a9c31c", "timestamp": "2026-02-24T00:54:36.727138+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-697-0002", "source_id": "SOURCE-20251229-697", "category": "praxis_hook", "content": "A third paradigm for knowledge injection into LLMs, beyond massive context windows and Retrieval Augmented Generation (RAG), is \"training things into weights,\" which involves efficiently injecting specific knowledge directly into model parameters.", "line_start": 24, "line_end": 27, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.8, 0.3, 0.2, 0.6, 0.7, 0.5], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-697", "entity_type": "PraxisHook", "name": "A third paradigm for knowledge injection into LLMs, beyond massive context windo", "content": "A third paradigm for knowledge injection into LLMs, beyond massive context windows and Retrieval Augmented Generation (RAG), is \"training things into weights,\" which involves efficiently injecting specific knowledge directly into model parameters.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-697", "line_start": 24, "line_end": 27, "atom_id": "ATOM-SOURCE-20251229-697-0002"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.8, 0.3, 0.2, 0.6, 0.7, 0.5], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "f34ad74d-5e80-5982-a150-56f19f453ee2", "timestamp": "2026-02-24T00:54:36.727138+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-697-0003", "source_id": "SOURCE-20251229-697", "category": "concept", "content": "The \"Long Tail\" Knowledge Problem refers to the critical failure mode in current LLMs where they excel at general knowledge but fail catastrophically at niche, specific tasks (e.g., optimizing an AMD GPU kernel or knowing terms of a specific partnership).", "line_start": 36, "line_end": 39, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-697", "entity_type": "Concept", "name": "The \"Long Tail\" Knowledge Problem refers to the critical failure mode in current", "content": "The \"Long Tail\" Knowledge Problem refers to the critical failure mode in current LLMs where they excel at general knowledge but fail catastrophically at niche, specific tasks (e.g., optimizing an AMD GPU kernel or knowing terms of a specific partnership).", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-697", "line_start": 36, "line_end": 39, "atom_id": "ATOM-SOURCE-20251229-697-0003"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.6, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "fd1b314a-af50-53a6-81f7-77c9c9381fa5", "timestamp": "2026-02-24T00:54:36.727138+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-697-0004", "source_id": "SOURCE-20251229-697", "category": "claim", "content": "The \"Long Tail\" Knowledge Problem arises because these niche tasks are either outside the LLM's training data, subject to knowledge cutoffs, or require private data.", "line_start": 40, "line_end": 41, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-697", "entity_type": "Claim", "name": "The \"Long Tail\" Knowledge Problem arises because these niche tasks are either ou", "content": "The \"Long Tail\" Knowledge Problem arises because these niche tasks are either outside the LLM's training data, subject to knowledge cutoffs, or require private data.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-697", "line_start": 40, "line_end": 41, "atom_id": "ATOM-SOURCE-20251229-697-0004"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "d9e80e7a-233a-55e2-b424-91b39cb63b85", "timestamp": "2026-02-24T00:54:36.727138+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-697-0005", "source_id": "SOURCE-20251229-697", "category": "claim", "content": "No amount of prompting or prompt engineering can force an LLM to know facts it does not have stored.", "line_start": 42, "line_end": 43, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-697", "entity_type": "Claim", "name": "No amount of prompting or prompt engineering can force an LLM to know facts it d", "content": "No amount of prompting or prompt engineering can force an LLM to know facts it does not have stored.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-697", "line_start": 42, "line_end": 43, "atom_id": "ATOM-SOURCE-20251229-697-0005"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "aad8e0ca-90ae-5d31-bfec-a4d616c4e3e5", "timestamp": "2026-02-24T00:54:36.727138+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-697-0006", "source_id": "SOURCE-20251229-697", "category": "framework", "content": "There are three paradigms for knowledge injection into LLMs: Full Context (stuffing all relevant data into the prompt), RAG (Retrieval Augmented Generation, retrieving only relevant chunks), and Training into Weights (injecting knowledge directly into the model's parameters).", "line_start": 46, "line_end": 51, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.5, 0.4, 0.2, 0.2, 0.6, 0.6], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-697", "entity_type": "Framework", "name": "There are three paradigms for knowledge injection into LLMs: Full Context (stuff", "content": "There are three paradigms for knowledge injection into LLMs: Full Context (stuffing all relevant data into the prompt), RAG (Retrieval Augmented Generation, retrieving only relevant chunks), and Training into Weights (injecting knowledge directly into the model's parameters).", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-697", "line_start": 46, "line_end": 51, "atom_id": "ATOM-SOURCE-20251229-697-0006"}, "metadata": {"category": "framework", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.5, 0.4, 0.2, 0.2, 0.6, 0.6], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "22ce8258-9c75-530d-8313-95ac2d7c5362", "timestamp": "2026-02-24T00:54:36.727138+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-697-0007", "source_id": "SOURCE-20251229-697", "category": "claim", "content": "The self-attention mechanism in Transformers has a quadratic compute cost because it requires every token to look at every other token.", "line_start": 55, "line_end": 56, "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.1, 0.9], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-697", "entity_type": "Claim", "name": "The self-attention mechanism in Transformers has a quadratic compute cost becaus", "content": "The self-attention mechanism in Transformers has a quadratic compute cost because it requires every token to look at every other token.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-697", "line_start": 55, "line_end": 56, "atom_id": "ATOM-SOURCE-20251229-697-0007"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "evidence", "tension_vector": [0.1, 0.9, 0.1, 0.1, 0.1, 0.9], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "f6c8638e-e0b7-5b26-a51a-bc494ca1f933", "timestamp": "2026-02-24T00:54:36.727138+00:00", "payload": {"atom_id": "ATOM-SOURCE-20251229-697-0008", "source_id": "SOURCE-20251229-697", "category": "claim", "content": "Increasing context length significantly impacts LLM output latency: 1,000 tokens of context allow 10,000 tokens per second output, while 128k tokens of context reduce output to 130 tokens per second.", "line_start": 57, "line_end": 59, "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.4, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20251229-697", "entity_type": "Claim", "name": "Increasing context length significantly impacts LLM output latency: 1,000 tokens", "content": "Increasing context length significantly impacts LLM output latency: 1,000 tokens of context allow 10,000 tokens per second output, while 128k tokens of context reduce output to 130 tokens per second.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20251229-697", "line_start": 57, "line_end": 59, "atom_id": "ATOM-SOURCE-20251229-697-0008"}, "metadata": {"category": "claim", "chaperone": {"context_type": "anecdote", "argument_role": "evidence", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.4, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
