# Extraction: SOURCE-20260214-019

**Source**: `SOURCE-20260214-x-thread-rryssf_-stanford_and_caltech_researchers_just_published.md`
**Atoms extracted**: 4
**Categories**: claim, framework, praxis_hook

---

## Claim (2)

### ATOM-SOURCE-20260214-019-0001
**Lines**: 2-3
**Context**: consensus / claim
**Tension**: novelty=0.80, consensus_pressure=0.50, contradiction_load=0.10, speculation_risk=0.10, actionability=0.20, epistemic_stability=0.70

> Stanford and Caltech researchers have published the first comprehensive taxonomy of how Large Language Models (LLMs) fail at reasoning.

### ATOM-SOURCE-20260214-019-0003
**Lines**: 10-10
**Context**: consensus / claim
**Tension**: novelty=0.70, consensus_pressure=0.60, contradiction_load=0.10, speculation_risk=0.20, actionability=0.70, epistemic_stability=0.70

> The research provides a structured perspective on systemic weaknesses in LLM reasoning, offering insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities.

## Framework (1)

### ATOM-SOURCE-20260214-019-0002
**Lines**: 10-10
**Context**: method / claim
**Tension**: novelty=0.90, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.10, actionability=0.60, epistemic_stability=0.80

> A novel categorization framework for LLM reasoning failures distinguishes reasoning into embodied and non-embodied types, with non-embodied further subdivided into informal (intuitive) and formal (logical) reasoning. Failures are classified along a complementary axis into three types: fundamental failures (intrinsic to LLM architectures), application-specific limitations, and robustness issues (inconsistent performance across minor variations).

## Praxis Hook (1)

### ATOM-SOURCE-20260214-019-0004
**Lines**: 10-10
**Context**: method / evidence
**Tension**: novelty=0.10, consensus_pressure=0.80, contradiction_load=0.00, speculation_risk=0.00, actionability=0.90, epistemic_stability=0.90

> A comprehensive collection of research works on LLM reasoning failures is available as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures.
