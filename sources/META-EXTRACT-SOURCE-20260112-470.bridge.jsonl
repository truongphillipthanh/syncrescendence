{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "b59cd338-7b39-5ce7-972c-e0e44099e218", "timestamp": "2026-02-24T00:54:20.934052+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260112-470-0001", "source_id": "SOURCE-20260112-470", "category": "concept", "content": "The \"holy grail\" problem in Large Language Models (LLMs) is the trade-off between speed and quality.", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260112-470", "entity_type": "Concept", "name": "The \"holy grail\" problem in Large Language Models (LLMs) is the trade-off betwee", "content": "The \"holy grail\" problem in Large Language Models (LLMs) is the trade-off between speed and quality.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260112-470", "line_start": 10, "line_end": 10, "atom_id": "ATOM-SOURCE-20260112-470-0001"}, "metadata": {"category": "concept", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "9f2ab5e8-8990-5bb2-864a-6b3ecf04fd90", "timestamp": "2026-02-24T00:54:20.934052+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260112-470-0002", "source_id": "SOURCE-20260112-470", "category": "claim", "content": "Standard Autoregression (AR) in LLMs provides high quality but is slow.", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260112-470", "entity_type": "Claim", "name": "Standard Autoregression (AR) in LLMs provides high quality but is slow.", "content": "Standard Autoregression (AR) in LLMs provides high quality but is slow.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260112-470", "line_start": 10, "line_end": 10, "atom_id": "ATOM-SOURCE-20260112-470-0002"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "35547956-553d-5c35-b642-ed7078182e6d", "timestamp": "2026-02-24T00:54:20.934052+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260112-470-0003", "source_id": "SOURCE-20260112-470", "category": "claim", "content": "Pure Diffusion models are fast but often lack coherence.", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260112-470", "entity_type": "Claim", "name": "Pure Diffusion models are fast but often lack coherence.", "content": "Pure Diffusion models are fast but often lack coherence.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260112-470", "line_start": 10, "line_end": 10, "atom_id": "ATOM-SOURCE-20260112-470-0003"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.1, 0.8, 0.1, 0.1, 0.1, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "650a2af5-5861-505c-a5eb-90a300a18971", "timestamp": "2026-02-24T00:54:20.934052+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260112-470-0004", "source_id": "SOURCE-20260112-470", "category": "concept", "content": "TiDAR (Think in Diffusion, Talk in Autoregression) is a hybrid approach that combines the strengths of Diffusion and Autoregression to maximize GPU efficiency.", "line_start": 10, "line_end": 10, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.2, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260112-470", "entity_type": "Concept", "name": "TiDAR (Think in Diffusion, Talk in Autoregression) is a hybrid approach that com", "content": "TiDAR (Think in Diffusion, Talk in Autoregression) is a hybrid approach that combines the strengths of Diffusion and Autoregression to maximize GPU efficiency.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260112-470", "line_start": 10, "line_end": 10, "atom_id": "ATOM-SOURCE-20260112-470-0004"}, "metadata": {"category": "concept", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.2, 0.6, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "a69b5b05-7712-5249-806f-ff4376a5f8ff", "timestamp": "2026-02-24T00:54:20.934052+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260112-470-0005", "source_id": "SOURCE-20260112-470", "category": "claim", "content": "LLM decoding is currently \"memory bound.\"", "line_start": 17, "line_end": 17, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260112-470", "entity_type": "Claim", "name": "LLM decoding is currently \"memory bound.\"", "content": "LLM decoding is currently \"memory bound.\"", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260112-470", "line_start": 17, "line_end": 17, "atom_id": "ATOM-SOURCE-20260112-470-0005"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.7, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "a1d9dbf4-2a0a-5b03-b4ab-2ef6b8c8370a", "timestamp": "2026-02-24T00:54:20.934052+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260112-470-0006", "source_id": "SOURCE-20260112-470", "category": "praxis_hook", "content": "TiDAR drafts in parallel and verifies sequentially.", "line_start": 19, "line_end": 19, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.2, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260112-470", "entity_type": "PraxisHook", "name": "TiDAR drafts in parallel and verifies sequentially.", "content": "TiDAR drafts in parallel and verifies sequentially.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260112-470", "line_start": 19, "line_end": 19, "atom_id": "ATOM-SOURCE-20260112-470-0006"}, "metadata": {"category": "praxis_hook", "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.7, 0.3, 0.1, 0.2, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
{"record_type": "source_atom", "schema_version": "1.0.0", "uuid": "9f159075-237b-51fe-9234-72ed876d0694", "timestamp": "2026-02-24T00:54:20.934052+00:00", "payload": {"atom_id": "ATOM-SOURCE-20260112-470-0007", "source_id": "SOURCE-20260112-470", "category": "claim", "content": "TiDAR models (1.5B & 8B) are significantly improving the speed-to-quality curve for LLMs.", "line_start": 21, "line_end": 21, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.8, 0.2, 0.1, 0.2, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}, "source_id": "SOURCE-20260112-470", "entity_type": "Claim", "name": "TiDAR models (1.5B & 8B) are significantly improving the speed-to-quality curve", "content": "TiDAR models (1.5B & 8B) are significantly improving the speed-to-quality curve for LLMs.", "confidence": 1.0, "provenance": {"source_id": "SOURCE-20260112-470", "line_start": 21, "line_end": 21, "atom_id": "ATOM-SOURCE-20260112-470-0007"}, "metadata": {"category": "claim", "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.8, 0.2, 0.1, 0.2, 0.7, 0.7], "opposes_atom_ids": []}, "extensions": {}}}
