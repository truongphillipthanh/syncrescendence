# Extraction: SOURCE-20260214-001

**Source**: `SOURCE-20260214-x-article-daviddondrej1-this_new_ai_benchmark_changes_everything.md`
**Atoms extracted**: 8
**Categories**: claim, concept

---

## Claim (5)

### ATOM-SOURCE-20260214-001-0001
**Lines**: 5-7
**Context**: consensus / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.20, speculation_risk=0.20, actionability=0.40, epistemic_stability=0.70

> A new benchmark called SWE-rebench has revealed that many Chinese AI companies have been optimizing their models on popular evaluations and benchmarks, rather than genuinely improving their models.

### ATOM-SOURCE-20260214-001-0003
**Lines**: 25-30
**Context**: consensus / evidence
**Tension**: novelty=0.50, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.20, actionability=0.30, epistemic_stability=0.70

> The original SWE-bench, a popular coding benchmark, became saturated because its questions were public for so long, leading labs (especially Chinese labs) to design training data to overfit on these specific problems.

### ATOM-SOURCE-20260214-001-0005
**Lines**: 39-50
**Context**: consensus / evidence
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.20, speculation_risk=0.10, actionability=0.40, epistemic_stability=0.80

> On SWE-rebench, Claude Code with Opus 4.6 scored 52.9% and Claude Opus 4.6 scored 51.7%, while Chinese models like MiniMax M2.5 dropped significantly to 39.6% from a reported 80.2% on the original SWE-bench.

### ATOM-SOURCE-20260214-001-0006
**Lines**: 51-54
**Context**: consensus / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.20, speculation_risk=0.20, actionability=0.40, epistemic_stability=0.80

> The performance gap between top Western AI models (Anthropic, OpenAI, Google) and Chinese models on SWE-rebench, which tests unseen problems, indicates that Chinese models were not comparable to Western models despite similar scores on older, saturated benchmarks.

### ATOM-SOURCE-20260214-001-0007
**Lines**: 58-65
**Context**: consensus / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.30, speculation_risk=0.30, actionability=0.20, epistemic_stability=0.70

> Chinese AI labs, having fewer resources like compute, talent, and data infrastructure compared to Anthropic and OpenAI, resort to 'cheating' by overfitting their models on popular, public benchmarks to appear world-class.

## Concept (3)

### ATOM-SOURCE-20260214-001-0002
**Lines**: 13-20
**Context**: method / claim
**Tension**: novelty=0.60, consensus_pressure=0.40, contradiction_load=0.10, speculation_risk=0.20, actionability=0.30, epistemic_stability=0.80

> A 'benchmark scam' in AI occurs when models are trained to overfit on public test answers, allowing them to score well without actually becoming smarter or more capable.

### ATOM-SOURCE-20260214-001-0004
**Lines**: 31-35
**Context**: method / claim
**Tension**: novelty=0.80, consensus_pressure=0.20, contradiction_load=0.10, speculation_risk=0.10, actionability=0.60, epistemic_stability=0.80

> SWE-rebench is a new benchmark that addresses the overfitting problem by pulling fresh GitHub tasks from recent repositories, ensuring models encounter new problems they haven't seen in training, while maintaining similar difficulty and format.

### ATOM-SOURCE-20260214-001-0008
**Lines**: 70-71
**Context**: hypothesis / claim
**Tension**: novelty=0.70, consensus_pressure=0.30, contradiction_load=0.10, speculation_risk=0.20, actionability=0.50, epistemic_stability=0.80

> A 'genuinely powerful general model' performs well across various tasks (coding, writing, analysis, reasoning) even in new situations, unlike models that are hyper-trained on specific benchmarks and collapse when faced with unseen problems.
