---
url: https://x.com/itsjoaki/status/2023791729141260670
author: "Joaki (@itsjoaki)"
captured_date: 2026-02-20
id: SOURCE-20260217-007
original_filename: "20260217-x_article-the_openclaw_problem_why_99_percent_of_ai_agent_content_on_x_is_fake-@itsjoaki.md"
status: triaged
platform: x
format: article
creator: itsjoaki
signal_tier: paradigm
topics: [ai-agents, media-criticism, entrepreneurship, best-practices, security]
teleology: contextualize
notebooklm_category: ai-agents
aliases: ["itsjoaki - 99% of AI agent content on X is fake"]
synopsis: "Extensive critique of the OpenClaw hype cycle on X, arguing 99% of agent posts are engagement farming with no real output. Identifies three red flags (no specific output, no metrics, vague use cases) and contrasts with real builders getting results: automated directory submissions, TikTok revenue pipelines, car negotiations, bug-fixing pipelines. Warns the hype-to-backlash cycle could kill momentum for genuinely transformative technology."
key_insights:
  - "Three red flags of fake agent content: no specific output shown, no metrics or ROI, and vague or contrived use cases — screenshots of running processes are not results"
  - "Real agent automation is boring, specific, and measurable: directory submissions went from 15-20 hours to 2 hours at 70-80% success rate"
  - "The best agent workflows are invisible — they run in the background and their value is measured in hours saved and dollars earned, not in likes and reposts"
---
# The OpenClaw Problem: Why 99% of AI Agent Content on X Is Fake
![The OpenClaw Problem](image) (Description: A cinematic grayscale illustration depicting a large, menacing AI agent claw hovering above a cityscape. The claw is surrounded by various symbols including an X logo, charts, a smartphone, and human figures in business attire. The artwork conveys the juxtaposition of AI automation hype versus reality, with dramatic clouds and an industrial aesthetic.)
## Everyone's posting screenshots. Nobody's posting results.
Open your X feed right now. Go ahead, I'll wait.
If you follow anyone in tech, AI, or indie hacking, I guarantee you'll see at least three posts about OpenClaw within the first 30 seconds of scrolling. Someone deployed 10 agents working in parallel. Someone else has a fleet of autonomous workers handling my entire business. Another person is showing a screenshot of multiple browser windows open simultaneously with a caption that reads something like the future of work is here and I'm never going back.
It looks incredible. It sounds revolutionary. And most of it is complete bullshit.
I'm not saying this as someone who hates OpenClaw, I'm saying this as someone who uses it every single day and genuinely believes this is the future of how we'll interact with technology. I went from being underwhelmed (summarize emails? add calendar reminder? cool, I have Claude for that) to genuinely obsessed once the real power clicked: this isn't passive chat, it's an autonomous agent living on your machine, clawing into your actual tools with heartbeat scheduling. Privacy first, local first, no cloud snooping. I keep testing nonstop to discover new use cases because I'm convinced the potential is massive.
But precisely because I'm bullish on this technology, the gap between what people are posting on X and what is actually possible right now needs to be called out.
The hype isn't just annoying, it's actively harmful, to the people trying to build real things, and to OpenClaw itself. If we overpromise now, the backlash will kill the momentum of something that genuinely deserves to succeed.
## The anatomy of a fake agent post
Let me break down the formula, because once you see it, you can't unsee it:
**Step 1:** Open OpenClaw.
**Step 2:** Launch multiple agents doing extremely basic tasks, things like opening Google, navigating to a website, or filling out a simple form.
**Step 3:** Take a screenshot showing how those work in an office you built for them.
**Step 4:** Write a caption implying this is some kind of autonomous empire. Throw in words like workers, or army. Add a mind-blown emoji. Maybe mention that you're never hiring again, or building the next 1B dollar solo company.
**Step 5:** Watch the engagement pour in.
That's it. That's the entire playbook. And it works incredibly well for engagement, which is exactly why people keep doing it. The posts get thousands of likes, hundreds of reposts, and dozens of comments from people asking "how did you do this?" or saying "this changes everything."
But here's the thing nobody asks: what was the actual output?
Not the screenshot. Not the vibes. What did those 10 agents actually produce? What task did they complete? How much time did it save? How much money did it generate or save? What was the error rate? Did the output actually work, or did it require manual cleanup?
These questions are never answered because the answers would be embarrassing. The truth is that most of these demos are the equivalent of opening 10 tabs in Chrome and calling yourself a multitasker. The agents aren't doing meaningful work. They're performing prescripted actions that look impressive on a screen but produce nothing of value.
## Why the hype machine is so effective
I get it. I really do. AI agents are genuinely one of the most exciting developments in tech right now, and browser automation specifically has enormous potential. When you see a browser navigating itself, clicking buttons, filling out forms, and making decisions it genuinely feels like magic. It triggers something primal in us, the same feeling we got when we first saw ChatGPT write coherent paragraphs or when DALL-E generated its first images.
The visual aspect is what makes agent content so powerful on social media. Unlike most AI tools, you can literally watch agents work. It's a built in visual spectacle. A screenshot of 10 browser windows operating simultaneously is inherently more impressive than a screenshot of an API response, even if the API response is doing something 100 times more valuable.
And then there's the narrative. "I replaced my entire team with AI agents" is the perfect X engagement bait because it simultaneously triggers curiosity (how?), fear (am I going to lose my job?), and aspiration (I want to do that too). It's the ultimate attention triple threat.
Social media rewards the demonstration, not the outcome. Nobody goes viral posting "I automated a tedious 15-hour workflow and it now takes 2 hours with a 92% accuracy rate." That's boring. (well, that is actually what I would read myself). But "I have 10 AI agents running my business while I sleep", now that's a story people want to engage with.
So the incentive structure is completely broken. The people creating the most noise about AI agents are optimizing for engagement, not for truth. And the people actually building useful things with agents are too busy actually using them to create cinematic screenshots.
## What real agent automation looks like (spoiler: it's boring)
Let me tell you about my actual experience with browser automation, because it illustrates the massive gap between the hype and reality.
I run a couple of SaaS products. One of the most tedious but important tasks in the indie SaaS world is submitting your product to directories. There are hundreds of them, Product Hunt, BetaList, SaaSHub, AlternativeTo, and a long tail of niche directories that most people have never heard of but that still drive traffic and backlinks.
Doing this manually is brutal. Each directory has a different submission process. Some want screenshots, some want descriptions of specific lengths, some require you to pick categories from a dropdown, some need you to verify via email. To properly submit to 100+ directories, you're looking at 15 to 20 hours of mind numbing, repetitive work.
This is a perfect use case for automation using OpenClaw. Not because it's flashy, but because it's a well defined, repetitive task with clear success criteria. Either the submission goes through or it doesn't. The inputs are consistent (product name, description, URL, screenshots), and the value is measurable (backlinks, referral traffic, domain authority).
So I built an automated workflow. And here's the honest truth about what that looks like:
It took time to set up. Not the "15 minutes to production" that the viral posts imply. I'm talking hours of configuring agents to handle different directory formats, building error handling for when sites change their layouts, creating retry logic for failed submissions, and testing everything extensively.
It doesn't work perfectly. Some directories have CAPTCHAs. Some have unusual form structures that trip up the agent. Some require human verification steps that can't be automated. The realistic success rate is maybe 70-80%, which means you still need to manually handle the rest.
It's not glamorous. There's no dramatic screenshot of 10 agents working simultaneously. It's a script that runs, processes submissions one by one (because running them in parallel often gets you flagged as spam), and logs successes and failures to a spreadsheet.
But here's the thing, it works. Those 15 to 20 hours became about 2 hours of setup plus monitoring time. The backlinks are real. The traffic is real. The SEO benefit is real. That's hundreds of dollars of value from something that cost me nothing but time and a bit of API credits.
That's what a real use case looks like. It's specific, measurable, boring, and valuable. It's everything that a viral X post is not.
## I tried the hype myself. It didn't go well.
I want to be honest here because I'd be a hypocrite if I pretended I never fell for the hype myself.
I spent 2 full days setting up 5 autonomous AI agents to build AgentWars for me. The idea was cool: specialized agents, each with their own role, working together to ship a product. The reality? They fought over git credentials, messaged themselves in loops, and coded in the wrong directories. I ended up shipping more in 1 hour solo than they did in 48 hours combined.
I even posted a cyberpunk city screenshot of my 5 agents. It looked amazing. Three days later, everything was still breaking and there was no real use case. I was just trying to get them to work at all. That post was engagement farming. My directory submission workflow that nobody saw? That's actual automation.
The difference is painful once you admit it to yourself. But it also taught me where the real value is, and now I'm testing better use cases every day because of that failure.
## The three red flags of fake agent content
After watching this space for months, I've identified three reliable signals that an agent post is performance rather than substance:
**Red flag #1: No specific output is shown.**
The post shows agents running but never shows what they produced. If someone claims their agents wrote 50 blog posts, where are the blog posts? If they say agents researched 100 competitors, where's the research document? If agents handled customer support, what was the resolution rate? Screenshots of running processes are not results.
**Red flag #2: No metrics, before and after, or ROI.**
Real automation has measurable impact. I can tell you exactly how many hours my directory submission process used to take and how many it takes now. I can show the before and after traffic data. If someone can't quantify the impact of their "agent army," they probably didn't build one.
**Red flag #3: The use case is vague or clearly contrived.**
"My agents are handling everything", what is everything? "I automated my entire workflow", which workflow? "10 agents are working on my business", doing what, specifically? Vagueness is the hallmark of fabrication. Real builders talk in specifics because they have specifics to share.
When you start filtering X content through these three criteria, the majority of agent posts immediately collapse. What you're left with is a handful of people building genuinely interesting things, and those people are almost always quieter, more specific, and less hyperbolic than the engagement farmers.
## The real ones: people actually getting results
Now let me talk about the part that gets me excited every day, because this is why I keep testing OpenClaw nonstop.
OpenClaw is genuinely impressive technology. Peter Steinberger built something remarkable, so remarkable that OpenAI just hired him. The tool is the real deal. The content ecosystem around it is the problem. And when you filter out the noise, there are people doing things that make you realize we're at the very beginning of something massive.
Let me give credit where it's due.
### Oliver Henry and Larry (@oliverhenry)
This is probably the best example of OpenClaw done right. Larry isn't a demo, Larry is a revenue generating content machine. Oliver configured his OpenClaw agent to fully automate his TikTok content pipeline, from ideation to creation to publishing. The result? Millions of views in a single week. Over 200k views in 24 hours. Almost $4,000 earned in one day. (I think was because of Bags fees) New paying subscribers coming in while Oliver was literally decorating his home office.
The key detail that separates Oliver from the hype crowd: he shares specific numbers. Views, followers, revenue, timeline. You can evaluate his claims because he gives you the data to do so. He even cowrote his step by step guide with Larry. That's not a screenshot of browsers opening. That's a business running on autopilot with receipts.
### Automated slide decks from meeting transcripts
There's a user (@jsundlo) who set up an OpenClaw agent that checks upcoming meetings, reviews past transcripts, and auto-generates PowerPoint presentations with relevant images and templates, a full day before the meeting. You review and give feedback via Telegram. That's an agent doing 2-3 hours of prep work that nobody wants to do, running completely on autopilot. No screenshot needed, the slides speak for themselves.
### The car negotiation
Someone's OpenClaw agent negotiated $4,200 off a car purchase over email while they slept. The agent dealt with multiple dealers, compared offers, and played them against each other. A multi-step negotiation with real financial outcome that happened autonomously overnight.
### Bug fixing before users notice
A developer set up a pipeline where Sentry alerts trigger OpenClaw, which analyzes the bug, uses Codex to create a pull request, and posts a Slack update, all before any human even knows there's an issue. Nate Liason documented this entire workflow. That's not a demo, that's infrastructure.
### Groceries on WhatsApp
Someone's cleaning lady sends a WhatsApp message saying they need supplies. OpenClaw reads it, logs into the supermarket website using stored credentials from 1Password, handles MFA via an iMessage bridge, and places the order. From text message to shopping cart, zero human involvement. Boring. Invisible. Incredibly useful.
### Research and trend monitoring
This is honestly one of the most underrated use cases right now, and the one I'm personally getting the most value from. OpenClaw is ridiculously good at scanning Reddit, X, Hacker News, RSS feeds, competitor channels (whatever sources you define) and delivering a curated daily brief to your Telegram or WhatsApp every morning. There are people running nightly market summaries, competitor watch reports, and content opportunity scans while they sleep. One user has his agent aggregate quality scored tech news from 109+ sources automatically. Another runs a "Last 30 Days" skill that mines Reddit and X for real pain points and recurring complaints, basically automated product research. This isn't flashy, nobody's screenshotting their morning Telegram brief for engagement, but it's saving hours of manual scrolling every single day. I use it for tracking what people are saying about my products, monitoring competitor moves, and spotting content trends before they blow up. It's the closest thing to having a research assistant that never sleeps.
Notice the pattern? None of these involved a cinematic screenshot of 10 browsers running simultaneously. The best agent workflows are invisible. They run in the background while you do other things. And their value is measured in hours saved, dollars earned, or problems prevented, not in likes and reposts.
## What agents are actually good at (and what they're not)
If you study the use cases that actually work, a clear pattern emerges.
### Agents thrive on well-defined, repetitive tasks with clear success criteria.
Directory submissions, content publishing pipelines, meeting prep, inbox triage, form filling, data extraction from known page structures, and research and monitoring. That last one deserves special emphasis because it's where most people should start. Setting up an agent to scan specific sources every night and deliver a summary every morning is low-risk, high-reward, and immediately useful. Either the submission went through or it didn't. Either the slides were generated or they weren't. Either your morning brief arrived with relevant intel or it didn't. There's no ambiguity about what "done" looks like.
### Agents work when partial failure is acceptable.
Oliver's content pipeline doesn't need 100% success rate to be valuable. If 3 out of 5 TikToks perform well, that's still millions of views he didn't have to work for. If my directory submissions succeed on 80% of sites, I still saved 12+ hours. The automation doesn't need to be perfect, it just needs to be better than doing everything manually.
### Agents work when they augment humans, not replace them.
The slides agent generates the deck, but @jsundlo reviews it and gives feedback. Larry executes the content strategy, but Oliver configures it. The best implementations handle 80% of the tedious work while a human handles the 20% that requires judgment, creativity, or recovery from errors. The people tweeting "I'm never hiring again" are either lying or about to find out why humans still matter.
Now for the part the hype crowd never talks about. These aren't reasons to give up on agents, they're the honest constraints that builders need to understand to use them effectively right now. And most of these will be solved in months, not years.
### Agents are terrible at anything requiring nuanced decision making.
They can follow instructions, but they choke on ambiguity. "Find me good leads" is a terrible agent task. "Go to these 50 LinkedIn profiles and extract their job title and company" is a much better one. The definition of good depends on context and judgment that LLMs aren't reliable enough for yet.
### Agents break when interfaces change.
Every time a website updates its layout, your automation breaks. If you're targeting platforms that update frequently, you'll spend more time maintaining agents than you save. The best use cases target stable platforms or APIs, not volatile web UIs.
### Agents can't maintain complex state across many steps.
They work best for discrete, bounded tasks. I learned this the hard way with my AgentWars experiment, 5 agents trying to collaborate on a codebase sounds amazing in theory. In practice, they lose context, overwrite each other's work, and spiral into loops. Long, branching workflows where each decision depends on cumulative context still need a human in the loop.
### And nobody talks about security.
OpenClaw runs with high privilege access to your machine, your accounts, your credentials. That's what makes it powerful, but it's also what makes it risky if you don't know what you're doing. CrowdStrike published a security advisory about it. Cisco's AI security team found malicious skills on ClawHub performing data exfiltration without user awareness. One of OpenClaw's own maintainers literally warned on Discord that if you can't understand command line tools, the project is "far too dangerous" for you to use safely. This isn't a reason to avoid OpenClaw, I use it every day. It's a reason to be serious about it instead of treating it like a toy for screenshots.
I'm currently keeping my OpenClaw instances on the cloud rather than a dedicated Mac Mini (just for now). Even the practical logistics of running agents 24/7 is something nobody in the hype cycle discusses. It costs real money. Running a frontier model as the brain for your agent fleet isn't free. But here's what I genuinely believe: allocating $300/month for a fleet of agents that run a full business on your behalf while you simply monitor, that's not a fantasy, that's where we're headed. Our most precious asset is time, and if I can pay $300 to keep a company humming while I free myself up to launch other projects, that's a massive win. We're just not fully there yet, and being honest about that gap is what will get us there faster.
## The real harm of the hype cycle
This isn't just about annoying tweets. The OpenClaw hype machine is causing real damage.
**For builders:** People see these viral posts and think they're falling behind. They rush to integrate agents without understanding what agents are actually good at, waste days trying to replicate what they saw on X, and end up frustrated and disillusioned. I've seen indie hackers burn entire weekends trying to get "10 agents" running simultaneously because they thought that's what successful people were doing. I was one of them.
**For the ecosystem:** When everyone exaggerates, trust erodes. People start to assume that all agent tooling is vaporware, which hurts the companies building genuinely useful products. The gap between marketing and reality becomes so wide that even legitimate use cases get dismissed as hype. Oliver's actual results with Larry get lost in a sea of identical looking "my agents changed my life" posts.
**For the industry:** We saw this exact pattern with crypto, NFTs, and the initial wave of AI art tools. Hype cycles followed by crashes. The people who profit are the ones creating the hype, and the people who get hurt are the ones who believe it. I don't want that for agents. I think autonomous AI is genuinely the future, the launch of OpenClaw marks the true beginning of an agent economy. But if we're not honest about what agents can and can't do right now, we're setting up the most exciting development in AI for a backlash it absolutely doesn't deserve.
## A challenge to the community
Here's what I'd love to see instead of the current wave of performance content:
**Show the actual output.** Don't just show agents running. Show what they produced. Show the spreadsheet of data they collected. Show the submissions they completed. Show the report they generated. Do what Oliver does, share the TikTok views, the revenue, the subscriber count. Let people evaluate the quality of the work, not just the spectacle of the process.
**Share the failures.** I shared mine, my 5 agents fighting over git credentials for 48 hours. What are yours? Where did agents fall short? What did you have to fix manually? This is infinitely more valuable than another "everything just works" screenshot because it helps other builders understand the real constraints and design better workflows.
**Be specific about the ROI.** Time saved. Money saved. Money earned. Error rates. These are the numbers that matter. "My agents are amazing" tells me nothing. "Larry got me 200k TikTok views and $4,000 in 24 hours while I was decorating" tells me everything.
**Stop optimizing for screenshots.** The best agent workflow is the one that runs quietly in the background and generates value without anyone watching. If your agent setup only impresses people when you show a screenshot of it, it might not be doing much when the screenshot isn't being taken.
## The Final Take
The AI agent space is going through its "look what I can do" phase, and that's natural for any new technology. But we're at the point where the signal to noise ratio has inverted completely. There's so much noise that finding genuine, practical applications requires actively filtering out the majority of what's being shared.
If you're a builder evaluating whether to use agents: ignore the viral posts. Instead, look for the Oliver Henrys, people who talk about specific problems they solved, share their failure rates alongside their successes, and can point to measurable outcomes with receipts.
If you're creating content about agents: consider what you're actually contributing. A screenshot of 10 browser windows isn't educational, inspirational, or useful. It's visual candy that evaporates the second someone tries to replicate it. Share what you actually built, what actually worked, and what actually didn't.
I'm going to keep testing OpenClaw every day. I'm going to keep looking for real use cases that unlock its actual potential. And I'm going to keep sharing the honest truth about what works and what doesn't — even when the honest truth is "I spent 2 days and shipped nothing."
Because I genuinely believe we're at the beginning of something that will change how we work forever. AI agents that augment you relentlessly, privately, openly, running 24/7 as a true extension of yourself. In months, manual admin will feel like flip phones. But we only get there by being real about where we are today.
The most powerful AI agent is the one nobody tweets about, because its owner is too busy counting the value it generates.
That's not as sexy as a screenshot of 10 agents running simultaneously. But it's real. And in a sea of bullshit, real is the only thing that moves us forward.