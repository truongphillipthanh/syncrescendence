---
id: CANON-30340
name: Implementation Patterns
identity: IMPLEMENTATION_PATTERNS
tier: CANON
type: asteroid
parent: CANON-30300
chain: INTELLIGENCE
comet: TECH_STACK
version: 2.0.0
status: canonical
created: 2025-12-31
updated: 2025-12-31
change_velocity: quarterly
dependencies:
  - CANON-30300
  - CANON-30330
  - CANON-30000
synopsis: >
  Production-validated implementation patterns for AI-augmented systems:
  architectural anti-patterns, memory bootstrapping, orchestration strategies,
  context engineering economics, security governance, and operational cadences.
---

# CANON-30340: IMPLEMENTATION PATTERNS
## Production Workflows and Operational Guidance

---

## I. Architectural Wisdom

### The Over-Engineering Anti-Pattern

**What fails:**

---

## CELESTIAL NAVIGATION

**Orbital Class**: ðŸª¨ Asteroid

**Parent**: [[CANON-30300-TECH_STACK-comet-INTELLIGENCE]] (Technology Stack Database)

**Siblings**:
- [[CANON-30310-MIGRATION-asteroid-TECH_STACK-comet-INTELLIGENCE]] (Tech Stack Migration)
- [[CANON-30320-WORKFLOW_INTEL-asteroid-TECH_STACK-comet-INTELLIGENCE]] (Workflow Intelligence)
- [[CANON-30330-RESEARCH_PROTOCOLS-asteroid-TECH_STACK-comet-INTELLIGENCE]] (Research Protocols)

---

1. Building infrastructure before validating concepts
2. Elaborate schemas before understanding requirements
3. Rigid taxonomies resisting evolution
4. Tool-centric organization instead of primitive extraction
5. Prescriptive workflows specified upfront

**Core corrections:**

| Anti-Pattern | Correct Approach |
|-------------|------------------|
| Database schemas â†’ theory | Theory â†’ derived implementation |
| Specify field types, intervals | Document concepts, let details emerge |
| Comprehensive tool catalogs | Stable primitives extracted when needed |
| Prescribe workflows upfront | Observe patterns, crystallize apparatus |
| Linear effort growth | Intelligence scales with data |

### Implementation Principles

**Start Lightweight, Scale Intelligently**: Don't build comprehensive databases. Start with simple tracking, let patterns emerge, formalize what proves valuable.

**Observation Over Prescription**: Don't design workflows upfront. Use tools naturally, track usage, identify apparatus from observation.

**Primitives Over Tools**: Don't catalog every feature. Extract primitives only when considering consolidation or encountering gaps.

**Intelligence Through Data**: Don't manually classify everything. Instrument systems, collect signals, let patterns emerge algorithmically.

**Governance As Enabler**: Design governance that enables capability while managing risk. Good governance accelerates, not restricts.

---

## II. Memory System Bootstrapping

### Phase Architecture

| Phase | Timeline | Goal | Success Criteria |
|-------|----------|------|------------------|
| Working | Week 1 | Context continuity | Multi-turn coherence |
| Episodic | Weeks 2-4 | Interaction logging | Pattern identification |
| Semantic | Months 2-3 | Knowledge extraction | Cross-session answers |
| Procedural | Months 3-4 | Workflow caching | Zero-reasoning execution |
| Integration | Months 4-6 | Unified retrieval | Multi-tier intelligence |

### Working Memory (Week 1)
Maintain conversation history in model context window. No external storage required.

**Pattern**: Append user/assistant messages to history, pass full history to LLM.

### Episodic Memory (Weeks 2-4)
Log interactions to structured format (JSON/SQLite): timestamp, input, output, metadata, session_id.

**Pattern**: Append-only log file, chronological retrieval, keyword search.

### Semantic Memory (Months 2-3)
Extract and retrieve factual knowledge.

**Option A (Simple)**: LLM-based fact extraction + keyword search in fact store.

**Option B (Production)**: Vector database (Pinecone, etc.) with embeddings for semantic retrieval.

### Procedural Memory (Months 3-4)
Cache high-frequency workflows as reusable functions.

**Pattern**: Analyze episodic log for repeated sequences â†’ convert to executable procedure â†’ invoke without re-reasoning.

### Unified Memory (Months 4-6)
Intelligent multi-tier retrieval hierarchy:
1. Working memory (fastest)
2. Procedural cache (fast)
3. Semantic knowledge (medium)
4. Similar episodes (slower)

### Memory Architecture Patterns

**Context Window Optimization**:
- Static knowledge at beginning (enables caching)
- Recent conversation in middle
- Current query at end
- Result: 90% cost reduction via prompt caching

**Hierarchical Summarization**:
- Detailed episodes â†’ session summaries â†’ weekly digests â†’ monthly insights
- Retrieval starts at appropriate abstraction level

**Sleep-Time Reorganization**:
- During idle: consolidate memories, update indices, prune low-value entries
- Background optimization improves retrieval quality

---

## III. Orchestration Patterns

### Decision Tree

```
Is workflow deterministic with clear stages?
â”œâ”€ Yes â†’ Sequential coordination (60%+ of workflows)
â””â”€ No
   â””â”€ Are subtasks independent?
      â”œâ”€ Yes â†’ Concurrent execution
      â””â”€ No
         â””â”€ Is brainstorming/creativity needed?
            â”œâ”€ Yes â†’ Group chat (max 3 agents)
            â””â”€ No
               â””â”€ Does expertise shift between phases?
                  â”œâ”€ Yes â†’ Explicit handoff
                  â””â”€ No â†’ Magnetic routing (adaptive)
```

### Pattern 1: Sequential Coordination

**When**: Linear pipeline with clear stage boundaries.

**Structure**: Research â†’ Draft â†’ Edit â†’ Format (stage outputs feed next stage).

**Tips**:
- Add checkpoints for failure recovery
- Log intermediate results
- Time stages to identify bottlenecks
- Cache outputs for common inputs

### Pattern 2: Concurrent Execution

**When**: Independent subtasks that parallelize.

**Structure**: Spawn parallel tasks â†’ gather results â†’ synthesize.

**Validation**: 45% faster than sequential for independent tasks.

**Tips**:
- Set timeout per task (prevent blocking)
- Retry with exponential backoff
- Monitor API rate limits
- Priority queue if some results more important

### Pattern 3: Critic-Refiner Loop

**When**: Quality > speed, iterative improvement beneficial.

**Structure**: Generate â†’ Critique â†’ Refine (loop until threshold met).

**Validation**: 30-40% quality improvement on complex generation.

**Tips**:
- Clear evaluation criteria
- Limit iterations (diminishing returns)
- Track iteration count as quality metric

### Pattern 4: Specialist Swarm

**When**: Complex problem benefiting from multiple expert perspectives.

**Structure**: Parallel specialists â†’ Coordinator synthesis.

**Validation**: 45% faster resolution, 60% higher accuracy than single-agent.

**Tips**:
- Ensure specialists are complementary, not redundant
- Weight by historical accuracy
- Surface disagreements (often reveals nuance)
- Include confidence scores

### Pattern 5: Hub-and-Spoke

**When**: Mission-critical applications requiring centralized oversight.

**Structure**: Hub creates plan â†’ routes to specialists â†’ monitors â†’ synthesizes.

**Tips**:
- Circuit breakers in hub
- Log all hub decisions
- Approval gates for high-impact actions
- Monitor hub (it's the bottleneck)

---

## IV. Context Engineering Economics

### Prompt Caching

**Economics**:
- Cache write: 1.25x (5min TTL) or 2x (1hr TTL)
- Cache read: 0.1x (90% discount)
- Cache miss: Full price

**Break-even**: Static docs cache after 2 uses. Semi-static content after 10+ daily uses.

**Pattern**: Structure prompts with static content first, dynamic last. Cache static portion.

**Observed savings**: 40-90% cost reduction, 10-85% latency reduction.

### RAG Strategy Selection

```
Is knowledge static or dynamic?
â”œâ”€ Static â†’ Semantic memory + caching
â””â”€ Dynamic
   â””â”€ Simple lookup or complex reasoning?
      â”œâ”€ Simple â†’ Basic RAG
      â””â”€ Complex
         â””â”€ Query ambiguous/evolving?
            â”œâ”€ No â†’ GraphRAG
            â””â”€ Yes â†’ Agentic RAG
```

### RAG Implementations

| Type | When | Cost | Latency | Accuracy |
|------|------|------|---------|----------|
| Basic | Simple Q&A | ~1Â¢ | 200-500ms | 70% |
| Hybrid | Production systems | ~1-2Â¢ | 400-800ms | 85-90% |
| Agentic | Research, high-stakes | ~5-15Â¢ | 2-8s | 92-96% |

**Basic RAG**: Embed query â†’ vector search â†’ inject context â†’ generate.

**Hybrid RAG**: Vector + keyword + metadata search â†’ rerank â†’ generate. 49% reduction in retrieval misses.

**Agentic RAG**: Iterative retrieval with agent evaluation and query refinement. Reserve for complex/ambiguous queries.

### Context Overflow Strategies

**Sliding Window**: Process chunks with overlap for continuity.

**Hierarchical Summarization**: Sections â†’ section summaries â†’ combined summary (recurse if needed).

**Context Offloading**: Heavy processing in separate agent, return summary only. 8x cleaner signal.

---

## V. Security and Governance

### Threat Model

- **Prompt injection**: Malicious inputs overriding instructions
- **Agent hijacking**: Tricking agent into unauthorized actions
- **Tool misuse**: Access beyond intended scope
- **Data exfiltration**: Sensitive info in outputs
- **Jailbreaking**: Bypassing safety constraints

### Defense Patterns

**1. Input Validation**
- Detect injection patterns
- Mask PII before processing
- Rate limit per user/IP

**2. Capability-Based Access Control**
- Explicit allowed capabilities per agent
- Fine-grained action permissions
- Log all violations

**3. Output Filtering**
- Remove leaked PII
- Content policy enforcement
- Detect exfiltration attempts

**4. Circuit Breaker**
- Track failure count
- Trip open after threshold
- Auto-reset after timeout
- Alert operators on trip

**5. Approval Gates**
- Assess action impact
- Pause for human approval on high-impact
- Provide reasoning and alternatives
- Timeout with escalation

### Production Security Checklist

**Input**: [ ] Injection validation [ ] PII masking [ ] Rate limiting [ ] File sanitization

**Agent**: [ ] Capability access control [ ] Action logging [ ] Resource limits [ ] Circuit breakers

**Output**: [ ] PII filtering [ ] Content policy [ ] Anti-exfiltration [ ] Watermarking

**Audit**: [ ] Security event logging [ ] Anomaly monitoring [ ] Violation alerts [ ] Regular audits

---

## VI. Bootstrap Roadmap

| Phase | Timeline | Goal | Investment |
|-------|----------|------|------------|
| Foundation | Week 1 | Baseline + observation | 4-6 hrs |
| Observation | Weeks 2-4 | Pattern understanding | 30min/day + 1hr/week |
| First Optimizations | Month 2 | 1-2 improvements | 6-8 hrs |
| Memory Infrastructure | Months 3-4 | Multi-tier memory | 12-16 hrs |
| Intelligent Routing | Months 5-6 | Context-aware selection | 8-10 hrs |
| Multi-Agent | Months 7-9 | Complex orchestration | 16-20 hrs |
| Meta-Orchestration | Months 10-12 | Self-optimization | 20-24 hrs |

**Total Year 1**: 60-80 hours investment â†’ 200-400 hours saved

---

## VII. Common Pitfalls

### Over-Delegation Too Early
**Symptom**: Complex tasks to AI before trust established.
**Prevention**: Start copilot mode, gradually increase delegation.
**Recovery**: Reduce scope, add monitoring, rebuild through small successes.

### Insufficient Context
**Symptom**: Low quality despite good prompts.
**Prevention**: Memory systems early, RAG for domain knowledge, examples.
**Recovery**: Audit received context, implement missing memory tiers.

### No Governance Until Crisis
**Symptom**: Deployment without security/compliance controls.
**Prevention**: Design governance from start.
**Recovery**: Immediate audit, retrofit controls, review past actions.

### Tool Accumulation Without Extraction
**Symptom**: Growing similar tools, each used occasionally.
**Prevention**: Default extraction mindset, quarterly consolidation.
**Recovery**: Tool audit, identify overlap, extract primitives, deprecate.

---

## VIII. Maintenance Cadences

| Cadence | Duration | Focus |
|---------|----------|-------|
| Daily | 5 min | Security alerts, failures, anomalies |
| Weekly | 30 min | Usage patterns, friction points, routing |
| Monthly | 2 hrs | Reuse stats, memory performance, costs |
| Quarterly | 4 hrs | Framework validity, consolidation, governance |
| Annual | 8 hrs | Paradigm shifts, strategic direction |

---

## IX. Success Metrics

### Individual
- **Cognitive overhead**: Tool management time â†“, decision fatigue â†“, context switching â†“
- **Capability expansion**: Tasks accomplishable â†‘, specialist needs â†“, quality â†‘
- **Synapticality**: Intention-to-execution latency â†“, conscious decisions â†“, flow state â†‘

### System
- **Memory**: Assembly time â†“, retrieval accuracy â†‘, reuse rate â†‘
- **Orchestration**: Completion time â†“, coordination overhead â†“, success rate â†‘
- **Primitives**: Reuse count â†‘, redundancy â†“, composition success â†‘
- **Cost**: Cost per task â†“, latency â†“, quality â†‘

### Meta
- **Learning rate**: Pattern recognition time â†“, apparatus crystallization â†‘, routing accuracy â†‘
- **Governance**: Security incidents = 0, compliance = 100%, gate effectiveness â†‘

---

## X. Implementation Imperatives

**The core insight**: Build for observation and learning first, add complexity only when patterns justify it.

**The path**:
1. Start lightweightâ€”observation before infrastructure
2. Let patterns emergeâ€”don't prescribe workflows
3. Extract primitivesâ€”build on what persists
4. Enable intelligenceâ€”observation creates learning
5. Implement governanceâ€”safety and quality matter
6. Iterate continuouslyâ€”system improves with use

**The progression**:
- Tool mastery â†’ intent specification
- Manual workflows â†’ apparatus patterns
- App adoption â†’ primitive composition
- Static systems â†’ reflexive improvement
- Human oversight â†’ intelligent autonomy

---

## VERSION HISTORY

**v2.0.0** (December 2025): Canonization from Technology Lunar - 4 Implementation_Guide.md
- Compressed from ~43K to ~16K (63% reduction)
- Converted Python code to pattern descriptions and tables
- Preserved all production-validated metrics and patterns
- Added CANON frontmatter and hierarchy placement

---

## CROSS-REFERENCES

- [[CANON-00000-SCHEMA-cosmos]] â€” Master Schema
- [[CANON-30000-INTELLIGENCE-chain]] â€” Intelligence Chain (Chain Root)
- [[CANON-00006-CORPUS-cosmos]] â€” Corpus Management

