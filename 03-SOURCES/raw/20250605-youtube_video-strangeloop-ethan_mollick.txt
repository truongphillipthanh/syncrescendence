https://www.youtube.com/watch?v=KEQjwE7hDjk
Every leader needs this AI strategy | Ethan Mollick explains
65,183 views  Jun 5, 2025  Strange Loop Podcast
Most companies are using AI to cut costs. Ethan Mollick argues that the biggest mistake companies make is thinking too small.

In the first episode of Strange Loop, Wharton professor and leading AI researcher Ethan Mollick joins Sana founder and CEO Joel Hellermark for a candid and wide-ranging conversation about the rapidly changing world of AI at work.

They explore how AI is not just an efficiency tool but a turning point—one that forces a choice between incremental optimization and transformational scale. The discussion covers the roots of machine intelligence, the relevance of AGI, and what it takes to build organizations designed from the ground up for an AI-native future.

What’s in this episode
Why most companies are underestimating what AI makes possible
The tension between using AI for efficiency vs. scaling ambition
How traditional org charts, built for a human-only workforce, are breaking
The collapse of apprenticeship and its long-term implications
How prompting is becoming a foundational business skill
Why “cheating” with AI may be the new form of learning
The risks of using AI to optimize the past instead of inventing the future
What it means to build truly AI-native teams and organizations

About Strange Loop
Strange Loop is a podcast about how artificial intelligence is reshaping the systems we live and work in. Each episode features deep, unscripted conversations with thinkers and builders reimagining intelligence, leadership, and the architectures of progress. The goal is not just to follow AI’s trajectory, but to question the assumptions guiding it.

Subscribe for more conversations at the edge of AI and human knowledge.

Interview chapters
00:20 - Origins: AI in the early days at MIT
01:53 - Defining and testing intelligence: Beyond the Turing test
06:35 - Redesigning organizations for the AI era
08:56 - Human augmentation or replacement
14:58 - Navigating AI's jagged frontier
17:18 - The 3 ingredients for successful AI adoption
23:31 - Roles to hire for an AI-first world
33:41 - Do orgs need a Chief AI officer?
39:45 - The interface for AI and human collaboration
43:50 - Rethinking the goals of enterprise AI
49:15 - The case for abundance
52:30 - Best and worse case scenarios
58:51 - Avoiding the trap of enterprise AI KPIs

---

0:00
You can either fire most of your staff and make more money per barrel of ale or you can be Guinness and hire 100,000 people and expand worldwide. And I
0:06
really worry about too many people taking the small path and not the big one.
0:14
[Music]
0:25
I would love to start from the very beginning when you were back at MIT with
0:30
you know the OJ and Marvin Minsky and and so on. Uh what were sort of the ideas at at that stage? So so this is a
0:38
little bit of like stolen technical glory because I was not the coder with Marvin. I was the person from the MBA
0:44
program who was trying to help the AI people explain what AI was to everybody else. So I worked with Marvin and a few
0:50
other people at the the media lab quite a bit on this and what was really interesting was a lot you know this was sort of during one of the various AI
0:57
winters right so it was no one was paying much attention to AI and it was all about sort of elaborate schemes for
1:03
how we can create intelligence and so there was projects to observe everything a baby did and maybe that would somehow
1:09
let us make AI there was Marv Minsky's society of mind of all these kind of complex interlocking pieces and um I I
1:15
think about how kind of ironic it was that the actual solution solution turned out to be just shove a lot of language into a learning system and you end up
1:22
with with LLMs. It's it's interesting because a lot of the the technical ideas
1:28
turned out um to to be incorrect. Uh but there was a lot of the core philosophies
1:34
there that I think are are are back in fashion now. You had Minsky and Angelbart. Uh Engelbart had this
1:41
philosophy of augmenting human intelligence and Minsky was a lot about replacing human intelligence and trying
1:47
to make machines conscious. Um what what were some of those sort of foundational ideas of how AI could be applied then
1:55
that you think can be relevant now? Well, I mean, I think that's what we're all kind of struggling with right now is now that we have these things in sight
2:01
and you know, we're back to what is sensient and what are the I mean, I think it was two weeks ago a new paper
2:07
came out showing that the actual original touring test, right, gets passed the three-party touring test that
2:12
GBD 4.5 is capable of passing it, right? And in fact, 70% of the time, um, people will pick the AI as the human in the
2:19
room. Uh, which I don't know what that means, but it's better than chance, but that that's interesting. Um and so I
2:24
think we're faced with all these exact set of issues that a few thinkers are worrying about for a long time. So does this replace humans and what do we use
2:31
that for? Right? And for augmentation, what does augmentation look like becomes the big question, right? Is it you know
2:36
and we we that that debate I think never got as far as as it could be partially because this was still kind of
2:41
fictional, right? So what do we do with these very intelligent also very limited machines and then where do humans fit
2:49
into the equation? And I don't think that was ever answered and now it's suddenly very very important. And the Turing test was um it was a beautiful
2:56
idea back back then. But if we were to design a new test now, a mollik test, what would be your mollik test for for
3:03
AGI? So I uh I'm struggle with AGI as this concept all the time, right? Which
3:09
is it's badly defined. I mean the reason why the touring test is interesting just like all the other tests is they were great when we didn't have anything to
3:14
test, right? Like the touring test was great when computers obviously failed it. And similarly, we have the issues where like the AI is acing all the
3:21
creativity tests we have, but those were designed and they were always mediocre for humans and now we're expecting AI to do them. The way we figure out whether
3:27
someone has empathy in social science is the best test is something called the reading the mind in the eyes test. We show a bunch of eyes and ask people what
3:33
emotion they have they have. Like none of these things were designed for AI stuff. So I think about this a lot and I
3:39
tend to be very practically oriented on this, right? So first of all, everyone kind of has their own AGIish test. Um,
3:45
you know, I'm a business school professor. some some of the easiest is can this agent go out in the world and make money and and do things as a useful
3:51
useful test. Can we discover new knowledge and actually test and come with results? But I mean I think what
3:57
we're starting to realize is AGI is going to be this sort of phase we're in rather than a moment in in time, right?
4:05
There's not going to be a you know fireworks going off. Tyler Cohen just said O3 is AGI and when asked why he
4:11
says it's like pornography I know when I see it. Um, and so we don't know what the answer to these questions are. And I
4:16
think it's kind of realizing the meaninglessness of it because it turns out also like as you guys have learned, if you connect AI to systems in the
4:22
right way and you connect with company processes, suddenly you have something that's much better than the sum of its parts versus something you're prompting.
4:28
If you're just doing conversation, that feels very different than can we do strategic decision-m for example. And
4:34
frequently when these models are released, it's always on the most hardcore math problems and science
4:39
problems. It's it's very rarely they take more business applications. If you were to design a benchmark that was more
4:47
focused on the applications that you see in in companies, what what would a benchmark uh for that look like? So I
4:54
think that is one of the most critical problems we're facing right now because all of the people in the labs are math
4:59
and science people and they view the only good thing you could do with your life as coding, right? And then add to that the fact that they want to use AI
5:05
to make better AI and coding and math becomes like the important things followed by biology because they all
5:10
want to live forever. So like that becomes the angle that that that this goes down and there are very few
5:16
benchmarks other things. So we know the AI companies build towards benchmarks. They build sketchy ways right of like optimizing for benchmarks but also in
5:22
more broad ways of they use this for testing and so the fact the lack of good business benchmarks is a real problem.
5:28
So I actually one thing I've been pushing is companies should be doing this themselves to some extent right like and some of this can be direct
5:34
number based like how often does it mess up in being asked to do an accounting process but some of this is vibes based
5:40
as as they say would you actually could have outside experts and we've done this for some of our experiments judge the quality of answers and is this as good
5:46
as a human or not have your own touring tests for various important parts of your job right is the analysis report
5:51
good enough what's the error rate on it you know if we use this to give us to give us strategy advice how good is it
5:57
how good is it at a selection decision. And those are questions that are not that hard to measure, right? They're not
6:03
that technical, but they do require a little bit of effort. I think that's that's one of the areas where products
6:08
have been largely lacking, too, especially when you deploy agents. The ability to test these agents and see
6:14
what knowledge they have and what knowledge they're lacking and correct them and run these test sets has has been really really limit limited. Um so
6:22
as we think about designing an an an AI first uh org um so you basically get a
6:28
thousand person company and you get to redesign the org to be completely AI native. How do you structure it? So the
6:36
first thing you say is redesign to be AI native is hard because it wasn't AI native right so we are we are in this
6:41
really interesting spot where we've had basically hundreds of years of organizational development that that is
6:47
paralleled you know industrial revolution the communications revolution I mean the first org chart came out in
6:53
1855 for the New York and eerie railroad and it solved a problem that never existed before which is how do we
6:58
coordinate vast amounts of you know traffic on train lines in real time using a telegraph and they came up
7:04
McKenzie the guy who came up with this came up with the org chart as a solution and we still use them today. 1910s huge
7:10
breakthroughs in organizing work. Henry Ford's production lines time clocks still use those today. Early 2000s agile
7:17
development right all of these things broke because they all depended on there being only one form of intelligence available which is human comes in
7:23
humansized packages can only be deployed with a span of control of five or seven people. Uh the you know two pizza
7:29
problem and now we're in a world where that isn't the case. So things have to be rebuilt from the ground up and I I
7:34
worry a little bit that um modern western companies have given up on organizational innovation as something
7:39
that they do. It used to be that the way Dowo Chemical would win or the way that IBM would win would they come up with
7:45
new approaches to sales or new approaches to working with organizations and now we've outsourced that. So enterprise software companies will tell
7:52
you how to build your company because Salesforce sells you a Salesforce product that tells you how you use sales or a large scale consulting company will
7:58
come in and tell you how how your organization should run. And now is a time where leaders actually need to innovate. So if I to return to your sort
8:05
of core question, it has to be building from both the idea that we're heading in a in a trend line where humans are less
8:11
necessary in the product and then where do you get you have to pick whether you augmentation or replacement and then you
8:18
have to start building the systems from that fewer people um doing more impressive work or more people doing
8:24
ever more work and trying to take over the world together. Does does this mean that we have sort of fewer 100x
8:31
employees or do we sort of boost double the productivity of everyone? Do do we create this sort of small clusters of
8:38
folks that are uh overseeing the orchestration of of of of agents and are you know orders of magnitude more more
8:44
more productive or is it sort of more deployed horizontally across the organization where a few people get more
8:50
more so I think those are key choices. I mean one of the things I really worry about is when I look at early
8:56
implementations what people view this is as an efficiency technology and I bear a little blame for that our earliest work
9:02
focused on productivity gains from AI and I still focus on that because it matters but I I worry a lot that at the
9:07
edge of industrial revolution what we're seeing happen or some sort of new revolution what we're seeing happen right now is that companies are viewing
9:14
this like a normal technology so they get a 25% cost savings from you know or efficiency gain in in customer service
9:22
let's cut 25% of people, right? Like I hear that all the time and there's a whole bunch of dangers with that. One of
9:27
them is nobody knows how to deploy AI in your organization other than you, right? You can build tools and the techniques
9:33
that are really useful, but ultimately it has to be people in the company that figure out is this good or bad. They're the ones with the experience, the
9:39
evidence to do it. If they're terrified of doing that because they'll get fired or punished for using AI or they'll be
9:44
replaced if there's an efficiency gain, they'll never show you an efficiency gain. Right? And then the second set of problems around that is if we're really
9:50
in a world where we're about to see an explosion of performance and productivity. The idea that you should be as small and lean as possible going
9:56
into that. Like it's like if you imagine the early industrial revolution and you are you're a you know a local brewer in
10:02
the early 1800s, you got steam power. You can either fire most of your staff and make more money per barrel of ale or
10:07
you can be Guinness and hire 100,000 people and expand worldwide. And I really worry about too many people taking the small path and not the big
10:13
one. and and you've generally advocated more for human augmentation and the idea
10:19
that you know the back in the days we used to talk about bicycles for the mind and now we might be getting you know
10:26
airplanes uh for for the minds to to to to some extent. Um in in what ways do
10:31
you think this will be augmenting uh human uh human intelligence? What what
10:37
because it's it's been quite counterintuitive. What what we thought historically was it would start with the
10:43
mundane repetitive tasks and then it would move on to knowledge work and coding and then the very last thing it
10:49
would take would be the creative tasks but it's almost been like the exact opposite um in the sense that you know
10:55
the creative tasks the knowledge work but the mundane repetitive has been really tricky to automate. So in what
11:01
what what ways do you think we'll actually be be implementing this? I mean it it is fascinating how much like
11:06
everyone you know the image of of AI would be that if you talk if you tried to explain the concept love it would explode right does not compute instead
11:13
we have like these weird systems that are super emotional and have to be convinced to do things right like we've
11:19
actually found in prompt engineering sometimes what you have to do is actually just justify to the AI why it should do a step rather than tell you to
11:25
do something it's like no this is why it's important and you should do it which is super weird um and so the thing
11:31
to think about with augmentation though is that Our jobs are that we do are bundles of many different tasks, right?
11:37
Nobody would have designed any job the way we have as a professor, right? What am I supposed to do? I'm supposed to be a good teacher and come up with good
11:43
ideas and be able to have a conversation with you and do research and run an academic department and you know like no
11:49
one would be a counselor, right? No one would want all of these jobs and a lot of them are sort of hot AIS jobs. I
11:55
don't mind giving away grading to an AI, right? If that helps. Like I wouldn't mind providing more counseling support
12:01
through AI if that helps. even though these are very human kinds of things. So I don't think that augmentation necessarily mean like just because it
12:07
does creative engaging sort of human knowledge work tasks at least at the current levels we're at it's definitely
12:13
below the expert level in these kind of cases whatever you're best at you're probably better than AI. So the question become augmentation level one is just
12:20
hand off stuff you're less good at as part of your job bundle and the second level is how do you use it to boost what you're doing right now and we're
12:26
starting to get some good evidence for that too. And what happens when the systems become uh more proactive than
12:33
than than reactive? We're so reliant on giving these systems input to what they
12:40
should give back to us and um prompting them and and and and so on. At some
12:45
point, we should be getting systems that are better than us at asking those questions too and can sort of
12:52
proactively serve this to us. Is that something you're seeing? If if you take your domain as an example, it could go
12:58
out and do all of the research for you and then come and say then sort of this this matches your research test. Here
13:04
are five papers I wrote. Um, pick the best one. Um, have you started seeing
13:10
any applications like that? Yeah, I mean I there's a couple things you said there that are really important. One of them
13:16
is actually the more minor point, which is the idea that it gives me 10 papers to pick from, right? This idea of of,
13:22
you know, it's a hot word now, but abundance. But we're not used to a situation where you can just get a lot of something and curate, right? So, one
13:28
of the things that actually matters a lot is taste and curation that I want to be able to pick out of a subset of options and that still matters a lot.
13:35
That kind of taste piece or what to pursue and start it starts to look like management, which is not the end of the
13:40
world, right? Like management is what most of us aspire to anyway, right? Or at least a lot of people aspire to. And
13:46
it starts to be giving your direction and taste where it goes. But like I think at the end of this, we don't know
13:51
how good these systems get. And that ultimately every question becomes downstream of how good you think AI gets, right? If it's good enough that it
13:58
does all of our work at the high level of what the work you do and your organization does and the work that I do as a professor, then, you know, we're
14:04
sort of in uncharted territory overall and I don't know what the answer is. I think that real organizations um are,
14:11
you know, work much more in much more complex ways than we think about. They're not always aimed for efficiency and AI remains very jagged in its
14:18
frontier of capability. So it can't quite do the whole paper because parts of it'll fail. But I I if I have
14:23
experience, I'll know where those fail and can intervene and shape in those places just like I would with a PhD
14:28
student. So I think we're going to be in a longer world of limited autonomy than people think where like direction
14:34
guidance, you know, is still going to be important. I think the jagged frontier is is probably one of the areas that's
14:41
most bottlenecking or organizations now. It's so incredibly confusing talking to a system that sometimes is genius and
14:48
sometimes is completely stupid and it also makes it very difficult to uh deploy it uh uned in organizations and a
14:58
bit similarly we've had with self-driving cars where the deployment took a very long time because it was
15:03
both sort of superhuman in some applications and other uh in other situations get quite tripped up. What
15:09
what do you think we'll we'll we'll see in uned agents and how how they will be
15:14
deployed? Will we'll be you know bottlenecked for another decade by the jagged frontier or will we start
15:20
trusting these systems and to end quite soon? I mean I think we're already in a place where narrow agents are very good
15:26
right so the the best example of those are the deep research agents that now have been rolled out by you know Google
15:32
openai and uh x right um and the perplexity as well they're all very good right and they do the narrow task of
15:39
finding information being you know giving you answers very well and that is a highly renumerated task right and
15:45
they're not quite there yet because they don't have access to the kind of private data that people need to be able to use these systems fully but you know they're
15:51
starting to get very good at legal research accounting and market research and finance research and like so I think that there'll that kind of delegation to
15:58
a fairly complex task to narrow agents feels very doable. I think there are clever ways to do generalized agents
16:04
with other agents watching them that no one's really pushing yet. Like we we're so new into this that the that you kind
16:10
of have to make two bets, right? One is the whole view of Frontier when when I came with the idea of Jagged Frontier is
16:16
like the frontier is constantly pushing out. So it's jagged, right? Some of those jaggedness will stick around for a
16:22
while. some it doesn't matter if it's still bad at it because it as the AIS get better overall it still beats humans
16:27
right and so I think part of this is the question do you wait for the frontier to move out and then solve the problems or
16:33
do you build around them today and I think part of the key is doing both right of like how do we but if you
16:38
invest too much on trying to solve the jaggedness today as long as models keep getting better you end up stuck with a
16:44
legacy system built around a jagged frontier that no longer exists makes a
16:49
makes a lot of sense and and one thing that's organizations uh is find it quite tricky is discovering the the the AI use
16:56
cases um and they have some bottoms up strategies where effectively most parts
17:02
of the organizations is already using these AI tools to some extent but just not telling their their their leadership
17:08
and then they have some top down initiatives where they're like let's build some AI SDRs or whatever that that
17:14
that might be. How would you approach you know discovering these use cases internally? What are some tactics there?
17:20
So I tend to say you need three things to make AI work in an organization. You need leadership, lab and crowd. So we
17:28
can talk more about leadership later but that's the idea that like this the organization needs to start grappling with the questions at the CEO level
17:35
seuite level of the kinds of things you've been talking about here. What does our organization do? How do we want it to look? What experiments do we want
17:41
to do in organizational form? Like those are fundamental questions. And by the way, if those aren't answered, then the incentives aren't set correctly for
17:48
people in the organization. And everyone in the company wants to know what the vision like you can't say people work
17:53
sideby-side with agents without giving people an articulation of what that actually looks like their day-to-day job. So that has to have come from the
17:58
leadership level. And one of the bottlenecks by the way has been that sea level people have not used these systems
18:04
enough. And you can see where they do because transformation happens much more quickly. Um, you know, uh, Mary Erdo
18:09
said JP Morgan, for example, is, you know, been very public about using AI and that's trickled down and part of why
18:15
JP Morgan does quite well on AI stuff. And so there's this leadership piece and then there's the crowd that you're talking about. Everybody gets access to
18:20
these tools in some way or another. Um, and then how do you create the incentive so they share what they're doing, right?
18:26
Because there's at least like seven or eight reasons why people use AI and don't tell you. Like everyone thinks they're a genius. They don't want to seem like a genius right now. They know
18:32
that efficiency gains get translated into people being fired. They don't want that to happen. they're working a lot less and why would they ever return the
18:38
extra value to the company itself? They they have come up with brilliant ideas that that they don't want to share
18:44
without you know taking a risk for it. Like there's lots of reasons people don't share this stuff. So you have to align that organization to do it. And
18:50
then the issue is like this is done through individual prompting. So to turn those into products, to turn those into agents, to test whether they work or
18:57
not, you need to then extract some of those and start doing some actual real R&D work, which doesn't mean necessarily
19:03
coding, right? Tool bases like the kinds you build are really important for what you're doing here, but it's also just
19:08
how do we start experimenting? How do we take what was a basic prompt and turn into an agent agentic system? How do we
19:14
benchmark that system? So you need all three of those pieces at the same time. And what use cases have have you found
19:20
uh over the last year? you've done a lot of research both as AI as a collaborator
19:26
in a in a team, AI as sort of assisting BCG consultants and and so on. Um what
19:32
type of of of use cases do you think are inside of the the frontier now where it's delivering meaningful value? So I
19:39
think it's really clear at this I mean so there's stuff that I think is that
19:44
like CSR people still struggle with right and I think that those are in some ways riskier things are external facing
19:49
human replacement the augmentation angle the results are really clear right individuals working with AI and
19:55
especially if you have way people sharing that information ideation it's absolutely useful to have you generate
20:00
better ideas working with AI in this right there's some methods that work better than others but that kind of approach for supplementing work of all
20:06
kinds right translation not just you know translation up and down levels of abstraction not just translation directly summarization um
20:14
but where you start to see the really interesting stuff is trying to accelerate cycles so I'm seeing a lot
20:20
more of like rapid prototyping and development so going from like let's take an idea then let's have the AI um
20:27
let's have the AI generate 25 ideas let's have it create a rubric and test those ideas then let's put simulated
20:32
people through those ideas and get their reactions to it refine the ideas further then let's go and and create a um you
20:38
know a prototype working prototype and interview me about how to make it better and then build a vibe coded first
20:43
version that is literally 25 minutes of work at this point right with just a command line in 03 so like we're in a
20:50
very weird spot where it's like but then the organization ends up tripping that up right because what do you do with the fact that now we have 45 great
20:56
prototypes where's the manufacturing capability build it where's the output so that augmentation piece is pretty
21:02
good at the beginning and then research agents are looking really interesting um and then knowledge management agents
21:07
also seem to have a lot of value, right? Which is like actually this is something you forgot or thought about. Where I'm
21:12
starting to see really interesting stuff happen is advisory. Like the idea that we're going to give you advice that's timely or un is is also really
21:18
interesting. What do you think happens to uh the economy when we have I mean
21:24
it's effectively a renaissance where we just have an abundance of everyone can code, everyone can do science, everyone
21:31
can go deep into so many different uh disciplines. uh if we um you know get
21:37
another sort of 10x uh the output from the medical community as a as an example
21:44
will we still be bottlenecked by by by the FDA or do you think the the system will will will adapt and both right
21:52
systems take a lot longer to change um I mean we've been talking to some of the deep mind people and they are saying
21:58
that there's getting real drug development results in a year that look really good right um so there'll be
22:03
pressure to adapt to those kind of things. And I think part of the question like part of the issue with the uncertainty in the regulatory
22:10
environment whether for different reasons in Europe versus the US for example um is is that it makes it hard
22:16
to figure out where to invest to make these kind of changes happen because we are going there's going to be societal
22:21
bottlenecks all over the place and there's also you know the AI only has limited ability to act in the physical world at this point right robotics lags
22:27
this organizational structure lags this so how do we start thinking about that becomes a really big deal I think part of why people find agents so appealing
22:34
is in part the idea that they solve some of this problem by just doing stuff so I don't have to worry about it. But at some point they're going to hit the real
22:40
world, right? And and at those friction points, that is where things slow down. On the other hand, if you can get up to
22:46
that friction point and deliver here's seven really good-looking, you know, like um compounds that might make a
22:52
difference, that is a huge gain anyway. So I think that the gain will be more spread out. Um but we just don't know. I
22:58
mean part of this also is how autonomous these systems get, right? Which roles do
23:03
you think will uh will end up being more useful in organizations as a function of
23:09
this? Oh, that's a tough one and based a lot on organizational choice, right? But I think I think management roles does
23:16
like um roles that are sort of thinking about systems are tend to be very valuable because there's systems are
23:22
problematic. I think experts anywhere become valuable, right? Uh it turns out expertise actually is really good. None
23:27
of these systems are as good as an actual expert at the top of their fields. we tend to measure against the average in a field and like the AI does
23:34
really well but if you're in the top 2% of something you're going to be beating the AI in that field and so expertise
23:39
actually matters a lot in this space so either deep subject matter expertise broad expertise across many areas um as
23:46
a system leader uh or really good taste tend to be the three things that help you one one thing that that I've been
23:53
thinking a lot about is um you know on on one end you could be hiring more
23:58
senior developers as an example where you say you know, we just hire the top 2%. Those are the only folks that are
24:04
going to be, you know, make a big difference to us. Another argument could be actually you could hire much more
24:09
junior developers nowadays because the junior developers will be able to execute at the quality of much more
24:16
senior uh developers. Um, what do you think there? Should does does the
24:22
democratization of expertise actually enable you to maybe staff your team with more junior junior talent and maybe
24:29
folks that are slightly more senior will actually not benefit as much from from from this technology. So there's
24:35
actually a few effects happening at once and I think it's worth unpacking them. Like our our our Boston Consulting Group study was the first one to document in
24:42
the real world the idea that like there was this performance gain for the lower performers got the highest gain. Uh but
24:48
people don't talk as much about the why we found out that happened which is we measure something called retainment which is how much of the AI's answers
24:54
the consultants only turn ultimately turn as their own and for sort of 80% of consulting tasks the only way to screw
25:00
up was to added your own thoughts or ideas into the AI's answer right as long as you were just turning in the AI's answer you did great as soon as you're
25:05
adding your own thoughts or ideas so it's basically work at the eighth percentile so when you say you're hiring a junior developer and the AI makes them
25:11
better I think it's worth specifying is it just that the the human is substituting for the things we can't do
25:17
agentically yet which is like I'll paste in the requirement and I'll attend the meeting and the AI is actually doing the work right is or is it actually bringing
25:24
people up to that level and at the same time at this sort of really good person level we're seeing effects where if
25:29
you're very good and you use AI the right way you can get 10 or 100 times performance improvement so I think you need to think about both things right
25:36
there is this sort of substitution effect and my view has been that a lot of the benefit comes from having
25:43
expertise and then using AI to supplement the areas that you're not you're bad at, right? Like I think about
25:49
founders all the time. I was an entrepreneur. I teach entrepreneurship. Like entrepreneurship is all about you being very bad at many things but really
25:56
really really good at one thing. And your whole task as an entrepreneur and the reason why I teach entrepreneurship is to have those, you know, the 95% of
26:03
stuff you're bad at not trip you up, right? Like the fact that you didn't know you needed a business plan or that you didn't know how to do a pitch like
26:09
because your idea is brilliant and you know how to execute it in this market. And so the fact that AI could bring you
26:14
to 80% in all of that is a really good thing, right? And that is replacing your work. But in the area where you're at
26:20
the 90 9.9th percentile, you get a 100 times multiplier. And I think that's the same kind of angle. And I think the
26:25
danger is is that if you're hiring junior people and expect them to use AI the whole time, how will they ever become senior? Becomes a real challenge.
26:33
What what do you think the answer to that is? like a lot of the law firms I speak to for example there's a a core
26:39
part of the training is the you know basic work you do and then as you become more more senior but you do more complex
26:47
legal analysis but when you look at actually what the juniors are are are doing I think most of that work is not
26:54
actually adding up to what the more senior role will will be doing it's very
26:59
simple repetitive work and so on do you think that will be an issue where people don't grow um you know through the
27:06
hierarchy to the same extent and as a function of that we don't have as many folks that can step into this more
27:12
senior roles or will you just go into the senior roles more quickly? No, I'm I'm really worried about that, right?
27:17
Because like any other university I at Wharton, you know, I teach really smart people and but I teach to be
27:23
generalists, right? I don't teach them to be, you know, I teach them about how to do analysis. I don't teach them how to be a Goldman Sachs analyst, right?
27:30
But then they go to Goldman Sachs or they go to a law firm or whatever it is and they learn the same way we've been teaching any white collar knowledge work
27:36
for 4,000 years which is apprenticeship right and it you're right they're asked to do repetitive work over and over again the repetitive work doing it over
27:43
and over again that's how you learn expertise right you get yelled at by your senior manager you're you know at the wrong kind of firm or else treated
27:49
nicely. Um but you're basically given correction over and over again till you write a deal memo. But it's not just that you're learning to write a deal
27:54
memo it's that you're also learning why this approach didn't work. you're absorbing a whole bunch of stuff from your mentor about what the goal of this
28:01
is. So, we let like it just happens, right? Apprenticeship, if you have a good uh mentor, apprenticeship is a
28:07
thing that happens. We don't spend a lot of time training people for. We just sort of it's magic and some people pick it up and then other people get fired,
28:12
right? And they might get fired because they're bad, but they might get fired because they got unlucky and got a men good bad mentor or didn't learn the
28:17
right things. That mentorship just snapped this summer. That chain that's kept going for a few thousand years.
28:23
Because what happens now is if you're a junior person, you go to a company, you don't want to show people you don't know
28:29
something. It's because you want a senior job. So you're going to use AI to do everything. So you've turned off your brain because the AI is better than you.
28:34
And every middle manager has realized that rather than going to an intern who sometimes like take messes up or cries,
28:40
um you could just have the AI do the work because it's better than an intern. And I really worry about that pipeline being snapped. And the problem is is
28:47
that we've viewed this as an implicit thing. Like there's very little work in law firms to teach you how to be good at
28:53
teaching a lawyer, right? To someone to be a good lawyer. Instead, you hope that you had a good mentor yourself and you replicate what they did, right? It's why
28:59
bankers will often, you know, like 120 hour weeks is part of your job. Why? Because that's always been part of your
29:05
job and somehow that teaches you something. And so I think we have to move much more formally to how do we want to teach people expertise and work
29:12
on it. that ironically the one place we do this really well is actually in sports because like that's an area where
29:17
we've learned how to build expertise right practice with a coach and yeah we're gonna have to do the same kind of
29:22
thing in other forms of learning as well. So how would you think about it if you started a new university now uh for
29:28
for the intelligence era. So assuming you know models keep getting better over
29:34
the next few decades how would you design a university around that? So there's a few things happening, right?
29:40
One is what should we teach and the other is how should we teach it. I'm more concerned about two than one. I
29:46
think there's a big thing of like we need to teach people AI skills and I think as somebody who's worked with these systems a lot, you know, like
29:52
there's not that like the skills are first of all there's like five classes worth of skills to learn, right? Unless you want to build an LLM, which you
29:58
shouldn't do. Um it's really like five or six skills classes and then there's a lot of experience. Um, and so I think
30:04
the qu it's less about teaching people to use AI and in fact I think a lot of the discipline stuff that we teach are
30:10
really important. We want people to still learn to be good writers. We want that broad knowledge, right? As well as deep knowledge. I think universities are
30:16
well suited to that. Where we break down is how we teach, right? And so everybody's cheating, right? And AI
30:22
detectors don't work. And they're already cheating, by the way, but now everyone's really cheating. There's a great study that shows that from the
30:27
beginning of um from like when the internet era and social media really kicked in in like 2007 or 2006 students
30:34
at Ruters um who did their homework almost all of them did better on tests and by the time you reach 2020 almost
30:41
none of them like 20% were getting better in test because everyone else was just cheating right so like you have to do the kind of hard work so AI doesn't
30:48
let us skip the hard work but it will let us with AI tutors on a onetoone basis you can actually teach people at
30:54
their level we can help excel accelerate the learning process in real ways. And so I'm much more interested in how you
30:59
ch and I already did this with my classes. How do you transform how we teach with AI becomes a really interesting question. I don't know if
31:04
the subject matter changes and I think we can increase scale also teach more people but I think that some of the core
31:11
subjects stay the same and you've done some really cool things and were probably one of the first to actually
31:17
ask your students to to shoot. What are some other things in in which you've uh deployed this and how how you teach how
31:25
everything my class are 100% AI based I mean so I teach entrepreneurship so the easiest version is it used to be at the
31:30
end of a class right and you know people have raised hundreds of millions of dollars from my class and the ones taught by my colleagues the same class
31:36
number but um you know you would you basically have a business plan and a powerpoint now at the end of a week I
31:41
have people have working products right like literally when I first introduced chatbt to my entrepreneurship class um
31:48
the Tuesday after it came out um you know the one student was really distracted came to me afterwards said I
31:53
just built my entire our product while we were talking, right? And that seemed entirely novel at the time that you could it would write code was like
31:59
shocking, right? And now we're in a very different world for where that is. Um, but I think that um so I I have my
32:05
students now have AI simulations they play. They have to teach the AI something. We have a purpose naive AI
32:11
student. There's AI mentors for all the class material. Uh they have to build cases with AI. There's AI um watching
32:18
what they do in a in team settings and giving feedback or acting as devil's advocate. So there's lots of cool stuff
32:23
you can do to supplement it, but that's all in service of having a classroom experience that's active and engaged.
32:29
And so I think that classrooms don't go away, right? But but what we do in them kind of transforms. So one thing we've
32:36
been discussing is is the organizational uh design and what it should be structured like. Should companies hire a
32:42
shift AI officer who sort of oversees all of the internal deployments? should have a model where they deploy someone
32:49
in each team to figure out the the use cases. What do you think like how how do
32:54
you structure your your AI or so I worry a little bit sometimes on the chief AI officer thing for the same problem that
33:00
everybody is having which is everybody wants answers and like I talk to all the AI labs on a regular basis I know you guys do too you've been doing this for
33:07
much longer than most people in the space and you know the horrible realization you have fairly quite quickly is that nobody knows anything
33:13
right it's not like the labs have an instruction manual out there that they haven't handed to you it's not like that there's like more data than what I'm
33:19
sharing with you guys about this or that I share online like there's no secrets right there. Like there isn't everyone's
33:24
like desperate to copy somebody else and there isn't. So like when you say hire a chief AI officer, how are they going to
33:29
have any more experience in the last two years than anyone else did? No one thought LLMs would be this good. Like
33:34
you guys were there before almost anyone else that like that gave you a year of head start, right? Like this is a weird place we're in. So there isn't someone
33:41
you can hire who's like an expert and they often I mean one of the major problems of AI in organizations is that
33:46
AI meant something very different from 2010 to 2022 that is still important by the way. large data, you know, going
33:53
ahead and actually boosting everything like still worth doing, right? But like that's a very different beast. So a
33:58
chief AI officer is kind of a hard hire. I really feel strongly that organizations have the expertise they
34:03
need to succeed internally because the only people who know how to use AI will be the people who are experts. It's very
34:08
easy for someone who's done a job a thousand times to, you know, run a model and figure out whether it works or not.
34:14
And in fact, in our BCG study, we have a second paper that shows that junior people are much worse at using AI than senior people, which is not something
34:20
people think about usually. They're like, "We need the digital generation to come in." Turns out not to be true because junior people produce a memo and
34:26
they show that memo and you're like, "It's a memo. It's great." And you're like, "Well, I've looked at this for 20 I've done this for 20 years. Here's seven things the memo doesn't do well,
34:32
right? So expertise and knowledge matter. So I think it's less about embedding people in teams." And then we
34:38
don't even know what makes someone good at AI. So what I tend to do is suggest the crowd and lab need to be linked
34:43
together. So what the crowd does is you're not just surfacing you know AI use cases. It basically by the way in
34:49
almost every organization you max out at 20 30% of people using your AI model internally and everyone else is either
34:55
not using it or they're cheating and using someone else some of their AI because they don't want to show you what they're doing. But you get like 20 30%
35:00
of your of your organization using it. And then you'll find like one or two% of your organization is just brilliant at
35:05
this stuff. They're amazing at it. Those are the people who will be able to lead you in your AI development effort. I
35:12
don't know who they're going to be at first, right? And you won't know either, but they will emerge. And then the danger is they're making so much profit
35:18
for you on the line that you don't want to pull them off the line. But those become the people that become the center of your lab and figure out how to use
35:25
it. So I really think building internal effort is the right way. And it's very hard for me to recommend hiring a bunch
35:30
of people for AI when we don't know what makes someone good or bad at this. And your organizational context actually
35:35
matters here. And what how do you think we set up the incentives? So if you have
35:40
the experts in each domain and uh you really hand it to them to figure out how to deploy AI and effectively automate
35:48
away their own role. How do you create the right incentives for them to do that? And that's why the leadership leg
35:54
matters so much, right? So there's a few things you need to do. One is this is easier for companies with good culture,
36:00
right? If the CEO says and in growth mode, right? If the CEO can, if you trust the CEO or the founder and they
36:05
say things like, "Listen, we're not going to fire anyone because of AI. We're going to expand what we can do. We're going to make this work for everybody and people are incentivized to
36:11
do it, you're in a much easier spot than if you're a large mature organization that has a tendency to use it funds to
36:17
cut people, right? People will know the difference. So, you have to acknowledge this to start off with, right? Like, if this is going to be a threat to people's
36:23
jobs, people want to know that and you have to start thinking through what you want to say. And then incentives can often be pretty crazy in these
36:29
situations. I've talked to one company that gave out $10,000 cash prizes at the end of every week to whoever did the best job automating their job. Um, and
36:36
you save money versus a typical IT deployment just shoving over a suitcase full of cash. I've talked to another
36:41
company that um before you hired anyone, you needed to show um you need to spend
36:46
two hours as a team trying to do the job with AI and then rewrite the job description around the fact that AI would be used or that you had to spend a
36:53
few when you proposed a project, you had to try using AI to do it and then resubmit the project proposal as a
36:58
result. So like you can incentivize people in lots of different ways but that clarity of vision matters so much
37:04
right if you say your job in four years will be working with AI to do something people are going to be like well what does that mean like am I sitting at home
37:10
you know giving instructions to an agent am I in a room doing things are there less of us so that vision actually
37:16
matters and I find way too many executives just want to kick that down the road and say AI will do great stuff
37:21
why would I ever want to share my productivity benefits with the organization without being compensated and so starting with that kind of piece
37:27
is really important So another research you you did was when when AI is embedded and collaborating
37:33
like more like a colleague and you studied folks that were working individually, folks that were working in
37:39
teams, folks that were working individually with AI, folks that were working in teams with w with with with with AI. What what did that sort of
37:46
teach us about how this might be embedded into teams? So we did this big study with my colleagues at MIT and
37:52
Harvard University of Warwick uh of 776 people at Proctor and Gamble the big consumer products company and um like
37:59
you said they were either teams of two cross functional teams or individuals working alone and then working with AI
38:04
and teams are alone. First off we found individuals and this is all real job tasks right not just like innovation
38:10
tasks. We found that individuals working alone with AI performed as well as teams um and um which was a pretty impressive
38:18
kind of boost and were actually happier too as a results of working with it. Like they got some of the social benefits of of working with these
38:23
systems and produced high quality results. Um and we also found that but the teams that work with AI were much
38:29
more likely to come through come up with really breakthrough ideas. We also found that expertise tended to even out. So if
38:36
you sort of mapped how technical a solution was and you had technical people in room they'd produce highly technical solutions you prod marketing
38:41
people produce highly marketing solutions as soon as you added AI the solutions were across the board so they
38:47
were much more even so it really turned out like this was a good supplement to kind of human work um you know and again
38:54
this was pretty naive like we gave them a bunch of prompts to work with but a lot of it was them just kind of playing with these systems back and forth so you
39:01
know this leaves the same problem that we've had before which is you need to make some decisions like the the typical
39:07
company that sort of sits back and waits for someone else to provide a solution to them is going to be less well off
39:13
than if you start experimenting now and figure out what works and what doesn't. And what do you think will be the interface for for collaboration? will
39:20
they just be embedded in natively into our Google Docs and our Slack and we'll
39:26
just communicate with them just the way we communicate with with all of our colleagues or do you think there will be
39:32
something that's more an agent native interface where we collaborate with
39:37
them? I I mean I think an agent native interface makes a lot more sense you know that born built around teams rather
39:42
than having each document have a co-pilot on them. I want something to maintain state across the various tasks.
39:48
I mean, we're close, right? Like, I've got my phone here and I can, you know, turn on if if we want to, I can even do
39:53
it. Uh, we can turn on, you know, chat GBD's agent. I can look around us and give feedback on what we're doing in the
39:58
world. And I think that that like that's a promising way forward. And again, it's
40:04
about that redesigning work. I think agentic systems are more less interesting almost because they automate
40:09
work than that they can bring together many threads of work. And you mentioned one example uh a while a while ago which
40:16
I think it was ship like hallucinated a quote from you and you actually thought that was your your your own your own
40:22
quote. When do you think we'll have the systems crit you know sort of ethanolic level research and what's required for
40:29
that? Is it just sort of feeding them more of of of your of your context? Do you think we'll get there quite soon?
40:36
And and what will that mean? Would that mean that you're basically just using your test to select among the best
40:41
papers that it's generating? I mean, I think a lot of this is already possible with the levels of models we have. I
40:46
mean, there's a paper that shows 01 preview, which is not even a cutting edge model at this point. You know, the hallucination rate on the New England
40:53
Journal of Medicine case studies went from like 25% in previous models like 0.25%. Like the hallucination problem
41:00
starts to drop when you connected data sources, when you have smarter models. I mean it's still there but like you
41:05
mentioned uh at one point that you know I used AI in classrooms and my first classroom policy was you could use AI in
41:11
class and that was great for three months right when chatb 3.5 came out my students are smarter than chat GPT and
41:17
it produced much more obvious errors and I let them use AI for anything they wanted because if they don't add their own thinking they would get like a B
41:23
right like AI was not capable of doing that GBT4 came out does as well as my students you know who aren't putting a huge amount of effort in so I think
41:29
we're in the same kind of boat here which is these systems are very good and as people who build agentic systems. I think you're probably realizing what you
41:36
know I've long realized what I think we know which is they're capable of a lot more when you start thinking about them agentically and you know Google's been
41:43
doing some stuff of building AI labs. There's been work out of Carnegie Melon doing the same sort of stuff. I actually
41:48
think it's more willpower than anything else to build a research system that does interesting work. And it's like so many other areas in AI where I'm like
41:54
wow we've already shown that this can work really well as a tutor. Where are the thousand tutor, you know, that are
42:00
actually well done as opposed to just prompting the AI to be a tutor? Where are the thousand science applications?
42:05
Where's the internal training systems? These are capable right now. Like, it's really just doing it. What What has been
42:10
some of the most surprising uh things you've gotten to work recently? What have you seen in the latest generation
42:17
of of of the models? Things that didn't work previously that are now starting to work really well. I mean, so at with the
42:24
latest versions of say Gemini, the hardest thing you have to do as an academic is writing what's called a tenure statement. So you do this
42:31
hopefully once in your life and you have to write a statement where you go up for tenure. And what you have to do is take all of the academic work you've done,
42:36
which is often 15 years of work, very complicated, and boil it down to a few themes and write an essay sort of about
42:41
why your research has these themes. I was able to recently with the new Gemini models dump in all of my academic papers
42:48
I wrote because the context one is huge and have it develop those themes and it found two of the three themes I ended up
42:53
took me two months to write on my own at a fairly high analytical level right like um you know or on the more fun
42:59
version I can now throw in any academic paper I've ever written and say turn this into a video game and get a good video working video game out of it um
43:06
you know I vibe coded some 3D games recently which was like I can't code um and you know building pretty good
43:11
working systems so I mean I like threshold after threshold kind of keeps falling uh and I'm sh surprised on a
43:18
regular basis like can't believe how much these systems can do and how how should we be thinking about this in
43:23
companies is this the equivalent to like deploying more IQ into the system is it deploying more labor into the system or
43:31
how should I view this as a company so there's a tactical and then there's a philosophic view on the philosophic view
43:38
we don't really know right like certainly in intelligence but like you've you know intelligence labor are
43:43
just sort of like two very simple inputs, right? But also what does it mean for better adi to get better
43:48
advice? What's it mean to get better mentoring? What's better to have a second opinion, right? Um the and on the
43:54
tactical side, I think that the thing to aim is to be maximalist. I think too few organizations are maximalist. Just push
44:00
the system to do everything. If it doesn't do it, great. You now have a benchmark for future systems to test and
44:05
it might actually just do all the stuff. If it does all the stuff, you've learned something valuable. So I really worry about the incrementalist sort of like
44:11
let's summarize our documents like that's fine but it could do that a long time ago. Why why are you having that document summarized? Let's just have it
44:17
do the thing as opposed to the intermediate step. I I I think that's a really interesting uh point because a
44:24
lot of companies now are like let's start with a small proof of concept and then we scale up and then it's sort of
44:29
six months in and they get stuck in that proof of concept and they never quite never quite scale. Whereas you see
44:36
others take the approach of let's actually deploy it everywhere, get everyone access to this and then double
44:41
down on the use cases that work really well. But even that isn't maximalist enough, right? Like you're absolutely
44:47
right because the problem with the use cases that work well is they worked well given the limits of the system and given what people were able to do at that
44:52
point. And the building apps is often the worst kind of angle because you end up with now a semisuccessful product
44:57
that you have like that you built around the limitations of llama 22 or whatever it is because that was I mean we can
45:03
talk about the problem one of the problems IT teams have with being the nexus of AI deployment is it is very
45:09
interested in low latency and low cost right and it turns out that low latency and low cost are the exact opposite of
45:15
high intelligence in these models. So there are times where you want to be low latency, low cost, but there's also times where it's like I'm willing to pay
45:20
15 cents for a really smart decision or new chemical, right? Like that's a reasonable amount to pay. Um, and so you
45:27
have to like that balancing act can be really hard because people tend to build off of cheap small models and then they
45:32
get stuck later on, right? Which is why being agnostic is so important, but also updating. So even when people do this,
45:37
they often don't find the maximalist approach. So that's where the lab comes in. You really need people building impossible things. And what what's the
45:45
difference between using it as a centaur versus a cyborg and and what do you recommend there? So the the centaur
45:53
definition is you know Gary Kasparov used that term at first. This idea that I kind of took from that was the half
45:59
person half horse, right? the idea that like you you're you're basically dividing up the work with AI and I know you know you know Kasparov's definition
46:06
was was vague around that right but like that's how we view this and this is sort of the beginning thing like I hate writing emails I'm good analysis I'll do
46:12
the analysis you do the emails cyborg work is more blended right so my book is a cyborg task and you know the system's
46:18
gotten much better since then but at that time it was very bad at writing I'm a very good writer I think or at least I'm proud of my writing so the I did
46:24
almost no writing but writing books is terrible and so all the things that made writing books terrible it helped me I
46:30
got stuck on a sentence. Give me 30 ways to end the sentence and pick one. Read this chapter and make sure that I'm, you know, like my Substack. I have the AIS
46:37
read my Substack all the time, two or three of them, and give me feedback. I rarely, you know, I use it for core
46:43
writing, but I absolutely get feedback all the time from it and make changes as a result. Read these academic papers and
46:48
make sure I'm citing them properly. Like those sorts of use cases are where the power really comes in. And there was
46:53
this other uh study where um the folks that that got advice from the AI ultimately ended up being more
47:00
productive, but it it was largely benefiting the the more more senior folks and and and and not as as much the
47:07
lower performers that they couldn't quite sort of internalize the the the advice. What does this mean for advice?
47:13
If everyone is sort of getting you know your advice on how to deploy AI in their
47:19
or organizations, what what will that mean for for the society? So, I mean, I think that part of the thing is it's not
47:25
always the same advice, right? Like the AI is good at context. I think the study you're talking about is the Kenya study of entrepreneurs, which was this great
47:31
controlled study that you only got advice from GPT4. They couldn't get it to make products for them or anything else. And what they found was that for
47:38
um that high performers got I forget what it was eight or 13% improvement profitability, which is by the way
47:43
insane for advice. Like if I could do that with my students and just give them advice and get a 13% profitability boost, that's amazing. And again,
47:50
remember people are jagged too. So like even if you like you're going to need different advice than someone else. So even if you're getting advice from the
47:56
AI, it's going to be about the thing you're weakest at, not the thing you're strongest at. And the low performers did worse because their business were
48:01
already struggling. So they couldn't implement the ideas. So I think it's very much true that the advisory role,
48:07
the second opinion role, there's some danger that does shape us all in the same direction, right? We find this in
48:12
ideation too. The AI has a bunch of themes. If you've worked with these models, you know that for example like
48:17
GPD40 loves to generate ideas that have to do with crypto. It loves to generate ideas that have to do with AR and VR and
48:24
it loves environmentally friendly ideas, right? Like just from the way it's post training worked, I assume and just churns these out. And but we found in
48:30
some of our other work that if you prompt it better, you can get as diverse ideas as a group of people. So part of this is about like what does the adviser
48:36
do for you? Maybe you wanted four or five advisors. you don't want Ethan Mollik to be the adviser or you want me but you also want Adam Grant and you
48:42
also want Garrett Kasparov and that can be valuable too and um if I if I take
48:48
the case of abundance uh here and and prompt you to give 30 examples of uh of
48:56
good things companies are doing deploying AI can you list as many as possible uh in now what how should you
49:04
you mentioned the example of you know handing out cash for the folks that are uh deploying it the best. What are some
49:11
of these crazy ideas you've seen and certainly work really well? I've been see I mean so there's tons of them. I can't give you 30 and I can't even talk
49:17
about all of them unfortunately because I'm not allowed to but um you know certainly right the easy stuff is all
49:23
your coders use these you know and but then you know change the reward systems around doing that. Um so every ideation
49:30
session you stop in the middle of meetings and you ask AI how's it going so far or whether or not you should continue the meeting at all um and then
49:37
drop out otherwise. uh if if the meeting if the AI think the meeting is done. Uh even in physical meetings just stopping
49:42
and having an AI conversation with the AI and thinking about what they're doing at that stage. Um I I have uh the I have
49:49
seen cases where people are using everyone gets an AI consultant or adviser that they kind of ask about
49:54
strategy decision-m uh on every point. Um there is some really interesting stuff being done on training, right? So
50:00
ask the AI to simulate a training environment or play through that one way or another. Turns out to be really cool. I don't know. I'm not going to be able
50:05
to hit 30 here in the room with you. But I think that the Ethan probably could.
50:10
Abs. Absolutely. And that's how you know I'm real is that I'm not doing a very good job. And I'm I'm kind of worried
50:16
now. You're not even responding to my prompts. You have enough footage of me that I'm desperately worried that this that you're going to get much better
50:21
answers. Yeah, we'll definitely try what what the what the AI version will will will do. And um and and what do you
50:28
think is the best case scenar scenario? So assuming everything get gets right, this gets deployed in into society. What
50:34
do you think is this the best case scenario a decade from now? I mean so I do think that the idea of sort of a
50:40
let's let's leave aside an ASI kind of world where there we're all watched over by machines of love and grace, right?
50:45
And let's just focus on on sort of I think what happens is that you know I
50:52
mean I the problem is a best case link also requires policy decisions because there is clearly going to be employment
50:58
impacts from this. We don't know what form they're going to take. It's very possible that everyone gets more jobs, but we need retraining. I don't know
51:04
what the future holds in that case. So, there has to be some policy piece. It's kind of missing on that right now. But I think that there is a place where your
51:10
jobs get more satisfying because you have to do less grunt work, where we have a world where productivity is now
51:15
flowing in in fun ways rather than just like productivity office is like are you typing enough stuff? But like if you're
51:20
architecting a system of agents that's building stuff for you, suddenly this is feels like a very different kind of world you're in. It's much more
51:26
satisfying, right? um where you work less and more stuff comes out and you add your humanness at the key elements
51:32
that you know the people who still have a sense of style or approach or perspective produce very different work
51:37
than somebody else so you have differentiation variation I mean that kind of looks like a world where AI gets
51:42
five to 10 times better than it does right now but it doesn't get beyond that you know which is sort of a weird thing to root for in some ways but that's the
51:49
easiest way to imagine a you know a kind of outcome that feels like the world of today if these systems get a lot smarter
51:55
then it's like well why do you come into work when it's like we could sit here and you've we've autogenerated this video here. I feel like 5 years to come
52:02
back like recreate the people, make it 3D, put us in a volcano um and have us
52:07
talk individually to everybody in their language and voice, right? We're close to that. So that starts to change jobs
52:13
much more dramatically. And what are some beliefs u that are in the field currently that that you really disagree
52:19
with? So I think that there is a a huge focus and I understand the safety focus
52:25
but I think there's a huge focus that we and there's a paper that just proves it that we need to either focus on existential risks or not. And I I think
52:32
that there's a lot on existential risks and it's worth thinking about but that worries me a lot less than agency over
52:38
the decisions we're making right now. And I worry that people are by treating AI as this technological thing which
52:43
we're even having this discussion here where it's like a steamroller. That's not actually how this is, right? like we have to figure out how this technology
52:49
is used and shaped and that's important and everybody who's at this you know at this event gets to make decisions right
52:55
about how AI is used and shaped and those will in turn shape where AI goes so I really worry about this lack of agency kind of approach which is like
53:02
the AI will do things to us we get to make choices and we can make those choices that defend what we think is
53:08
important to be human what our customers need what society needs and so I that concerns me is avoiding that kind of
53:14
conversation um I also think that a lot of people in the technical field of AI don't understand how actual organizations work and that they're
53:20
messier and that you know even super smart agents won't necessarily change how companies work overnight right which
53:26
is why I always struggle five or 10 years we don't know when the change happens and it will happen in bursts but
53:32
you know you know there's a naive sometimes sort of like I have I have a sister who who's a Hollywood producer
53:37
and every time I hear that AI will replace Hollywood I'm like you don't understand how much work goes into a Hollywood film and some of that will
53:43
disappear in fact they're using AI actually to accelerate performance is one fun example. So, she is um she's
53:48
made a movie with Michelle Fefeifer and every time uh and when they have to do test audio dubs and they now have a fake
53:54
Michelle Feifer voice that they could test the audio dubs with, but they never can use that for actual theater crowds
54:00
because there's good union protections around the actor. So, it's a test bed to do experiments, but Michelle Fefeifer
54:06
still has to come in and record in her human voice with what she wants to do or not. So, I think we can build a world
54:11
where we defend that humanness, but we have to make choices to do it. And if if you had to prompt a a model and to
54:19
basically make all of your decisions from from now on, what what would you prompt it? Okay, so I'd probably do
54:25
something of, you know, I so first of all, I'd like to give it a lot of context, right? Something you guys know a lot about um about me and my choices.
54:32
So paste in a couple couple million characters of stuff. But I would probably say, you know, the good thing I have this advantage disadvantage, which
54:38
is I've written enough that the AIs care about that they have opinions about me. Um, and um, so I I get a pretty good act
54:45
like Ethan Mollik. I get pretty good answers. It tends to be a little overenthusiastic and it likes hashtags for some reason uh, that I don't
54:51
recommend and really loves emojis and I'm not really an emoji person. So I think it thinks I'm more millennial than I am. But aside from that, um, if I was
54:58
asking for it, I'd be like, okay, so you know, taking on the person realizing that you're working for Ethan Mllock to help make decisions and knowing that,
55:04
you know, here are four or five things that he values that are very important. Before making a decision, I want you to
55:09
go and uh pick four or five possible options that we might follow in the decision. At least a couple of them should be very radical. Then u I want
55:17
you to compare those decisions versus each other and give for each one give two or three simulated outcomes. Then I
55:23
want you to create a a um a expedient version of Ethan and a thoughtful version of Ethan. Have them argue over
55:28
which path is best. Then I want you to give me a set of pros and cons for each of them and then select the best of
55:34
those. So a little chain of thought, little perspective taking. It's a very good very good one. We should we should
55:39
should try it. One thing I actually I I I did a couple of years back is I trimmed one on everything that Steve
55:45
Jobs had ever said because it was very interesting to get one that was founded in in his principle. So during COVID for
55:52
example, I asked it you know should should we go remote? Should we become a remote first company? And uh Steve
55:59
replied to me no 95% of all communication problems are solved by putting people in the same room. Always
56:05
colllocate teams. And it's quite interesting if you ground it in a in a person's writing and so on. It gets a
56:12
specific point of view that's not like the average of of of the internet. Yes. And that's what's so important when you
56:17
going back to that idea of where you get advice from like and that's why companies are important like your founder can have an influence on this.
56:23
Your principles if you give the AI a manual if this is what we believe that will get very different results than
56:28
someone who isn't. I think the idea of viewing this is this you know universal mind that is always giving you the right
56:34
answer. It's giving you opinions and points of view and that is a shapable thing. And if you believe your principles about the world are right,
56:40
giving those principles to the AI to have it help you execute those principles is a lot better than just letting it tell you stuff. One thing I
56:46
find quite quite interesting is that the systems are are yet to be optimized for for engagement. So we basically just
56:53
train them to predict the next the next token. But um if we know anything about
56:58
the sort of consumer services, they'll very soon um start um evolving to engage
57:04
us deep in deeper conversations. You can can imagine a a bot deployed in our
57:10
organization and we want to maximize the engagement with it and it starts enticing people and asking them
57:15
interesting questions and so on. What do you think will happen when this once these systems get optimized for for
57:21
engagement, which hasn't really been the case yet? Yeah, I'm nervous about that. Um, I think that that is uh is starting
57:27
to play with fire and the bigger labs are starting to realize they can do that, right? I think if you kind of look at the trend of OpenAI's stuff, it's
57:34
become more casual, more chatty. Um, there's a a fun incident where the new Llama 4 model was just released and it
57:41
was top of the leaderboards. Uh, and then it was revealed that the version that he had that was the top of the leaderboards was not the same model as
57:47
the model that was released to everybody. And if you look at the transcript from the leaderboard one, it's full of emojis. It tells you how
57:53
great you are. it like makes little jokes that are kind of semi funny and that's not the model they released,
57:58
right? There's there's an optimized for engagement thing that throws out a lot more tokens trying to flatter you and so I do worry about that, right? We have
58:04
some early evidence that it makes things more sticky and that you know that that optimizing for engagement is what made
58:10
social media such a risky place to be and I really do worry about that kind of outcome. Um and I think it's inevitable
58:16
though. Uh and so this is kind of you know what we do with that becomes a really big question and and and what um
58:25
one thing that I get asked a lot is you know how should we measure the outcome of of this? So you made a business
58:32
leader and they they want to measure one thing which showed that we deployed this and it um improved uh productivity. Um
58:40
what do you think we should be measuring? So, I'm going to this is one of my uh opinions I feel most strongly
58:46
about, which is in the early R&D phase, the worst thing you do is have a bunch of KPIs, right? We just talked about
58:51
maximizing for engagement. You if you maximize for something, you'll get the thing you maximized for and probably not the other stuff. We don't know what
58:57
these systems do. You're spending R&D cash on this. Like, we know you get performance improvements because we'll see those. But if you're optimizing for
59:03
performance, is that how many word documents are produced every day? Is that how fast people turn around their
59:09
reports? Like, is that what you want? Like part of the problem is organizations aren't built for the KPIs
59:14
that you need to have. Like people are like it it used to be valuable to produce as many words as possible. Like
59:20
if you can write a good report or four PowerPoint presentations or cover six companies now do you want people covering 25 companies 300 PowerPoints a
59:27
week like what what are we maximizing the number of lines of code that people are writing? I mean you can imagine some
59:33
cases how quickly clear the backlog is important but is that what we want to have people do? So I I really worry
59:38
about KPIs, measurable KPIs being doom, especially because they end up always end up falling to cost savings and
59:44
they're always 30% cost savings and they're always let's fire people which undermines everything you're doing. So I think people do need to adopt an an R&D
59:51
mindset like the productivity gaines are pretty clear and will happen pretty quickly and fine throw them into coding
59:56
because like coding there's clear productivity gains but I really worry about people who's like productivity gains for document writing feels like a
1:00:02
risky thing to do because what are you optimizing for?
1:00:08
[Music]