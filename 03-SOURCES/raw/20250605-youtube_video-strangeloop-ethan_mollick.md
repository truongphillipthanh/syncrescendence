# The AI Threshold: Thinking Beyond Incremental Optimization

You can either fire most of your staff and make more money per barrel of ale, or you can be Guinness and hire 100,000 people and expand worldwide. The choice before every company confronting AI is precisely that stark. Too many organizations are taking the first path when the opportunity lies in the second. This fundamental tension—between cost optimization and transformational ambition—shapes everything about how companies should approach AI strategy.

## The Roots of Modern AI Thinking

The intellectual foundations for this moment were laid decades ago. At MIT, working alongside Marvin Minsky and others at the Media Lab during one of the various AI winters, the dominant approach assumed that creating intelligence required elaborate, complex schemes. Some projects attempted to observe everything a baby did in hopes of reverse-engineering human learning. Minsky developed his Society of Mind, a theory of interlocking pieces that might generate consciousness. All of these approaches were intellectually sophisticated and ultimately incorrect.

What actually worked turned out to be almost deceptively simple: take enormous amounts of language data, feed it into a learning system, and you end up with large language models. The irony is striking—the solution bypassed the complex theorizing entirely.

But beneath those technical details, two competing philosophical traditions did emerge from that era, and both remain profoundly relevant today. Douglas Engelbart championed the vision of augmenting human intelligence—technology as amplifier of human capability. Marvin Minsky, by contrast, was focused on replacement, on making machines conscious and achieving artificial intelligence that could substitute for human thought. That debate never fully resolved, partially because it remained largely theoretical. Now it's suddenly urgent. We must ask: Do we use AI for augmentation or replacement, and what does each actually look like in practice?

## Redefining What Intelligence Means

The question of what constitutes artificial intelligence has taken on new urgency. Recently, a new paper demonstrated that GPT-4.5 can pass the original three-party Turing Test. In fact, 70% of the time, people identify the AI as the human in the conversation. Whether that actually means anything is debatable, but it's certainly interesting.

Yet the original Turing Test was designed for a world where we had nothing to test. It worked brilliantly as a theoretical marker when computers obviously failed it. The problem now is that AI is acing all the creativity tests we created, but those tests were always mediocre benchmarks for human performance anyway. When the field relied on something like the "reading the mind in the eyes" test—where subjects identify emotions from photographs of eyes—we were using measures designed for humans, not machines.

I tend toward practical definitions. Can an agent go out into the world and make money? Can it discover new knowledge, test hypotheses, and produce results? These seem like better measures than abstract philosophical tests.

The deeper realization is that AGI itself is likely not a moment but a phase—a period we're moving through rather than a threshold we'll dramatically cross. There won't be fireworks. Tyler Cowen recently claimed that O3 is AGI, and when asked why, he said it's like pornography—he knows it when he sees it. The meaninglessness of the distinction is becoming clear.

What matters more than the philosophical question is what happens when you connect AI intelligently to real systems and company processes. When AI is embedded into organizational workflows rather than just used for conversation, something emerges that's much better than the sum of its parts. That's where the real power lies.

## The Organizational Challenge: AI's Jagged Frontier

AI capability is fundamentally jagged. It excels in specific domains and completely fails in others—often in unexpected ways. You might have a system that's brilliant at legal analysis but terrible at summarizing complex financial documents. It can write Python code competently but fail at simple logical reasoning tasks. This jaggedness creates a critical organizational design problem.

Traditional org charts were built for a human workforce with roughly predictable capability profiles. A senior analyst can do virtually any analytical task. A junior engineer can learn from a senior engineer. But an AI system? It might be better than any human in the room at data synthesis yet completely incapable of making judgment calls that a high schooler could handle.

This forces companies to redesign how they think about work. You can't just plug AI into an existing organizational structure and expect it to work. The jagged frontier means you need to map AI's actual capabilities ruthlessly, then restructure work around those capabilities rather than hoping the technology will adapt to your processes.

## Three Essential Ingredients for Successful AI Adoption

Based on what we're seeing across organizations attempting serious AI implementation, three ingredients consistently separate success from disappointment.

**First, you need domain expertise.** Someone who understands what the work actually requires. This person knows the difference between what looks like good output and what actually solves the problem. Without domain expertise, you're flying blind—you can't evaluate whether the AI is actually competent in this space.

**Second, you need prompt craft skill.** Prompting is becoming a foundational business skill, though most organizations haven't recognized it yet. This isn't about typing clever instructions to ChatGPT. It's about understanding how to structure information, what context to include, how to frame a problem so the AI understands what you actually need. This is learnable and teachable, but it requires intentional development.

**Third, you need workflow integration.** AI doesn't work as a standalone tool. It needs to be woven into how work actually gets done. This might mean changing when in a process the AI step occurs, what happens before and after AI involvement, how outputs are validated and refined. The workflow has to be designed for AI collaboration.

## The Case Against Efficiency and the Case for Abundance

Most companies are approaching AI as a cost-reduction tool. Reduce headcount, flatten organizational layers, do the same work with fewer people. This logic is understandable and wrong.

The problem isn't efficiency itself—productivity gains from AI are real and will materialize. The problem is narrowing your ambition to efficiency when AI opens doors to abundance. What if instead of asking "How can we do the same work with fewer people?" you asked "What new work becomes possible? What couldn't we do before that we could do now? Where is there currently unmet demand in our market that we were rationally avoiding because it was too expensive?"

This is the fundamental choice. You could optimize the past more efficiently, or you could invent the future differently. Most companies are still optimizing the past.

A concrete example: A company might currently review their top fifty customers in depth annually. With AI, reviewing the top two hundred becomes feasible. Which is more valuable—saving the cost of three analyst reviews of the top fifty, or gaining insight into a hundred new customers? The abundance mindset opens completely different strategic options.

## Redesigning Roles, Not Eliminating Them

The question of what roles to hire for—and whether traditional org structures make sense—is urgent and complex. The Chief AI Officer position has become common, but it's often misunderstood. 

The effective model isn't a single person responsible for AI as a siloed function. That typically fails because every department needs AI competency embedded within it. A better approach is thinking of AI adoption as a capability that needs to be distributed across the organization, supported by people who understand both AI and the specific domain.

What you actually need to hire for are people who can do domain work and learn to work with AI as a tool. You need people who understand what good looks like in their domain and can evaluate whether the AI is actually producing it. You need people who can design workflows that incorporate AI intelligently. And you need some number of people who understand the technology deeply enough to troubleshoot and innovate.

But the traditional hierarchical organization chart—where information flows up and decisions flow down—is fundamentally mismatched to an AI-native environment. When your analyst has AI assistance at their fingertips, they need autonomy to experiment and iterate. The control structures that worked for human-only organizations create friction in an AI-augmented one.

## Human and Machine Collaboration: The Interface Problem

How humans and AI should collaborate is still unsolved territory. There's the philosophical question—do we prefer to preserve certain human functions as fundamentally human, regardless of efficiency? There's also the practical question of what actually works best.

Consider the Hollywood example: An actress working with a synthetic version of her own voice can test audio dubs instantly, without waiting for her schedule to align. The AI voice is a testing tool. But union protections (appropriately) ensure that the actual recorded performance for distribution must be her genuine voice. This creates a world where the humanness is preserved intentionally, through structural choice, while still leveraging AI's capabilities.

What this suggests is that we can build systems where machines augment human work at specific high-leverage points, while the overall creative or decision-making authority remains human. But this requires deliberate design—it won't happen automatically.

## The Danger of Knowledge Collapse

An underappreciated risk with AI adoption is the potential collapse of apprenticeship and learning structures. Historically, you learned a trade by working alongside a master, gradually building tacit knowledge through practice and correction. The journey itself built deep understanding.

With AI, there's a shortcut available. You can prompt an AI to do the work and get good output without developing mastery. This feels efficient in the moment. Over time, it creates organizations where fewer people actually understand their domain deeply. If that AI system breaks or doesn't work in a novel situation, there's no fallback of human expertise. The knowledge has effectively disappeared.

This suggests a different kind of discipline is needed. Organizations need to be intentional about which work they outsource to AI and which work they reserve for human development and learning, even when the AI could do it faster or better. This is an investment in organizational resilience and future capability.

## Measuring What Matters: The KPI Trap

Every business leader eventually asks: "How do we know this is working? What metrics should we track?"

The honest answer is that in the early R&D phase of AI adoption, KPIs are counterproductive. You optimize for what you measure. If you measure documents produced, you'll get more documents. If you measure backlog clearance, work will get cleared faster. But none of these necessarily maps to value created.

The deeper problem is that the work organizational structures were built for didn't require these metrics. It used to be valuable to produce as many words as possible—longer reports, more documents, more output volume. Now you can ask: Do you want people covering five companies or twenty-five? Do you want four PowerPoint presentations or three hundred a week? What are you actually optimizing for?

With AI, productivity gains are real and will be visible. They'll happen quickly. But that doesn't mean those gains are capturing the value you should be capturing. For work like document production, "productivity" metrics can be actively misleading. They incentivize volume over impact.

The better approach is adopting an R&D mindset. Experiment. Learn where AI creates genuine advantage. Let productivity improvements happen as a side effect of better work, not as the metric driving the work. This requires patience and tolerance for ambiguity that most organizations struggle with.

## Building an AI-Native Mentality

The companies that will thrive aren't those that implement AI most efficiently. They're the ones that think from first principles about what becomes possible with AI and then rebuild their organizations around that possibility.

This means starting with a different set of questions. Not "How can we use AI to do what we do now faster?" but "What couldn't we do before that's now possible?" Not "How can we preserve our current org chart?" but "What structure makes sense if every person has AI assistance?" Not "How do we cut costs?" but "Where is there unmet demand we can now serve?"

It means recognizing that prompting—teaching people to communicate their intent clearly to machines—is a learnable skill that's becoming foundational. It means defending human elements you actually care about through structural choice, not hoping they'll survive market forces. It means thinking about knowledge preservation alongside efficiency gains.

Most of all, it means resisting the gravitational pull toward incremental thinking. The easy path is optimization. The harder, more valuable path is transformation. That's the choice every leader needs to make, and it needs to be made deliberately, not inherited from assumptions about what technology demands.