# The Next Phase of Accelerated Computing and AI: NVIDIA's Vision for America's Technological Leadership

Welcome to GTC. We have a lot to cover today. GTC is where we talk about industry, science, computing, the present, and the future.

NVIDIA invented a new computing model for the first time in 60 years. A new computing model rarely comes about—it takes an enormous amount of time and a set of conditions. We invented this computing model because we wanted to solve problems that general-purpose computers, normal computers, could not. We also observed that someday transistors will continue—the number of transistors will grow—but the performance and the power of transistors will slow down, that Moore's law will not continue beyond, limited by the laws of physics. That moment has now arrived. Dennard scaling has stopped nearly a decade ago, and in fact the transistor performance and its power has slowed tremendously, and yet the number of transistors continued. We made this observation a long time ago, and for 30 years we've been advancing this form of computing we call accelerated computing. We invented the GPU. We invented the programming model called CUDA. And we observed that if we could add a processor that takes advantage of more and more transistors, apply parallel computing, add that to a sequential processing CPU, we could extend the capabilities of computing well beyond. That moment has really come. We have now seen that inflection point.

## The Architecture of Accelerated Computing

In the beginning, we started with just two things: a GPU and a sequential processor, the CPU. We then realized, because of the laws of physics, that we need to put these chips as close as possible together to save energy. Energy is the scarcest resource today. And so we created system architecture where we took the GPU, we took the CPU, and we connected them together in the fastest possible way with what we call NVLink. NVLink is essentially a super high-speed interface that makes it possible for these two chips to work together. But that wasn't enough, because this is accelerated computing. And when you're doing accelerated computing, you're processing data at an incredible rate, and you need to be able to feed these GPUs and CPUs. So we needed to increase the data rate. And so we created switches—network switches that allowed us to connect every single GPU to every single GPU inside a system.

This architecture I just described is one of the most fundamental computer architectures today. In fact, it's a new computing model. This architecture is designed to run CUDA and CUDA-X, and it has millions of cores. Today's data center—we call them AI factories—has thousands of these systems. Each one is several hundred thousand dollars, and you connect thousands of them together. You connect them with our network interface called ConnectX, the world's most advanced network interface, the world's fastest, and you connect them with our switches—these network switches are the most advanced in the world. The reason for that is because we have to disaggregate the computers so that they can work together as one giant brain. And so the network is unbelievably complicated. And so our network switches have four CPUs, a control plane, several DPUs for all kinds of data processing units, GPUs, to handle all kinds of AI tasks within the network. These network switches are computers unto themselves.

## The CUDA-X Ecosystem

The foundation of everything we do is CUDA—a programming model that makes it possible to take algorithms and accelerate it on our GPUs. Over the last 30 years, we've created libraries that sit on top of CUDA. These libraries we call CUDA-X. CUDA-X are full stack AI. The reason why it's called full stack AI is because we're accelerating the entire computing stack from the algorithms of signal processing—LTE and 5G and now 6G—the algorithms that relate to video compression and video codecs, the algorithms that relate to databases and data processing and data analytics and data warehousing—all the way up to large language models and computer vision and speech recognition, robotics, graphics and visualization, high-performance computing for scientific computing. The entire computing stack has been accelerated on CUDA-X running on top of CUDA running on top of our GPU systems. This is the new computing model.

It took about 30 years to get to where we are, because remember, at a time when transistor performance was still scaling, when Dennard scaling was still working, you didn't need accelerated computing. In fact, CPU makers that are making general-purpose computing were scaling quite nicely. But for the last decade, CPU performance per transistor has not been scaling. That's the reason why accelerated computing is now at an inflection point—we're now growing very, very fast because we've hit that inflection. That first inflection is going from general-purpose computing to accelerated computing.

The second inflection point is now upon us. Imagine going from something you've done for 60 years—writing software explicitly, codifying all the rules of what the software should do—and now to this new world where you collect data, you compile the data, you learn from the data, you essentially learn the patterns from the data, the insights from the data, and you turn that into software. It's an entirely different way of creating software. We call that artificial intelligence. This is the second platform transition. Two platform transitions happening at the same time, which is the reason why we're feeling such incredible growth.

## Opening New Markets Through Acceleration

These CUDA-X libraries have opened up new markets for us. Each one of these markets is very large. The telecommunications industry—we're now working with practically everybody on 5G advanced and 6G. The reason for that is because telecommunications is now about digital signal processing and artificial intelligence. All of the signals that come from the environment around us—the wireless transmissions, the wireless antennas, and radios—it is collected as data, and from that data we can learn the channels, we can learn the environment around us, and we could figure out how to do beamforming and beam-steering so that we could deliver wireless communication to customers all over the world.

The data processing industry—we're now working with practically every single one of them: Snowflake, Databricks, and many, many others. The reason for that is because accelerated computing is also at an inflection point. Before, the cost of accelerators relative to the performance did not justify acceleration because the CPUs were still doing fine. Today, the CPUs have run out of steam. Accelerated computing is absolutely necessary. And the amount of data that's in the world is growing exponentially—we all know that.

Financial services—we're working with practically all of them. Every single one of the companies that trades or manages risk, they're all accelerating their dataframes and their computing. We're working with all the major scientific computing companies. And today you're going to hear that we're announcing a brand new supercomputer with the Department of Energy.

Sovereign AI—every country is standing up sovereign AI. Every country realizes that the data they collect, the culture that comes from that data, the intelligence that comes from that data, should be their own intelligence. They should own and produce that intelligence. Every country is now investing in sovereign AI, building AI infrastructure all over the world.

Cybersecurity is now fundamentally driven by AI. We're working with CrowdStrike, one of the largest cybersecurity companies in the world, as well as many others. Healthcare—practically every single healthcare company is now using Hopper. In fact, clinical evidence says that we've extended the lives of cancer patients by about a year, which is a landmark achievement, through a whole bunch of different AI algorithms.

The automotive industry—by now everybody knows that the world's car companies are using our computing stack. We just announced a whole bunch more yesterday. And going up the stack, SaaS companies—every single SaaS company is now building AI into their SaaS. Robotics, earth science, materials science, and so on.

## The Telecommunications Revolution: 6G and AI-Native Networks

Let me show you one example—telecommunications. 5G and 6G is really different than 4G. 4G is really about high-speed data for smartphones. 5G is really about devices—billions and billions of devices, all kinds of different devices. And these devices require not just high-speed data, they also require very low latency, ultra-low latency, and ultra-high reliability. It's not just about high-speed data anymore. The combination of these three things—high-speed data, low latency, and high reliability—are necessary for all of the sensors in the autonomous vehicles around us. 

We need to use machine learning, the AI, to learn from the environment around us, to not just increase the channel width but increase the channel throughput. We want to increase the throughput, but we also want to save energy. The way that we do that is by learning from the environment around us and by tuning the radios in a way that takes advantage of the environment around us through something called beamforming and beam-steering. It's all done using AI.

We're building the entire 5G and 6G stack on top of our GPU systems. You could do everything from what we call the radio unit, the distributed unit, the centralized unit, to the core—you could do it all using CUDA on top of our systems. This is the most open, large ecosystem of anybody in the world, and it allows you to go use anybody's radio, plug into it, integrate into it. It's designed from the ground up for multivendor, which is the reason why it's so important and so unique. If you architect your 6G network from the ground up on top of our platform, it is AI-native. Then going forward, the entire network could be tuned and optimized using AI.

One of the most world-renowned 5G companies is partnering with us, and for 6G we're building it from the ground up. Let's take a look at that video. Nokia is building the world's first AI-native 6G on our new platform called ARC. This is a landmark because, remember, the cellular industry is several trillion dollars. This is a major win in an important industry, an important inflection point for us.

## Quantum Computing: The Next Frontier

I'd like to move on to quantum computing. There are about a billion transistors today inside a CPU, and if you're fortunate, maybe you can get a few gigahertz out of it. We can calculate somewhere around a billion billion floating-point operations per second. But if you can have a quantum computer where you have a thousand qubits, it would be able to do two to the power of a thousand calculations—unimaginable amounts of computing. Quantum computing promises to open up entirely new fields.

We believe the path to quantum computing is through the hybrid computing model—just like accelerated computing, where we have a general-purpose computer, the CPU, connected to a specialized processor, the GPU. We believe the path to quantum computing is connecting a general-purpose computer to a quantum computer, and then you would orchestrate the computing back and forth. The quantum computer would be used for computing workloads that have something to do with physics—quantum physics related: quantum chemistry, quantum materials, biology, medicines, drug discovery—because nature is quantum, not digital. We believe we're going to discover all kinds of new algorithms, all kinds of new opportunities. The key is to create the connection that scales between a general-purpose computer and a quantum computer. The connection has to be coherent. It has to be deterministic. It has to be lossless. It can't be on Ethernet. It can't be on long haul. It has to be incredibly close to each other.

We believe that quantum computing has a bright future. The path to quantum computing should be that we simulated it here in the data center, in our AI factory. We call this quantum in the cloud. We created a simulation platform for quantum computing. We're partnering with Google. We're partnering with Microsoft Azure. We're partnering with every major quantum computer company—all the way from qubits that are trapped ion photonic to superconducting, so that we could simulate it in this cloud, in this AI factory. Then we create the libraries for NVIDIA CUDA Quantum. CUDA Quantum would make it possible for the researchers all over the world to prepare themselves for quantum computing even before the quantum computers arrive.

Today, we're announcing that we're going to connect quantum computers with our GPU systems using a brand new fabric that we created called NVQLink. This fabric would make it possible for you to scale from one GPU to many GPUs connected to many quantum computers so that you could have a hybrid computing system. We're partnering with every single major quantum company in the world building their quantum computers on this fabric so that the researchers could access these quantum computers. This is a really exciting moment for quantum computing, and I'm looking forward to it.

## National AI Infrastructure: Department of Energy Partnership

I'd like to announce something that I'm really proud of. We're partnering with the Department of Energy to build seven brand new AI supercomputers. These AI supercomputers will be spread all over the country—east coast, west coast, middle of the country—and they will support all of the national labs and the large community of researchers. This is a truly extraordinary partnership. These new supercomputers will be extraordinary. They're designed from the ground up for AI simulation and AI physics. Let me show you what they look like.

This is called the AI Factory. All these AI factories will be connected together. This is a fabulous partnership, and I'm really proud to be working with the Department of Energy and all the national labs to help extend supercomputing to the next era of accelerated computing and AI.

## The New Industrial Revolution

I want to talk about AI. We've seen the personal computer revolution. We've seen the internet revolution. We've seen the mobile revolution. We've seen the cloud revolution. I think we're at the beginning of the next revolution. People call it the AI revolution, but I think it's probably more appropriate to call it the new industrial revolution. The reason for that is because it is going to affect practically every industry. The agricultural industry, the manufacturing industry, the transportation industry, healthcare, financial services, retail, telecommunications, media and entertainment, education, government services, legal services, customer service—practically every single industry will be affected.

The reason for that is because every industry has an enormous amount of data about their business, about their customers, about their products, about their services. That data, if you process it, if you learn from it, if you can gain insight from it, if you can forecast from it, if you can understand it, you can make better decisions, you can be more productive, you can deliver better services. This is the reason why this is the new industrial revolution.

The AI revolution is going to create whole new industries. It's going to reinvent every single industry we have. And because of that, the amount of computing is going to grow. Remember, the amount of computing that you need is determined by two things: how much data you have and how much you want to learn from it, and how much computing you're willing to do. Those two things together determine how much computing you need. Well, in this new world, we're going to be computing on all of the data that we have in the world. So the amount of computing is going to be extraordinary. We're going to be computing not just on text, not just on images, but we're going to be computing on videos, we're going to be computing on proteins, we're going to be computing on chemicals, we're going to be computing on materials, we're going to be computing on climate, we're going to be computing on weather, we're going to be computing on simulations of all kinds. The amount of computing is going to be extraordinary.

## The Three Scaling Laws

There are three scaling laws. The first scaling law—and I think everybody knows this by now—is called pre-training scaling. Pre-training scaling says that the more data you have, the more parameters you have in your neural network, and the more computing you throw at it, the more you'll learn, the better the AI will be. This is pre-training scaling. Pre-training scaling is what got us here. It's what created ChatGPT. It's what created all of these amazing large language models.

The second scaling law is called post-training scaling, and post-training scaling is really about reinforcement learning, reinforcement learning from human feedback. You essentially teach the AI what good looks like after you've trained it. This is the way that you could take an AI that's very capable but not very aligned with your preferences, not very aligned with what you think is helpful, and you teach it to be helpful, you teach it to be harmless, you teach it to be honest. This is post-training scaling.

The third scaling law—and this is a new scaling law that I think is going to be really exciting—is called test-time scaling. Test-time scaling is about reasoning. When you give the AI a prompt, the AI reasons about it. It thinks about it. It tries different things. It explores different possibilities before it answers you. The longer it reasons, the better the answer. That's test-time scaling.

These three scaling laws—pre-training scaling, post-training scaling, and test-time scaling—are all about computing. They're all about more computing equals better AI. And so the demand for computing is going to be extraordinary.

The way that we're going to do that is we're going to continue to scale our systems. We're going to continue to innovate. We're going to continue to do what we call extreme co-design. Extreme co-design is where we design every single component from the transistor all the way to the chip, all the way to the system, all the way to the rack, all the way to the data center at the same time. We optimize for end-to-end performance and end-to-end energy efficiency. This is the only way that we're going to be able to continue to scale to meet the demand.

## Grace Blackwell: The Thinking Machine

Let me show you our new system. This is the Grace Blackwell NVL72. We call it a thinking machine. This is not just a computer. This is a machine that reasons. This is a machine that thinks. Let me show it to you.

This is the Grace Blackwell NVL72. Each one of these liquid-cooled racks has 72 GPUs. Each GPU has 208 billion transistors. These 72 GPUs are all connected together as if it's one giant chip. The way that they're connected is through our NVLink switch. These NVLink switches make it possible for any GPU to talk to any GPU at 1.8 terabytes per second. The system has 130 terabytes of memory, and the system bandwidth is 130 terabytes per second. This is an extraordinary system. The entire system is run by 36 Grace CPUs. The Grace CPUs are the world's highest-performance CPUs for AI. They run all of the operating system software. They run all of the CUDA-X libraries. They feed the GPUs. They basically orchestrate this entire data center.

This system has 1.4 exaflops of AI computing. When you configure it the right way, you could get three exaflops of AI computing. This is a truly extraordinary system. The power consumption is about 120 kilowatts, and when you configure it for test-time scaling for reasoning, the power consumption goes up to about 180 kilowatts. But the amount of computing that comes out of it is extraordinary—three exaflops.

The Grace Blackwell NVL72 is designed from the ground up for the three scaling laws. It's designed for pre-training. It's designed for post-training. And it's designed for test-time scaling. When you configure it for test-time scaling, we change the configuration. We disaggregate the system in a way that's different. We want to maximize the memory capacity and the memory bandwidth so that the AI could think longer, reason longer. When you do that, you get three exaflops out of this 120-kilowatt system. This is a truly extraordinary achievement.

## The Demand for AI Infrastructure

The demand for this system is exceptional. Every single cloud service provider is buying them. Every single enterprise is building AI factories. The reason why is really clear. If you bought a data center filled with CPUs just a few years ago, you would be generating about one to two billion dollars of revenue per year. If you bought that same data center and filled it with our Hopper systems—the previous generation—you'd be generating about 10 to 20 billion dollars of revenue per year. The reason why is because the cloud service providers are now selling something called AI as a service, tokens. Tokens are essentially the output of these large language models. The amount of tokens that you could generate is directly related to how much computing power you have.

If you generate about 20 billion dollars of revenue per year from a Hopper data center, and you fill that data center with Blackwell, you'll generate about 40 to 45 billion dollars of revenue per year. The reason for that is because Blackwell is about twice as efficient as Hopper in generating these tokens. This is the reason why the demand is so extraordinary. The cloud service providers understand this. The enterprises understand this. This is the most valuable computing infrastructure that has ever been built.

The capital expenditures of the cloud service providers—the hyperscalers—is somewhere north of 200 billion dollars this year. Next year it's going to grow to over 300 billion dollars. The vast majority of that is coming to us. This is a truly extraordinary moment for our company, but also for the entire industry. The reason why this is so important is because this infrastructure is essential. It's not optional. Every company is going to need AI. Every country is going to need AI. This infrastructure is as essential as electricity was to the last industrial revolution. AI is the electricity of this industrial revolution.

## Made in America for the World

I'm proud to announce that Blackwell is made in America. We're manufacturing Blackwell with TSMC in Arizona. We're manufacturing Blackwell with Samsung in Texas. We're manufacturing our CoWoS advanced packaging with Amkor in Arizona. We're manufacturing our networking switches with Foxconn in Mexico. Blackwell is made in America and made for the world.

This is a truly important milestone. The supply chain that we've built is extraordinary. We're working with all of the major partners to make sure that we can deliver these systems to customers all over the world. The demand is so great that we have to scale our manufacturing capacity incredibly fast. I want to thank all of our partners—TSMC, Samsung, Amkor, Foxconn—for working with us to make this possible.

## The Innovation Cadence: Annual X-Factors

We deliver these systems on a one-year rhythm. Every single year, we deliver a new system that is a multiple—an X-factor—better than the previous generation. This is extraordinary. This is unprecedented in the computer industry. No computer company in the history of computing has ever delivered X-factor improvements year after year after year. The reason why we can do that is because of extreme co-design. We design everything from the transistor to the data center at the same time, and we optimize for end-to-end performance.

Let me show you what's coming next. After Blackwell comes Vera Rubin. Vera Rubin is named after the famous astronomer who discovered dark matter. Vera Rubin will be another X-factor improvement over Blackwell. The Vera Rubin GPU will have more transistors, more memory, more memory bandwidth. The Vera Rubin system will have a new CPU—a more powerful Grace CPU. It will have a new network interface—BlueField-4. BlueField-4 is a DPU, a data processing unit, that has a 400-gig network interface, and it has AI engines built into it so that it could process all of the data coming off the network.

The Vera Rubin system will have a new NVLink switch. The new NVLink switch has even more bandwidth, even more ports, so that we could scale to even larger systems. This is going to be extraordinary. Vera Rubin will be available next year. We're on a one-year cadence—Hopper, Blackwell, Vera Rubin. Each one of them is an X-factor improvement over the previous generation. This is unprecedented, and this is the reason why we're able to continue to lead the industry.

## AI Factories: The New Power Plants

Let me talk about AI factories. An AI factory is a data center that produces AI. Just like a power plant produces electricity, an AI factory produces intelligence. These AI factories are being built all over the world. They're being built by cloud service providers, by enterprises, by countries building sovereign AI. These AI factories are built with our DGX systems—our DGX Cloud systems and our DGX SuperPOD systems.

But an AI factory is not just about the computers. An AI factory has to have the entire software stack. It has to have the operating system. It has to have all of the frameworks. It has to have all of the tools. It has to have all of the applications. We've created an entire software stack that makes it possible for you to stand up an AI factory very, very quickly. This software stack is called NVIDIA AI Enterprise. It runs on top of our DGX systems, and it makes it possible for you to build AI applications, train your models, and deploy your models into production.

But there's another type of AI factory that's emerging—the digital twin factory. A digital twin factory is where you build a simulation of your physical factory, and you use that simulation to optimize your factory, to train your robots, to test your processes before you deploy them in the physical world. This is going to be the way that we're going to build factories in the future. Every factory will have a digital twin. We call this platform Omniverse. Omniverse is a platform for building physically accurate digital twins.

We've created a new offering called Omniverse DSX. DSX stands for digital simulation experience. This is a turnkey offering that makes it possible for any company to build their digital twin. It comes with all of the software, all of the physics engines, all of the AI, all of the rendering—everything that you need to build a physically accurate digital twin of your factory, your warehouse, your building, your city. This is going to be transformational for manufacturing.

## Open Models and the AI Ecosystem

Let me talk about open models. The world of AI is going to have closed models and open models. Closed models are models that are provided as a service. You go to OpenAI, you go to Anthropic, you use their models. These models are incredibly capable. They're at the frontier of AI. But there are many companies, many industries, many countries that want to have their own models. They want to customize the models for their own data, for their own domain, for their own language, for their own culture. This is where open models come in.

Open models are models that you could download. You could fine-tune them. You could customize them. You could deploy them on your own infrastructure. We're partnering with every single major open model company—Meta with Llama, Mistral, Hugging Face. We're working with all of them to optimize these models on our systems, to make sure that they run incredibly efficiently. We're also working with them to make sure that these models are safe, that they're aligned, that they're reliable.

The open model ecosystem is thriving. There are thousands and thousands of models on Hugging Face today. Every single day, hundreds of new models are being created. This open ecosystem is incredibly important because it democratizes AI. It makes AI accessible to everyone. You don't have to be a giant company. You don't have to have a billion dollars. You can download an open model, you can fine-tune it on your data, and you can create incredible AI applications.

We're seeing this happening in every single industry. In healthcare, people are taking these open models, fine-tuning them on medical data, creating diagnostic models, drug discovery models. In financial services, people are taking these models, fine-tuning them on financial data, creating fraud detection models, risk management models. In manufacturing, people are creating quality inspection models, predictive maintenance models. The open model ecosystem is absolutely thriving, and we're supporting it with all of our might.

## Physical AI: The Next Wave

Let me talk about physical AI. Physical AI is AI that interacts with the physical world. Until now, most of AI has been about understanding language, understanding images, understanding documents. But the next wave of AI is about understanding the physical world, interacting with the physical world, manipulating the physical world. This is physical AI.

Physical AI requires three things. First, it requires perception—the ability to see, to sense the world around you. Second, it requires understanding—the ability to understand what you're seeing, to understand the physics, to understand the dynamics. Third, it requires action—the ability to plan, to control, to manipulate. These three things together—perception, understanding, and action—that's physical AI.

The applications of physical AI are extraordinary. Autonomous vehicles—that's physical AI. Robots in factories, robots in warehouses, robots in hospitals—that's physical AI. Drones, agricultural robots, construction robots—all of that is physical AI. This is going to be one of the largest opportunities in computing.

To enable physical AI, we've created an entirely new platform. We call it the Omniverse platform. Omniverse is where you build digital twins of physical things. You build a digital twin of a factory, a digital twin of a warehouse, a digital twin of a city, a digital twin of a robot. In that digital twin, you train your AI. You simulate your robot. You test it. You validate it. Only after you've done that do you deploy it in the physical world. This is the way that physical AI is going to work.

## America's Reindustrialization

I want to talk about something that I'm really excited about. America is reindustrializing. For the last several decades, we've been offshoring manufacturing. We've been offshoring production. But now we're bringing it back. We're bringing it back because of AI, because of robotics, because of automation. The economics have changed. It's now possible to manufacture in America economically, competitively, because we can automate so much of it.

We're seeing this happening in every single industry. The automotive industry is reindustrializing in America. Tesla is building the world's most advanced factories right here. The semiconductor industry is reindustrializing. TSMC is building fabs in Arizona. Samsung is building fabs in Texas. Intel is building fabs all over the country. The electronics industry is reindustrializing. Foxconn is building factories in Wisconsin, factories in Mexico to serve the American market.

This reindustrialization is being enabled by AI and robotics. The factories of the future are going to be highly automated. They're going to be orchestrated by AI. Robots are going to work side-by-side with people. This is the future of manufacturing. And it's happening right here in America.

Let me show you an example. We're working with Foxconn to build the factory of the future. This is a factory where AI agents monitor operations, alert engineers of anomalies and safety violations and quality issues. AI agents power interactive coaching systems for worker onboarding. People and robots work together. This is the future of manufacturing.

The age of US reindustrialization is here with people and robots working together. The factory is essentially a robot that's orchestrating robots to build things that are robotic. The amount of software necessary to do this is so intense that unless you could do it inside a digital twin, to plan it, to design it, to operate it inside a digital twin, the hopes of getting this to work is nearly impossible.

We're also working with Caterpillar, a hundred-year-old company, incorporating digital twins in the way they manufacture. These factories will have robotic systems, and one of the most advanced is Figure. Brett Adcock founded this company three and a half years ago. They're worth almost 40 billion dollars. We're working together—training the AI, training the robot, simulating the robot, and the robotic computer that goes into Figure is really amazing. It is very likely that humanoid robots—and Elon is also working on this—this is likely going to be one of the largest new consumer electronics markets and surely one of the largest industrial equipment markets.

We're working with Agility on robots for warehouse automation, with Johnson & Johnson on surgical robots that will perform completely noninvasive surgery at a precision the world's never seen before. And of course, the cutest robot ever—the Disney robot. We're working with Disney Research on an entirely new simulation platform based on revolutionary technology called Newton. That Newton simulator makes it possible for the robot to learn how to be a good robot inside a physically aware, physically based environment.

Everything you're seeing—that is not animation, it's not a movie, it's a simulation. That simulation is in Omniverse, the digital twin. These digital twins of factories, digital twins of warehouses, digital twins of surgical rooms, digital twins where robots could learn how to manipulate and navigate and interact with the world—all completely done in real time. This is going to be the largest consumer electronics product line in the world.

## Robotics: The Inflection Point

Now, humanoid robots are still in development. But meanwhile, there's one robot that is clearly at an inflection point and is basically here, and that is a robot on wheels—a robotaxi. A robotaxi is essentially an AI chauffeur. 

We're announcing the NVIDIA DRIVE Hyperion. This is a big deal. We created this architecture so that every car company in the world could create vehicles—commercial, passenger, or dedicated to robotaxi—that are robotaxi-ready. The sensor suite with surround cameras and radars and lidar makes it possible for us to achieve the highest level of surround cocoon sensor perception and redundancy necessary for the highest level of safety.

DRIVE Hyperion is now designed into Lucid, Mercedes-Benz, Stellantis, and many other cars coming. Once you have a basic standard platform, then developers of autonomous vehicle systems—and there's so many talented ones: Wayve, Waabi, Aurora, Momenta, Neuron, WeRide, and so many others—can then take their AV system and run it on the standard chassis. The standard chassis has now become a computing platform on wheels. And because it's standard and the sensor suite is comprehensive, all of them could deploy their AI to it.

The robotaxi inflection point is about to get here. A trillion miles a year are driven. A hundred million cars are made each year. There's some 50 million taxis around the world. It's going to be augmented by a whole bunch of robotaxis. So it's going to be a very large market. To connect it and deploy it around the world, we're announcing a partnership with Uber. We're working together to connect these NVIDIA DRIVE Hyperion cars into a global network. In the future, you'll be able to hail one of these cars, and the ecosystem is going to be incredibly rich. We'll have Hyperion robotaxi cars all over the world. This is going to be a new computing platform for us, and I'm expecting it to be quite successful.

## The Path Forward

Let me summarize what we talked about today. At the core of this are two platform transitions. First, from general-purpose computing to accelerated computing. NVIDIA CUDA and those suite of libraries called CUDA-X have enabled us to address practically every industry, and we're at the inflection point. It is now growing as a virtual cycle would suggest.

The second inflection point is now upon us—the second platform transition from classical handwritten software to artificial intelligence. Two platform transitions happening at the same time, which is the reason why we're feeling such incredible growth.

We spoke about quantum computing. We spoke about open models. We spoke about enterprise with CrowdStrike and Palantir accelerating their platforms. We spoke about robotics—potentially one of the largest consumer electronics and industrial manufacturing sectors. And we spoke about 6G.

NVIDIA has new platforms for 6G—we call it ARC. We have a new platform for robotics cars—we call that Hyperion. We have new platforms even for factories. Two types of factories: the AI factory we call DSX, and factories with AI we call Mega. And now we're also manufacturing in America.

Thank you for joining us today, and thank you for allowing us to bring GTC to Washington D.C. We're going to do it hopefully every year.