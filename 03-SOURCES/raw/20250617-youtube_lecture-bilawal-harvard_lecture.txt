https://www.youtube.com/watch?v=4NqhYrucp6Y
The AI & 3D Tools No One Is Talking About (Harvard Lecture)
9,119 views  Aug 29, 2025  #aivideo #XR #aitools
In this talk, we dive deep into the AI primitives that are revolutionizing content creation and consumption. Starting with the evolution of computer graphics and animation workflows, we explore how AI simplifies what once took weeks into days.

Covered in this video:
Spatial Intelligence: Photogrammetry, neural radiance fields (NeRFs), and 3D Gaussian splatting for digitizing reality.

Visual Intelligence: AI-powered pose estimation, segmentation, depth inference, and relighting.

Hybrid Workflows: Vibe coding, Model Context Protocol (MCP), and how LLMs interface with Blender, Unreal, and Runway.

Content to Content Paradigm: Moving beyond text-to-video into fully personalized, dynamic, and just-in-time media.

The Future of Media: Personalized podcasts, AR tours, generative AI games, and the blurring of software with content.

With real-world examples from Google Maps, Niantic, Wunder Dynamics, Autodesk, ByteDance, Meta, and startups like Luma, Odyssey, and Simulon, this video is both a crash course and a forward-looking blueprint for creators, studios, and innovators.

Chapters:
00:00 â€“ The New AI Primitives
 00:24 â€“ Background
 01:21 â€“ Automation & Democratization
 03:36 â€“ Spatial AI
 12:33 â€“ Visual AI
 15:40 â€“ Hybrid Workflows
 17:59 â€“ The Future of Media

Subscribe for more in-depth AI & creative tech videos! ðŸ‘‰  @bilawalsidhu  

Join My Newsletter: https://spatialintelligence.ai
Connect with me on X/Twitter here: https://x.com/bilawalsidhu
Everywhere else here: https://bilawal.ai
Business inquiries: team@metaversity.us 

Bio: 
Bilawal Sidhu is a creator, engineer, and product builder obsessed with blending reality and imagination using art and science. Bilawal is the technology curator for TED Talks, and a venture scout for Andreessen Horowitz. With more than a decade of experience in the tech industry, he spent six years as a product manager at Google, where he worked on spatial computing and 3D maps. His work has been featured in major publications including Bloomberg, Forbes, BBC, CNBC, and Fortune, among others. Bilawalâ€™s journey into computer graphics began at 11, when he fell in love with seamlessly blending 3D into real life footage. Since then, he's captivated over 1.5M subscribers, garnering more than 500M+ views across his platforms. Driven by a mission to empower the next generation of artists and entrepreneurs, Bilawal openly shares AI-assisted workflows and industry insights on social media. When heâ€™s not working, you can find Bilawal expanding his collection of electric guitars.

TED: https://www.ted.com/speakers/bilawal_...

#aitools #spatialai #aivideo #googledeepmind #XR

---

The New AI Primitives
0:00
So today I'm going to talk to you about
0:02
the many AI primitives that are at your
0:04
disposal and how they both change
0:06
content creation as well as content
0:08
consumption. So the rough agenda for the
0:09
next 30 minutes is I'll give you a
0:11
little bit about my background and then
0:12
we'll dive into the menu of options at
0:14
your disposal. Spatial AI, visual AI,
0:17
and then we'll talk about hybrid
0:18
workflows that sort of take classical
0:20
techniques, these emerging techniques
0:22
and talk about those implications of
Background
0:24
creation and consumption. All right, so
0:26
the computer graphics bug bit me pretty
0:27
early. I was 11 when I saw this TV show
0:29
called Mega Movie Magic. And at that
0:31
time, I was into Flash 5. Basically like
0:34
dorky vector cartoony animations on a
0:36
computer. And I realized I could use the
0:38
same computer to create visuals that
0:40
were indistinguishable from reality.
0:42
Now, professionally, as it was
0:43
mentioned, I spent a decade in tech
0:45
predominantly at Google, working on AR,
0:47
VR, and 3D mapping. I had a chance to do
0:49
some really cool stuff from working on
0:50
VR camera systems, YouTube VR content to
0:53
turning the world into an AR canvas with
0:55
a geospatial API and even advancing the
0:57
next generation of Google maps with
0:59
immersive view and yes, you know, doing
1:01
those IoT tech talks, the sun keynotes,
1:03
but perhaps even cooler, moving
1:05
satellites and planes around in the sky
1:07
because the geo team is just that cool.
1:10
Now, I've come full circle to what I
1:11
call being a creator. I'm lucky to have
1:13
built a following about a million and a
1:15
half subscribers on my channels and I
1:17
like to say that I entertain on YouTube
1:18
on Tik Tok and then I educate on Twitter
Automation & Democratization
1:21
and LinkedIn. All right, so AI has
1:23
entered the chat, right? Like I
1:24
throughout my career I've witnessed the
1:26
emergence of AI as this sort of
1:27
transformative force. It amplifies and
1:30
commodifies many of our abilities,
1:32
instincts, and impulses in remarkable
1:34
ways. But while all of this may seem
1:37
groundbreaking, it often mirrors
1:39
concepts that have been around for a
1:40
really long time. In other words, this
1:43
has never happened before, yet has
1:45
happened time and time again. And I
1:47
think spatial representations are a good
1:49
example. Whether it was 1800's paper
1:51
prototypes or Disney's multiplane
1:53
cameras or Google's multiplane images,
1:56
you see the same core idea repeat itself
1:59
again and again. The evolution of
2:01
animation follows a similar pattern. It
2:03
was manual rotoscoping and now it's AI
2:06
powered videotovideo tools as you see on
2:08
the right. It's basically this
2:10
progression towards automation and ease
2:12
of use. So all these things that kind of
2:14
required complex workflows and say after
2:16
effects just a couple years ago are a
2:18
few clicks in tools like pollabs. These
2:21
tools are kind of democratizing all
2:23
these esoteric capabilities that I had
2:25
to learn growing up. Now you can just
2:26
hit the ground running and focus on an
2:29
even bigger vision.
2:31
So on the left you see the old way of
2:32
creating content. You know, it's this
2:34
sort of classical waterfall workflow
2:36
where you take a bunch of specialized
2:38
tools and put them together in this
2:40
serial fashion. And on the right, you
2:42
see the new way. So, what took me one
2:44
week now takes me one day. And so, the
2:47
question is obvious, right? Like, are we
2:49
all going to pull a Tim Ferris 4-hour
2:51
work week, sip on pita coladas, and kick
2:53
it with all this extra time? I would
2:55
argue no. We will put more value on the
2:58
screen. We're going to take on more
2:59
ambitious projects. So, just to give you
3:01
an example, here's a video I created
3:03
last year using this workflow that I
3:05
described that got 37 million views.
3:08
There was a moment in the zeitgeist
3:10
where it made sense to make this video
3:12
and so I made it. If it would have taken
3:14
me a week to make it, I wouldn't have
3:15
made it at all. And so, you know, I'm
3:17
doing a lot of other things and this has
3:19
been really fun for me because I can
3:20
still still scratch that childhood itch
3:23
I have to create fun whimsical visual
3:25
effects and do it on the weekend. But
3:27
what that means is indies like me can
3:30
start rivaling the outputs of dare I say
3:31
a studio but studios will start setting
3:34
whole new standards altogether.
Spatial AI
3:37
All right. So you might say blah that's
3:39
super cute. You know Tik Tok videos
3:41
whatever. What about the AI primitives I
3:42
could integrate into higherend
3:44
workflows? That's where I want to get
3:46
into the meat of this presentation.
3:48
Starting with spatial intelligence.
3:50
So you probably heard about
3:52
photoggramometry. It's the art and
3:54
science of measuring stuff in the real
3:56
world using images and other sensors.
3:58
It's also defined in the literature as
4:01
the ability to understand and interpret
4:03
spatial data. Essentially teaching
4:05
machines to gro the 3D world we live in
4:08
every single day. Now photoggramometry
4:10
isn't new. It's been around since before
4:12
computers. But what is new is the
4:14
commodification we talked about. What
4:16
took data centers and teams of experts
4:18
in the 2000s became commodity by the
4:21
2010s. And similarly, volutric rendering
4:24
isn't new either, but all of these
4:26
techniques collectively have gotten a
4:28
huge boost with machine learning.
4:31
Enter neural radiance fields. I mean,
4:33
it's kind of wild for me to see an AI
4:35
literally like spitballing array tracing
4:38
using a 100 photos I provided to create
4:40
this like 3D world that you can step
4:42
inside. And the progress is nuts, right?
4:44
Just from the seminal Nerf paper from
4:45
2020. In the last 5 years, we've seen
4:48
insane progress. But again, it didn't
4:50
happen overnight. The multiplane images
4:52
I talked about were one of the first
4:54
instantiations of these learned uh
4:56
approaches to representing the
4:58
complexity of reality. So, I'm not going
5:00
to give you a full definition on neural
5:02
radiance fields. I've got a video on
5:03
this on my YouTube channel if that's of
5:05
interest, but really what you should
5:07
think of them as is like 3D
5:08
representations that change their
5:10
appearance when you look at them from
5:12
different directions. And while it's not
5:14
technically accurate, it may be helpful
5:17
to think about nerfs as a neural version
5:19
of light field photography. So all the
5:22
things that classical photoggramometry
5:24
has a hard time with, right?
5:25
Reflections, refra refractions,
5:27
translucency, transparency, thin
5:30
structures, all of that is fair game.
5:32
And then with the advent of 3D Gaussian
5:35
splatting, which I'm sure you're hearing
5:36
a lot about on social media and in
5:38
academia, instead of an implicit
5:40
blackbox representation, you know, where
5:42
you're modeling the complexity of
5:44
reality and the weights of this
5:45
multi-layer perceptron, you get this
5:47
explicit representation that is super
5:49
easy to render and even edit. Basically,
5:52
nerfs without the end, the neural
5:54
rendering bit. And so this is wild,
5:56
right? Suddenly, you can digitize spaces
5:58
and places that you care about for
6:00
storytelling purposes, for memory
6:02
capture purposes, and get insane frame
6:05
rates. Like on the bottom left here, I'm
6:06
getting 400 frames per second on the
6:09
scan I did in my parents' house before
6:11
they moved out of it. And now it's
6:12
immortalized for time for the time to
6:14
come. Same thing on the right. I've got
6:16
a a capture of my backyard in Austin,
6:18
Texas, running an Unreal Engine at 100
6:21
frames per second. Totally wild. Why
6:23
would you model something from scratch
6:24
if it exists in the real world? just
6:26
capture it. You can also do really cool
6:29
view dependent effects. And this is what
6:30
I meant by the representation changes
6:33
based on how you're looking at it. And
6:35
it's funny, 3D Gaussian splatting uses
6:37
spherical harmonics, this old physics
6:39
concept to basically give you uh this
6:41
ability. So all the light transport
6:43
effects through the leaves are
6:44
beautifully modeled.
6:46
Now, there's a bunch of tools out there
6:48
to create these kind of scans. Uh here's
6:50
just a few of them. Uh feel free to take
6:52
a photo, but two I want to call out is
6:54
post shots. If you got a Windows machine
6:56
and an Nvidia GPU, absolutely try it
6:58
out. It's free. You'll do all your
7:00
training locally. You're not sending
7:01
your data up to the cloud. And it's got
7:03
a friendly guey. And the same thing with
7:05
Scanverse. Uh this is a Niantic company.
7:08
And basically, it also does the training
7:10
on the phone in your pocket. And yes,
7:12
you can optionally contribute it to
7:14
expand their map as the previous speaker
7:16
mentioned.
7:18
Now, it's not just about creating
7:19
photorealistic renditions of reality.
7:21
For certain use cases, you want to
7:23
create a semantically meaningful
7:25
abstraction of reality. And so in these
7:27
cases, Polycam has a really cool mode
7:30
powered by Apple's room plan API that
7:32
does exactly that. It finds these higher
7:35
order primitives in your scenes and
7:36
effectively gives you a 3D floor plan.
7:39
And just imagine that the phone in your
7:40
pocket on device can make this right
7:42
now. There's also some very compelling
7:45
research in academia such as spatial LM
7:47
that allow you to to take any capture
7:49
and do offline processing to get even
7:52
more semantically rich results.
7:55
Let's also talk about manipulating
7:57
reality. As I mentioned, nerfs were
7:58
these implicit blackbox representation.
8:01
Since Gaussian splatting gives you this
8:03
explicit representation, it's far easier
8:06
to transform. If you've got crazy point
8:08
cloud shaders that you made in the past,
8:10
like a lot of folks that do EDM visuals
8:12
for concerts on screen, do things like
8:14
this or playing with a connect, you can
8:16
bring all that stuff to bear with a 3D
8:18
gaussian splat capture. Uh there's also
8:20
really cool editors out there, too. I'll
8:22
call out or super splat and spline if
8:24
you want to clean up some of your
8:25
captures. And there's also really great
8:27
plugins for After Effects. Postshot has
8:29
one and then I reel has one. So, if
8:32
you're already doing stuff in After
8:33
Effects, this is a way you could do
8:34
virtual sets and some really cool
8:36
transitions.
8:40
Yeah, of course it's being used in uh
8:42
music videos and commercials. We also
8:44
just use 3D Gaussian splatting made by
8:45
the homie Don Allen the third for the
8:48
opening of TAD 2025.
8:51
Um I'll probably skip over this in the
8:53
interest of time, but you know you can
8:54
capture stuff with a phone in your
8:56
pocket and you know the DSLR, but
8:58
they're very compelling capture systems
9:00
that are basically taking LAR slam. So
9:02
simultaneous localization and mapping.
9:04
You're figuring out where you are in the
9:06
space and building continually building
9:07
this refined map of the space to get a
9:10
LAR artifact, but also get 3D Gaussian
9:12
splats. And the kicker here really is
9:14
the capture time. Like if I was
9:16
capturing a monument like this with a
9:18
DSLR, even with a fisheye lens, I'd be
9:20
like walking around for 2 or 3 hours.
9:22
Now you can do it in 15 minutes and get
9:25
both artifacts for free. So very, very
9:27
cool stuff. Uh check out X Grids if you
9:29
want to look at capture platforms like
9:30
this.
9:32
Now, what's also cool here is that if
9:34
you built a 3D model of the world, you
9:36
can also query it to do very interesting
9:38
things. And let me see if I can just
9:40
pause, right? Can we pause right here
9:42
very quickly? And so, we talked about
9:45
this photorealistic human readable model
9:47
of the world. We also talked about
9:48
creating this abstracted version of
9:50
reality. There's a third one, the
9:52
machine readable model, right? It's
9:54
cool. We could just go to the next
9:55
slide. And so, you probably so enter VPS
9:57
here. You probably heard about, you
9:59
know, global positioning system. You
10:00
obviously use it every single day, you
10:02
know, to find your latte or your
10:03
classrooms. But there's also a VPS,
10:06
visual positioning system. Essentially,
10:08
you can take a photo and then you use
10:09
machine learning uh to extract salient
10:12
features from it, match it up against
10:14
this like 3D model that's created with
10:16
all of street view imagery and figure
10:18
out your exact location on the globe
10:20
with submeter positional accuracy and a
10:23
couple of degrees of rotational
10:24
accuracy. It's absolutely wild what you
10:26
can do. And the craziest thing about it,
10:28
the reason I flashed it to a bunch of
10:30
students especially and young
10:32
professionals is that it's free and it's
10:34
available in over a 100 countries
10:36
basically everywhere that you have
10:38
Street View. And another interesting
10:40
primitive to go with it is Google's 3D
10:42
tiles which makes all of the Google
10:44
Earth photoggramometry data available in
10:47
this open OGC 3D tiles format. Uh this
10:50
is something I had a chance to work with
10:51
Cesium and Patrick Cozy to get that data
10:53
out there in this sort of interoperable
10:55
format. Now, if you combine visual
10:57
positioning system and 3D tiles, you
11:00
could do some amazing things like you
11:03
can annotate a global model of the
11:05
world. Kind of what you see on the left,
11:06
right? You're on your desktop
11:07
essentially. Can we play that video clip
11:09
again? Possibly. Uh, you know, annotate
11:11
this global model of the world. You
11:12
could see that place, drop these
11:13
annotations, and then when you're at the
11:15
place with your phone today and glasses
11:17
or headset tomorrow, you could see them
11:19
precisely anchored where you left them.
11:21
And vice versa as well. it'll drop
11:23
anchors in the real world and then see
11:24
it in this global view.
11:27
Uh, of course, what you can also do here
11:28
is if you're doing these like Gaussian
11:30
splatting captures, you can capture the
11:32
thing you want in detail and then use
11:35
Google's 3D tiles for the surrounding
11:37
context and the set dressing, if you
11:39
will.
11:41
A really cool tool that's being used in
11:43
virtual production and film making right
11:45
now that I want to bring to your
11:46
attention is Cyclops. It's basically
11:48
what I just described, right? Like you
11:50
can have a real and virtual camera. You
11:52
can take a tracked iPad and suddenly
11:54
start figuring out your shot list. You
11:55
know the Q lines for where you going to
11:57
place the talent. Where is the sun going
11:59
to be at that point in day and basically
12:00
figure out your shot list? And then when
12:02
you go there, you can see that digital
12:04
model precisely overlaid with that
12:07
amazing accuracy when you're scoping
12:09
things out in person to set it up for
12:11
real. So, if there's one thing I'd ask
12:14
you to walk away with is to focus and
12:16
think about what are the applications
12:18
for VPS plus 3D tiles, especially if you
12:21
care about building XR experiences that
12:23
are anchored in the in the real world.
12:25
And not just because I worked on them,
12:27
but but because I think they're
12:28
terribly, terribly underpriced and
12:30
people aren't paying enough attention to
12:31
them.
Visual AI
12:34
All right, so let's change gears to talk
12:35
about visual intelligence. I kind of
12:37
think of visual and spatial intelligence
12:39
as two halves of the same coin. And I
12:41
basically distill it down to teaching
12:43
machines to perceive the world and
12:45
understand the contents within it. So
12:47
think of this as like what computer
12:48
vision would define as like classical CV
12:50
tasks of estimating human pose depth you
12:54
know inferring you know physically based
12:55
rendering shaders doing semantic
12:57
segmentation tasks like that.
13:00
And so two really cool companies I want
13:02
to flag here are Wonder Dynamics and
13:03
Move AI. And it's funny, you know, I had
13:05
Palmer Lucky come and speak at my
13:07
session at TED this last week and he had
13:09
a really funny line which is basically
13:11
your Snapchat filters have better
13:12
perception than the most advanced
13:14
military systems. And that's actually
13:17
true because most of this talent have
13:19
been in large companies and the canvas
13:21
that they've been targeting are all
13:23
these consumer applications. Now we're
13:25
seeing reverse pollination happening not
13:27
just in defense but also in higherend
13:29
content creation. So, Wonder Dynamics
13:31
recently acquired by Autodesk does
13:33
stellar moninocular pose estimation. And
13:35
the same thing with Move AI, which
13:37
essentially gives you the ability to
13:38
take like a bunch of iPhones or machine
13:41
vision cameras to do this type of
13:43
real-time performance capture without
13:44
getting your talent into a crazy suit or
13:47
something like that. So, this is wild,
13:49
right? Like suddenly you can start
13:50
reskinning reality and it takes a
13:52
fraction of the effort. On the left you
13:53
see me reskinning, relighting myself
13:56
using a tool called Bibble out of South
13:58
Korea. really amazing tool partnered
13:59
with NYU. And on the right, I'm
14:01
basically taking this like the blocked
14:03
out 3D scan that I did, kit bashed a
14:06
couple of things together and then
14:07
reskinning it with runway ML.
14:10
On the segmentation side, it's really
14:12
interesting too. Meta's obviously
14:13
released some amazing things. Segment
14:15
anything 2 is released under a very
14:17
permissive Apache 2.0 license and track
14:21
anything is also available in this very
14:22
permissive MIT license. And this is wild
14:25
to me having grown up on Adobe tools
14:26
like After Effects. This stuff is order
14:28
of magnitude better and it just doesn't
14:31
seem to stop. It's wild, right? Samurai
14:33
is the one thing that came out that just
14:34
introduced the old school component
14:36
filter with SAM 2 and does very robust
14:39
uh tracking. As you can see, the
14:40
tracking is maintained across the board.
14:42
And Meta is doing some very cool things
14:44
by using these capabilities to create
14:46
synthetic training data to create video
14:48
models that can do that type of fine
14:50
grain editing where you just swap the
14:51
background or just swap a certain object
14:54
and respect the lighting and everything
14:55
else in the scene. Depth is another key
14:57
component of this and so bite dance
14:59
released something called depth anything
15:00
and this is under Apache 2.0 as well. A
15:03
bunch of people are already building
15:04
very cool products with it and this is
15:06
particularly uh popular in the you know
15:08
sort of stereo 3D community for the
15:10
Apple Vision Pro, the Quest 3 and and
15:13
soon project Muhan. Basically converting
15:15
2D content uh to 3D very very uh simply.
15:19
So an example of a company that's
15:20
bringing all of this together is
15:21
Simulon. Uh, I'm an angel investor in
15:23
this, so full disclosure there. But
15:25
they're basically taking all of these
15:26
little pieces like lighting estimation,
15:29
high quality motion tracking, geometry
15:31
estimation. So you can skip all those
15:33
steps of drudgery in visual effects and
15:35
get to that first render quicker and
15:37
then start iterating from there.
Hybrid Workflows
15:41
All right, so that brings me to hybrid
15:43
workflows,
15:44
approaches that get you the best of both
15:46
worlds. Now, unless you've been living
15:48
under a rock, you've probably heard
15:49
about the term vibe coding. Uh you
15:51
probably also heard about model context
15:53
protocol. Uh essentially think of it as
15:55
a way for LLMs to interface with
15:57
existing tools like Blender. And so this
15:59
those are the examples that you see and
16:01
this is very exciting to me because
16:03
you're tapping into the world knowledge
16:04
of these large language models but using
16:06
the tools that you know and love that
16:08
give you explicit control. So on the
16:10
right what was happening there is you
16:12
tell this model, hey like tell Claude,
16:14
hey this is what I want to create and
16:16
it's not only going to build the scene
16:18
out for you. It's doing tool use and
16:20
calling a text to 3D AI model called
16:22
tripo to basically create those assets
16:24
for you and then iteratively refine and
16:26
place them. And so how cool is that?
16:29
It's starting to feel like you're almost
16:30
standing over the shoulder of a human
16:33
and asking them to do stuff. And I think
16:35
that's very very powerful. So what I'd
16:36
say here is I want you to start thinking
16:39
of this not like text to content. I want
16:42
you to start thinking about this as
16:43
content to content. any piece of content
16:46
comes in, any piece of content comes
16:48
out.
16:51
You know, there's so much, you know,
16:53
like there's a lot of focus on the
16:54
visual creation models, but I think all
16:56
these like multimodal large language
16:58
models are equally excited. I am so
17:00
stoked with Gemini 2.5 Pro. Same thing,
17:03
OpenAI's O3, all these multimodal models
17:05
that are capable of reasoning. They've
17:07
got huge context windows. You can throw
17:09
in so much cool stuff. Like for my
17:11
YouTube videos, I'll often throw in like
17:13
four or five different introductory
17:14
takes and just tell it which one do you
17:16
think would be the best for retention,
17:18
which one has the highest energy, and
17:19
it'll give me a very detailed rundown of
17:21
exactly that. Just imagine how your
17:23
workflows could transform if you start
17:25
thinking of it more flexibly and not
17:28
just as text prompt in and video comes
17:29
out.
17:32
So if you keep thinking content to
17:33
content, you can also decide what are
17:36
the bits you're going to make the
17:37
classical way, right? and what to lean
17:39
into AI models for. I'm very bullish on
17:42
mixing classical 3D and with generative
17:44
AI. And this example by the Homie 8bit
17:47
is a great example of this, right? You
17:49
can exert fine grain control on the
17:50
stuff that matters and then use
17:52
generative AI is sort of this final pass
17:54
to take it all the way and in this case
17:56
make this rather tubby person look like
17:58
Kanye West. Now, I think content to
The Future of Media
18:00
content also hints at the future of
18:01
content consumption. I'm going to give
18:03
you a rather dry example. If you haven't
18:05
had a chance to use Notebook LM and
18:06
audio overviews already, I highly highly
18:09
encourage it. That's how I like consume
18:10
an insane amount of academic research.
18:13
But imagine personalized weekly podcasts
18:15
for your employees, right? Like you take
18:18
all your all hands videos, you take all
18:20
your emails, all your presentations, and
18:23
bam, you get this easy to digest
18:25
summary. And I think this is the shape
18:28
of where content is going. Personalized,
18:30
dare I say disposable, and made just in
18:33
time. And it doesn't matter if it's
18:35
like, you know, some enterprise use case
18:36
or you're building a new kind of podcast
18:38
that you subscribe to. I think you can
18:40
reimagine how you deliver content in a
18:42
personalized fashion. Now, even before
18:45
we fully automate this, uh, take my
18:47
friends Rowan Schwang and Von Ma as
18:49
examples. I believe they built what I
18:52
call the fastest breaking news to
18:54
Instagram reel pipeline in the tech
18:56
media game. So, where most other
18:58
channels are like busy reposting the
19:00
same like PR approved assets as a text
19:03
post or a blog, they've already got a
19:05
video out there on Instagram that's like
19:07
meeting people where they are and giving
19:08
them content to an in an easy to digest
19:11
fashion. And they're not doing anything
19:13
fancy. They're using Hey Gen, 11 Labs
19:15
and essentially eliminating themselves
19:17
as the bottleneck. They're still human
19:19
editors and producers that are doing all
19:22
of the, you know, bringing it all
19:23
together. But given where MCP is going,
19:25
you can imagine even those processes
19:27
will be automated and the VLM will be
19:30
capable of using uh you know Premiere or
19:32
Da Vinci Resolve for you. The best part
19:36
about all of this is that their audience
19:38
doesn't even care that it's AI. A lot of
19:40
creators are worried that oh my god what
19:41
if I put a deep fake version of myself
19:43
like where is the authenticity? You're
19:45
delivering them a news bite and if it's
19:47
valuable they're absolutely going to
19:48
consume it. And I think the numbers
19:50
speak for themselves.
19:52
Similar things happening in the
19:53
advertising game. You know, this company
19:55
that Adobe just acquired made this
19:57
personalized campaign in India where
19:59
basically every single ad was like, you
20:01
know, uh uh tailored to these 2500 local
20:04
businesses. So when you saw the ad, it
20:07
referred the local shop where you could
20:08
go buy Cadbury products by name. Just
20:11
imagine where advertising is going to
20:13
go.
20:16
Speaking of where this is going, I mean,
20:18
I'd me mentioned this notion of
20:20
disposable, you know, dynamic content.
20:23
You can start injecting you sort of the
20:25
context of your user, their needs, their
20:27
preferences, almost as if somebody made
20:29
a piece of media, whatever that piece of
20:31
media is for them on the platform
20:33
they're choosing to consume it. It's not
20:35
going to be static. But it's not just
20:37
video stuff, too, right? Like we talked
20:38
about location-based AR and historically
20:40
we talk about oh if we want to build
20:42
like you know location-based tour guide
20:44
experience. How are we going to create
20:46
this content at scale? Now you take
20:48
these visual language models that have
20:49
insane world knowledge. Take something
20:51
like VPS. Use something like you know
20:53
the Google Maps API as tool use and then
20:56
text to speech and you can make that
20:58
tour for their AR glasses or just their
21:01
freaking phone with a pair of AirPods
21:03
like on the fly. That's really really
21:05
wild.
21:07
But just don't just take my word for it.
21:09
If you ask Jensen Hong, and I had the
21:11
chance to ask him this at GTC last year,
21:14
he thinks we're 5 to 10 years out from
21:16
fully generative AI games and that every
21:20
pixel in the future is going to be
21:22
generated and not rendered. Now,
21:23
obviously, he's got a vested interest in
21:25
it. Jensen trying to sell a lot of GPUs,
21:27
but I actually think it's a very, very
21:29
tame prediction, especially when you
21:31
look at some of the recent research
21:33
that's coming out. Sorry, can we play
21:34
this video? Oh, there we go. Uh, coming
21:36
out. uh such as uh this one coming out
21:38
of Google paper is called Genie2. You
21:40
take a photo and then suddenly you get a
21:42
minute of a playable world. And I think
21:44
another leading indicator or another
21:46
sign that this shift is happening is if
21:48
you take some of the 3D first AI
21:51
startups that exist like Luma and
21:53
Odyssey, they've all switched to video
21:56
diffusion as the path to interactive
21:58
worlds. And uh literally the Odyssey CEO
22:00
replied to my tweet yesterday saying
22:02
Jensen is always right. So there you
22:04
have it.
22:06
What this also means is the lines
22:08
between software and content are
22:10
blurring, right? You can already code
22:12
things up faster than it would have
22:14
taken you to make it in Blender or
22:15
Cinema 4D. Here's an example. I
22:17
vibecoded the vibe coded this thing in
22:20
Can we play that again? Vibe coded this
22:22
thing in Claude. And I literally felt
22:24
like a creative director or product
22:25
manager. I was like, "Write a PRD for
22:27
this." And then, hey, tech lead, I want
22:29
you to sequence this out. No, cut these
22:31
features. And then I ended up with this
22:33
really cool visualization that you see
22:35
over here. Oh, let me try that again.
22:38
There we go. See over here? And then I
22:40
ran it into uh ran it through Runway to
22:42
reskin it to make it look like Legos. I
22:44
didn't even touch Blender. Like that's
22:46
mind-blowing to me. So you can just
22:47
imagine where these capabilities will be
22:49
in just another year.
22:52
So what I think that means is that in
22:55
this future instead of getting lost in
22:57
the details and endless variations,
22:59
you'll start authoring content at this
23:01
higher level of abstraction. And the
23:03
best analogy that comes to my mind right
23:05
now is sort of, you know, HTML and the
23:07
document object model or a 3D scene
23:10
graph if you're a 3D nerd. And it's
23:12
going to start with the examples that I
23:13
gave you last mile customization. and
23:15
you know all the drudgery that agencies
23:17
are doing today tailoring creatives or
23:19
ads to like different platforms, viewers
23:21
and interests, but at some point you'll
23:23
be able to create a lot of content on
23:25
demand. So the way to maybe think about
23:27
it is we're not just building content
23:29
anymore. We're building systems and
23:32
workflows that do the creation for us
23:34
and then video or 3D or audio is just
23:37
the means for final delivery.
23:40
All
23:41
right, to wrap things up, now that we've
23:43
painted a picture for the future, I want
23:45
to bring this home and leave you with
23:46
some, you know, key takeaways.
23:50
So, big picture, you got to play test
23:52
with all this stuff. There's seemingly a
23:54
new primitive that drops like every
23:55
other week. If you're not playing with
23:56
it, I truly think you're missing out.
23:58
And the way you should think about doing
24:00
this is having two modes. I call it
24:02
sandbox mode and architect mode. In
24:05
sandbox mode, you're sort of like
24:07
building castles by the beach. You're
24:08
totally cool with the waves of water
24:10
coming in washing everything away, but
24:12
you're going to pull out interesting
24:13
insights. Happy accidents will happen.
24:16
And then you take those insights and
24:17
you'll start applying them in a more
24:19
structured fashion towards a deliverable
24:21
for architect mode. The third thing I'd
24:24
say is build an island of influence. You
24:26
know, a lot of especially like students
24:27
and like you know, early founders come
24:29
to me and say things like, "Oh, building
24:30
in public is cringe." And I say, "I
24:33
don't think it's cringe. I think it's a
24:34
damn near necessity." Right? Like as
24:37
these barriers to creation keep
24:38
shattering, I think it's going to get
24:40
increasingly difficult to cut through
24:41
the noise. So I think it's important you
24:43
start building your island of influence
24:45
now. And then finally, this is a little
24:47
bit contrarian. Some people are like,
24:49
"Oh, I don't want to learn Blender,
24:51
Premiere, whatever, After Effects, the
24:53
incumbent tool of choice in your
24:54
industry, Unreal Engine." You do have to
24:57
learn these things because a maybe these
24:59
things are just running headless in the
25:01
cloud and AI is using them so you need
25:02
to know how they work and b for now
25:05
they're going to be the bailing wire and
25:06
duct tape you need to ship stuff. So
25:09
look, I get it. This all feels very
25:11
exhausting. We do get a new Lego
25:14
primitive every couple weeks. Um but
25:16
even the existing primitives have not
25:18
been put together in the most obvious
25:21
combinations. And I gave you just a few
25:23
examples, let alone all the nonobvious
25:26
insights that you're going to get as you
25:27
apply these primitives to your own
25:29
business and creative problems. So, in
25:31
my opinion, this is the moment to act
25:33
because in this new world, the only
25:34
limit is your imagination.
25:37
Thank you very much.