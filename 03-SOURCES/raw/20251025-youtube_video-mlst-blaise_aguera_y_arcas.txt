https://www.youtube.com/watch?v=rMSEqJ_4EBk
Google Researcher Shows Life "Emerges From Code"
31,485 views  Oct 21, 2025  Staff Favourites
Blaise Agüera y Arcas explores some mind-bending ideas about what intelligence and life really are—and why they might be more similar than we think (filmed at ALIFE conference, 2025 - https://2025.alife.org/).

Life and intelligence are both fundamentally computational (he says). From the very beginning, living things have been running programs. Your DNA? It's literally a computer program, and the ribosomes in your cells are tiny universal computers building you according to those instructions.

*SPONSOR MESSAGES*
—
Prolific - Quality data. From real people. For faster breakthroughs.
https://www.prolific.com/?utm_source=...
—
cyber•Fund https://cyber.fund/?utm_source=mlst is a founder-led investment firm accelerating the cybernetic economy
Oct SF conference - https://dagihouse.com/?utm_source=mlst - Joscha Bach keynoting(!) + OAI, Anthropic, NVDA,++
Hiring a SF VC Principal: https://talent.cyber.fund/companies/c...
Submit investment deck: https://cyber.fund/contact?utm_source...
— 

Blaise argues that there is more to evolution than random mutations (like most people think). The secret to increasing complexity is merging i.e. when different organisms or systems come together and combine their histories and capabilities.

Blaise describes his "BFF" experiment where random computer code spontaneously evolved into self-replicating programs, showing how purpose and complexity can emerge from pure randomness through computational processes.

https://en.wikipedia.org/wiki/Blaise_...
https://x.com/blaiseaguera?lang=en

TRANSCRIPT:
https://app.rescript.info/public/shar...

TOC:
00:00:00 Introduction - New book "What is Intelligence?"
00:01:45 Life as computation - Von Neumann's insights
00:12:00 BFF experiment - How purpose emerges
00:26:00 Symbiogenesis and evolutionary complexity
00:40:00 Functionalism and consciousness
00:49:45 AI as part of collective human intelligence
00:57:00 Comparing AI and human cognition

REFS:
What is intelligence [Blaise Agüera y Arcas]
https://whatisintelligence.antikyther... [Read free online, interactive rich media]
https://mitpress.mit.edu/978026204995... [MIT Press]

Large Language Models and Emergence: A Complex Systems Perspective
https://arxiv.org/abs/2506.11135

Our first Noam Chomsky MLST interview
   • Why Does Noam Chomsky Say AI Failed?   

Chance and Necessity [Jacques Monod]
https://monoskop.org/images/9/99/Mono...

Wonderful Life: The Burgess Shale and the History of Nature [Stephen Jay Gould]
https://www.amazon.co.uk/Wonderful-Li... 

The major evolutionary transitions [E Szathmáry, J M Smith]
https://wiki.santafe.edu/images/0/0e/...

Don't Sleep, There Are Snakes: Life and Language in the Amazonian Jungle [Dan Everett]
https://www.amazon.com/Dont-Sleep-The... 

The Nature of Technology: What It Is and How It Evolves [W. Brian Arthur]
 https://www.amazon.com/Nature-Technol... 

The MANIAC [Benjamin Labatut]
https://www.amazon.com/MANIAC-Benjam%... 

When We Cease to Understand the World [Benjamin Labatut]
https://www.amazon.com/When-We-Cease-... 

The Boys in the Boat [Dan Brown]
https://www.amazon.com/Boys-Boat-Amer... 

How something can be said about Telling More Than We Can Know [Petter Johansson] (Split brain)
https://www.lucs.lu.se/fileadmin/user...

If Anyone Builds It, Everyone Dies [Eliezer Yudkowsky, Nate Soares]
https://www.amazon.com/Anyone-Builds-... 

The science of cycology: Failures to understand how everyday objects work [Rebeca Lawson]
https://link.springer.com/content/pdf... 

SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction [Kushin Mukherjee, Judith Fan et al]
https://arxiv.org/pdf/2312.03035 

Brain-Score [Martin Schrimpf]
https://www.biorxiv.org/content/10.11... 

Feature Visualization [Chris Olah]
https://distill.pub/2017/feature-visu...

THE REVERSAL CURSE [Lukas Berglund]
https://arxiv.org/pdf/2309.12288

---

Introduction - New book "What is Intelligence?"
0:00
the new book. So, it's called What is intelligence? Uh, thank you for asking. Uh, it was just published by MIT Press,
0:06
uh, you know, three or so weeks ago. So, it's it's very it's fresh off the presses. Uh, there's an online version
0:12
of it as well that is free um and and very rich. It's t, you know, full of full of all kinds of um all kinds of
0:18
rich media. Um, chapter one of that book is called What is Life? So, what is Life is sort of the the single to the album
0:25
uh and works as a book in its own right. So that that book is also for sale from MIT Press. Um I think I've explained why
0:32
I think of life as being a subset of intelligence, you know, and why the story of of artificial life and
0:38
abiogenesis and so on is relevant to the story of intelligence and what it is. But uh yeah, the subtitle of the book is
0:44
lessons from AI about uh evolution and minds and something something. Uh so you
0:50
know it's it's basically um documenting the time since about 2020 when I when I
0:56
got really shocked by seeing that that these uh large sequence models uh seemed
1:02
to be generally intelligent and just starting to think through the implications of that. You know what would it mean if you know if we believe
1:08
our eyes and that is what intelligence is. What does that tell us about ourselves about the properties of
1:13
intelligence more broadly? uh and and the whole intellectual journey that that that that has taken us on in the last in
1:19
the last few years. MLST is supported by Cyber Fund. So, I'm Falm. I'm the co-founder and CEO of
1:25
Prolific and uh Prolific is a human data infrastructure company. So, we make it easy for people developing frontier AI
1:34
models uh and running research to get access to trustworthy high-quality
1:39
participants for high quality online data collection.
Life as computation - Von Neumann's insights
1:47
I am at Google. I've been there for about 12 years now. And uh I am their CTO of technology and society and also
1:55
the founder of a new research group. Well, newish. We've been around for a couple of years called Paradigms of
2:00
Intelligence or PI. It's much smaller than the previous organization that I uh that I ran at Google research. Uh it's
2:07
about 50 people. Uh so, you know, enough to do some real damage. uh and the idea is to really focus on fundamentals of uh
2:15
of artificial intelligence uh and go beyond the the sort of exploiting of the
2:20
current uh models and paradigms that are working well. We we believe in those but we also think that we have to sort of uh
2:26
refill the bucket with with uh new insights and new ideas as well. I've just watched your talk and you said
2:33
that life and intelligence are the same thing. They are both computational. What
2:38
do you mean by that? Yeah, this is a a surprising claim. I know it I know it sounds a bit odd, but
2:45
what I mean by that is u well let's let's begin with life and why life is computational.
2:51
So um in the middle of the 20th century, John Vonoyman, who was one of the founders of computer science, of course,
2:58
realized that in order for a robot paddling around on a pond, let's suppose
3:04
the robot is made out of Legos, and its job is to make another robot uh out of loose Legos that it finds floating
3:10
around in the pond just like itself. Uh in order to do that, it needs to have instructions inside itself. So he
3:16
imagined a tape uh with with instructions for how to assemble a MI. And the the robot would also have to
3:22
have a machine inside itself that would be able to walk along that tape and follow those instructions to take the
3:27
loose Legos and put them together into into its own form. Uh and it would also have to have a tape copier uh so that it
3:35
could endow the the offspring with with that tape. And the tape would have to include the instructions for the copier
3:42
and the the assembler, the universal constructor as he called it. Uh so the
3:47
cool thing is that he he uh made all of those predictions if you like uh on the basis of pure theory before Watson and
3:54
Crick uh and their unagnowledged collaborators uh had figured out the structure and function of DNA before we
4:00
knew how ribosomes worked which are in fact exactly that that universal constructor before we had discovered DNA
4:06
polymerase which is the tape copier. Um and uh and in fact, you know, all of
4:12
those things have to exist in order for for uh an organism to not only be able to reproduce itself, but to do so
4:18
heritably such that if you make a change in its genome in what's on the tape, then the offspring will also have that
4:24
change. Uh the kicker is that this universal constructor is a universal
4:29
touring machine. In other words, you have to have a computer inside yourself
4:34
as a cell in order to make another cell. And DNA is in this very literal sense a
4:40
touring tape. So uh you know it's it's a very profound insight because you know basically he's saying you cannot be a
4:46
living organism without literally being a computer a universal computer. Very interesting. So are you saying that
4:52
DNA is basically a computer program? DNA is a computer program. Yes. Very cool. Because many folks in the
4:58
audience um would have been inspired by Conway's game of life for example and you were talking about computational
5:04
equivalence. So the game of life of course is to incomplete because it has
5:09
an expandable memory. Um just as DNA has an expandable memory the the grid size
5:14
um could just keep growing. As you pointing to when we see these weakly emergent behaviors they appear very
5:22
lifelike but does that in any way downplay the biochemical and
5:28
thermodynamic realities in the physical world? Of course, I mean, a cellular
5:33
automaton like like the game of life is a lot less um is a lot less complex than
5:39
the than the real world. Uh it's in two dimensions rather than three. There's no thermal randomness which turns out to be
5:45
very important actually. Um and uh and and the fact that that the computation
5:51
is is deterministic uh is a little different from you know from real life where where those thermal fluctuations
5:57
mean that there's always a a probabilistic element to things. And you do have to extend Turing's original
6:03
ideas about computation to make a so-called stochastic touring machine in order to really do a proper job. Um but
6:09
but what what Vonoyman was getting at and and I'm glad you you bring up cellular automat because they're really a generalization of the touring machine
6:17
to um the laws of physics where uh where you essentially every pixel on your game
6:22
board if you like is performing a computation uh as so it has a state and
6:27
it's performing some very simple computation to say what's the next state on the basis of the neighbors. Uh so uh
6:34
the the idea is that those those rules that are that are determining the next state of a particular pixel are the laws
6:41
of physics of that universe. And the reason that that van came up with with this idea with solar automata is because
6:48
he wanted um a system that would that would allow one to do computation uh but
6:54
in which the computation is embodied. Uh, and what I mean by that is, you know, in a touring machine, there's a a
7:01
tape and there's a head and then there are the symbols that are written on the tape, but the symbols are not the same
7:06
stuff as the tape and the head and and the instruction uh or the or the um the the table of of rules, right? Those
7:13
things are are abstract and they're separate from the symbols that are written. Whereas in a in a cellular
7:18
automaton, the machine can literally print itself. So it's like not just a laptop if you like but like a laptop and
7:25
a 3D printer in one that can print another laptop. So embodied computation is computation where the the memory uh
7:33
is uh is written and read in atoms rather than in bits uh and and and
7:39
therefore the machine can make another of itself. David Krakow said to me that I mean you
7:44
know we can agree that intelligence I mean he says it's in um adaptivity, inference and representation. adaptivity
7:51
is very important. So yes, DNA is is adaptive but it's very slow. And he was
7:57
saying that the nervous system and the brain is and culture is is evolution at light speed because it allows us to kind
8:04
of overcome the information transfer with successive generations. So does it make sense to think of the program of
8:10
intelligence at the DNA level when so much of the adaptivity seems to be happening higher up?
8:16
A lot of the adaptivity very much happens higher up. So, you know, in humans, we have cultural evolution, which goes, you know, as as David says,
8:22
at light speed relative to genetic evolution. You know, when I say that that, you know, DNA is a is a touring
8:28
tape and that, you know, ribosomes are universal computers that construct life, that's really only the ground level. Uh,
8:34
or maybe it's level one or level two. I mean, there's physics underneath that, but there there's a level three, a level
8:40
four, level five, and so on. There are computers built out of computers built out of computers. The thing about it is that once you have that ground level,
8:47
then then you can build as many uh as many floors above that as you like. The
8:52
the point is that the moment you have life uh meaning that you have something that can build a copy of itself, you
8:58
have a general computer uh which allows you to do anything. And what that means is that life from the very beginning uh
9:05
is computational and can start to compute in parallel. This gets us into symbioenesis which I guess we'll we'll
9:11
cover in a bit. The fact that that ground floor is computational answers the question, you know, why are brains
9:16
computational. It's because cells were computational from long before there were action potentials and other fast
9:23
fast electrical processes allowing us to think. Can you speak to this notion of recursion that you were just pointing
9:29
to? So um Carl Friston for example, he he thinks about this division um in
9:34
systems called a marov blanket like a statistical independence. And we seem to
9:40
observe um empirically that complex intelligent adaptive systems have nesting. And you were just kind of
9:47
speaking to having levels upon levels upon levels. And what does that buy you? Is is is it a kind of recursion? How
9:55
does that in improve the sophistication of the intelligence? Yeah. Well, two things happen. Uh one of
10:02
them is things inside things inside things and the other one is parallelism.
10:07
In other words, a lot of things uh at the same level happening at once. Uh and
10:12
and they're both important. They're both important parts of the story. So, first of all, uh you know, when you have a
10:18
cellular automaton like like Vonoyman was imagining, that's already massively parallel computation because every pixel
10:23
is like a little computer doing its thing. Uh in the same way that in physical space, uh you know, every there
10:29
can be molecules, you know, in a lot of spots, you know, all of which are doing something. You can think of them as as operations as computational operations
10:35
perhaps and they're all happening at once. Uh so you know in your in your body you have quintilions of ribosomes
10:42
and all of those ribosomes are little tiny universal computers working in all of your cells at once. All of your cells
10:47
are working at once. But also there is nesting because um you know you are a
10:53
person uh of course you're already a part of a society which in some sense is is an intelligence uh bigger than an
10:59
individual human. You're made out of cells. uh those cells are made out of organels. Those organels are made out of
11:05
proteins. Those proteins are made out of molecules. You know that that sort of nestedness is really important as well.
11:11
You're you're not only a lot of computers working together in parallel, but you're also uh you know a system of
11:18
computers made of computers. One thing I find fascinating that you've thought a lot about Blaze is where the purpose comes from. Folks like Harrison
11:25
for example, they have a nononsense um physics interpretation that it's the second law of thermodynamics at at the
11:31
end of the day because there's some notion of veilance that we build these complex adaptive systems and there needs
11:39
to be something that drives them forward something that propels them in a certain direction and your experiments have
11:45
shown that this kind of falls out of computation. What do you mean by that?
11:51
Yes. Uh and to be clear, computation and the second law very much work together here. So um the the experiment that that
11:59
uh that I did a couple of years ago that I I that that really uh sort of you know got me started with with artificial life
BFF experiment - How purpose emerges
12:06
uh is called BFF. Uh it's uh it's based on a language called brain [ __ ] which is
12:12
where the first BF come from. I didn't I didn't name it that. Uh although I admit I was you know I enjoyed that that it
12:18
was called that. This is a a minimal touring complete language uh designed by uh by a by a grad student. I think he
12:25
was a grad student in physics uh Urban Miller in the9s. It's a very minimal language. It's only only eight
12:30
instructions. Uh I only use seven of them. And uh the basic setup is that I begin with a bunch of tapes of length
12:37
64. So just 64 bytel long tapes. Uh like those touring tapes or or or um or
12:44
vonoyman's tapes. uh they start off filled with random bytes. There are only seven instructions. So um the great
12:52
majority of those bytes, about 31 out of 32 of them are no ops, meaning that they don't they don't code for any
12:57
instruction at all. So they start off random and very much purposeless. You have a thousand of them in your soup.
13:03
And the procedure is really simple. It's just plucking two of those tapes at random out of the soup, sticking them
13:09
end to end. Uh so you make one tape that is 128 long, and then running it. And uh
13:15
this modification of brain [ __ ] is self-modifying, meaning that when you run it, it can modify values on on that
13:22
combined tape and then pulling the tapes back apart and dropping them back in the soup. And that's it. And you just repeat
13:28
that process. Uh if you do that a few million times, you start off with nothing much going on. I mean, again,
13:34
you know, the huge majority of those of those bites are not even instructions. They're only an average of two of them or so on each tape. So the likelihood of
13:40
them doing anything is almost zero. And you know you might once in a while see one bite somewhere change but um after a
13:48
few million interactions something apparently magical happens which is that uh suddenly the entropy of the soup
13:55
drops dramatically. So it goes from being incompressible because it's all random bytes to being very highly
14:01
compressible and programs emerge on those tapes and those programs are are complex. they they take some real effort
14:08
to reverse engineer and um and you can see that they are uh they're occurring in a lot of copies. Uh that's why it's
14:14
compressible. The fact that they're occurring in a lot of copies tells you what the programs are doing. They're they're reproducing. They're copying
14:20
themselves. So, you know, what's so cool about this experiment is that it really shows you how life emerges from nothing.
14:26
Uh and and the emergence of life is in some sense the emergence of purpose. Uh
14:32
you know, in this case, you know, what is the purpose of one of these programs? it is to reproduce. If you were to mess
14:37
with one of those bites, if you were to change it, you would in most cases break the program and when you break the
14:43
program, it no longer functions to reproduce. So, you know, something that can break is something that is
14:50
functional or that has purpose. Absolutely fascinating. So, you said there was a phase change which was quite
14:55
sudden. Uh, would David Crackau acknowledge that as being a form of emergence?
15:01
I think so. Um, yeah. I mean, I've actually never asked David that question. We we disagree on a lot of
15:07
things AI related, but I I think he would acknowledge that this is a phase change and that it is an example of
15:13
emergence. Yes, I think he would because um he he has a bunch of criteria, but one of them is is a fundamental coarse graining and um
15:20
reorganization of the micro substrate such that the new phenomena can be described with with a you know with
15:25
within you know simple new variable and this seem this seems to to to match that description. Is it possible that there's
15:32
some kind of design bias or you know like when we design machine learning architectures? There's so much
15:38
information in the architecture and and in this case there's so much information in the brain [ __ ] language and and and
15:44
the terms and so on. Could that have kind of influenced it to emerge in a certain way?
15:50
Yes. Yes. And and uh the structure of those programs does change depending on the language. Um we have tried this with
15:57
other languages. We've tried it with Z80 assembly language, which is the the um the assembly language of these uh Xylog
16:04
uh chips that I that were were invented sometime in the 70s and and just got discontinued last year. A very
16:09
longunning uh microprocessor architecture. It's so the phenomenon is very generic. What those programs look
16:15
like is is shaped by uh by the specifics of the language. But the reason that those programs emerge uh the reason that
16:22
they develop purpose is actually thermodynamic. Uh so you know that that might seem puzzling because you would
16:28
think thermodynamics is about things becoming more random and the apparently the exact opposite is happening here.
16:33
You start with randomness and you get order and you know how could that be? Well um I I think the answer was
16:39
actually well characterized by uh a chemist by an organic chemist Adi Pros
16:45
uh who uh at the at the University of the Negv in in Israel who um is now ameritus and he did a lot of work on
16:51
so-called dynamic kinetic stability. uh the idea being that it's an extension of the second law that says things seek
17:00
their most stable state, their most stable form. You know, usually we think about those stabilities as only being
17:05
fixed points, but those stabilities can be cycles too. So, if something dynamically uh makes itself, if
17:13
something forms more copies of itself, that's more stable than something that just settles. Uh you know, it's like the
17:19
old joke about DNA being the stablest molecule in the universe. Obviously DNA is fragile but at the same time if the
17:26
DNA makes more DNA then you know it will be around a long time after granite you know which can only erode. In terms of
17:32
this veilance question though does that imply to you that there is a natural drive to survive almost I mean for for
17:40
these systems to kind of maintain their existence they assuming that's a primary force they would need to have a degree
17:47
of sophistication they would need to be doing modeling they would need to be doing sophisticated things but is that something that just always you know is
17:54
it a convergent property yes it is uh in in that sense evolution
17:59
is the second law at work. Uh meaning if you have a bunch of things that are not copying themselves in the BFF soup and
18:06
you have something that emerges that can copy itself, then that thing that can copy itself will write over the things
18:12
that can't copy themselves, which means it's more fit uh or more stable if you like. Uh and so that that is written
18:20
into the laws of statistics in just the same way that the second law is. It's just the the kinetic or the or the
18:26
cyclic form of that of that same law rather than than just the steady state. You said in your talk that merging is
18:33
more important than mutation. Tell me more. Yes. Uh so the the usual thing what we
18:40
learned in school uh was that Darwinian evolution consists of mutation and selection or what Jacqu Mon the Nobel
18:48
winner called uh chance and necessity. Uh so you know in other words um
18:53
mutations maybe from cosmic rays or whatever to our DNA uh sort of throw spaghetti at the wall and um and
18:59
whatever sticks is what is what remains whatever doesn't kill us and and and whatever hopefully makes us stronger.
19:06
That was my assumption as well. Uh, you know, starting out with these BFF experiments and and so I I had a
19:11
mutation rate, you know, where a bite could change at random with probability one in 10,000 or something. Uh, you
19:16
know, with with every interaction and then I began playing with the mutation rate and found that this uh this
19:22
emergence of these complex programs occurred even when the mutation weight rate was turned down to zero. Uh, which
19:28
is really a surprising finding, you know. tells you that that this emergence of purpose uh comes about you know even
19:35
without any any random changes in in the code. Uh it's it's not explainable in in
19:41
purely Darwinian terms. But you know the other things that are not explainable in purely Darwinian terms are the emergence
19:47
of life in the first place. This greatly puzzled Darwin. You know he he thought this problem of abiogenesis or the
19:53
emergence of life was just you know impossible to to reckon with. you know, you might as well talk about the origin of matter is how he put it in one of his
20:00
letters. And the other thing I can't explain is the increases in complexity that occur. You know, why is life now
20:07
more complex than bacterial life? You know, a million, you know, a billion years after it began on Earth. Um, you
20:12
know, why are do we have human societies now? And if we go back, you know, 100 million years, we had only uh, you know,
20:18
things with much simpler brains. We had octopuses. They had pretty complex brains. But anyway um but you know that
20:24
the tendency has been toward toward greater complexity. Uh there there are some people who have argued against that. Uh you know famously uh Steven J.
20:31
Gould you know has said things like you know everything on earth is is this is the same amount evolved. We've all been
20:37
evolving for you know three billion years. It's all equally evolved. I think I think G was wrong uh when he said
20:43
this. Uh the reason being um symbioenesis uh when when you have uh a ukareote
20:50
formed by uh a mitochondrian uh being um sort of uh you know finding itself
20:56
inside an archa and then you know becoming a ukarote that resulting composite organism is more complex than
21:03
either of the two parts that made it up in the same way that a spear is more complex than a stick and a stone point.
21:10
you know, you put two things together, you now have a you have something more complex than the parts. And um and if if
21:16
this if this idea that symbiosis or symbioenesis is is um an essential part
21:22
of evolution is correct, then uh you absolutely get more sophisticated things
21:27
coming about later in evolution because they're being put together from pre-existing parts.
21:33
Yeah, I wanted to touch on the because I said the the same thing on the show many times that inspired by Kenneth Stanley
21:39
actually that we see this monotonic increase in complexity in in in evolution in in standard Darwinian
21:45
evolution there is no reason for things to become more complex. So in other words, if you if you just do
21:50
the spaghetti throwing at the wall thing, then you could get simplifications or complexifications uh
21:56
you know and they're they're equal. There's there's nothing to favor one over the other a priori. So uh ordinary
22:01
Darwinian evolution um you know can can either make things simpler or more complex. But symbioenesis which is uh
22:09
you know the the this coming together of parts to make holes uh and and these major evolutionary transitions that you
22:15
just mentioned uh this is the theory of uh Yor Safmari and John Maynard Smith
22:21
that they published in Nature in 1995. They they only had it like eight of them in their original paper and they've
22:26
since extended it to maybe a dozen, you know, but things like single cells coming together to make bodies, uh
22:31
individuals coming together to make colonies, se the emergence of sexual reproduction, the endo symbiosis of
22:37
chloroplasts and of mitochondria, there a few others, right? Those are very clearly steps upward in complexity. And
22:43
and the reason that that it's trivial to prove that there are steps upward is
22:48
because if you have A which is reproducing uh and can make more of itself and you have B which is
22:54
reproducing and can make more of itself. Think of them each as having a tape right that says how to make a me. Then
23:00
when they come together the result has to both know how to make A and how to
23:05
make B and how to put them together. and that little extra bit of information how to put them together is what makes the
23:12
whole necessarily more complex than the parts. So that that is the the the
23:17
latter uh and and where where I go beyond what Smith and Smari say is that for them major transitions are are a
23:24
rare and exceptional event. But I I think that if you look more closely at the way biology works um that's just the
23:31
tip of the iceberg. Those are just the really big transitions that involved, you know, two large uh, you know, highly
23:38
consequential things uh, you know, merging in some way or or many cells, you know, merging into something
23:43
qualitatively extremely different. But, but when you look more closely, you see horizontal gene transfer in bacteria all
23:49
the time. That's also a form of of symbioenesis where, you know, parts of one thing get muddled up in another. Uh,
23:56
you see horizontal gene transfer in ukarotes like us all the time. Apparently a quarter of the cow genome
24:02
is this bove B transposon which has jumped around among lizards and all kinds of other animals as well. Viruses
24:08
do this all the time. They they you know retroviruses insert big chunks of their genomes into ours. Um and you know the
24:14
big shock when you look at at our genome when it was first sequenced in 2001 is that only one and a half% of that is
24:21
even you know our proteins. And what the hell is the rest of it? You know that there's the junk DNA. Uh now we know
24:28
it's not all junk. Uh you know a lot of it has regulatory functions and so on. But even so that's a lot you know there's a lot of other stuff in there
24:34
and a huge amount of it is retrotransposons and retroviruses that have been indogenized. They serve
24:39
functional purposes in many cases. Uh the mamalian placenta was made out of uh a virus uh related to uh the RSV virus
24:47
which fuses lung cells together and can make babies sick. That fuses together the cells in our placenta to make this
24:52
this um this blood blood barrier. uh or there's an ARC virus. Uh we don't really
24:58
understand how it works, but it lives in our brains and we know that if we knock it out in mice, they can't form new
25:03
memories. Uh and uh and it goes on and on. You know, especially in the last decade, we we find more and more
25:09
examples of functional instances of bits of genome from one thing ending up in
25:14
another and changing it. One thing I want to touch on is is the importance of the merge operator. We were talking about that earlier and even
25:20
um Chomsky spoke about this and you could argue whether the merge operator in language evolution was the Prometheus
25:26
moment whether it was phoggenetic or ontogenic um because you were just talking about this um uh you know this
25:33
the symb symbiosis and merging in in a physical substrate but it also happens in the information substrate you get you
25:38
get this kind of like mometic computer programs that ensconce themselves and maybe language was that you know I don't
25:44
know but I I have a theory why merge is so important as opposed to to random selection So I think creativity is is
25:51
about grounding. Um it's about path dependence basically. So um even the
25:56
retroviruses and and all of these things they they actually they form a lineage and I think that if you don't use merge
Symbiogenesis and evolutionary complexity
26:04
you lose the lineage and also something about the recursive merge operation allows you to build more complex
26:11
computer programs but allowing for this kind of reuse and canalization. There's something very natural about that.
26:17
Yeah. I I completely buy everything that you're saying. I think that's exactly right. Except that I dislike Chomsky.
26:23
So, I think he's wrong. He's wrong about language. Um I'm I'm much more of a fan
26:28
of Dan Everett. Uh I don't know if you're familiar with his work with the Pir. Oh, it's it's wonderful. Uh so, he
26:35
he spent a long time with the Pir uh uh in in Brazil, who are a people uh whose
26:41
language does not obey uh Chsky's uh you know, requirements for language. They don't have uh they don't have recursion.
26:47
Uh they don't have anything like center embedding. They also don't have numbers and they don't have past and future
26:52
tenses. You know, he uh Everett wrote a wrote a great book uh some years ago called don't sleep there are snakes
26:58
which uh talks both about um his experiences among the among the piranha and their language and also his big
27:04
fight with Chsky over this. uh Chsky's papers are filled with theory and pseudo math uh and and have no time to give to
27:12
ethnography or to actually studying any real languages. But anyway, I'm digressing. Um you know, but uh putting
27:18
Chsky aside though, what what you're saying about merge or as I would see it, composition, uh functional composition,
27:24
uh I think is absolutely fundamental. It's it's how all technology is built. Uh W Brian Arthur uh has written about
27:31
this and how technology evolves. uh you know that every every technological invention you know it's funny like it it
27:37
every technology gets invented a dozen times around the same time as if everybody's in telepathic communication
27:43
and the reason is that every technology has precursors you know you can't get a light bulb until you know how to blow
27:49
glass how to make a vacuum how to draw a filament uh how to generate electric current and when all those things were
27:55
there and you know the need for light was there the light bulb was going to get invented but it was invented you
28:01
know a dozen times by different inventors with different contingent choices. You know, they might be, you
28:06
know, which kind of filament do you use or is it prongs or do you screw in the light bulb, which way do you screw it
28:11
in, uh, you know, what's the diameter? And and and those decisions as they get locked in determine the course of of
28:17
everything after that that incorporates light bulbs. So, uh, you know, in a way this this
28:23
contingency, this these choices about exactly which way those combinations go is actually what the entire genome or
28:30
whatever it is is made out of. Um, in the case of BFF, the original replicators are really just single
28:37
instructions that sometimes randomly weekly might copy themselves. One bite that moves here and there, but as those
28:43
bites get copied around, sometimes a couple of them end up together and then they'll they'll they'll copy as a group.
28:49
They'll do better together. And uh and so the contingent thing, you know, which way they ended up getting copied, was it
28:55
AB or BA? You know, that they stuck together. That's that's the information that the bigger thing is made out of.
29:01
that little extra bit uh because you know in this case you just had single bytes. There was nothing there was no information there to begin with. So so
29:08
the merger tree uh you know ends up being exactly the information that is encoded uh in the final genome. It's all
29:16
about the history. Yes. Absolutely fascinating. I mean in in a sense I'm surprised you're not a fan of Chsky because he he was talking
29:21
about automter and tour incompleteness and he was the ultimate computationalist and and in a sense what you're
29:28
describing is Chsky's ideas just applied lower down the stack. That's right. So in in that sense I
29:34
think I think he was correct but I also think you know all of those ideas were already there in in Vonoyman uh in the
29:41
1950s uh even Neil's Alaricelli the first artificial life researcher who uh
29:47
you know worked on one of Vonoyman's machines I think he I think he he he sort of snagged time on the maniac uh to
29:54
to do some of his first a life experiments. They're kind of pseudo documented in in um in Midkin Labatut's
30:00
uh book Maniac. It's really was really fun. Um but um or no that was his it was
30:06
in his first book I think when we cease to understand the world. Um but anyway so so yeah my point is those ideas were
30:11
there before uh before Chsky. Uh the thing that Chomsky really pushed um you
30:16
know during his reign of terror over linguistics. Sorry I'm being a little bit mean. But the thing that he really
30:22
pushed was was the um the movement in artificial intelligence that we now call uh goi good oldfashioned AI which held
30:29
that you could you could formalize what AI is as grammarss and programs uh which
30:35
turned out to be you know wrong that turned out to be a false start in in AI
30:40
and why there were so many AI winters there seems to be a bit of a tension because the goi folks they had some very
30:46
interesting ideas I mean I'm a big fan of um FOD and pollition for example and and they spoke about this strong
30:52
compositionality. We have semantics and intentions and you know it's possible to to build these cognitive representations
30:58
but we have the issue that we can't really design them to represent the world in in in a in a high fidelity way
31:05
and we have the the semantics divergence and then you're pointing to this this very interesting um constructive thing
31:11
and I think um a constructive form of AI and compositionality solves a lot of problems because of this path dependence
31:19
problem and this canalization that we're talking about that when you build intelligence brick by brick, you can
31:24
build artifacts of incredible sophistication, but unfortunately we can't design the artifacts to do exactly
31:31
what we want. We have we can gently steer them in a certain direction. And even with um with Friston, I I feel that
31:37
even though he's talking about the what of intelligence is is prediction and and adaptivity, I think the implementation
31:44
matters. I think adaptivity means structure learning. I think there's something about having a substrate which
31:50
actually does this form of of composition that you're talking about that seems to be like a mechanistic
31:56
necessary condition for intelligence, right? Yes. I I think in many ways what we're talking about is sort of the the tension
32:03
between analog and digital ways of thinking or bottom up and top down ways of thinking. Uh so for instance um
32:10
let's talk about how you would recognize a bicycle. uh you know, in the good old fashioned AI world, you would say, well,
32:16
you've got a circle detector, uh, you know, and and a, you know, and a line detector that will detect, you know, the the lines that make up the frame of the
32:22
bike and and so on. And you'll write you'll handwrite code for all of those things. And, um, and of course, the
32:28
problem is that, you know, there are many ways of looking at a bike where you're not you're not going to see the the wheels at once or maybe the bike is
32:34
of a weird design. And you know, they're those funny bikes that have shoes instead of wheels, you know, and when you look at one of those in a Gestalt
32:40
sort of way, you recognize a bike immediately, even if all of the rules are broken, as it were. Um, and and
32:47
that's really important because when when you're when you're looking as an intelligent being at the world, you have
32:52
to cluster. You have to um you have to find regularities in the world that um
32:58
you know, whose shapes are not well defined by uh by a set of rules. um you
33:03
know they're not they're not just sort of carved up by hyperplanes they're blobby and uh and so you know
33:09
intelligence requires methods that are very neural netlike you know that look more like continuous function approximators and that's why gradient
33:16
descent is a good idea uh for instance uh you know and learning these things via smooth functions is a good idea and
33:21
learning them or or not learning them but trying to encode them with rules never never worked out well now on the
33:27
other hand DNA is is discrete right there there there are you know four symbols and you order them in a certain
33:34
way and and you know that's it doesn't mean that there's no randomness in the way you know the way uh proteins are
33:39
folded and so on but uh composition at the level of DNA really does have to do with you know chopping up uh programs
33:46
essentially made of discrete symbols and inserting you know bits of code and and so on so you know when you're looking
33:51
from the bottom up it's a very very quantized world um but when you start to you know look at at you know giant
33:58
complex things like us uh you know from from a high level you have to begin from
34:03
a more uh more continuous perspective. Um I think you've hinted that there are natural convergent patterns in in
34:10
computation. I mean can we sort of get a convex hull of your philosophy? We could try. Um I mean I I hesitate to
34:16
say I'm in anythingist but um but probably functionalist comes closest
34:22
functionalist. Yeah. Um so the reason for that is that
34:28
uh you know in in the old days um in the 19th century we used to think that you know to be alive meant that there was
34:35
some vital spirit or vital force you know that right that living things have and dead things don't and as we started
34:40
to figure out that the laws of chemistry were the same for living things and dead things and you know ura can be synthesized in a test tube and so on you
34:47
know those ideas really went out of fashion and we went into a very strict materialist kind of perspective
34:53
right or everything is just physics and you know I I I mean I was trained as a physicist you know I think I I I believe
34:59
in physics fully um but I also think that there is more to life uh in the
35:05
sense that you know if everything is just physics then you have no way of saying what it means for you or me to be
35:10
alive and to understand what what that is what it means to be alive I think you have to come to grips with the idea of
35:17
of purpose you have to bring teiology back into the equation uh what I mean by that is you know a kidney is not just a
35:24
collection of atoms. It's it's an organ that performs a function, right? The function is to filter ura. And if you
35:31
implant an artificial kidney that works on totally different principles but also filters ura, it's an artificial kidney.
35:37
You know, it's still meaningful to say that. So that that means that there is something about that word kidney that
35:43
means something that that that goes beyond the the matter that the kidney is
35:49
made out of. uh you know conversely if I come back from the future and show you an object and and and you're like what
35:54
is that and I I tell you it's an artificial kidney you know there's nothing about this set of weird carbon
35:59
nano tubes and so on inside that would that would say to you kidney it's just that you know if you happen to implant
36:05
it in a body you know and set it and sew it you know sewed it in the right way and so on then all of those relationships would be would would show
36:12
up in the right way for your body to persist. So this this idea of things serving functions for other things and
36:18
functions only have meaning in the context of yet other functions. So there's something ecological about this idea of functions. I think this is
36:25
really central. Uh you know a rock on an inanimate planet has no function. If I break it in half I I now have two rocks.
36:32
But a living thing has function. uh and you know the the the hallmark of function is multiple realizability just
36:40
like just like uh Turing talked about for touring machines because you know Turing and Vonoya are functionalists
36:45
meaning that uh you know if you if you have a need to make ATP for energy
36:50
inside your cells uh you know you're going to have multiple pathways for doing it because sometimes the aerobic
36:55
way works sometimes you need the anorobic way whenever you start to have multiple uh pathways you know wings and
37:01
insects wings and bats you know that there is a function in play the alternative position would be essentialism. So folks like Anil Seth
37:08
and um John S, they they think that certain types of material have a certain
37:15
type of causal graph and you know so for example brains might give rise to
37:20
consciousness and if we simulated a brain it wouldn't have the same causal graph therefore it'd be different. But I
37:25
would like to I mean we'll just park that you know just just for the moment. It seems a little bit like you're you're talking about this like a computer
37:31
software architecture diagram and we can you know it's like that ship of thesis type thing where we can kind of you know
37:37
swap things out and is it still the same thing but I think um path dependence is very important so the kidney evolved it
37:45
it has this kind of this this rich um fogyny of of of evolution and when you
37:50
replace it with something you know which came from a different substrate which has a different provenence then it it's
37:56
it's almost like it it is a kidney now and it works now, but it breaks the ecology. Like imagine in an ecology if I
38:03
swapped a plant out with an artificial plant and I kept doing that. Um it might work now, but doesn't that affect its
38:10
future trajectory? Yes, it does. Um but that's exactly what symbioenesis is all
38:15
about. Uh you know often often you will have a repurposing of something that was that was designed if you like you know
38:22
by nature, right? And and what one of the cool things about the BFF experiment is that it shows you how if you you know
38:27
if you like intelligent design can happen without any intelligent designer. Um but you know something that was
38:32
designed for one purpose or to serve one function can come back around and serve another function. And uh yeah that
38:39
brings a whole different contingent history uh with it. Right? the the that that RSV example that I gave, you know,
38:45
the the ability to fuse cell membranes together came from a vi, you know, a virus whose original purpose had nothing
38:52
to do with building placentas, but you know, it gets incorporated and repurposed. Uh, you know, and this is
38:57
the kind of bricklage that uh that life is made out of. So, uh, so yeah, I think I think that that kind of uh, you know,
39:03
not only replacement but uh, you know, parallel pathing uh, etc., you know, it
39:08
it doesn't just happen when we make artificial kidneys. it's happening, you know, all the time in nature. Uh, and is
39:14
the very hallmark of life. So, yes, I disagree strongly with with Anil Seth and with John Surl on on this point. Um,
39:21
you know, the the brain of thesis kind of experiments that you've alluded to, right? The idea that if you took a an
39:26
emulator or a simulator of a neuron and you plugged it into your brain, you know, so that it's its inputs and
39:32
outputs are connected to the other neurons, you know, well, you know, then the other neurons wouldn't know the difference. Well, what if you do that
39:37
for half of your neurons, for all of them? you know, will your consciousness get dialed down even if you behave the same way? Of course not. Uh, you know,
39:44
for me, your consciousness is is obviously a function of of the functions of the relationships of all of those
39:49
things with each other. Um, it doesn't mean that it's so simple as a computer program where you can just, you know,
39:55
substitute a subruine for another one. I mean, you we've made computers very kind of um abstract in that way. Uh, you
Functionalism and consciousness
40:02
know, and biology is wet and messy. The interfaces are are complex and hard. But but this this same idea of multiple
40:08
realizability and repurposibility is is the very stuff of life. What is your position on consciousness?
40:15
So um what is it? What's its purpose? Is it epifenomenal? Can it be measured? Etc. etc.
40:21
Yes. Uh great question. So um I think that the idea of philosophical zombies
40:28
which uh uh you know David Chmer's has talked about right that maybe something could behave just like you or me but be
40:34
dead on the inside you know not not not have any experiences not feel anything is um uh is actually a lot less coherent
40:41
than it sounds. Uh so I'm a functionalist about consciousness too and what I mean by that is twofold. One
40:48
is that I don't think cons consciousness is some kind of epiphenomenon that you know just weirdly you know we happen to
40:55
have for reasons that have nothing to do with our behavior. Uh nor do I think that it is um uh that is that it is
41:01
somehow tied to anything about our the way we're physically made. Uh I think it is functional. So why do we have it?
41:08
Well, um, in in, uh, my team, Paradigms of Intelligence, we've been doing a lot of work, uh, over the last year on
41:15
multi-agent reinforcement learning. Uh, and the reason is that we're very interested in the precondition for
41:22
symbioenesis, which is symbiosis, cooperation. You know, when two things or 700 things or whatever start to
41:29
cooperate closely, you know, that that's the that's the beginning of them really fusing together and becoming one thing.
41:36
And um and in order for two agents that are intelligent to cooperate uh it turns
41:42
out they have to have theory of mind. Uh they have to model each other. They have to be able to put themselves in the
41:47
place of the other. And uh we have a you know a whole long theory called mupai about how that all works. uh but it you
41:55
know and and I guess the the cliff's note version of it is that it requires that you do induction over a universe
42:01
that includes not only the game that we're playing but that also includes what is happening in your head and what
42:08
is happening in my head. In other words, you have to have a universe that that that includes yourself in it and the
42:15
other in it and that allows you to generalize over the class of you and me. you know, so that I know, you know, my
42:21
internal state is happy when I smile and when I see you smile, I know that you're happy as well on the inside, I can make
42:27
that inference. Uh, you know, in the same way that if I see a bunch of peaches, you know, then I I know that they're all the same object and I know
42:33
what the back side of it will look like and so on. So, um, this ability to do psychological um, uh, you know,
42:40
induction is is really important for uh, for cooperation and uh, and that's why
42:45
we have it. So um you know and one of the consequences of that is that we model ourselves and we model our own
42:51
models of others models of our models and so on. There's a kind of a strange loop as Douglas Hoffetter would have
42:56
called it in that. Yes, I love Douglas Hoffatter. So so there's this kind of selfmodeling and then second order selfodeling and third
43:03
order selfodeling which could be applied to other agents and uh of course you know in in the real world we are computationally bounded right you know
43:10
we we can't make sense of all of the complexity. So when we do this modeling of other agents, um, our modeling is
43:16
quite cartoonish and it's quite structured and and it only it only goes up to sixth
43:22
order as well at most. Oh, interesting. Interesting. I mean, how does this affect I mean, we haven't
43:28
really spoken about agency yet. Presumably you could have a strong agent which is just doing something quite
43:33
trivial. But when we have this collective intelligence and this information synchrony between agents,
43:39
how does that affect your ideas of you know purposeful behavior? I sometimes use the example of rowing uh to to
43:47
describe what's happening when um purposes merge into a single purpose and and consciousnesses you know merge into
43:54
a single consciousness. Um there's there's this term u that I learned from Dan Brown's book, The Boys in the Boat,
44:01
uh swing, which is when when this the six horsemen uh or eight horsemen, sorry, uh you know, all achieve this
44:08
kind of state where they're in perfect sync with each other and and you know, you know it when you when you when you
44:13
experience it, the the boat acquires a soul as it were. You know, you all feel like you're like you're you're pulling
44:19
as one. And um and boats with that property go a lot faster than than boats where people haven't quite achieved that
44:25
sink. Um that that I think is kind of what happens when um uh you know when we
44:30
uh when we think of ourselves as being a self despite the fact that our brain actually consists of a lot of parts you
44:36
know like the the same way that that as the horsemen that you know in some sense began with their own purposes and their
44:42
own self models and their own models of the other parts of the brain. thought uh you know this process of of subjective
44:48
symbioenesis I guess you could call it uh where where all of those wills become one and all of those selves become
44:55
oneself in hiring for example you you want folks with high agency but you also want
45:01
alignment which is the potential for this kind of synchrony and we often have we do a thought experiment on MLST that
45:07
you can look at a boat or a flatillaa of boats and you're trying to draw a boundary and the boundary for the agent
45:14
should be the minimal description It should be, you know, where where is most of the agency? Where is most of the
45:19
planning and future modeling happening? And usually it's it's the pilot. It's it's the driver of of the boat. But
45:25
you're talking about the situation where there is such a synchrony and alignment between the agencies that almost like
45:30
the the best intentional stance, if you like, is to draw a boundary around all of them. Yeah. Um I also think that that there's
45:37
no there's not necessarily a single right answer. So uh you know, in in um my book, what is intelligence? I talk
45:43
about a few interesting cases. Uh, one of them is, uh, for instance, uh, the conjoined twins, Abby and Brittany
45:50
Hansel. Uh, who who, uh, I I don't know if you've seen, uh, them on on YouTube or, you know, the TV shows. Fascinating
45:57
case. And, and, um, you know, so these are, uh, two people who share one pair
46:02
of arms and one pair of legs. Uh, you know, each each of them controls one arm and one leg, so they're in a sort of three-legged race. Uh, two-legged race.
46:10
Um, they often speak in synchrony. Um and um you know they they play
46:15
volleyball and sports and stuff. They drive they drive a car. They can write emails no problem. Um uh and you know
46:22
they also sometimes uh you know have um you know differences of opinion. So you know they'll they'll they'll they'll
46:27
sort of you know come together and apart in in a in a remarkably fluid way and all of that is done purely with
46:33
behavioral cross queuing as Mike Gazanaga would call it meaning their nervous systems are separate separate
46:39
brains separate spinal cords. Um so you know in that case they they are able to model each other extremely well because
46:46
you know their entire lives they've been right next to each other. Another interesting case would be uh split brain
46:52
patients of the kind that Kazaniga you know spent a lot of his career studying and you know those are cases where in
46:59
adulthood the the brain is essentially cut in half. So you know each hemisphere can only see the left or the right
47:04
hemisphere. uh controls one arm, one leg and um the the the most fascinating
47:10
thing about these split brain experiments, you know, is that from the outside point of view, it is obvious
47:15
that there are two consciousnesses in there. You know, each hemisphere is conscious of different things. Uh you
47:20
can make disjunctions between what shows up in the left and right hemfield and you know, the left and right hands can
47:26
be drawing different things, you know, and so on. Um, but if you talk to somebody uh, you know, who's with a
47:32
split brain patient, they're always like, "Yeah, I'm I'm still one person." They will never admit that there are two people in there. So, you know, is there
47:39
somebody who is right and somebody who is wrong? No. Uh, you know, this is entirely um relational. It's a it's a
47:46
relational description. Uh, and the fact that for them, uh, they are, you know,
47:51
they're the same person they always were, just, you know, occasionally something takes a little more work. Occasionally one hand will be buttoning
47:57
the shirt while the other hand is unbuttoning it. You know it's just an inconvenience. There are split brain experiments as
48:02
well even even just with a normal brain. And I can believe that we are we are sort of separately conscious in in
48:08
different parts of our brain. You get out of bed in the morning and you must be a slightly different person. But we kind of gloss over that, don't we?
48:14
Absolutely. We make a narrative. The the the best the coolest experiments about this I think are the ones from Peter Johansson uh at at uh at University of
48:22
Salah. So he's done a bunch he he was the one who discovered uh choice blindness. Uh in these experiments a um
48:29
a subject is I think the very first one was face choice blindness. So you'd be shown two faces uh on cards and asked
48:35
which one is more attractive and you pick and you know every you know every so often uh the one that you're handed
48:43
to then explain why you thought that face was more attractive is the one you didn't pick. So there's a kind of slate of hand trick. And the cool thing is
48:49
that very very few people notice that uh you know that they're that they're being handed the wrong face. And there is no
48:56
difference in the fluency uh or the latency of the description. You know,
49:02
you have an inner lawyer ready to spring up and justify whatever choice you made even if it's not the choice you made. Uh
49:08
and and that narrative that you invent uh you know then influences your future choices. It's as if we all, you know,
49:14
make up a story about ourselves. And and of course, the reason is that, you know, you're we're all split brain patients in
49:20
a way. You know, the the the left hemisphere interpreter that that generates the speech, you know, is
49:26
likely not the same part of the brain that actually, you know, sort of did the the choosing if you if you know, you
49:31
know, and and yet all of those parts of your brain are invested in the idea that they're all in the same boat, you know,
49:36
that it's all one me. So, they're all covering for each other. In the same way that in a split brain patient uh you
49:42
know if you show to um you know to the to the non leftbrain interpreter hemisphere um you know stand up the
AI as part of collective human intelligence
49:49
person stands up and you ask them why did you stand up and you know they'll say oh I I was thirsty I'm going to the kitchen for a drink of water same thing
49:56
artificial intelligence it's becoming more sophisticated and there's the social question and I suppose actually
50:02
you can think of it as a ship of thesis for society so we're going to be having agents embedded in society and we're
50:09
going to form a large collective intelligence. Do you do you worry about that future? I
50:14
mean, what do you predict is going to happen? Well, um I mean, there are certainly things that I worry about. I don't want
50:20
to I don't want to come across as a polyiana. I'm worried about um polarization. I'm worried about
50:25
disinformation. I'm worried about our political and economic systems uh you know, not necessarily being fit for
50:31
purpose uh in the world that we'll all be living in in 20 years. But I'm certainly not concerned about uh a lot
50:37
of the kinds of things that I hear Elazar Yudkowski talking about for instance. Um and in particular uh you
50:44
know one of the reasons that I that I I feel very differently is because I feel like human intelligence in the usual
50:51
sense that we think of it is already a collective phenomenon. Uh we're not that smart individually. We're not that much
50:57
better uh individually than our our our primate cousins. Um it's only you know
51:03
because we get together in large societies of millions and billions of people that we can do these amazing
51:08
things you know that we can transplant organs and and go to space and you know and so on. Um you know individually
51:14
we're just we're just not all that. So for me um you know AI is actually a part
51:20
of human intelligence. It's it's literally already uh the the same thing. uh you know I find it very interesting
51:26
that we only achieved general AI when we began to literally train the models on reams and reams and reams of human
51:32
language. So you know AI was human intelligence from the start because I suppose the the thesis of Eliza
51:40
is that it is possible to have artifacts which are dramatically more intelligent
51:45
than than we are. Maybe you think there's some kind of a limit but but do you think in principle that we could
51:50
build artifacts which are significantly more intelligent? Well, I think that collective humanity is already vastly
51:56
more intelligent than individual humans. Uh so in that you know and and in many cases operates at very different time
52:01
scales for instance I think these things are already true. Um now you know the
52:06
the ideas about um so in in a sense in a sense our difference our biggest difference is about thinking about it as
52:13
an other versus already a part of ourselves. You know what do we even mean by human? Um there was a wonderful paper
52:21
um from 2006 called the science of psychology uh in which um I I'm not
52:27
remembering her name but uh she is a psychologist and the science of psychology is spelled cyc uh she asks
52:34
people to draw bicycles uh you know first to say do you know how a bicycle works everybody says yeah of course I
52:40
know how a bicycle works okay draw one nobody can draw it even if it's just you know looking at a sketch of a bicycle
52:45
and saying okay where does the chain go you know or where are the pedals Most people don't know you know they
52:50
make some very fundamental error in this and um you know it's it's a it's a very funny it's a very funny paper but you
52:56
know the point is we all have these illusions about what our own knowledge is our own capabilities our own
53:02
intelligence are. We already have swing in the sense that we are we identify what we think of as our intelligence
53:08
with something that is actually in a bunch of other people and a bunch of other stuff around us. Um you know and
53:14
and and we do that kind of unconsciously. So, uh, for me, you know, there's there's not really a
53:19
discontinuity between, you know, what we what's already going on and AI. It's
53:25
it's really just more of that. Interesting. You know, I think they would make the argument that you could build a single artifact which is more
53:32
intelligent than the totality of of humans. But just parking that to one side, um, I spoke with Judith Fan. She's
53:37
a wonderful professor at at Stanford and she's done studies on on drawing. So, comparing how humans draw to um
53:43
computers using clip models and stuff like that. And she found she found something fascinating which is that because we have quite an abstract
53:49
understanding when we make sketches you know and she was kind of um grading it on you know like um the progression one
53:55
progression two progression three and we we kind of start very coarse and very abstract and AI systems they start with
54:04
the edges and and the details and that to me indicates that AI models
54:09
today they don't really understand things at a very deep abstract level like we do perhaps because we have this
54:14
this compositional synthesis of knowledge that we were alluding to earlier. Do you see that as a gap?
54:20
There are a few questions I guess hidden in there. Uh you know one of them is uh you know do I think of of LLMs for
54:26
instance of of today's you know sort of frontier models as being um less than um
54:33
or different than in some basic way you know our our brains. What are those gaps? Um so first of all I mean they're
54:40
obviously very different. I mean you know their their their architectures are different. they're trained in a very different way. Um the fact that you know
54:47
for me the remarkable thing is actually how convergent a lot of a lot of their properties are with those of brains
54:53
despite all of that. Um you know the fact that you find internal representations and many of them that that that surprisingly resemble uh you
55:01
know ones that you can measure in human brains. these brain score uh type measures of of Martin Shrimp and Co. um
55:08
you know or or um uh you know sensory modalities you know uh in humans can be reproduced remarkably well even by
55:15
models trained on pure language which is really remarkable uh you know and speaks to how much is encoded in language and
55:21
how much of what is encoded in language is a reflection of architectural properties of our brains and umvelts and
55:28
how much of that is then reconstructed essentially by those models. Now uh the question of you know what we draw first
55:35
when we draw a picture and how that all works. I mean remember that you know image synthesis models like clip or what
55:41
have you are working in pixel space uh to begin with and uh you know diffusion
55:46
models by the way you know work very differently from various other kinds of models. I mean we now know that you know
55:51
you you can drive a robot with a transformer. So if you give one of those robots a paintbrush and you say or you
55:57
know a pen and you say now draw what it will draw is going to be very very different from what you get from a
56:03
diffusion model that starts filling in pixels and for that matter all of that is different from what happens in your own head when you're visualizing
56:09
something. So you know I think a lot of this is is not so straightforward to to
56:15
analyze uh because of all the differences in the way that IO and the representation space works. Um I do
56:21
think that today's models are highly compositional. I mean, even with a lot of those original uh uh you know, image
56:27
synthesis models, the fact that you could say, you know, a teddy bear at the bottom of the sea playing with the speak
56:32
and spell or whatever and it'll do it, you know, tells you that that they they can compose. Uh again, are there
56:38
capabilities like ours are, you know? No. I mean, there there are definitely places where they're better, places
56:44
where they're worse, places where they have surprising gaps. So it's different but uh but I wouldn't say that there's a
56:50
fundamental lack of composition there at all. Um I think if if anything the biggest gap between uh transformerbased
56:58
models and what we do is actually narrative memory um right or being able to form long-term memories and and and
Comparing AI and human cognition
57:05
that way have a kind of persistence of a self over over long periods of time. They they don't have that yet.
57:11
I'm conflicted. You are pointing to this universal representation hypothesis. I think Chris Oler um popularized it with
57:18
some of his visualization experiments and it's true the representations are very convergent and and other things
57:23
lead me to to um believe that the models produce these kind of superficial
57:28
imposters that they give you exactly the right answer but for the wrong reasons
57:33
and one of the hints of that is when you um do variations on on the input it's
57:38
not robust there's the there's the touring machine argument as well so you know these LLMs are finite state
57:44
automter But they can access tools which are tour incomplete. So you know perhaps we could say the system is tour
57:50
incomplete but I don't believe that um chat GPT is is effectively searching the
57:55
space of touring machine algorithms. It hasn't been trained to do that but it is surprisingly robust with the arc
58:01
challenge. It can actually um you know it can it can do really well especially if you do some evolution do do some
58:06
refinement and so on. So it feels like we're we're knocking on the door but it but but there's something missing. I
58:12
think that in many of those cases, we're not doing the we're not doing a fair human comparison. Uh, you know, we we
58:18
often uh, you know, and this is a little bit similar to our illusions about knowing how bicycles work and so on. You know, we I I hear a lot of people uh,
58:26
you know, say things like, well, you know, but but look at this case where we just flip the logic, you know, uh, you
58:31
know, we change it from do to don't and then, you know, it gets it wrong 30% more often and so on. Um, you know, my
58:37
my first question is always, have we done the human baseline? Uh, and and it turns out that surprisingly often the
58:44
human baseline shows the same uh the same property, you know, and this doesn't mean that humans are incapable
58:50
of doing, you know, the fully robust, fully general version of these things, right? If you're a logician or if you
58:55
think about it carefully, you know, you can you can really write down your premises and be super robust to, you
59:01
know, flipping the knots, you know, in the way something is is formulated. But most of us don't operate that way most
59:06
of the time. Uh, you know, and we're highly susceptible to logical illusions, cognitive illusions, etc., which turn
59:13
out to be in many cases surprisingly similar to the to the machine case. So I I'm I'm I'm kind of unmoved by, you
59:19
know, by by a lot of those. Uh, and I I think often um often we're we're being a
59:25
little sloppy about how how we do it. It's certainly the case that that um you know, transformers don't aren't
59:31
searching systematically over all possible touring machines. I mean, we don't know how to how to do that. You know, you you have to take shortcuts of
59:37
various kinds in order to make that that whole problem of of induction over over programs computationally tractable,
59:44
whether you're a brain or a, you know, or a transformer. L, thank you so much for joining us today. It's been an honor.
59:49
Thank you. Uh, thank you for the the really thoughtful questions.