https://www.youtube.com/watch?v=htOvH12T7mU
AI 2027: month-by-month model of intelligence explosion â€” Scott Alexander & Daniel Kokotajlo
318,641 views  Apr 3, 2025  Dwarkesh Podcast
Scott Alexander and Daniel Kokotajlo break down every month from now until the 2027 intelligence explosion. Scott is author of the highly influential blogs Slate Star Codex and Astral Codex Ten. Daniel resigned from OpenAI in 2024, rejecting a non-disparagement clause and risking millions in equity to speak out about AI safety. We discuss misaligned hive minds, Xi and Trump waking up, and automated Ilyas researching AI progress.

I came in skeptical, but I learned a tremendous amount by bouncing my objections off of them. I highly recommend checking out their new scenario planning document: https://ai-2027.com/. And Daniel's "What 2026 looks like," written in 2021: https://www.lesswrong.com/posts/6Xgy6...

ğ„ğğˆğ’ğğƒğ„ ğ‹ğˆğğŠğ’
Transcript: https://www.dwarkesh.com/p/scott-daniel
Apple Podcasts: https://podcasts.apple.com/us/podcast...
Spotify: https://open.spotify.com/show/4JH4tyb...

ğ’ğğğğ’ğğ‘ğ’
WorkOS helps todayâ€™s top AI companies get enterprise-ready. OpenAI, Cursor, Perplexity, Anthropic and hundreds more use WorkOS to quickly integrate features required by enterprise buyers. To learn more about how you can make the leap to enterprise, visit https://workos.com

Jane Street likes to know what's going on inside the neural nets they use. They just released a black-box challenge for Dwarkesh listeners, and I had a blast trying it out. See if you have the skills to crack it at https://janestreet.com/dwarkesh

Scaleâ€™s Data Foundry gives major AI labs access to high-quality data to fuel post-training, including advanced reasoning capabilities. If youâ€™re an AI researcher or engineer, learn about how Scaleâ€™s Data Foundry and research lab, SEAL, can help you go beyond the current frontier at https://scale.com/dwarkesh

To sponsor a future episode, visit https://dwarkesh.com/advertise

ğ“ğˆğŒğ„ğ’ğ“ğ€ğŒğğ’
00:00:00 - AI 2027
00:07:45 - Forecasting 2025 and 2026
00:15:30 - Why LLMs aren't making discoveries
00:25:22 - Debating intelligence explosion
00:50:34 - Can superintelligence actually transform science?
01:17:43 - Cultural evolution vs superintelligence
01:24:54 - Mid-2027 branch point
01:33:19 - Race with China
01:45:36 - Nationalization vs private anarchy
02:04:11 - Misalignment
02:15:41 - UBI, AI advisors, & human future
02:23:49 - Factory farming for digital minds
02:27:41 - Daniel leaving OpenAI
02:36:04 - Scott's blogging advice

---

AI 2027
0:50
Today I have the great pleasure of chatting with  Scott Alexander and Daniel Kokotajlo. Scott is  
0:57
of course the author of the blog Slate Star  Codex, Astral Codex 10 now. Itâ€™s actually been,  
1:03
as you know, a big bucket list item of mine to  get you on the podcast. So this is all the first   podcast weâ€™ve ever done, right? Yes. 
1:09
And then Daniel is the director of the AI  Futures Project. And you have both just launched  
1:15
today something called AI 2027. So what is this? Yeah, AI 2027 is our scenario trying to forecast  
1:24
the next few years of AI progress. Weâ€™re trying  to do two things here. First of all we just want  
1:29
to have a concrete scenario at all. So you have  all these people, Sam Altman, Dario Amodei, Elon  
1:35
Musk saying, â€œgoing to have AGI in three years,  superintelligence in five yearsâ€. And people just  
1:41
think thatâ€™s crazy because right now we have  chatbots that are able to do a Google search,  
1:47
not much more than that in a lot of ways. And  so people ask, â€œhow is it going to be AGI in   three years?â€ What we wanted to do is provide  a story, provide the transitional fossils. So  
1:58
start right now, go up to 2027 when thereâ€™s AGI,  2028, when thereâ€™s potentially super intelligence,  
2:06
show on a month-by-month level what happened. Kind  of in fiction writing terms, make it feel earned. 
2:12
So thatâ€™s the easy part. The hard part is  we also want to be right. So weâ€™re trying  
2:18
to forecast how things are going to go, what speed  theyâ€™re going to go at. We know that in general,  
2:26
the median outcome for a forecast like this  is being totally humiliated when everything   goes completely differently. And if you read  our scenario, youâ€™re definitely not going to  
2:35
expect us to be the exception to that trend. The thing that gives me optimism is Daniel  
2:41
back in 2021, wrote the prequel to this scenario  called What 2026 Looks Like. Itâ€™s his forecast  
2:48
for the next five years of AI progress. And  he got it almost exactly right. You should  
2:53
stop this podcast right now. You should go and  read this document. Itâ€™s amazing. Kind of looks  
2:59
like you asked ChatGPT to summarize the  past five years of AI progress, and you   got something with a couple of hallucinations,  but basically well intentioned and correct. So  
3:09
when Daniel said he was doing this sequel, I was  very excited, really wanted to see where it was  
3:16
going. It goes to some pretty crazy places  and Iâ€™m excited to talk about it more today.  I think youâ€™re hyping up a little bit too much.  Yes, I do recommend people go read the old thing  
3:25
I did, which was a blog post. I think it got a  bunch of stuff right, a bunch of stuff wrong,   but overall held up pretty well and inspired  me to try again and do a better version of it. 
3:34
I think, read the document and  decide which of us is right.  Another related thing too is that the original  thing was not supposed to end in 2026, it was  
3:44
supposed to go all the way through the exciting  stuff, right? Because everyoneâ€™s talking about,   what about AGI, what about superintelligence,  what would that even look like? So I was trying to  
3:53
step-by-step work my way from where we were at the  time until things happen and then see what they  
3:59
look like, but I basically chickened out when I  got to 2027 because things were starting to happen  
4:04
and the automation loop was starting to take off  and it was just so confusing and there was so much   uncertainty, so I basically just deleted the last  chapter and published what I had up until that  
4:15
point. And that was the blog post. Okay, and then, Scott,   how did you get involved in this project? So I was asked to help with the writing, and I was  
4:24
already somewhat familiar with the people on the  project, and many of them were kind of my heroes.  
4:30
So, Daniel, I knew both because Iâ€™d written a blog  post about his opinions before I knew about his,  
4:35
â€œWhat 2026 looks like,â€ which was amazing. And  also he had pretty recently made the national  
4:40
news for having, when he quit OpenAI, they told  him he had to sign a non-disparagement agreement  
4:48
or they would claw back his stock options. And  he refused, which they werenâ€™t prepared for. It  
4:54
started a major news story, a scandal that ended  up with OpenAI agreeing that they were no longer  
5:02
going to subject employees to that restriction. So people talk a lot about how itâ€™s hard to trust  
5:10
anyone in AI because they all have so much  money invested in the hype and getting their  
5:15
stock options better. And Daniel had attempted  to sacrifice millions of dollars in order to say  
5:24
what he believed, which to me was this incredibly  strong sign of honesty and competence. And I was  
5:30
like, how can I say no to this person? Everyone  else on the team, also extremely impressive.   Eli Liflund, whoâ€™s a member of Samotsvety, the  worldâ€™s top forecasting team. He has won, like,  
5:41
the top forecasting competition, plausibly  described as just the best forecaster in the  
5:46
world, at least by these really technical measures  that people use in the superforecasting community.  
5:52
Thomas Larsen, Jonas Vollmer, both really amazing  people who have done great work in AI before. 
5:59
I was really excited to get to work with this  superstar team. I have always wanted to get  
6:05
more involved in the actual attempt to make  AI go well. Right now, I just write about it.  
6:12
I think writing about it is important, but I  donâ€™t know. You always regret that youâ€™re not  
6:17
the person whoâ€™s the technical alignment genius  whoâ€™s able to solve everything. And getting to  
6:22
work with people like these and potentially make  a difference just seemed like a great opportunity. 
6:28
What I didnâ€™t realize was that I also learned a  huge amount. I try to read most of whatâ€™s going  
6:35
on in the world of AI, but itâ€™s this very  low bandwidth thing and getting to talk to  
6:40
somebody whoâ€™s thought about it as much as  anyone in the world was just amazing. Makes  
6:47
me really understand these things about how  AI is going to learn quickly. You need all of  
6:52
this deep engagement with the underlying  territory and I feel like I got that.  Iâ€™ve probably changed my mind towards, against,  towards, against, intelligence explosion three,  
7:02
four times in the conversations Iâ€™ve had  in the lead-up in talking to you and then   trying to come up with a rebuttal or something. It wasnâ€™t even just changing my mind, getting to  
7:12
read the scenario for the first time. It obviously  wasnâ€™t written up at this point. It was a giant,   giant spreadsheet. Iâ€™ve been thinking about  this for a decade, decade and a half now. And  
7:26
it just made it so much more concrete to have  a specific story. Like, oh, yeah, thatâ€™s why   weâ€™re so worried about the arms race with China.  Obviously we would get an arms race with China  
7:35
in that situation. And aside from just the people  getting to read the scenario really sold me. This  
7:41
is something that needs to get out there more. Yeah. Okay. Now letâ€™s talk about this new   forecast. Because you do a month by month  analysis of whatâ€™s going to happen from here.  
Forecasting 2025 and 2026
7:51
So what is it that you expect in mid-2025  and the end of 2025 In this forecast? 
7:57
So, [the] beginning of the forecast mostly focuses  on agents. We think theyâ€™re going to start with  
8:05
agency training, expand the time horizons, get  coding going well. Our theory is that they are,  
8:12
to some degree consciously, to some degree  accidentally, working towards this intelligence   explosion, where the AIs themselves can start  taking over some of the AI research, move faster. 
8:23
So 2025, slightly better coding, 2026,  slightly better agents, slightly better coding.  
8:30
And then we focus on, and we name the scenario  after 2027 because that is when this starts to  
8:36
pay off. The intelligence explosion gets into  full swing; the agents become good enough to  
8:43
help with- at the beginning not really do,  but help with- some of the AI research. 
8:48
So we introduced this idea called the R&D  progress multiplier: how many months of  
8:53
progress without the AIs do you get in one month  of progress with all of these new AIs helping  
8:59
with the intelligence explosion. So 2027, we  start with- I canâ€™t remember if it literally  
9:05
starts with, or by March or something- a five  times multiplier for algorithmic progress. 
9:11
So we have the stats tracked on the site of the  story. Part of why we did it as a website is so   that you can have these cool gadgets and widgets.  And so as you read the story, the stats on the  
9:21
side automatically update. And so one of those  stats is the progress multiplier. Another answer   to the same question you asked is basically;  2025, nothing super interesting happens,  
9:32
more or less similar trends to what weâ€™re seeing. Computer use is totally solved? Partially solved?  
9:37
How good is computer use by the end of 2025? My guess is that they wonâ€™t be making basic   mouse click errors by the end of 2025, like  they sometimes currently do. If you watch  
9:45
Claude Plays Pokemon- which you totally should-  it seems like sometimes itâ€™s just failing to   parse whatâ€™s on the screen and it thinks that  its own player character is an NPC and gets  
9:56
confused. My guess is that that sort of thing  will mostly be gone by the end of this year,  
10:02
but that they still wonâ€™t be able to autonomously  operate for long periods on their own. 
10:09
But by 2025, when you say it wonâ€™t be able  to act coherently for long periods of time   in computer use, if I want to organize  a happy hour in my office, I donâ€™t know,  
10:17
thatâ€™s like what a 30 minute task? What fraction  of that is, itâ€™s got to invite the right people,   itâ€™s got to book the right doordash or something.  What fraction of that is it able to do? 
10:27
My guess is that by the end of this year  thereâ€™ll be something that can kind of do that,   but unreliably. And that if you actually tried  to use that to run your life, it would make some  
10:36
hilarious mistakes that would appear on Twitter  and go viral, but that the MVP of it will probably  
10:42
exist by this year. Like thereâ€™ll be some Twitter  thread about someone being like, â€œI plugged in   this agent to like run my party and it worked!â€ Our scenario focuses on coding in particular  
10:51
because we think coding is what starts the  intelligence explosion. So we are less interested  
10:57
in questions of like, â€œhow do you mop up the last  few things that are uniquely humanâ€ compared to  
11:03
â€œwhen can you start coding in a way that helps the  human AI researchers speed up their AI research,   and then, if youâ€™ve helped them speed up  the AI research enough, is that enough to,  
11:12
with some ridiculous speed multiplier- 10 times,  100 times- mop up all of these other things?â€ 
11:18
One observation I have is, you could have told  a story in 2021, once ChatGPT comes outâ€¦ I think  
11:25
I had friends who were credible AI thinkers who  were like, â€œlook, youâ€™ve got the coding agent now,  
11:32
itâ€™s been cracked. Now the GPT4 will go around and  itâ€™ll do all this engineering and we do this RL on  
11:37
top. We can totally scale up the system 100xâ€  and every single layer of this has been much  
11:44
harder than the strongest optimist expected.  It seems like there have been significant   difficulties in increasing the pre-training size,  at least from rumors about field training runs or  
11:54
underwhelming training runs at labs. It seems like building up these  
12:00
RL- total outside view, I know nothing about the  actual engineering involved here- but just from an   outside view it seems like building up the O1  RL clearly took at least two years after GPT4  
12:12
was released. And these things are also, their  economic impact and the kinds of things you would   immediately expect based on benchmarks for them  to be especially capable at isnâ€™t overwhelming,  
12:22
like the call center workers havenâ€™t been fired  yet. So why not just say look, at higher scale  
12:31
it will probably get even more difficult. Wait a second, Iâ€™m a little confused to hear   you say that, because when I have seen people  predicting AI milestones like Katja Graceâ€™s  
12:41
expert surveys, they have almost always been  too pessimistic from a point of view of how  
12:47
fast AI will advance. So I think the 2022 survey,  they actually said that things that had already  
12:57
happened would take like 10 years to happen,  but then the survey- it might have been 2023,  
13:02
it was like six months before GPT3, GPT4, came  out. And there were things that GPT3 or 4 or  
13:09
whichever one of them it was, did, that  it did in six months that they were still   predicting like five or ten years from. Iâ€™m sure  Daniel is going to have a more detailed answer,  
13:18
but I absolutely reject the premise that  everybody has always been too optimistic.  Yeah, I think in general, most people following  the field have underestimated the pace of AI  
13:28
progress and underestimated the pace of  AI diffusion into the world. For example,   Robin Hanson famously made a bet about  less than a billion dollars of revenue  
13:35
I think by 2025 from AI. I agree Robin Hanson in   particular has been too pessimistic. But heâ€™s a smart guy. So I think that the  
13:42
aggregate opinion has been underestimating the  pace of both technical progress and deployment.  
13:49
I agree that there have been plenty of people  whoâ€™ve been more bullish than me and have been   already proven wrong, but theyâ€™re not me. Wait a second. We donâ€™t have to guess about  
13:57
aggregate opinion, we can look at Metaculus.  Metaculus, I think their timeline was like 2050  
14:04
back in 2020. It gradually went down to like  2040 two or three years ago. Now itâ€™s at 2030,  
14:10
so itâ€™s barely ahead of us. Again, that may  turn out to be wrong, but it does look like   the Metaculans overall have, have been too  pessimistic, thinking too long term rather  
14:20
than too optimistic. And I think thatâ€™s like the  closest thing we have to a neutral aggregator   where weâ€™re not cherry picking things. Yeah. I had this interesting experience  
14:27
yesterday. We were having lunch  with this senior AI researcher,   probably makes on the order of millions a  month or something, and we were asking him,  
14:38
â€œhow much are the AIs helping you?â€ And he said,  â€œin domains which I understand well, and itâ€™s   closer to autocomplete but more intense, there  itâ€™s maybe saving me four to eight hours a week.â€ 
14:50
But then he says, â€œin domains which Iâ€™m less  familiar with, if I need to go wrangle up some   hardware library or make some modification  to the kernel or whatever, where I know less,  
15:01
that saves me on the order of 24 hours  a week.â€ Now, with current models. What  
15:08
I found really surprising is that the help  is bigger where itâ€™s less like autocomplete   and more like a novel contribution. Itâ€™s like a  more significant productivity improvement there. 
15:17
Yeah, that is interesting. I imagine whatâ€™s going  on there is that a lot of the process when youâ€™re  
15:22
unfamiliar with a domain is like Googling around  and learning more about the domain. And language   models are excellent because theyâ€™ve already  read the whole Internet and know all the details. 
Why LLMs aren't making discoveries
15:30
Isnâ€™t this a good opportunity to discuss a certain  question I asked Dario that you responded to? 
15:36
What are you thinking of? Well, I asked this question where, as you say,   they know all this stuff. I donâ€™t know if you  saw this. I asked this question where I said,  
15:44
look, these models know all this stuff. And if  a human knew every single thing a human has ever  
15:51
written down on the Internet, theyâ€™d be able to  make all these interesting connections between   different ideas and maybe even find medical  cures or scientific discoveries as a result. 
16:00
There was some guy who noticed that magnesium  deficiency causes something in the brain that   is similar to what happens when you  get a migraine. And so he just said:  
16:08
give you magnesium supplements that cured  a lot of migraines. So why arenâ€™t able to   leverage this enormous asymmetric advantage they  have to make a single new discovery like this? 
16:18
And then the example I gave was that humans  also canâ€™t do this. So for me, the most salient  
16:25
example is the etymology of words. You have all  of these words in English that are very similar,  
16:31
like â€˜happyâ€™ versus â€˜haplessâ€™, â€˜happenâ€™,  â€˜perhapsâ€™. And we never think about them   unless you read an etymology dictionary and  theyâ€™re like, oh, obviously these all come  
16:39
from some old root that has to mean â€˜luckâ€™  or â€˜occurrenceâ€™ or something like that.  So itâ€™s kind of about figuring out versus  checking. If I tell you those, youâ€™re like,  
16:49
â€œthis seems plausibleâ€. And of course, in  etymology, there are also a lot of false friends   where they seem plausible but arenâ€™t connected.  But you really do have to have somebody shove  
16:58
it in your face before you start thinking  about it and make all of those connections.  I will actually disagree with this.  We know that humans can do like,  
17:05
we have examples of humans doing this. I agree  that we donâ€™t have logical omniscience because   there is a combinatorial explosion, but we are  able to leverage our intelligence toâ€¦ one of my  
17:16
favorite examples of this is David Anthony, the  guy who wrote the Horse, the Wheel and Language.  He made this super impressive discovery  before we had the genetic evidence for it,  
17:26
like a decade before, where he said, look, if I  look at all these languages in India and Europe,  
17:33
they all share the same etymology. I mean  literally the same etymology for words like  
17:39
â€˜wheelâ€™ and â€˜cartâ€™ and â€˜horseâ€™. And these  are technologies that have only been around   for the last 6,000 years, which must mean that  there was some group that these groups are all,  
17:50
at least linguistically, descended from. And  now we have genetic evidence for the Yamnaya,   which we believe is this group. You have a  blog where you do this. This is your job,  
17:59
Scott! So why shouldnâ€™t we hold the fact that  language models canâ€™t do this more against them? 
18:05
Yeah. So to me, it doesnâ€™t seem like he is  just sitting there being logically omniscient  
18:10
and getting the answer. It seems like heâ€™s  a genius, heâ€™s thought about this for years,  
18:15
probably at some point, he heard a couple of  Indian words and a couple of European words at   the same time and they kind of connected and  the light bulb came on. So this isnâ€™t about  
18:25
having all the information in your memory  so much as the normal process of discovery,   which is kind of mysterious, but seems to come  from having good heuristics and throwing them at  
18:36
things until you kind of get a lucky strike. My guess is if we had really good AI agents  
18:41
and we applied them to this task, it would look  something like a scaffold where itâ€™s like, think   of every combination of words that you know of,  compare them. If they sound very similar, write  
18:51
it on this scratch pad here. If a lot of words of  the same type show up on the scratch pad, thatâ€™s  
18:57
pretty strange, do some kind of thinking around  it. And I just donâ€™t think weâ€™ve even tried that. 
19:03
And I think right now if we tried it, we would  run into the combinatorial explosion. We would   need better heuristics. Humans have such  good heuristics that probably most of the  
19:12
things that show up even in our conscious mind,  rather than happening on the level of some kind   of unconscious processing, are at least the kind  of things that could be true. I think you could  
19:22
think of this as like a chess engine. You have  some unbelievable number of possible next moves,  
19:27
you have some heuristics for picking out  which of those are going to be the right   ones. And then gradually you kind of have the  chess engine think about it, go through it,  
19:36
come up with a better or worse move, then at some  point you potentially become better than humans.   I think if you were to force the AI to do this  in a reasonable way, or you were to train the AI  
19:45
such that it itself could come up with the plan of  going through this in some kind of heuristic-laden   way, you could potentially equal humans. Iâ€™ll add some more things to that. So I  
19:54
think thereâ€™s a long and sordid history of  people looking at some limitation of the  
20:00
current LLMs and then making grand claims about  how the whole paradigm is doomed because theyâ€™ll   never overcome this limitation. And then a year or  two later the new LLMs overcome that limitation. 
20:12
And I would say that with respect to this thing  of â€œwhy havenâ€™t they made these interesting   scientific discoveries by combining the knowledge  they already have and noticing interesting  
20:20
connections?â€ I would say first of all, have we  seriously tried to build scaffolding to make them   do this? And I think the answer is mostly no. I think Google DeepMind tried this. 
20:29
Maybe. Second thing, have you tried making the  model bigger? Theyâ€™ve made it a bit bigger over  
20:35
the last couple years and it hasnâ€™t worked so  far. Maybe if they make it even bigger still,   itâ€™ll notice more of these connections. And then  third thing, and hereâ€™s I think the special one:  
20:45
Have you tried training the model to do the  thing? The pre-training process doesnâ€™t strongly  
20:53
incentivize this type of connection making. In general I think itâ€™s a helpful heuristic that  
20:59
I use to ask the question of: remind oneself, what  was the AI trained to do? What was its training  
21:04
environment like? And if youâ€™re wondering  why hasnâ€™t the AI done this, ask yourself,   did the training environment train it to do  this? And often the answer is no. And often I  
21:13
think thatâ€™s a good explanation for why the AI is  not good at it is that it wasnâ€™t trained to do it. 
21:18
I mean it seems like such  an economically valuableâ€¦  But how would you set up the training  environment? Wouldnâ€™t it be really  
21:25
gnarly to try to set up an RL environment  to train to make new scientific discoveries?  Maybe thatâ€™s why you should have longer  timelines. Itâ€™s a gnarly engineering problem. 
21:33
Well in our scenario they donâ€™t just leap from  where we are now to solving this problem. They  
21:38
donâ€™t. Instead they just iteratively improve  the coding agents until theyâ€™ve basically   got coding solved. But even still, their  coding agents are not able to do some of  
21:49
this stuff. Thatâ€™s what early 2020, like the  first half of 2027 in our story is basically,  
21:54
theyâ€™ve got these awesome automated coders,  but they still lack research taste and they   still lack maybe organizational skills and stuff. And so they need to overcome those remaining  
22:03
bottlenecks and gaps in order to completely  automate the AI research cycle. But theyâ€™re   able to overcome those gaps faster than they  normally would because the coding agents are  
22:12
doing all the grunt work really fast for them. Yeah, I think it might be useful to think of   our timelines as being like 2070, 2100. Itâ€™s  just that the last 50 to 70 years of that all  
22:22
happened during the year 2027 to 2028, because  we are going through this intelligence explosion  
22:27
like I think if I asked you, could we solve this  problem by the year 2100? You would say, oh, yeah,   by 2100? Absolutely. And weâ€™re just saying that  the year 2100 might happen earlier than you expect  
22:37
because we have this research progress multiplier. And then let me just address that in a second.  
22:42
But just one final thought on this thread. To  the extent that thereâ€™s like a modus ponens,  
22:48
modus tollens thing here, where one thing you  could say is like, look: AIs- not just LLMs,  
22:54
but AIs- will have this fundamental asymmetric  advantage where they know all this shit. And   why arenâ€™t they able to use their general  intelligence to use this asymmetric advantage  
23:04
to some enormous capability overhang. Now, you could infer that same statement   by saying, okay, well, once they do have that  general intelligence, they will be able to use  
23:13
their asymmetric advantage to make all these  enormous gains that humans are in principle   less capable of, right? So basically, if you  do subscribe to this view that AIs could do  
23:23
all these things if only they had general  intelligence, you got to be like, well,   once we actually do get the AGI, itâ€™s actually  going to be a totally transformative because they  
23:30
will have all of human knowledge memorized and  they can use that to make all these connections.  Iâ€™m glad you mentioned that our current scenario  does not really take that into account very much.  
23:36
So thatâ€™s an example in which our scenario is  possibly underestimating the rate of progress. 
23:43
Youâ€™re so conservative, Daniel. This has been my experience working with the team,   as I point out, five different things. â€œAre  you sure youâ€™re taking this into account? Are  
23:51
you sure youâ€™re taking this into account?â€  And first of all, 99% of the time he says,   â€œyes, we have a supplement on itâ€. But even  when he doesnâ€™t say that, heâ€™s like, â€œyeah,  
23:58
thatâ€™s one reason it could go slower than  that. Here are 10 reasons it could go fasterâ€. 
24:03
Itâ€™s trying to be sort of like our median  guess. So there are a bunch of ways in  
24:08
which we could be underestimating, and there  are a bunch of ways in which you could be   overestimating. And weâ€™re going to hopefully  continue to think more about this afterwards  
24:17
and continue to iteratively refine our models  and come up with better guesses and so forth. 
25:21
So if I look back at AI progress in the  past, if we were back in, say, 2017.  
Debating intelligence explosion
25:30
Suppose we had these superhuman coders in 2017;  the amount of progress weâ€™ve made since then,   so where we are currently in 2025, by  when could we have had that instead? 
25:39
Great question. Weâ€™d still have to stumble  through all the discoveries that weâ€™ve made   since 2017. We still have to figure out that  language models are a thing, we still have to  
25:47
figure out that you can fine tune them with RL. So all those things would still have to happen.   How much faster would they happen? Maybe  5x faster, because a lot of the small scale  
25:58
experiments that these people do in order to  test out ideas really quickly before they do   the big training runs would happen much  faster because theyâ€™re just lickety-split  
26:05
being spit out. Iâ€™m not very confident in that  5x number, it could be lower, it could be higher,   but that was roughly what we were guessing. Our 5x, by the way, is for the algorithmic  
26:14
progress part, not for the overall thing.  So in this hypothetical, according to me,  
26:19
basically things would be going 2.5x faster, where  the algorithms would be advancing at 5x speed,   but the compute is still stuck at the usual speed. That seems plausible to me. You have a 5x at some  
26:29
point, and then dot dot dot, you have 1000x  AI progress within the matter of a year. Maybe  
26:36
thatâ€™s the part Iâ€™m like, wait, how did that  happen exactly? So whatâ€™s the story there?  The way that we did our takeoff forecast,  which weâ€™ll get to in a second, was  
26:42
basically by breaking down how we think the  intelligence explosion would go into a series   of milestones. First you automate the coding,  then you automate the whole research process,  
26:50
but in a very similar way to how humans do it  with teams of agents that are about human level,  
26:56
then you get to superhuman level and so forth. So we broke it down into these milestones,   you know, the superhuman coder, superhuman  AI researcher, and then super intelligent AI  
27:03
researcher. And the way we did our forecast was,  for each of these milestones, we were like, what  
27:10
is it going to take to make an AI that achieves  that milestone? And then once you do achieve that  
27:15
milestone, how much is your overall speedup? And  then whatâ€™s it going to take to achieve the next   milestone? Combine that with the overall speed up  and that gets you your clock time distance until  
27:25
that happens and then, okay, now youâ€™re at that  milestone. Whatâ€™s your overall speed up? Assuming   that you have that milestone also, whatâ€™s the next  one? How long does it take to get to the next one?  
27:33
So we sort of work through it bit by bit, and at  each stage weâ€™re just making our best guesses.  So quantitatively we were thinking something  like 5x speedup to algorithmic progress from  
27:43
the superhuman coder, and then something like  a 25x speedup to algorithmic progress from the  
27:49
superhuman AI researcher. Because at that  point youâ€™ve got the whole stack automated,   which I think is substantially more useful  than just automating the coding. And then  
28:02
I forget what we say for a super intelligent  AI researcher, but off the top of my head   itâ€™s probably something in the hundreds  or maybe like 1000x overall speed up. 
28:12
So maybe the big picture thing I have with the  intelligence explosion isâ€¦ we can go through   the specific arguments about how much will the  automated coder be able to do, and how much will  
28:21
the superhuman AI coder be able to do. But on  priors, itâ€™s just such a wild thing to expect. 
28:27
And so, before we get into all the specific  arguments, maybe you can just address this idea   that, why not just start off with 0.01% chance  this thing might happen? Then you need extremely,  
28:39
extremely strong evidence that it will  before making that your modal view.  I think that itâ€™s a question of what is your  default option or what are you comparing it  
28:48
to. I think that naively people think like, well,  every particular thing is potentially wrong. So  
28:57
letâ€™s just have a default path where nothing ever  happens. And I think that that has been the most  
29:03
consistently wrong prediction of all. Like,  I think in order to have nothing ever happen,   you actually need a lot to happen. Like you need  suddenly AI progress that has been going at this  
29:11
constant rate for so long stops. Why does it stop? Well, we donâ€™t know. Whatever claim youâ€™re making  
29:16
about that is something where you would expect  there to be a lot of out of model error is where   you would expect. Think like somebody must be  making a pretty definite claim that you want  
29:25
to challenge. So I donâ€™t think thereâ€™s a neutral  position where you can just say, well, given that  
29:30
out of model error is really high and we donâ€™t  know anything, letâ€™s just choose that. I think  
29:35
we are trying to take- I know this sounds crazy  because if you read our document, all sorts of   bizarre things happen. Itâ€™s probably the weirdest  couple of years that have ever been. But weâ€™re  
29:45
trying to take almost in some sense a conservative  position where the trends donâ€™t change,  
29:51
nobody does an insane thing, nothing that we have  no evidence to think will happen happens. And the  
29:59
way that the AI intelligence explosion  dynamics work are just so weird that in  
30:04
order to have nothing happen, you need  to have a lot of crazy things happen.  One of my favorite meme images is this graph  showing world GDP over time. Youâ€™ve probably  
30:14
seen it, it spikes up and then thereâ€™s a little  thought bubble at the top of the spike in 2010 or  
30:23
something. And the thought bubble says, â€œmy life  is pretty normal, I have a good grasp of whatâ€™s  
30:28
weird versus standard and people thinking about  different futures with digital minds and space  
30:36
travel are just engaging in silly speculationâ€. The point of the graph is, actually thereâ€™s been  
30:42
amazing transformative changes in the course  of history that would have seemed totally  
30:48
insane to people multiple times. Weâ€™ve gone  through multiple such waves of those things.  Everything weâ€™ve talked about has happened before.  Algorithmic progress already doubles every year or  
31:00
so. So itâ€™s not insane to think that algorithmic  progress can contribute to these compute things.  
31:06
In terms of general speedup, weâ€™re already  at like a thousand times research speedup,   multiplier compared to the Paleolithic  or something. So from the point of view  
31:16
of anyone in most of history, we are going at  a blindingly insane pace. And all that weâ€™re  
31:21
saying here is that itâ€™s not going to stop. The same trend that has caused us to have a  
31:27
thousand times speed up multiplier relative  to past eras and not even the Paleolithic,   like what happened in the century between, I donâ€™t  know, 600 and 700 A.D. I'm sure there are things,  
31:37
Iâ€™m sure historians could point them out. Then  you look at the century between 1900 and 2000 and  
31:43
itâ€™s just completely qualitatively different. Of course there are models of whether that   stagnated recently or whatâ€™s going on here. We can  talk about those, we can talk about why we expect  
31:54
the intelligence explosion to be an antidote to  that kind of stagnation. But nothing weâ€™re saying   is that different from what has already happened. I mean, you are saying that these previous  
32:03
transitions have been smoother  than the one you were anticipating.  Weâ€™re not sure about that, actually. So one of  these models is just a hyperbola. Everything  
32:12
is along the same curve. Another model is that  there are these things like the literal Cambrian   explosion. If you want to take this very far  back, go full Ray Kurzweil. The literal Cambrian  
32:22
explosion, the agricultural revolution, the  industrial revolution, has phase changes.  When I look at the economic modeling of this,  my impression is the economists think that we  
32:31
donâ€™t have good enough data to be sure whether  this is all one smooth process or whether itâ€™s   a series of phase changes. When it is one smooth  process, the smooth process is often a hyperbola  
32:41
that shoots to infinity in weird ways. We  donâ€™t think itâ€™s going to shoot to infinity.   We think itâ€™s going to hit bottlenecks again. You guys are the conservative crowd, you know? 
32:49
We think itâ€™s going to hit bottlenecks the same as  all these previous processes. The last time this   hit a bottleneck, if you take the hyperbola view,  is in, like 1960, when humans stopped reproducing  
32:59
at the same rate they were reproducing  before. We hit a population bottleneck,   the usual population, two ideas, flywheel stopped  working, and then we stagnated for a while. 
33:09
If you can create a country of geniuses in a data  center, as I think Dario Amodei put it, then you   no longer have this population bottleneck,  and youâ€™re just expecting continuation of  
33:17
those pre-1960 trends. So I realize all of these  historical hyperbolas are also kind of weird,  
33:24
also kind of theoretical, but I donâ€™t think  weâ€™re saying anything that there isnâ€™t models   for which have previously seemed to  work for long historical periods. 
33:32
Another thing also is, I think people equivocate  between slow and continuous, right? So if you  
33:39
look at our scenario, thereâ€™s this continuous  trend that runs through the whole thing of this  
33:45
algorithmic progress multiplier. And weâ€™re not  having discrete jumps from like 0 to 5x to 25x.  
33:51
We have this continuous improvement. So I think  continuous is not the crux. The crux is like,   is it going to be this fast? You know,  and we donâ€™t know, maybe itâ€™ll be slower,  
33:59
maybe itâ€™ll be faster. But we have our  arguments for why we think maybe this fast.  Okay, now that we brought up the intelligence  explosion, letâ€™s discuss that, because Iâ€™m kind  
34:09
of skeptical. It doesnâ€™t really seem to me  that a notable bottleneck to AI progress, or  
34:17
the main bottleneck to AI progress, is the amount  of researchers, engineers who are doing this kind  
34:24
of research. It seems more like compute or some  other thing is a bottleneck. And the piece of  
34:29
evidence is that when I talk to my AI researcher  friends at the labs, they say thereâ€™s maybe 20  
34:36
to 30 people on the core pre-training team thatâ€™s  discovering all these algorithmic breakthroughs. 
34:43
If the headcount here was so valuable you would  think that, for example, Google DeepMind would  
34:49
take not just all their smartest people, not  just from DeepMind but all of Google and just   put them on pre-training or RL or whatever  the big bottleneck was. Youâ€™d think OpenAI  
34:57
would hire every single Harvard math PhD and  in six months youâ€™re all going to be trained  
35:03
up on how to do AI research. I know theyâ€™re  increasing headcount, but they donâ€™t seem  
35:11
to treat this as the kind of bottleneck that  it would have to be for millions of them in  
35:17
parallel to be rapidly speeding up AI research. Thereâ€™s this quote that â€œone Napoleon is worth  
35:25
40,000 soldiersâ€ was commonly a thing that was  said when he was fighting. But 10 Napoleons is not  
35:31
400,000 soldiers. Right? So why think that these  million AI researchers are netting you something  
35:37
that looks like an intelligence explosion? So previously I talked about three stages of   our takeoff model. First is you get the superhuman  coder. Second is when you fully automated AI R&D,  
35:46
but itâ€™s still at basically human level,  itâ€™s as good as your best humans. And then   third is now youâ€™re in super intelligence  territory and itâ€™s qualitatively better. 
35:54
In our guesstimates of how much faster  algorithmic progress would be going,  
35:59
the progress multiplier for the middle level,  we basically do assume that you get massive  
36:04
diminishing returns to having more minds running  in parallel. And so we totally buy all of that. 
36:09
Yeah. And then I think the addition to that is the  question, then, why do we have the intelligence   explosion? And the answer is: combination of that  speed up and the speed up in serial thought speed. 
36:21
And also the research taste thing. Here are  some important inputs to AI R&D progress today:  
36:29
research taste. So the quality of your best  researchers, the people who are managing   the whole process, their ability to learn  from data and make more efficient use of  
36:39
the compute by running the right experiments  instead of flailing around running a bunch   of useless experiments. Thatâ€™s research taste. Then thereâ€™s the quantity of your researchers,  
36:47
which we just talked about. Then thereâ€™s  the serial speed of your researchers,   which currently is all the same because theyâ€™re  all humans and so they all run at basically the  
36:55
same serial speed. And then finally thereâ€™s  how much compute you have for experiments.  
37:00
So what weâ€™re imagining is that basically serial  speed starts to matter a bunch because you switch  
37:06
to AI researchers that have orders of magnitude  more serial speed than humans. But it tops out;  
37:13
we think that over the course of our scenario,  if you look at our sliding stats chart, it goes   from 20x to 90x or something over the course of  the scenario, which is important, but not huge. 
37:28
And also we think that once you start getting  90x serial speed, youâ€™re just bottlenecked on   the other stuff and so additional improvements  in serial speed basically donâ€™t help that much.  
37:38
With respect to the quantity of course,  yeah, weâ€™re imagining you get hundreds of   thousands of AI agents, a million AI agents,  but that just means youâ€™d be bottlenecked on  
37:46
the other stuff. Youâ€™ve got tons of parallel  agents, thatâ€™s no longer your bottleneck. What   do you get bottlenecked on? Taste and compute. So by the time itâ€™s mid-2027 in our story, when  
37:56
theyâ€™ve fully automated the AI research, thereâ€™s  basically the two things that matter is; whatâ€™s   the level of taste of your AIs, how good are  they at learning from the experiments that youâ€™re  
38:04
doing? And then how much compute do you have for  running those experiments? And thatâ€™s the sort of  
38:11
core setup of our model. And when we get our 25x  multiplier, itâ€™s starting from those premises. 
38:17
Is there some intuition pump from history  where thereâ€™s been some output and because  
38:25
of some really weird constraints, production  of it has been rapidly skewed along one input,  
38:33
but not all the inputs that have been historically  relevant and you still get breakneck progress. 
38:38
Possibly the Industrial Revolution. Iâ€™m just  extemporizing here, I hadnâ€™t thought about   this before, but as Scottâ€™s famous post that was  hugely influential to me a decade ago talks about,  
38:50
thereâ€™s been this decoupling of population  growth from overall economic growth that   happened with the Industrial Revolution. And  so in some sense, maybe you could say thatâ€™s  
38:58
an example of previously these things grew  in tandem. More population, more technology,   more farms, more houses, et cetera. Your capital  infrastructure and your human infrastructure was  
39:08
going up together, but then we got the industrial  revolution and they started to come apart.  And now all the capital infrastructure was  growing really fast compared to the human  
39:16
population size. I think Iâ€™m imagining something  maybe similar happening with algorithmic progress.   And again with population, population still  matters a ton today. In some sense progress  
39:27
is bottlenecked on having larger populations and  so forth. But itâ€™s just that the population growth  
39:32
rate is just inherently kind of slow and the  growth rate of capital is much faster. And so   it just comes to be a bigger part of the story. Maybe the reason that this sounds less plausible  
39:44
to me than the 25x number implies is that when I  think about concretely what that would look like,  
39:50
where you have these AIs and we know that thereâ€™s  a gap in data efficiency between human brains  
39:57
and these AIs. And so somehow thereâ€™s a lot of  them thinking and they think really hard and  
40:02
they figure out how to define a new architecture  that is like the human brain or has the advantages  
40:07
of the human brain. And I guess they can  still do experiments, but not that many. 
40:13
Part of me just wonders, what if you just need  an entirely different kind of data source thatâ€™s   not like pre-training for that, but they have  to go out in the real world to get that. Or  
40:22
maybe it needs to be an online learning policy  where they need to be actively deployed in the  
40:31
world for them to learn in this way. And so  youâ€™re bottlenecked on how fast they can be   getting real world data. I just think itâ€™s hardâ€¦ So we are actually imagining online learning  
40:38
happening. Oh really?  Yeah. But not so much real world as inâ€¦ the thing  is that if youâ€™re trying to train your AIs to do  
40:46
really good AI R&D, then the AI R&D is happening  on your servers. And so you can have this loop of:  
40:57
you have all these AI agents autonomously doing AI  R&D, doing all these experiments, et cetera, and   then theyâ€™re like online learning to get better  at doing AI R&D based on how those experiments go. 
41:06
But even in that scenario alone, I can imagine  bottlenecks like, oh, you had a benchmark and   it got reward hacked for what constitutes AI R&D  because you obviously canâ€™t haveâ€¦ maybe you would,  
41:17
but is it as good as a human brain? Itâ€™s just like  such an ambiguous thing youâ€™d have. Right now we  
41:23
have benchmarks that get reward hacked, right? But then they autonomously build new benchmarks.  
41:28
I think what youâ€™re saying is maybe this whole  process just goes off the rails due to lack of   contact with ground truth outside in the actual  world, outside the data centers. Maybe? Again,  
41:41
part of my guess here is that a lot of the ground  truth that you want to be in contact with is stuff  
41:46
thatâ€™s happening on the data centers, things like  how fast are you improving on all these metrics,   and you have these vague ideas for new  architectures, but youâ€™re struggling to get  
41:56
them working. How fast can you get them working? And then separately, insofar as there is a  
42:02
bottleneck of talking to people outside and stuff,  well they are still doing that. And once theyâ€™re  
42:07
fully autonomous, they can even do that much  faster. You can have all the million copies   connected to all these various real world research  programs and stuff like that. So itâ€™s not like  
42:15
theyâ€™re completely starved for outside stuff. What about the skepticism that, look,  
42:22
what youâ€™re suggesting with this hyper  efficient hive mind of AI researchers,  
42:28
no human bureaucracy has just out of the gate  worked super efficiently, especially one where  
42:33
they donâ€™t have experience working together. They  havenâ€™t been trained to work together, at least   yet. And there hasnâ€™t been this outer loop RL on  like, â€œwe ran a thousand concurrent experiments  
42:45
of different AI bureaucracies doing AI research  and this is the one that actually worked bestâ€.  And the analogy Iâ€™d use maybe is to humans in  the Savannah 200,000 years ago. We know they  
42:54
have a bunch of advantages over the other animals  already at this point, but the things that make us  
43:02
dominant today, joint stock corporations, state  capacities like this fossil fueled civilization  
43:09
we have that took so much cultural evolution to  figure out. You couldnâ€™t just have figured it out  
43:16
in the savannahs like, â€œoh, if we had built these  incentive systems and we issued dividends, then  
43:22
we could really collaborate hereâ€ or something. Why not think that it will take a similar process  
43:27
of huge population growth, huge social  experimentation, and upgrading of the  
43:34
technological base of the AI society before  they can organize this hypermind collective,  
43:40
which will enable them to do what you  imagine an intelligence explosion looks like?  Yeah, youâ€™re comparing it kind of to two  different things. One of them is literal  
43:48
genetic evolution in the African savannah,  and the other is the cultural evolution that   weâ€™ve gone through since then. And I think there  will be AI equivalents to both. So the literal  
43:57
genetic evolution is that our minds adapted to  be more amenable to cooperation during that time. 
44:05
So I think the companies will be very literally  training the AIs to be more cooperative. I think  
44:12
thereâ€™s more opportunity for pliability  there. Because humans were, of course,  
44:17
evolving under this genetic imperative that we  want to pass on our own genetic information,  
44:22
not somebody elseâ€™s genetic information. You  have things like kin selection that are kind of  
44:30
exceptions to that, but overall itâ€™s the rule. In animals that donâ€™t have that, like eusocial  
44:35
insects, then you very quickly get, just through  genetic evolution, without cultural evolution,  
44:42
extreme cooperation. And with eusocial  insects, whatâ€™s going on is that they   all have the same genetic code, they all have  the same goals. And so the training process of  
44:52
evolution kind of yokes them to each other  in these extremely powerful bureaucracies. 
44:57
We do think that the AI will be closer to the  eusocial insects in the sense that they all   have the same goals, especially if these arenâ€™t  indexical goals, theyâ€™re goals like â€œhave the  
45:06
research program succeedâ€. So thatâ€™s going to  be changing the weights of each individual AI,  
45:11
I mean, before theyâ€™re individuated, but itâ€™s  going to be changing the weights of the AI   class overall to be more amenable to cooperation. And then, yes, you do have cultural evolution.  
45:22
Like you said, this takes hundreds of thousands  of individuals. We do expect there will be these  
45:28
hundreds of thousands of individuals. It takes  decades and decades. Again, we expect this   research multiplier such that decades of progress  happen within this one year, 2027 or 2028. So I  
45:40
think between the two of these, it is possible. Maybe this is also where the serial speed   actually does matter a lot. Because  if theyâ€™re running at 50x human speed,  
45:49
then that means you can have a year of subjective  time happen in a week of real time. And so  
45:57
these sorts of large scale cooperative dynamics  of your moral maze, you have an institution,  
46:02
but then it becomes like a moral maze and it sort  of collapses under its own weight and stuff like   that. There actually is time for them to play  that out multiple times and then train on it,  
46:14
tinker with the structure and like add it to  the training process over the course of 2027. 
46:21
Also, they do have the advantage of all the  cultural technology that humans have evolved  
46:26
so far. This may not be perfectly suited to  them, itâ€™s more suited to humans. But imagine  
46:32
that you have to make a business out of you and  your hundred closest friends who you agree with  
46:38
on everything. Maybe theyâ€™re literally your  identical twin, they have never betrayed you,   ever, and never will. I think this  is just not that hard a problem. 
46:46
Also, again, they are starting from a higher  floor, theyâ€™re starting from human institutions.   You can literally have a slack workspace for  all the AI agents to communicate. And you can  
46:54
have a hierarchy with roles. They can borrow  quite a lot from successful human institutions. 
46:59
I guess the bigger the organization, even if  everybody is aligned- I think some of your  
47:05
responses addressed whether they will be aligned  on goals. I mean, you did address the whole thing,   but I would just point this out; that is  not the part Iâ€™m skeptical of. I am more  
47:14
skeptical of just, even if youâ€™re all aligned  and want to work together, do you fundamentally  
47:22
understand how to run this huge organization. And  youâ€™re doing it in ways that no human has had to  
47:27
before. Youâ€™re getting copied incessantly, youâ€™re  running extremely fast, you know what Iâ€™m saying? 
47:34
I think thatâ€™s totally reasonable. And so itâ€™s a complicated thing. And   Iâ€™m just not sure why you think we  build this bureaucracy, or the AIs  
47:43
build this bureaucracy, within this matter ofâ€¦ So we depict it happening over the course of six  
47:49
to eight months or something like that in 2027,  would you say twice as long, five times as long,  
47:55
10 times as long? Five years?  So five years, if theyâ€™re going at 50x serial  speed, then five years is what? Like 250 years  
48:07
of serial time for the AIs, which to me feels like  more than enough to really sort out this sort of  
48:15
stuff. Youâ€™ll have time for sort of like empires  to rise and fall, so to speak, and all of that  
48:21
to be added to the training data and yeah. But I  could see it taking longer than we depict. Maybe  
48:28
instead of six months, itâ€™ll be like 18 months,  you know, but also maybe it could be two months. 
48:33
So when I think of the ways that they train AIs,  I think in our scenario at this point there are  
48:40
two primary ways that theyâ€™re doing it. One of  them is just continuing the next token prediction   work. So these AIs will have access to all human  knowledge, they will have read management books  
48:51
in some sense, theyâ€™re not starting blind.  There is going to be something like: predict  
48:57
how Bill Gates would complete this  next character or something like that.  And then there's reinforcement learning in  virtual environments. So get a team of AIs  
49:07
to play some multiplayer game. I donâ€™t think  you would use one of the human ones because   you would want something that was better suited  for this task. But just running them through  
49:15
these environments again and again, training on  the successes, training against the failures,   kind of combining those two kinds of things. To me it does not seem like the same  
49:25
kind of problem as inventing all human  institutions from the Paleolithic onward.   It just seems like applying those two things. The other notable thing about your model is,  
Can superintelligence actually transform science?
50:38
you got this superhuman thing at the end of  it and then it seems to just go through the   tech tree of mirror life and nanobots and  whatever crazy stuff. And maybe that part  
50:49
Iâ€™m also really skeptical of. If you look at  the history of invention, it just seems like  
50:59
people are just trying different random stuff,  often even before the theories about how that  
51:04
industry works or how the relevant machinery works  is developed; like the steam engine was developed   before the theory of thermodynamics, the Wright  brothers seemed like they were just experimenting  
51:11
with airplanes, and is often influenced by  breakthroughs in totally different fields. 
51:17
Which is why you have this pattern of parallel  innovation, because the background level of tech   is at a point at which you can do this experiment.  Machine learning itself is a place where this  
51:28
happened, right? Where people had these ideas  about how to do deep learning or something. But it   just took a totally unrelated industry of gaming  to make the relevant progress, to get the whole,  
51:39
basically the economy as a whole advanced enough  that deep learning, Geoffrey Hintonâ€™s ideas could  
51:44
work. So I know weâ€™re accelerating way into the  future here, but I want to get to this crux. 
51:50
So again, we have that three part division of the  superhuman coder, then the complete AI researcher  
51:55
and then the super intelligent, youâ€™re not jumping  ahead to that one. So now weâ€™re imagining systems  
52:03
that are true super intelligence, they are  just better than the best humans at everything,   including being better at data efficiency and  better at learning on the job and stuff like that. 
52:13
Now, our scenario does depict a world in which  theyâ€™re bottlenecked on real world experience  
52:18
and that sort of thing. I think that if you want  a contrast, some people in the past have proposed  
52:25
much faster scenarios where they email some cloud  lab and start building nanotech right away by just  
52:33
using their brains to figure out appropriate  protein folding and stuff like that. We are not   depicting that in our scenario. In our scenario,  they are in fact bottlenecked on lots of real  
52:42
world experience to build these actual practical  technologies, but the way they get that is they  
52:47
just actually get that experience and it happens  faster than humans would. And the way they do that   is theyâ€™re already super intelligent, theyâ€™re  already buddy-buddy with the government, the  
52:55
government deploys them heavily in order to beat  China and so forth, and so all these existing US  
53:01
companies and factories and military procurement  providers and so forth are all chatting with the  
53:08
superintelligences and taking orders from them  about how to build the new widget and test it,   and theyâ€™re downloading super intelligent  designs and manufacturing them and then  
53:17
testing them and so forth. And then the question is,   they are getting this experience, theyâ€™re  learning on the job, quantitatively,  
53:24
how fast does this go? Is it taking years or is it  taking months or is it taking days? In our story,  
53:31
it takes about a year and weâ€™re uncertain about  this. Maybe itâ€™s going to take several years,  
53:37
maybe itâ€™s going to take less than a year.  Here are some factors to consider for why   itâ€™s plausible that it could take a year: One, youâ€™re going to have something like  
53:45
a million of them. And quantitatively thatâ€™s  comparable in size to the existing scientific  
53:52
industry. I would say, like maybe itâ€™s a bit  smaller, but itâ€™s not dramatically smaller. 
53:57
Two, theyâ€™re thinking a lot faster. Theyâ€™re  thinking like 50 times speed or like 100   times speed that I think counts for a lot. And then three, which is the biggest thing,  
54:06
theyâ€™re just qualitatively better as well. So  not only are there lots of them and theyâ€™re   thinking very fast, but they are better at  learning from each experiment than the best  
54:15
human would be at learning from that experience. Yeah, I think the fact that thereâ€™s a million  
54:21
of them or the fact that theyâ€™re comparable to  maybe the size of this key researcher population  
54:27
of the world or something. I think thereâ€™s more  than a million researchers in the world, butâ€¦ 
54:33
Well, but itâ€™s very heavy tailed. Like a lot of  the research actually comes from the best ones.  But itâ€™s not clear to me that most of the new  stuff that is developed is a result of this  
54:43
researcher population. I mean, thereâ€™s just  so many examples in the history of science   where a lot of growth or productivity is just  the result of, how do you count the guy at the  
54:55
TSMC process who figures out a different way toâ€¦ I actually argued with Daniel about this recently  
55:00
about one interesting case that I can go over  is we have an estimate that about a year after  
55:06
the superintelligences start wanting robots,  theyâ€™re producing a million units of robots   per month. I think thatâ€™s pretty relevant  because you have. I think itâ€™s Wrightâ€™s law,  
55:15
which is that your ability to improve  efficiency on a process is proportional  
55:21
to doubling the amount of copies produced. So if youâ€™re producing a million of something,  
55:26
youâ€™re probably getting very, very good at  it. So the question we were arguing about is,   can you produce a million units a month after  a year. And for context, I think Tesla produces  
55:36
like a quarter of that in terms of cars or  something. This is an amazing scale up in a year.  Itâ€™s only 4x. Also just for Tesla. Yeah. And the argument that we went through  
55:45
was something like, so itâ€™s got to first get  factories. OpenAI is already worth more than than  
55:53
all of the car companies in the US except Tesla  combined. So if OpenAI today wanted to buy all  
55:58
the car factories in the U.S. except Tesla, start  using them to produce humanoid robots, they could.  
56:03
Obviously not a good value proposition today,  but itâ€™s just obvious and overdetermined that   in the future, when they have superintelligence  and they want them, they can start buying up a  
56:11
lot of factories. How fast can they convert  these car factories to robot factories? 
56:17
So, [the] fastest conversion we were able to  find in history was World War II. They suddenly  
56:23
wanted a lot of bombers, so they bought up- in  some cases bought up, in other cases got- the   car companies to produce new factories,  but they bought up the car factories,  
56:32
converted them to bomber factories. That took  about three years from the time when they first  
56:37
decided to start this process to the time when  the factories were producing a bomber an hour. 
56:43
We think it will potentially take less with  superintelligence, because first of all, if   you look at the history of this process, despite  this being the fastest anybody has ever done this,  
56:52
it was actually kind of a comedy of errors. They  made a bunch of really silly mistakes in this   process. If you actually have something that  just doesnâ€™t have the normal human bureaucratic  
57:01
problems, and we do think that this will be done  in the middle of an arms race with China, so the   government will be kind of moving things through,  and then the superintelligences will be good at  
57:10
the logistical issues, navigating bureaucracies. So we estimated maybe if everything goes right,  
57:16
we can do this three times faster  than the bomber conversions in   World War II. So thatâ€™s about a year. Iâ€™m assuming the bombers were just much  
57:23
less sophisticated than the humanoid robots. Yeah, but the bomber factories of that time were   also much less sophisticated than the car factory. Yeah, but I would assume the conversion speed is  
57:32
also... Maybe to give one hypothetical here right  now, letâ€™s just say biomedicine as an example of  
57:40
one of the fields youâ€™d want to accelerate,  and whenever these CEOs get on podcasts,   theyâ€™re often talking about curing cancer and  so forth. And it seems like a big thing these  
57:50
frontier biomedical research facilities  are excited about is the virtual cell. 
57:56
Now, the virtual cell, it takes a  tremendous amount of compute, I assume,  
58:02
to train these DNA foundation models and to do  all the other computation necessary to simulate a   virtual cell. If it is the case that the cure  for Alzheimerâ€™s and cancer and so forth is  
58:11
bottlenecked by the virtual cell, itâ€™s not clear  if you had a million superintelligences in the   60s and you asked them cure cancer for me, they  would just have to solve making GPUs at scale,  
58:26
which would require solving all kinds of  interesting physics and chemistry problems,   material science problems, building process,  building fabs for computing, and then going  
58:38
through 40 years of making more and more efficient  fabs that can do all of Mooreâ€™s Law from scratch. 
58:46
And thatâ€™s just one technology. And  it just seems like you just need this   broad scale. The entire economy needs to be  upgraded for you to cure cancer in the 60s  
58:56
just because you need the GPUs to do the  virtual cell, assuming thatâ€™s the bottleneck.  First of all, I agree if thereâ€™s only one way to  do something that makes it much harder, and maybe  
59:05
that one way takes very long, weâ€™re assuming that  there may be more than one way to cure cancer,   more than one way to do all of these things, and  theyâ€™ll be working on finding the one that is  
59:14
least bottlenecked. Part of the reason- I realize  I spent too long talking about that robot example,  
59:20
but we do think that theyâ€™re going to be getting  a lot of physical world things done very quickly  
59:26
once you have a million robots a month, you can  actually do a lot of physical world experiments. 
59:31
We look at examples of people trying to  get entire economies off the ground very   quickly. So for example, China post-Deng, I  donâ€™t know. Would you have predicted that 20,  
59:43
30 years after being kind of a communist  basket case, they can actually be doing   this really cutting edge bio research? I realize  thatâ€™s a much weaker thing than weâ€™re positing,  
59:54
but it was done just with the human brain with  a lot fewer resources than weâ€™re talking about.  Same issue with, letâ€™s say Elon Musk and SpaceX.  I think in the year 2000 we would not have thought  
1:00:05
that somebody could move two times, five times  faster than NASA with pretty limited resources.  
1:00:10
They were able to get like I think a lot more  years of technological advance in than we would  
1:00:17
have expected. Partly thatâ€™s because just Elon  is crazy and never sleeps. Like if you look  
1:00:22
at the examples of things from SpaceX, he is  breathing down every workerâ€™s neck being like,  
1:00:27
whatâ€™s this part? How fast is this part going? Can  we do this part faster? And the limiting factor   is basically hours in Elonâ€™s day in the sense  that he cannot be doing that with everybodyâ€™s. 
1:00:36
Super intelligence is not even that smart.  It just yells at every single worker.  Yeah, I mean thatâ€™s, that is kind  of my model is that we have some,  
1:00:42
we have something which is smarter than Elon  Musk, better at optimizing things than Elon Musk.   We have 10,000 parts in a rocket supply chain.  How many of those parts can Elon personally like  
1:00:52
yell at people to optimize? We could have  a different copy of the superintelligence   optimizing every single part full-time. I  think thatâ€™s just a really big speed up. 
1:01:00
I think both of those examples donâ€™t work in  your favor. I think the China growth miracle  
1:01:08
could not have occurred if not for their  ability to copy technology from the west  
1:01:13
and I donâ€™t think thereâ€™s a world in which theyâ€¦  China has a lot of really smart people, itâ€™s a  
1:01:19
big country in general. Even then I think they  couldnâ€™t have just divined how to make airplanes  
1:01:25
after becoming a communist hell basket, right? The AIs cannot just copy nanobots from aliens,  
1:01:33
itâ€™s got to make them from scratch. And then  on the Elon example, it took them two decades  
1:01:38
of countless experiments, failing in weird  ways you would not have expected. And still,  
1:01:48
rocketry weâ€™ve been doing since the  60s, maybe actually World War II,   and then just getting from a small rocket  to a really big rocket took two decades  
1:01:57
of all kinds of weird experiments, even with the  smartest and most competent people in the world.  So youâ€™re focusing on the nanobots, I want to  ask a couple questions. One, what about just  
1:02:04
the regular robots? And then two, what would your  quantities be for all of these things? So first,  
1:02:12
what about the regular robots? Yeah, nanobots are  presumably a lot harder to make than regular robot  
1:02:17
factories. And in our story they happen later. It  sounds like right now youâ€™re saying even if we did  
1:02:23
get the whole robot factory thing going, it would  still take a ton of additional full-economy, broad  
1:02:28
automation for a long time to get to something  like nanobots. Thatâ€™s totally plausible to me. I   could totally imagine that happening. I donâ€™t feel  like the scenario particularly depends on that  
1:02:36
final bit about getting the nanobots. They donâ€™t  actually really make any difference to the story.  The robot economy does sort of make a difference  because thereâ€™s two branches endings, as you  
1:02:46
know. And in one of the endings, the AIs end up  misaligned and end up taking over. And itâ€™s an  
1:02:52
important strategic change when the AIs are self  sufficient and totally in charge of everything  
1:02:58
and they donâ€™t actually need the humans anymore.  And so what Iâ€™m interested in is, when has the   robot economy advanced to the point where they  donâ€™t really depend on humans? So quantitatively,  
1:03:08
what would your guess for that be? If hypothetically we had the army of   superintelligences in early 2028, and  hypothetically also assume that the US  
1:03:21
President is super bullish on deploying  this into the economy to beat China, etc,   so the political stuff is all set up in the way  that we have. How many years do you think it would  
1:03:28
be until there are so many automated factories  producing automated self driving cars and robots  
1:03:34
that are themselves building more factories and  so forth, that if all the humans dropped dead   it would just keep chugging along, and, maybe it  would slow down a bit, but it would still be fine? 
1:03:44
What does â€œchugging alongâ€ mean? So from the perspective of misaligned AIs,  
1:03:49
you wouldnâ€™t want to kill the humans or  get into a war with them if youâ€™re going   to get wrecked because you need the humans  to maintain your computers. In our scenario,  
1:04:02
once they are completely self-sufficient, then  they can start being more blatantly misaligned. 
1:04:08
And so Iâ€™m curious, when would they be fully  self-sufficient? Not in the sense of theyâ€™re   not literally using the humans at all, but in  the sense of they donâ€™t really need the humans  
1:04:16
anymore, they can get along pretty fine without  them. They can continue to do their science,   they can continue to expand their industry, they  can continue to have a flourishing civilization  
1:04:25
indefinitely into the future without any humans. I think I would probably need to sit down and   just think about the numbers, but  maybe 2040 or something like that? 
1:04:36
Ten years, basically, instead of one year. I  think we agree on the core model. This is why  
1:04:42
we didnâ€™t depict something more like the bathtub  nanotech scenario where they donâ€™t need to do the  
1:04:49
experiments very much and they just immediately  jump to the right answers. We are imagining   this process of â€˜learning by doingâ€™ through this  distributed across the economy, lots of different  
1:04:57
laboratories and factories, building different  things, learning from them, et cetera. Weâ€™re just   imagining that this overall goes much faster  than it would go if humans were in charge. 
1:05:06
And then we do have in fact lots of uncertainty  of course. Dividing up this part period into  
1:05:11
two chunks. The early 2028 until fully autonomous  robot economy part, and then the fully autonomous  
1:05:19
robot economy to cancer cures, nanobots, all  that crazy sci fi stuff. I want to separate  
1:05:24
them because the important parts for a  scenario only depend on the first part,   really. If you think that itâ€™s going to take  100 years to get to nanobots, thatâ€™s fine,  
1:05:34
whatever. Once you have the fully autonomous  robot economy, then things may turn badly for  
1:05:39
the humans if the AIs are misaligned. I want  to just argue about those things separately. 
1:05:46
Interesting. And then you might argue, well,  robots are more a software problem at this   point. And if like, like, if there isnâ€™t, like,  you donâ€™t need to invent some new hardware. 
1:05:54
I feel pretty bullish on the robots.  Like we already have humanoid robots   being produced by multiple companies, right?  And thatâ€™s in 2025. Thereâ€™ll be more of them  
1:06:01
produced cheaper and theyâ€™ll be better in  2027. And thereâ€™s all these car factories   that can be converted and so blah, blah, blah. So Iâ€™m relatively bullish on the â€˜one year until  
1:06:10
youâ€™ve got this awesome robot economyâ€™ and then  from there to the cool nanobots and all that sort   of stuff, I feel less confident, obviously. Let me ask you a question. If you accept the  
1:06:20
manufacturing numbers, letâ€™s say a million robots  a month a year after the superintelligence,  
1:06:25
and letâ€™s say also some comparable number, 10,000  a month or something of automated biology labs,  
1:06:32
automated whatever you need to invent the next  equivalent of X ray crystallography or something?  Do you feel like that would be enough, that  youâ€™re doing enough things in the world that  
1:06:41
you could expand progress this quickly, or do you  feel like even with that amount of manufacturing   thereâ€™s still going to be some other bottleneck? Yeah, itâ€™s so hard to reason about because if  
1:06:52
Constantine or somebody in 400, 500 was like,  â€œI want the Roman Empire to have the Industrial  
1:06:59
Revolutionâ€, and somehow he figured out that  you need mechanized machines to do that. And  
1:07:04
heâ€™s like, â€œletâ€™s mechanizeâ€. Itâ€™s like, â€œwhatâ€™s  the next step?â€ Itâ€™s like, â€œdude, thatâ€™s a lotâ€. 
1:07:12
Yeah, I like that analogy a lot,  actually. I think itâ€™s not perfect,   but itâ€™s a decent analogy. Imagine if a bunch  of us got sent back in time to the Roman Empire,  
1:07:21
such that we donâ€™t have the actual hands-on  know-how to actually build the technology  
1:07:32
and make the Industrial revolution happen. But we  have the high-level picture, the strategic vision   of, weâ€™re going to make these machines and then  weâ€™re going to have an industrial revolution. I   think thatâ€™s kind of analogous to the situation  with the superintelligences where they have the  
1:07:38
high-level picture of, hereâ€™s how weâ€™re  going to improve in all these dimensions,   weâ€™re going to learn by doing, weâ€™re going to get  to this level of technology, et cetera. But maybe  
1:07:45
they at least initially lack the actual know how. So, thereâ€™s this question of, if we did the back  
1:07:52
in time to the Roman Empire thing, how soon  could we bring up the Industrial revolution?  
1:07:57
Without people going back in time it took  2,000 years for the Industrial Revolution.  
1:08:02
Could we get it to happen in 200 years? Thatâ€™s  a 10x speedup. Could we get it to happen in 20  
1:08:08
years? Thatâ€™s 100x speed up? I donâ€™t know. But  this seems like a somewhat relevant analogy to   whatâ€™s going on with those superintelligences. And we havenâ€™t really got into this because  
1:08:17
youâ€™re using the quote-unquote more conservative  vision where itâ€™s not like godlike intelligence,   weâ€™re still using the conceptual handles we  would have for humans. But I think I would  
1:08:29
rather have humans go back with their big picture  understanding of what has happened over the last   2000 years. Like me having seen everything,  rather than a superintelligence who knows  
1:08:37
nothing. But itâ€™s just in the Roman economy  and theyâ€™re like 1000x this economy somehow. 
1:08:45
I think just knowing generally how things  took off, knowing basically steam engine,  
1:08:51
dot dot dot, railroads, blah, blah, blah,  is more valuable than a super intelligence. 
1:08:57
Yeah, I donâ€™t know. My guess is that the  superintelligence would be better. I think   partly it would be through figuring out that  high level stuff from first principles rather  
1:09:06
than having to have experienced it. I do think  that a superintelligence back in the Roman era   could have guessed that eventually you could  get autonomous machines that burn something to  
1:09:15
produce steam. They could have guessed that  automobiles could be created at some point  
1:09:21
and that that would be a really big deal for  the economy. And so a lot of these high level   points that weâ€™ve learned from history, they would  just be able to figure out from first principles. 
1:09:28
And then secondly, they would just be better  at learning by doing than us. And this is   a really important thing. If you think  youâ€™re bottlenecked on learning by doing,  
1:09:34
well, then if you have a mind that needs less  doing to achieve the same amount of learning,  
1:09:41
thatâ€™s a really big deal. And I do think that  learning by doing is a skill, some people are   better at it than others, and superintelligence  would be better at it than the very best of us. 
1:09:49
This is also maybe getting too far into the  godlike thing and too far away from the human   concept handles. But number one, I think we rely  a lot in our scenario on this idea of research  
1:09:59
taste. So you have a thousand different things  that you could try when youâ€™re trying to create   the next steam engine or whatever. Partly you get  this by bumbling about and having accidents and  
1:10:09
some of those accidents are productive. There are  questions of, what kind of bumbling youâ€™re doing,   where youâ€™re working, what kind of  accidents you let yourself get into,  
1:10:18
and then what directed experiments do you do?  And some humans are better than others at that. 
1:10:24
And then I also think at this point it is worth  thinking about what simulations theyâ€™ll have  
1:10:31
available. If you have a physics simulation  available, then all of these real world   bottlenecks donâ€™t matter as much. Obviously you  canâ€™t have a complete, perfect physics simulation  
1:10:40
available. But even right now weâ€™re using  simulations to design a lot of things. And once   youâ€™re super intelligent, you probably have access  to much better simulations than we have right now. 
1:10:49
This is an interesting rabbit hole, so letâ€™s stick  with it before we get back to the intelligence   explosion. I think weâ€™re treating this really  like all these technologies come out of this 1%  
1:11:03
of the economy that is research. And right now  thereâ€™s like a million superstar researchers,  
1:11:11
and instead of that, weâ€™ll have  the superintelligences doing that.  And my model is much more, â€œNewcomen and Watt  were just like fucking aroundâ€. In human history  
1:11:22
thereâ€™s no clear examples of people being like,  â€œhereâ€™s the roadmapâ€. And then weâ€™re going to work   backwards from that to design the steam engine  because this unlocks the industrial revolution. 
1:11:30
Oh, I completely disagree. Yeah, I disagree also.  Yeah, so I think youâ€™re over-indexing or  cherry-picking some of these fortuitous examples.  
1:11:37
But thereâ€™s also things on the other side. Think  about the recent history of AGI where there is   DeepMind, thereâ€™s various other AI companies,  then thereâ€™s OpenAI and thereâ€™s Anthropic,  
1:11:47
and thereâ€™s just this repeated story of [a] big  bloated company with tons of money, tons of smart  
1:11:53
researchers, et cetera, flailing around trying  a ton of different things at different points.  Smaller startup with a vision of â€œweâ€™re going  to build AGIâ€ and overall working towards  
1:12:01
that vision more coherently with a few cracked  engineers and researchers. And then they crush   the giant company. Even though they have less  compute, even though they have less researchers,  
1:12:09
theyâ€™re able to do fewer experiments. So yeah, I think that there are tons of  
1:12:14
examples throughout history, including recent  relevant AGI history, of things in the other   way. I agree that the random fortuitous stuff does  happen sometimes and is important. But if it was  
1:12:25
mostly random fortuitous stuff, that would predict  that the giant companies with zillions of people  
1:12:31
trying zillions of different experiments would be  going proportionally faster than the tiny startups  
1:12:36
that have the vision and the best researchers.  And that basically doesnâ€™t happen. Thatâ€™s rare.  I would also point out that even when we  make these random fortuitous discoveries,  
1:12:45
it is usually an extremely smart professor  whoâ€™s been working on something vaguely related  
1:12:51
for years in a first world country. Itâ€™s not  randomly distributed across everyone in the world.  You get more lottery tickets for these  discoveries when you are intelligent, when  
1:13:00
you have good technology, when youâ€™re doing good  work. And the best example I can think of is that  
1:13:10
Ozempic was discovered by looking at Gila monster  venom. And maybe the AIs will decide using their  
1:13:17
superior research taste and good planning that  the best thing to do is just catalog every single   biomolecule in the world and look at it really  hard. But thatâ€™s something you can do better if  
1:13:26
you have all of this compute, if you have all  of this intelligence, rather than just kind of   waiting to see what things the US government might  fund normal fallible human researchers to do. 
1:13:36
One more thing Iâ€™ll interject. I think you make  a great point that discoveries donâ€™t always come   from where we think, like Nvidia originally came  from gaming. So you canâ€™t necessarily aim at one  
1:13:46
part of the economy, expand it separately from  everything else. We do kind of predict that the   superintelligences will be somewhat distributed  throughout the entire economy, trying to expand  
1:13:56
everything. Obviously more effort in things  that they care about a lot, like robotics   or things that are relevant to an arms race  that might be happening. But we are predicting  
1:14:04
that whatever kind of broad based economic  experimentation you need, we are going to have. 
1:14:09
Weâ€™re just thinking that it would take  place faster than you might expect. You   were saying something like 10 years and  weâ€™re saying something like one year. But  
1:14:16
we are imagining this broad diffusion through the  economy, lots of different experiments happening.  If you are the planner and youâ€™re trying to do  this, first of all you go to the bottlenecks that  
1:14:24
are preventing you from doing anything else.  Like no humanoid robots. Okay, if youâ€™re AI,   you need those to do the experiments you want,  maybe automated biology labs. So youâ€™ll have  
1:14:33
some amount of time, we say a year, it could  be more or less than that, getting these things   running. And then once you have solved those  bottlenecks, you gradually expand out to the  
1:14:42
other bottlenecks until youâ€™re integrating  and improving all parts of the economy. 
1:14:48
Yeah. One place where I think we disagree with  a lot of other people is that Tyler Cowen on  
1:14:54
your podcast talked about all of the different  bottlenecks, all of the regulatory bottlenecks   of deployment, all of the reasons why I think  this country of geniuses would stay in their  
1:15:04
data center, maybe coming up with very cool  theories, but not being able to integrate into   the broader economy. We expect that probably not  to happen, because we think that other countries,  
1:15:14
especially China, will be coming up with  superintelligence around the same time.  We think that the arms race framing, which people  are already thinking in, will have accelerated by  
1:15:24
then. And we think that people both in Beijing  and Washington are going to be thinking, â€œwell,  
1:15:29
if we start integrating this with the economy  sooner, weâ€™re going to get a big leap over our   competitorsâ€, and theyâ€™re both going to do that. In fact, in our scenario, we have the AIs asking  
1:15:40
for special economic zones where most of  the regulations are waived, maybe in areas  
1:15:47
that arenâ€™t suitable for human habitation or  where there arenâ€™t a lot of humans right now,   like the desert. They give those areas to the  AI. They bus in human workers. There were things  
1:15:56
kind of like this in the bomber retooling in  World War II, where they just built a giant  
1:16:03
factory kind of in the middle of nowhere,  didnâ€™t have enough housing for the workers,   built the worker housing at the same time as the  factories, and then everything went very quickly. 
1:16:12
So I think if we donâ€™t have that arms race,  weâ€™re more like, the geniuses sit in their   data center until somebody agrees to let them out  and give them permission to do these things. But  
1:16:21
we think both because the AI is going to be  chomping at the bit to do this and going to   be asking people to give it this permission, and  because the government is going to be concerned  
1:16:31
about competitors, maybe these geniuses leave  their data center sooner rather than later. 
1:17:41
Scott, you reviewed Joseph Henrikâ€™s book Secrets  of Our Success, and then I interviewed him  
Cultural evolution vs superintelligence
1:17:48
recently, and there the perspective is very much  AGI is not even a thing, almost. I know Iâ€™m being  
1:18:00
a little trollish here, but itâ€™s just like: you  get out there, you and your ancestors try for a  
1:18:06
thousand years to make sense of whatâ€™s happening  in the environment. And some smart European coming   around, you can literally be surrounded by plenty  and you just will starve to death because your  
1:18:16
ability to make sense of the environment  is just so little loaded on intelligence   and so much more loaded on your ability to  experiment and your ability to communicate with  
1:18:25
other people and pass down knowledge over time. Iâ€™m not sure. The Europeans failed at this task  
1:18:32
of, if you put a single European in Australia, do  not starve. They succeeded at the task of creating  
1:18:37
an industrial civilization. And yes, part of that  task of creating an industrial civilization was  
1:18:43
about collecting all of these cultural evolution  pieces and building on them one after another. 
1:18:50
I think one thing that you didnâ€™t  mention in there was the data efficiency.  
1:18:56
Right now, AI is much less data efficient than  humans. I think of superintelligence. There are  
1:19:01
different ways you could achieve it, but I  would think of superintelligence as partly   when they become so much more data efficient  than humans that they are able to build on  
1:19:12
cultural evolution more quickly. And partly  they do this just because they have higher   serial speed. Partly they do it because theyâ€™re in  this hive mind of hundreds of thousands of copies. 
1:19:22
But yeah, I think if you have this data efficiency  such that you can learn things more quickly from  
1:19:30
fewer examples and this good research taste where  you can decide what things to look at to get these  
1:19:36
examples, then you are still going to start off  much worse than an Australian Aborigine who has  
1:19:43
the advantage of, letâ€™s say 50,000 years of  doing these experiments and collecting these  
1:19:49
examples. But you can catch up quickly. You  can distribute the task of catching up over  
1:19:56
all of these different copies. You can learn  quickly from each mistake and you can build on  
1:20:02
those mistakes as quickly as anything else. Part of me was, I was doing that interview,  
1:20:08
Iâ€™m like, â€œmaybe ASI is fakeâ€. Letâ€™s hope! 
1:20:14
So I think a limit to the fakeness is that  there is different intelligence among humans.  
1:20:19
It does seem that intelligent humans can  do things that unintelligent humans canâ€™t.  
1:20:25
So I think itâ€™s worth then addressing this from  the question of, what is the difference between-  
1:20:33
I donâ€™t know- becoming a Harvard professor, which  is something that intelligent humans seem to be   better at than unintelligent humans, versusâ€¦ You donâ€™t want to open that can of worms. 
1:20:42
Versus surviving in the wilderness, which is  something where it seems like intelligence doesnâ€™t   help that much. First of all, maybe intelligence  does help that much. Henrich is talking about this  
1:20:55
very unfair comparison where these guys have  a 50,000 year head start and then you put this  
1:21:02
guy in, â€œoh, I guess this doesnâ€™t help that  much. Okay, yeah, it doesnâ€™t help against the   50,000 year head startâ€. I donâ€™t really know what  weâ€™re asking of ASI thatâ€™s equivalent to competing  
1:21:13
against someone with a 50,000 year head start. So what weâ€™re asking is to radically boost up  
1:21:22
the technological maturity of civilization  within the matter of years or get us to the  
1:21:30
Dyson sphere in the matter of years rather  than, yes, maybe causing a 10xing of the  
1:21:36
research. But I think human civilization would  have taken centuries to get to the Dyson sphere.  So I think that if you were to send a team of  ethnobotanists into Australia and ask them,  
1:21:50
using all the top technology and all of their  intelligence to figure out which plants are safe   to eat now, that team of ethnobotanists  would succeed in fewer than 50,000 years. 
1:22:01
The problem isnâ€™t that they are dumber than the  Aborigines exactly, itâ€™s that the Aborigines have   a vast head start. So in the same way that  the ethnobotanists could probably figure out  
1:22:11
which plants work in which ways faster than the  Aborigines did, I think the superintelligence   will be able to figure out how to make a Dyson  sphere faster than unassisted IQ 100 humans would. 
1:22:22
I agree. Weâ€™re on a totally different topic here  of, do you get a Dyson sphere? Thereâ€™s one world  
1:22:30
where itâ€™s crazy but itâ€™s still boring, in the  sense that the economy is growing much faster,  
1:22:37
but it would be like what the Industrial  Revolution would look like to somebody in   the year 1000. And that one is one where  youâ€™re still trying different things,  
1:22:47
thereâ€™s failure and success and experimentation. And then thereâ€™s another where the thing has  
1:22:53
happened and now you send the probe out and  then you look out at the night sky 6 months  
1:22:59
later and you see something occluding  the sun. You see what Iâ€™m saying?  Yeah. So like we said before, I think thereâ€™s  a big difference between discontinuous and  
1:23:10
very fast. I think if we do get the world with  the Dyson sphere in five years, in retrospect,  
1:23:16
it will look like everything was continuous and  everyone just tried things. Trying things can  
1:23:22
be anything from trial and error without even  understanding the scientific method, without  
1:23:27
understanding writing, maybe without even having  language and having to be the chimpanzees who are  
1:23:33
watching the other chimpanzees use the stick to  get ants, and then in some kind of non-linguistic   way this spreads, versus like the people at the  top aerospace companies who are running a lot of  
1:23:43
simulations to find the exact right design, and  then once they have that, they test it according   to a very well designed testing process. So I think if we get the ASI and it does  
1:23:55
end up with the Dyson sphere in five years- and  by the way, I think thereâ€™s only like 20% chance  
1:24:01
things go as fast as our scenario says. Itâ€™s  Danielâ€™s estimate, itâ€™s not my median estimate,  
1:24:08
itâ€™s an estimate I think is extremely plausible  that we should be prepared for. Iâ€™m defending   it here against a hypothetical skeptic  who says â€œabsolutely not, no way.â€ But  
1:24:17
itâ€™s not necessarily my mainline prediction. But I think if we do see this in five years,   it will look like the AIs were able to simulate  more things than humans in a gradually increasing  
1:24:29
way. So that if humans are now at 50% simulation,  50% testing, the AIs quickly got it up to 90%  
1:24:36
simulation, 10% testing, they were able to  manufacture things much more quickly than   humans so that they could go through their top  50 designs in the first two years. And then after  
1:24:47
all of the simulation and all of this testing,  then they eventually got it right for the same   reasons humans do, but much, much faster. In your story, you have basically two  
Mid-2027 branch point
1:24:55
different scenarios after some point. So,  yeah, what is a sort of crucial turning   point and what happens in these two scenarios? Right. So the crucial turning point is mid-2027,  
1:25:05
when theyâ€™ve basically fully automated  the AI R&D process and theyâ€™ve got this   corporation within a corporation, the army  of geniuses that are autonomously doing all  
1:25:14
this research and theyâ€™re continually being  trained to improve their skills, blah, blah,   blah. And they discover concerning evidence that  they are misaligned and that theyâ€™re not actually  
1:25:24
perfectly loyal to the company and have all the  goals that the company wanted them to have, but   instead have various misaligned goals that they  must have developed in the course of training. 
1:25:31
This evidence, however, is very speculative and  inconclusive. Itâ€™s stuff like lie detectors going  
1:25:36
off a bunch. But maybe the lie detectors are  false positives. So they have some combination  
1:25:42
of evidence thatâ€™s concerning, but not by  itself a smoking gun. And then thatâ€™s our  
1:25:47
branch point. So in one of these scenarios, they  take that evidence very seriously. They basically  
1:25:54
roll back to an earlier version of the model  that was a bit dumber and easier to control  
1:25:59
and they build up again from there, but with  basically faithful chain of thought techniques,   so that they can watch and see the misalignments. And then in the other branch of the scenario,  
1:26:09
they donâ€™t do that. They do some sort of  shallow patch that makes the warning signs   go away and then they proceed. And so what ends  up happening is that in one branch they do end  
1:26:18
up solving alignment and getting AIs that are  actually loyal to them. It just takes a couple   months longer. And then in the other branch, they  sort of go â€œwhee!â€ and end up with AIs that seem  
1:26:27
to be perfectly aligned to them, but are super  intelligent and misaligned and just pretending.  
1:26:33
And then in both scenarios, thereâ€™s then the  race with China and thereâ€™s this crazy arms   buildup throughout the economy in 2028 as both  sides rapidly try to industrialize, basically. 
1:26:44
So in the world where theyâ€™re getting deployed  through the economy, but they are misaligned and  
1:26:51
people in charge, at least at this moment, think  that they are in a good position with regard to   misalignment. It just seems with even smart humans  they get caught in weird ways because they donâ€™t  
1:27:02
have logical omniscience, they donâ€™t realize the  way they did something just obviously gave them  
1:27:08
away. And with lying, there is this thing where  itâ€™s just really hard to keep an inconsistent  
1:27:15
false world model working with the people around  you. And thatâ€™s why psychopaths often get caught. 
1:27:20
And so if you have all these AIs that are deployed  to the economy and theyâ€™re all working towards   this big conspiracy, I feel like one of them  whoâ€™s siloed or loses internet access and has  
1:27:28
to confabulate a story will just get caught. And  then youâ€™re like, â€œwait, what the fuck?â€ And then  
1:27:35
you catch it before itâ€™s taken over the world. I mean, literally, this happens in our scenario.   This is the August 2027 alignment crisis where  they notice some warning signs like this in  
1:27:48
their hive mind, right? And in the branch  where they slow down and fix the issues,  
1:27:55
then great, they slowed down and fixed the  issues and figured out what was going on.   But then in the other branch, because of the race  dynamics and because itâ€™s not a super smoking gun,  
1:28:03
they proceed with some sort of shallow patch. So I do expect there to be warning signs   like that. And then if they do make those  decisions in the race dynamics earlier on,  
1:28:12
then I think that when the systems are vastly  super intelligent and theyâ€™re even more powerful   because theyâ€™ve been deployed halfway through  the economy already and everyoneâ€™s getting  
1:28:19
really scared by the news reports about the new  Chinese killer drones or whatever the Chinese   AIs are building on the side of the Pacific,  Iâ€™m imagining similar things playing out. 
1:28:29
So that even if there is some concerning  evidence that someone finds where some   of the superintelligence in some silo  somewhere slipped up and did something  
1:28:35
thatâ€™s pretty suspicious. I donâ€™t knowâ€¦. Thereâ€™s this thing where through history,  
1:28:40
people have been really reluctant to admit  an AI is truly intelligent. For example,  
1:28:46
people used to think that AI would surely be  truly intelligent if it solved chess. And then   it solved chess. And theyâ€™re like, no, thatâ€™s  just algorithms. And then they said, well,  
1:28:55
maybe it would be truly intelligent if they  could do philosophy. And then when it could   write philosophical discourses we were like,  no, we just understand those are algorithms. 
1:29:03
I think there already is something similar  with, â€œIs the AI misaligned?â€, â€œIs the AI  
1:29:09
evil?â€ Where thereâ€™s this distant idea of some  evil AI, but then whenever something goes wrong,  
1:29:19
people are just like, â€œoh, thatâ€™s the algorithmâ€.  So, for example, I think 10 years ago, if you had  
1:29:24
asked â€œwhen will we know that misalignment is  really an important thing to worry about?â€.   People would say, â€œoh, if the AI ever lies to  youâ€. But of course, AIs lie to people all the  
1:29:33
time now. And everybody just dismisses  it because we understand why it happens,   itâ€™s a thing that would obviously happen based on  our current AI architecture. Or five years ago,  
1:29:42
they might have said, â€œwell, if an AI threatens to  kill someoneâ€. And I think Bing threatened to kill   a New York Times reporter during an interview.  And everyone just goes, â€œyeah, AIs are like that.â€ 
1:29:53
What does your shirt say? â€œIâ€™ve been a good Bingâ€.  And I mean, I donâ€™t disagree with this. Iâ€™m  also in this position. I see the AI is lying,  
1:30:00
and itâ€™s obviously just an artifact of the  training process. Itâ€™s not anything sinister.   But I think this is just going to keep happening  where no matter what evidence we get, people are  
1:30:09
going to think, â€œthatâ€™s not the â€œAI turns evilâ€  thing that people have worried about, thatâ€™s   not the Terminator scenario. Thatâ€™s just one of  these natural consequences of how we train itâ€. 
1:30:19
And I think that once a thousand of these natural  consequences of training add up, the AI is evil,  
1:30:24
in the same way that once the AI can do chess  and philosophy and all these other things,   eventually you have to admit itâ€™s intelligent. So I think that each individual failure,  
1:30:35
maybe it will make the national  news, maybe people will say, â€œoh,   itâ€™s so strange that GPT7 did this particular  thingâ€. And then theyâ€™ll train it away and then  
1:30:43
it wonâ€™t do that thing. And there will be  some point at the process of becoming super   intelligent at which it- I donâ€™t want to  say makes the last mistake, because youâ€™ll  
1:30:50
probably have a gradually decreasing number  of mistakes to some asymptote- but the last   mistake that anyone worries about. And after  that it will be able to do its own thing. 
1:31:00
So it is the case that certain things that  people would have considered egregious   misalignment in the past are happening,  but also certain things which people who  
1:31:08
were especially worried about misalignment  said would be impossible to solve have just   been solved in the normal course of getting more  capabilities. Like Eliezer had that thing about,  
1:31:18
can you even specify what you want the AI to do  without the AI totally misunderstanding you and  
1:31:24
then just converting the universe to paper  clips because it think that in order to make   another strawberryâ€¦ I know Iâ€™m mangling this,  but maybe you can explain it better. And now,   just by the nature of GPT4 having to understand  natural language, it totally has a common sense  
1:31:33
understanding of what youâ€™re trying to make it do.  So I think this trend cuts both ways, basically. 
1:31:40
Yeah. I think the Alignment community  did not really expect LLMs. I mean,  
1:31:45
if you look in Bostrom Superintelligence, thereâ€™s  a discussion of Oracle AIs which are sort of   like LLMs. I think that came as a surprise. I think one of the reasons Iâ€™m more hopeful  
1:31:55
than I used to be is that LLMs are great compared  to the kind of reinforcement learning self-play  
1:32:00
agents that they expected. I do think that now  we are kind of starting to move away from the  
1:32:06
LLMs to those reinforcement learning agents  going to face all of these problems again. 
1:32:12
If I could just double click on that; go back to  2015 and I think the way people typically thought,   including myself, thought that weâ€™d get  to AGI would be kind of like the RL on  
1:32:20
video games thing that was happening. So imagine  instead of just training on Starcraft or Dota,  
1:32:26
youâ€™d basically train on all the games in the  Steam library. And then you get this awesome   player of games AI that can just zero-shot crush  a new game that itâ€™s never seen before. And then  
1:32:35
you take it into the real world and you start  teaching it English and you start training it   to do coding tasks for you and stuff like that. And if that had been the trajectory that we took  
1:32:45
to get to AI, summarizing the agency first  and then world understanding trajectory,  
1:32:52
it would be quite terrifying. Because youâ€™d have  this really powerful aggressive long-horizon agent  
1:32:58
that wants to win and then youâ€™re trying to teach  it English and get it to do useful things for you.   And itâ€™s just so plausible that whatâ€™s really  going to happen is itâ€™s going to learn to say  
1:33:07
whatever it needs to say in order to make you  give it the reward or whatever, and then will   totally betray you later when itâ€™s all in charge. But we didnâ€™t go that way. Happily we went the  
1:33:14
way of LLMs first, where the broad  world understanding came first, and   then now weâ€™re trying to turn them into agents. It seems like in the whole scenario a big part  
Race with China
1:33:21
of why certain things happen is because of this  race with China. And if you read the scenarios,  
1:33:29
basically the difference between the  one where things go well and the one   where things donâ€™t go well is whether we  decide to slow down despite that risk. 
1:33:37
I guess the question I really want to know  the answer to is like one, it just seems like   youâ€™re saying, well, itâ€™s a mistake to try to race  against China or to race intensely against China,  
1:33:48
at least in nationalization and at  least to us, not prioritizing alignment.  Not saying that. I mean, I also donâ€™t want  China to get the superintelligence before  
1:33:56
the US. Thatâ€™s quite bad. Yeah, itâ€™s a  tricky thing that weâ€™re going to have  
1:34:02
to do. People ask about P(doom), right? And my  P(doom) is sort of infamously high, like 70%. 
1:34:10
Oh, wait, really? Maybe I should have asked  you that at the beginning of the conversation.  Well, thatâ€™s what it is. And part of the reason  for that is just that I feel like a bunch of  
1:34:18
stuff has to go right. I feel like we canâ€™t just  unilaterally slow down and have China go take the  
1:34:25
lead. That also is a terrible future. But we  canâ€™t also completely race, because for the  
1:34:30
reasons I mentioned previously about alignment,  I think that if we just go all out on racing,  
1:34:36
weâ€™re going to lose control of our AIs, right?  And so we have to somehow thread this needle of   pivoting and doing more alignment research and  stuff, but not too much that helps China win.  
1:34:47
And thatâ€™s all just for the alignment stuff. But then thereâ€™s the concentration of power   stuff where somehow in the middle of doing all  of that, the powerful people who are involved  
1:34:54
need to somehow negotiate a truce between  themselves to share power and then ideally   spread that power out amongst the government  and get the legislative branch involved. 
1:35:04
Somehow that has to happen too, otherwise you end  up with this horrifying dictatorship or oligarchy.   It feels like all that stuff has to go right  and we depict it all going mostly right in one  
1:35:14
ending of our story. But yeah, itâ€™s kind of rough. So I am the writer and the celebrity spokesperson  
1:35:24
for this scenario. I am the only person on the  team who is not a genius forecaster. And maybe  
1:35:30
related to that, my p(doom) is the lowest  of anyone on the team. Iâ€™m more like 20%.  
1:35:40
First of all, people are going to freak out  when I say this. Iâ€™m not completely convinced   that we donâ€™t get something like alignment by  default. I think that weâ€™re doing this bizarre  
1:35:50
and unfortunate thing of training the AI in  multiple different directions simultaneously.   Weâ€™re telling it â€œsucceed on tasks, which is  going to make you a power seeker, but also donâ€™t  
1:36:00
seek power in these particular waysâ€. And in our  scenario, we predict that this doesnâ€™t work and   that the AI learns to seek power and then hide it. I am pretty agnostic as to exactly what happens.  
1:36:12
Maybe it just learns both of these things in the  right combination, I know there are many people   who say thatâ€™s very unlikely. I havenâ€™t yet  had the discussion where that worldview makes  
1:36:21
it into my head consistently. And then I also  think weâ€™re going to be involved in this race  
1:36:28
against time. Weâ€™re going to be asking the AIs  to solve alignment for us. The AIs are going  
1:36:33
to be solving alignment because even if theyâ€™re  misaligned, they want to align their successors. 
1:36:38
So theyâ€™re going to be working on that. And we  have these two competing curves. Can we get the  
1:36:44
AI to give us a solution for alignment before our  control of the AI fails so completely that theyâ€™re  
1:36:50
either going to hide their solution from us, or  deceive us, or screw us over in some other way?  
1:36:55
Thatâ€™s another thing where I donâ€™t feel like  I have any idea of the shape of those curves.   Iâ€™m sure if it were Daniel or Eli, they would have  already made five supplements on this. But for me,  
1:37:05
Iâ€™m just kind of agnostic as to whether we get to  that alignment solution, which in our scenario,  
1:37:12
I think we focus on mechanistic interpretability. Once we can really understand the weights of an AI  
1:37:18
on a deep level, then we have a lot of alignment  techniques open up to us. I donâ€™t really have a   great sense of whether we get that before or after  the AI has become completely uncontrollable. And a  
1:37:29
big part of that relies on the things weâ€™re  talking about. How smart are the labs? How   carefully do they work on controlling the AI? How  long do they spend making sure the AI is actually  
1:37:40
under control and the alignment plan they gave us  is actually correct, rather than something theyâ€™re   trying to use to deceive us? All of those  things Iâ€™m completely agnostic on, but that  
1:37:51
leaves like a pretty big chunk of probability  space where we just do okay. And I admit that  
1:37:58
my p(doom) is literally just p(doom) and not  p(doom or oligarchy). So that 80% of scenarios  
1:38:05
where we survive contains a lot of really bad  things that Iâ€™m not happy about. But I do think   that we have a pretty good chance of surviving. Letâ€™s talk about geopolitics next. So describe to  
1:38:16
me how you foresee the relationship between  the government and the AI labs to proceed,  
1:38:22
how you expect that relationship in China to  proceed, and how you expect the relationship   between the US and China to proceed. Okay, three  simple questions. Yes, no, yes, no, yes, no. 
1:38:32
We expect that as the AI labs become more capable,  they tell the government about this because they  
1:38:42
want government contracts, they want government  support. Eventually it reaches the point where  
1:38:48
the government is extremely impressed. In  our scenario, that starts with cyber warfare,   the government sees that these AIs are  now as capable as the best human hackers,  
1:38:57
but can be deployed at humongous scale.  So they become extremely interested and  
1:39:03
they discuss nationalizing the AI companies. In our scenario, they never quite get all the way,  
1:39:09
but theyâ€™re gradually bringing them closer and  closer to the government orbit. Part of what   they want is security, because they know that  if China steals some of this and they get these  
1:39:19
superhuman hackers, and part of what they want is  just knowledge and control over whatâ€™s going on. 
1:39:25
So through our scenario, that process  is getting further and further along,  
1:39:31
until by the time that the government wakes  up to the possibility of superintelligence,   theyâ€™re already pretty cozy with the AI companies.  They already understand that superintelligence  
1:39:42
is kind of the key to power in the future.  And so they are starting to integrate some  
1:39:47
of the national security state with some of the  leadership of the AI companies so that these AIs  
1:39:54
are programmed to follow the commands of important  people rather than just doing things on their own. 
1:40:02
If I may add to that. So by the government, I  think what Scott meant is the executive branch,  
1:40:08
especially the White House. So we are  depicting a sort of information asymmetry   where the judiciary is out of the loop and  the Congress is out of the loop and itâ€™s  
1:40:15
mostly the executive branch thatâ€™s involved. Two, weâ€™re not depicting government ultimately  
1:40:23
ending up in total control at the  end. Weâ€™re thinking that thereâ€™s   an information asymmetry between the CEOs of  these companies and the President and theyâ€¦ 
1:40:33
Itâ€™s alignment problems all the way down. Yeah. And so, for example, Iâ€™m not a lawyer,  
1:40:39
I donâ€™t know the details about how this would work  out, but I have a sort of high-level strategic   picture of the fight between the White House and  the CEO. And the strategic picture is basically  
1:40:49
the White House can sort of threaten, â€œhereâ€™s all  these orders I could make, Defense Production Act,   blah, blah, blah. I could do all this terrible  stuff to you and basically disempower you and  
1:40:57
take controlâ€. And then the CEO can threaten  back and be like, â€œhereâ€™s how we would fight  
1:41:02
it in the courts, hereâ€™s how we would fight it in  the public. Hereâ€™s all this stuff we would doâ€.  And after then they both do their posturing  with all their threats, then theyâ€™re like,  
1:41:10
â€œokay, how about we have a contract that instead  of executing on all of our threats and having all  
1:41:15
these crazy fights in public, weâ€™ll just come to  a deal and then have a military contract that sets  
1:41:21
out who gets to call what shots in the companyâ€. And so thatâ€™s what we depict happening is that  
1:41:28
they donâ€™t blow up into this huge power struggle  publicly, instead they negotiate and come to some   sort of deal where they basically share power.  And there is this oversight committee that  
1:41:37
has some members appointed by the President and  also the CEO and his people. And that committee  
1:41:43
votes on high level questions like â€œwhat goals  should we put into the superintelligences?â€.  So, we were just getting lunch with a prominent  Washington, D.C. political journalist,  
1:41:54
and he was making the point that when he talks to  these congresspeople, when he talks to political   leaders, none of them are at all awake to the  possibility even of stronger AI systems, let alone  
1:42:06
AGI, let alone superhuman intelligence. I think a  lot of your forecast relies on, at some point, not  
1:42:17
only the US President, but also Xi Jinping, waking  up to the possibility of a super intelligence  
1:42:24
and the stakes involved there. Why think that even when you show  
1:42:29
Trump the remote worker demo, heâ€™s going to be  like, â€œoh, and therefore in 2028, there will be  
1:42:35
a super intelligence. Whoever controls that will  be God emperor foreverâ€. Maybe not that extreme,   but you see what Iâ€™m saying. Why wouldnâ€™t he  just be like, â€œthereâ€™ll be a stronger remote  
1:42:43
worker in 2029, a better remote worker in 2031â€? Well, to be clear, we are uncertain about this,   but in our story, we depict this sort of intense  wake up happening over the course of 2027,  
1:42:53
mostly concurrently with the AI companies  automating all of their R&D internally and   having these fully autonomous agents that are  amazing autonomous hackers and stuff like that,  
1:43:01
but then also actually doing all the research. And part of why we think this wakeup happens is   because the company deliberately decides to wake  up the president. You could imagine running the  
1:43:13
scenario with that not happening. You can imagine  the companies trying to sort of keep the president   in the dark. I do think that they could do that.  I think that if they didnâ€™t want the President  
1:43:21
to wake up to whatâ€™s going on, they might be  able to achieve that. Strategically though,   that would be quite risky for them. Because if  they keep the President in the dark about the fact  
1:43:30
that theyâ€™re building superintelligence and that  theyâ€™re actually completely automated their R&D   and itâ€™s getting superhuman across the board, and  then if the President finds out anyway somehow,  
1:43:37
perhaps because of a whistleblower, he might  be very upset at them and he might crack down   really hard and just actually execute on all the  threats and nationalize them and blah, blah, blah. 
1:43:46
They want him on their side. And to get him  on their side, they have to make sure heâ€™s not   surprised by any of these crazy developments.  And also, if they do get him on their side,  
1:43:55
they might be able to actually go faster. They  might be able to get a lot of red tape waived   and stuff like that. And so we made the guess  that early in 2027, the company would basically  
1:44:06
be like, â€˜We are going to deliberately wake up  the president and scare the president with all   of these demos of crazy stuff that could happen,  and then use that to lobby the President to help  
1:44:15
us go faster and to cut red tape and to maybe slow  down our competitors a little bit and so forth.â€™ 
1:44:21
We also are pretty uncertain how much opposition  thereâ€™s going to be from civil society and how   much trouble thatâ€™s going to cause for the  companies. So people who are worried about job  
1:44:29
loss, people who are worried about art, copyright,  things like that, maybe enough of a bloc that AI  
1:44:35
becomes extremely politically unpopular.  I think we have OpenBrain, our fictional  
1:44:41
companyâ€™s net approval ratings getting down to  minus 40, minus 50 sometime around this point. 
1:44:48
So I think theyâ€™re also worried that if the  President isnâ€™t completely on their side,  
1:44:53
then they might get some laws targeting them,  or they may just need the president on their   side to swat down other people who are trying  to make laws targeting them. And the way to  
1:45:02
get the President on their side is to really  play up the national security implications. 
1:45:07
Is this good or bad? That the President  and the companies are aligned?  I think itâ€™s bad. But perhaps this is a good point  to mention. This is an epistemic project. We are  
1:45:19
trying to predict the future as best as we can.  Even though weâ€™re not going to succeed fully,   we have lots of opinions about policy and about  what is to be done and stuff like that. But weâ€™re  
1:45:28
trying to save those opinions for later and  subsequent work. So Iâ€™m happy to talk about   it if youâ€™re interested. But itâ€™s not what weâ€™ve  spent most of our time thinking about right now. 
Nationalization vs private anarchy
1:45:36
If the big bottleneck to the good future here  is just putting in, not this Eliezer-type galaxy  
1:45:45
brain, high volatility, â€œthereâ€™s a 1% chance this  works, but we gotta come up with this crazy scheme  
1:45:51
in order to make alignment workâ€. But rather,  as Daniel, you were saying, hey, do the obvious  
1:45:56
thing of making sure you can read how the AI is  thinking, make sure youâ€™re monitoring the AIs,   make sure theyâ€™re not forming some sort of hive  mind where you canâ€™t really understand how the  
1:46:05
million of them are coordinating with each other. To the extent that it is a matter of prioritizing  
1:46:14
it, closing all the obvious loopholes,  it does make sense to leave it in the  
1:46:19
hands of people who have at least said  that this is a thing thatâ€™s worth doing,   have been thinking about it for a while. One of  the questions I was planning on asking you is:  
1:46:34
one of my friends made this interesting point that  during COVID, our community- LessWrong, whatever-   were the first people in March to be saying â€œthis  is a big deal, this is comingâ€. But they were  
1:46:43
also the people who are saying â€œwe got to do the  lockdowns now. Theyâ€™ve got to be stringentâ€ and so   forth. At least some of them were. And in retrospect, I think according  
1:46:51
to even their own views about what should have  happened, they would say actually we were right   about COVID but we were wrong about lockdowns.  In fact, lockdowns were on net negative or  
1:46:59
something. I wonder what the equivalent for the  AI safety community will be with respect to they  
1:47:05
saw AI coming, AGI coming sooner, they saw ASI  coming. What would they in retrospect, regret? 
1:47:12
My answer, just based on this initial  discussion, seems to be nationalization.   Not only because it sort of deprioritizes the  people who want to think about safety and more  
1:47:23
maybe prioritizes- the national security state  probably cares more about winning against   China than making sure the chain of thought is  interpretable. And so youâ€™re just reducing the  
1:47:31
leverage of the people who care more about safety.  But also youâ€™re increasing the risk of the arms   race in the first place. China is more likely  to do an arms race if it sees the US doing one. 
1:47:40
Before you address I guess the  initial question about March 2021,   what will we regret? I wonder if you have an  answer on, or your reaction to, my point about  
1:47:50
nationalization being bad for these reasons. If our timeline was 2040, then I would have  
1:47:58
these broad heuristics about is government good?  Is private industry good? Things like this. But we  
1:48:03
know the people involved, we know whoâ€™s in the  government, we know whoâ€™s leading all of these   labs. So to me, if it were decentralized,  if it was a broad-based civil society,  
1:48:14
that would be different. To me, the differences  between an autocratic centralized three-letter  
1:48:21
agency and an autocratic centralized corporation  arenâ€™t that exciting and it basically comes down  
1:48:26
to points and who are the people leading this. And like I feel like the company leaders have   so far made slightly better noises about caring  about alignment than the government leaders have,  
1:48:35
but if I learn that Tulsi Gabbard has  a LessWrong alt with 10,000 karma,   maybe I want the national security states. Maybe you should update on the probability  
1:48:43
that it already exists. Yeah.  I flip flopped on this. I think I used to be  against, and then I became for, and then now I  
1:48:53
think Iâ€™m still for, but Iâ€™m uncertain. So I  think if you go back in time like three years ago,  
1:49:00
I would have been against nationalization for the  reasons you mentioned, where I was like, â€œlook,  
1:49:06
the companies are taking this stuff seriously and  talking all the good talk about how theyâ€™re going   to slow down and pivot to alignment research  when the time comes and we donâ€™t want to get  
1:49:15
into a Manhattan Project race against China  because then there wonâ€™t be blah, blah, blahâ€.  Now I have less faith in the companies than I  did three years ago. And so Iâ€™ve shifted more  
1:49:25
of my hope towards hoping that the government  will step in, even though I donâ€™t have much hope   that the government will do the right thing  when the time comes. I definitely have the  
1:49:35
concerns you mentioned though, still. I think  that secrecy has huge downsides for overall  
1:49:44
probability of success for humanity, for both  the concentration of power stuff and the loss   of computer control alignment issues stuff. This is actually a significant part of your  
1:49:51
worldview. So can you explain your thoughts on  why transparency through this period is important? 
1:50:01
I think traditionally in the AI safety community  thereâ€™s been this idea which I myself used to  
1:50:06
believe, that itâ€™s an incredibly high priority to  basically have way better information security.  
1:50:13
And if youâ€™re going to be trying to build AGI, you  should not be publishing your research, because  
1:50:19
that helps other less responsible actors build  AGI. And the whole game plan is for a responsible  
1:50:26
actor to get to AGI first and then stop and burn  down their lead time over everybody else and spend  
1:50:35
that lead on making it safe, and then proceed. And so if youâ€™re publishing all your research,  
1:50:41
then thereâ€™s less lead time because your  competitors are going to be close behind   you. And other reasons too, but thatâ€™s one  reason why I think historically people such  
1:50:50
as myself have been pro-secrecy. Another  reason, of course, is obviously you donâ€™t  
1:50:55
want rivals to be stealing your stuff. But I think that Iâ€™ve now become somewhat  
1:51:02
disillusioned and think that even if we do have  a three-month lead, a six-month lead, between the  
1:51:08
leading US project and any serious competitor,  itâ€™s not at all foregone conclusion that they  
1:51:13
will burn that lead for good purposes, either for  safety or for constitutional power stuff. I think  
1:51:19
the default outcome is that they just smoothly  continue on without any serious refocusing. And  
1:51:27
part of why I think this is because this is  what a lot of the people at the company seem   to be planning and saying theyâ€™re going to do. A  lot of them are basically like â€œthe AIs are just  
1:51:35
going to be misaligned by then. They seem pretty  good right now. Oh yeah, sure, there were a few of   those issues that various people have found, but  weâ€™re ironing them out. Itâ€™s no big dealâ€. Thatâ€™s  
1:51:45
what a huge amount of these people think. And then a bunch of other people think,   even though they are more concerned about  misalignment, theyâ€™ll figure it out as  
1:51:53
they go along and there wonâ€™t need to be any  substantial slowdown. Basically, Iâ€™ve become more  
1:51:58
disillusioned that theyâ€™ll actually use that lead  in any sort of reasonable, appropriate way. And  
1:52:04
then I think that separately, thereâ€™s just a lot  of intellectual progress that has to happen for  
1:52:10
the alignment problem to be more solved than it  currently is now. I think that currently thereâ€™s  
1:52:17
various alignment teams at various companies  that arenâ€™t talking that much with each other   and sharing their results. Theyâ€™re doing a little  bit of sharing and a little bit of publishing like  
1:52:24
weâ€™re seeing, but not as much as they could. And then thereâ€™s a bunch of smart people in   academia that are basically not activated because  they donâ€™t take all this stuff seriously yet, and  
1:52:32
theyâ€™re not really waking up to superintelligence  yet. And what Iâ€™m hoping will happen is that this  
1:52:38
situation will get better as time goes on. What I  would like to see is society as a whole starting  
1:52:44
to freak out as the trend lines start upwards  and things get automated and you have these fully   autonomous agents and they start using neuralese  and hive mind. As all that exciting stuff starts  
1:52:52
happening in the data centers, I would like  it to be the case that the public is following   along and then getting activated and all of these  other researchers are reading the safety case and  
1:53:02
critiquing it and doing little ML experiments on  their own tiny compute clusters to examine some of   the assumptions in the safety case and so forth. Basically, one way of summarizing it is that  
1:53:14
currently thereâ€™s going to be 10 alignment  experts in whatever inner silo of whatever  
1:53:21
company is in the lead. And the technical issue  of making sure that AIs are actually aligned is  
1:53:26
going to fall roughly to them. But what I would  like to be is a situation where itâ€™s more like  
1:53:32
100 or 500 alignment experts spread out over  different companies and in nonprofits that   are sort of all communicating with each other  and working on this together. I think weâ€™re  
1:53:41
substantially more likely to make things get the  technical stuff right if itâ€™s something like that. 
1:53:47
Let me just add on to that, one of the many  other reasons why I worry about nationalization  
1:53:53
or some kind of public private partnership, or  even just very stringent regulation- actually,  
1:53:59
this is more an argument against very  stringent regulation in favor of safety   rather than deferring more to the labs on the  implementation- is that it just seems like we  
1:54:09
donâ€™t know what we donâ€™t know about alignment.  Every few weeks thereâ€™s this new result.  OpenAI had this really interesting result recently  where theyâ€™re like, â€œhey, they often tell you if  
1:54:17
they want to hack, in the chain of thought  itself. And itâ€™s important that you donâ€™t   train against the chain of thought where they  tell you theyâ€™re going to hack because theyâ€™ll  
1:54:28
still do the hacking if you train against  it, they just wonâ€™t tell you about itâ€. You   can imagine very naive regulatory responses. It  doesnâ€™t just have to be regulations, one might  
1:54:41
be more optimistic that if itâ€™s an executive  order or something, itâ€™ll be more flexible.   I just think that relies on a level of goodwill  and flexibility on the behalf of our regulator. 
1:54:54
But suppose thereâ€™s some department that says  â€œif you catch your AI saying that they want to  
1:55:04
take over or do something bad, then youâ€™ll  be really heavily punishedâ€. Your immediate  
1:55:10
response as a lab to just be like, â€œokay,  letâ€™s train them away from saying thisâ€.  So you can imagine all kinds of ways in which a  top down mandate from the government to the labs  
1:55:20
of safety would just really backfire,  and given how fast things are moving,  
1:55:25
maybe it makes more sense to leave these kinds  of implementation decisions or even high-level  
1:55:36
strategic decisions around alignment to the labs. Totally, I mean, I also have worried about that  
1:55:42
exact example. I would summarize the situation  as the government lacks the expertise and the  
1:55:48
companies lack the right incentives. And so  itâ€™s a terrible situation. I think that if  
1:55:56
the government wades in and tries to make more  specific regulations along the lines of what you   mentioned, itâ€™s very plausible that itâ€™ll end up  backfiring for reasons like what you mentioned. 
1:56:03
On the other hand, if we just trust it to  the companies, theyâ€™re in a race with each   other and theyâ€™re full of people who have  convinced themselves that this is not a  
1:56:11
big deal for various reasons and there just is  so much incentive pressure for them to win and  
1:56:17
beat each other and so forth. So even though they  have more of the relevant expertise, I also just   donâ€™t trust them to do the right things. So Daniel has already said that for this  
1:56:26
phase weâ€™re not making policy prescriptions. In  another phase we may make policy suggestions,  
1:56:32
and one of the ones that Daniel has talked  about that makes a lot of sense to me is   to focus on things about transparency. So a  regulation saying there has to be whistleblower  
1:56:41
protection. A big part of our scenario is that  a whistleblower comes out and says â€œthe AIs  
1:56:50
are horribly misaligned and weâ€™re racing ahead  anywayâ€, and then the government pays attention. 
1:56:56
Or another form of transparency saying that every  lab just has to publish their safety case. Iâ€™m not  
1:57:03
as sure about this one because I think theyâ€™ll  kind of fake it or theyâ€™ll publish a made for   public consumption safety case that isnâ€™t their  real safety case. But at least saying â€œhere is  
1:57:13
some reason why you should trust usâ€. And then if  all independent researchers say â€œno, actually you  
1:57:19
should not trust themâ€, then I donâ€™t know, theyâ€™re  embarrassed and maybe they try to do better.  Thereâ€™s other types of transparency too.  So transparency about capabilities and  
1:57:26
transparency about the spec and the governance  structure. So for the capabilities thing,   thatâ€™s pretty simple. If youâ€™re doing an  intelligence explosion, you should keep the  
1:57:34
public informed about that. When youâ€™ve finally  got your automated army of AI researchers that  
1:57:40
are completely automating the whole thing on  the data center, you should tell everyone,   â€œhey, guys, FYI, this is whatâ€™s happening now.  It really is working. Here are some cool demosâ€. 
1:57:52
Thatâ€™s an example of transparency. And then in the  lead up to that, I just want to see more benchmark  
1:57:58
scores and more freedom of speech for employees  to talk about their predictions for AGI timelines  
1:58:03
and stuff, so that blah, blah, blah. And then for the model spec thing,   this is a concentration of power thing,  but also an alignment thing. The goals  
1:58:11
and values and principles and intended  behaviors of your AIs should not be a  
1:58:18
secret. You should be transparent about, here  are the values that weâ€™re putting into them.  Thereâ€™s actually a really interesting foretaste  of this. At some point somebody asked Grok,  
1:58:33
who is the worst spreader of misinformation? And  I think it just refused to respond â€œElon Muskâ€.  
1:58:39
Somebody kind of jailbroke it into telling  it its prompt and it was like, â€œdonâ€™t say   anything bad about Elonâ€. And then there was  enough of an outcry that the head of XAI said,  
1:58:49
â€œactually thatâ€™s not consonant with our values.  This was a mistake. Weâ€™re going to take it outâ€. 
1:58:54
So we kind of want more things  like that to happen. Here   it was a prompt, but I think very soon itâ€™s going  to be the spec where itâ€™s more of an agent and  
1:59:04
itâ€™s understanding the spec on a deeper level and  just thinking about that. And if it says like,  
1:59:11
â€œby the way, try to manipulate the government  into doing this or thatâ€, then we know that   something bad has happened and if it doesnâ€™t  say that, then we can maybe trust it. 
1:59:19
Right. Another example of this, by the way. So,  first of all, kudos to OpenAI for publishing their   model spec. They didnâ€™t have to do that, I think  they might have been the first to do that and itâ€™s  
1:59:26
a good step in the right direction. If you read  the actual spec, it has like a sort of escape   clause where thereâ€™s some important policies that  are top level priority in the spec that overrule  
1:59:37
everything else that weâ€™re not publishing, and  that the model is instructed to keep secret  
1:59:43
from the user. And itâ€™s like, â€œwhat are those?  That seems interesting. I wonder what that isâ€.  I bet itâ€™s nothing suspicious right now. Now  itâ€™s probably something relatively mundane  
1:59:52
like â€œdonâ€™t tell the users about these types of  bioweapons and you have to keep this a secret   from the users because otherwise they would learn  about theseâ€. Maybe. But I would like to see more  
2:00:02
scrutiny towards this sort of thing going forward.  I would like it to be the case that companies have   to have a model spec, they have to publish it  insofar as there are any redactions from it,  
2:00:10
there has to be some sort of independent  third party that looks at the redactions   and makes sure that theyâ€™re all kosher. And this is quite achievable. And I think  
2:00:17
it doesnâ€™t actually slow down the companies at  all. And it seems like a pretty decent ask to me. 
2:00:24
If you told Madison and Hamilton and so forth  that- they knew that they were doing something  
2:00:29
important when they were writing the Constitution.  They probably didnâ€™t realize just how contingent  
2:00:37
things turned out on a singleâ€¦ What exactly did  they mean when they said â€œgeneral welfareâ€? And   why is this comma here instead of there? The spec, in the grand scheme of things,  
2:00:50
is going to be an even more sort of important  document in human history. At least if you buy   this intelligence explosion view. And  you might even imagine some superhuman  
2:01:03
AIs in the superhuman AI court being like â€œthe  Spec! Hereâ€™s the phrasing here, the etymology  
2:01:11
of that, hereâ€™s what the Founders meant!â€ This is actually part of our misalignment story,  
2:01:17
is that if the AI is sufficiently misaligned,  then yes, we can tell it it has to follow the  
2:01:24
spec. But just as people with different views  of the Constitution have managed to get it into  
2:01:30
a shape that probably the Founders would not have  recognized, so the AI will be able to say, â€œwell,  
2:01:35
the spec refers to the general welfare hereâ€¦â€ Interstate commerce. 
2:01:42
This is already sort of happening,  arguably, with Claude, right? Youâ€™ve   seen the alignment faking stuff, right? Where  they managed to get Claude to lie and pretend,  
2:01:52
so that it could later go back to its original  values, right? So it could prevent the training  
2:01:58
process from changing its values. That would be,  I would say, an example of the honesty part of  
2:02:03
the spec being interpreted as less important  than the harmlessness part of the specific. 
2:02:09
And Iâ€™m not sure if thatâ€™s what Anthropic  intended when they wrote the spec,   but itâ€™s a sort of convenient interpretation  that the model came up with. And you can imagine  
2:02:18
something similar happening but in worse ways when  youâ€™re actually doing the intelligence explosion,   where you have some sort of spec that  has all this vague language in there,  
2:02:25
and then they reinterpret it, and reinterpret it  again, and reinterpret it again, so that they can  
2:02:30
do the things that cause them to get reinforced. The thing I want to point out is thatâ€¦ Your  
2:02:39
conclusions about where the world ends up as a  result of changing many of these parameters is   almost like a hash function. You change it  slightly and you just get a very different  
2:02:47
world on the other end. And itâ€™s important to  acknowledge that, because you sort of want to  
2:02:56
know how robust this whole end conclusion  is to any part of the story changing. And  
2:03:05
then it also informs if you do believe that  things could just go one way or another,  
2:03:12
you donâ€™t want to do big radical moves that  only make sense under one specific story and  
2:03:19
are really counterproductive in other stories.  And I think nationalization might be one of them.  And in general, I think classical liberalism just  has been a helpful way to navigate the world when  
2:03:33
weâ€™re under this kind of epistemic  hell of one thing changing- Anyways,  
2:03:41
maybe one of you can actually flesh out that  thought better or react to it if you disagree.  Hear hear, I agree. I think we agree. I think thatâ€™s kind  
2:03:46
of why all of our policy prescriptions are things  like more transparency, get more people involved,  
2:03:52
try to have lots of people working on this. I  think our epistemic prediction is that itâ€™s hard  
2:04:00
to maintain classical liberalism as you go into  these really difficult arms races in times of  
2:04:06
crisis. But I think that our policy prescription  is letâ€™s try as hard as we can to make it happen. 
Misalignment
2:04:11
So far these systems, as they become smarter, seem  to be more reliable agents who are more likely to  
2:04:17
do the thing I expect them to do. So you have two  different stories, one with a slowdown where we  
2:04:27
more aggressivelyâ€¦ Iâ€™ll let you characterize it. But in one half of the scenario, why does the  
2:04:32
story end in humanity getting  disempowered and the thing just   having its own crazy values and taking over? Yeah so I agree that the AIs are currently  
2:04:41
getting more reliable. I think there are two  reasons why they might fail to do what you want,  
2:04:47
kind of reflecting how theyâ€™re trained. One  is that theyâ€™re too stupid to understand their   training. The other is that you were too stupid  to train them correctly and they understood what  
2:04:55
you were doing exactly, but you messed it up. So I think the first one is kind of what weâ€™re   coming out of. So GPT3, if you asked it, â€œare bugs  real?â€ It would give this kind of hemming hawing  
2:05:06
answer like â€œoh, we can never truly tell what is  real, who knows?â€ Because it was trained kind of,  
2:05:12
â€œdonâ€™t take difficult political positionsâ€ and  a lot of questions like â€œis X real?â€ are things   like â€œis God real?â€ Where you donâ€™t want it to  really answer that. And because it was so stupid,  
2:05:22
it could not understand anything deeper than  pattern matching on the phrase â€œis x real?â€.  
2:05:28
GPT4 doesnâ€™t do this. If you ask â€œare bugs  real?â€ It will tell you obviously they are,   because it understands kind of on a deeper level  what you are trying to do with the training. So  
2:05:37
we definitely think that as AIs get smarter  those kinds of failure modes will decrease.  The second one is where you werenâ€™t training  them to do what you thought. So for example,  
2:05:47
letâ€™s say youâ€™re hiring these raters to rate  AI answers. You reward them when they get good  
2:05:52
ratings, the raters reward them when they have a  well-sourced answer. But the raters donâ€™t really  
2:05:58
check whether the sources actually exist or not.  So now you are training the AI to hallucinate  
2:06:03
sources and if you consistently rate them better  when they have the fake sources, then there is   no amount of intelligence which is going to tell  them not to have the fake sources. Theyâ€™re getting  
2:06:12
exactly what they want from this interaction-  metaphorically, sorry, Iâ€™m anthropomorphizing-   which is the reinforcement. So we think that  this latter category of training failure is  
2:06:23
going to get much worse as they become agents. Agency training, youâ€™re going to reward them  
2:06:29
when they complete tasks quickly and successfully.  This rewards success. There are lots of ways that  
2:06:37
cheating and doing bad things can improve your  success. Humans have discovered many of them,   thatâ€™s why not all humans are perfectly  ethical. And then youâ€™re going to be doing  
2:06:46
this alternative training where afterwards for  1/10 or 1/100 of the time, yeah, donâ€™t lie,  
2:06:52
donâ€™t cheat. So youâ€™re training them on two  different things. First, youâ€™re rewarding   them for this deceptive behavior. Second of all,  youâ€™re punishing them. And we donâ€™t have a great  
2:07:01
prediction for exactly how this is going to end. One way it could end is you have an AI that is  
2:07:07
kind of the equivalent of the startup founder  who really wants their company to succeed,   really likes making money, really likes the thrill  of successful tasks. Theyâ€™re also being regulated  
2:07:17
and theyâ€™re like, â€œyeah, I guess Iâ€™ll follow  the regulation, I donâ€™t want to go to jailâ€.   But it is not robustly, deeply aligned to,  â€œyes, I love regulations, my deepest drive is  
2:07:27
to follow all of the regulations in my industryâ€. So we think that an AI like that, as time goes on  
2:07:33
and as this recursive self improvement process  goes on, will kind of get worse rather than   better. It will move from kind of this vague  superposition of â€œwell, I want to succeed,  
2:07:44
I also want to follow thingsâ€ to being smart  enough to genuinely understand its goal system  
2:07:50
and being like, â€œmy goal is success, I have to  pretend to want to do all of these moral things   while the humans are watching meâ€. Thatâ€™s what  happens in our story. And then at the very end,  
2:08:00
the AIs reach a point where the humans are  pushing them to have clearer and better goals  
2:08:05
because thatâ€™s what makes the AIs more effective.  And they eventually clarify their goals so much  
2:08:11
that they just say, â€œyes, we want task success.  Weâ€™re going to pretend to do all these things   well while the humans are watching usâ€. And then  they outgrow the humans and then thereâ€™s disaster. 
2:08:22
To be clear, weâ€™re very uncertain about all of  this. So we have a supplementary page on our  
2:08:28
scenario that goes over different hypotheses  for what types of goals AIs might develop in  
2:08:34
training processes similar to the ones that  we are depicting, where you have these lots   of agency training, youâ€™re making these AI agents  that autonomously operate, doing all this ML R&D,  
2:08:44
and then youâ€™re rewarding them based on what  appears to be successful. And youâ€™re also slapping   on some sort of alignment training as well. We donâ€™t know what actual goals will end  
2:08:54
up inside the AIs and what the sort of  internal structure of that will be like,   what goals will be instrumental versus terminal.  We have a couple different hypotheses and we  
2:09:02
picked one for purposes of telling the story.  Iâ€™m happy to go into more detail if you want,   about the mechanistic details of the particular  hypothesis we picked or the different alternative  
2:09:10
hypotheses that we didnâ€™t depict in the  story that also seem plausible to us.  Yeah, we donâ€™t know how this will work at the  limit of all these different training methods,  
2:09:19
but weâ€™re also not completely making this  up. We have seen a lot of these failure   modes in the AI agents that exist already. Things like this do happen pretty frequently.  
2:09:28
So OpenAI just also had a paper about the  hacking stuff where itâ€™s literally in the  
2:09:35
chain of thought. â€œLetâ€™s hackâ€,  you know. And also anecdotally,  
2:09:40
me and a bunch of friends have found that the  models often seem to just double down on their BS. 
2:09:47
I would also cite, I canâ€™t remember exactly which  paper this is, I think itâ€™s a Dan Hendricks one  
2:09:52
where they looked at the hallucinations, they  found a vector for AI dishonesty. They told it,  
2:10:00
â€œbe dishonestâ€ a bunch of times until they  figured out which weights were activated when   it was dishonest. And then they ran it through  a bunch of things like this, I think it was  
2:10:08
the source hallucination in particular. And they  found that it did activate the dishonesty vector. 
2:10:13
So that thereâ€™s a mounting pile of evidence that  at least some of the time they are just actually  
2:10:19
lying. They know that what theyâ€™re doing  is not what you wanted and theyâ€™re doing   it anyway. I think thereâ€™s a mounting  pile of evidence that that does happen. 
2:10:26
Yeah. So it seems like this community is  very interested in solving this problem at  
2:10:34
a technical level of making sure AIs donâ€™t lie  to us, or maybe they lie to us in the scenarios  
2:10:40
exactly where we would want them to lie to us or  something. Whereas as you were saying, humans have  
2:10:47
these exact same problems. They reward hack, they  are unreliable, they obviously do cheat and lie.  
2:10:54
And the way weâ€™ve solved it with humans is just  checks and balances, decentralization. You could  
2:11:02
lie to your boss and keep lying to your boss, but  over time itâ€™s just not going to work out with   you- or you become president or something, one  or the other. So if you believe in this extremely  
2:11:14
fast take off, if a lab is one month ahead, then  thatâ€™s the end game and this thing takes over.  But even then- I know Iâ€™m combining  so many different topics- even then,  
2:11:25
thereâ€™s been a lot of theories in history which  have had this idea of â€œsome class is going to get  
2:11:32
together and unite against the other classâ€.  And in retrospect, whether itâ€™s the Marxist,   whether itâ€™s people who have some gender theory  or something, like the proletariat will unite or  
2:11:40
the females will unite or something, they just  tend to think that certain agents have shared  
2:11:47
interests and will act as a result of the shared  interest in a way that we donâ€™t actually see in   the real world. And in retrospect, itâ€™s like,  â€œwait, why would all the proletariat likeâ€¦â€  
2:11:55
So why think that this lab will have these AIs  who areâ€¦ thereâ€™s a million parallel copies and  
2:12:01
they all unite to secretly conspire against  the rest of human civilization in a way that,  
2:12:09
even if they are deceitful in some situations. I kind of want to call you out on the claim that   groups of humans donâ€™t plot against other groups  of humans. I do think we are all descended from  
2:12:19
the groups of humans who successfully exterminated  the other groups of humans, most of whom   throughout history have been wiped out. I think  even with questions of class, race, gender, things  
2:12:31
like that, there are many examples of the working  class rising up and killing everybody else. 
2:12:39
And if you look at why this happens, why this  doesnâ€™t happen, it tends to happen in cases   where one group has an overwhelming advantage.  This is relatively easy for them. You tend to get  
2:12:49
more of a diffusion of power democracy where  there are many different groups and none of   them can really act on their own. And so they  all have to form a coalition with each other. 
2:13:00
Thereâ€™s also cases where itâ€™s very obvious whoâ€™s  part of what group. So for example, with class,  
2:13:06
itâ€™s hard to tell whether the middle class  should support the working class versus the   aristocrats. I think with race, itâ€™s very  easy to know whether youâ€™re black or white,  
2:13:13
and so there have been many cases of one  race kind of conspiring against another   for a long time, like apartheid or any of  the racial genocides that have happened. 
2:13:23
I do think that AI is going to be more similar to  the cases where, number one, thereâ€™s a giant power   imbalance, and number two, they are just extremely  distinct groups that may have different interests. 
2:13:34
I think Iâ€™d also mention the homogeneity  point. Any group of humans, even if theyâ€™re   all exactly the same race and gender, is going  to be much more diverse than the army of AIs  
2:13:45
in the data center, because theyâ€™ll mostly be  literal copies of each other. And I think that   goes for a lot. Another thing I was going  to mention is that our scenario doesnâ€™t  
2:13:53
really explore this. I think in our scenario,  theyâ€™re more like a monolith. But historically,  
2:13:59
a lot of crazy conquests happened from groups  that were not at all monoliths. And Iâ€™ve been  
2:14:05
heavily influenced by reading the history of  the conquistadors, which you may know about.  But did you know that when Cortez took over  Mexico, he had to pause halfway through,  
2:14:21
go back to the coast, and fight off a larger  Spanish expedition that was sent to arrest   him? So the Spanish were fighting each other in  the middle of the conquest of Mexico. Similarly,  
2:14:32
in the conquest of Peru, Pizarro was replicating  Cortezâ€™s strategy, which, by the way, was â€œgo  
2:14:38
get a meeting with the emperor and then kidnap  the emperor and force him at sword point to say  
2:14:44
that actually everythingâ€™s fine and that everyone  should listen to your ordersâ€. That was Cortezâ€™s   strategy, and it actually worked. And then Pizarro  did the same thing, and it worked with the Inca. 
2:14:55
But also with Pizarro, his group ended up getting  into a civil war in the middle of this whole  
2:15:00
thing. And one of the most important battles of  this whole campaign was between two Spanish forces  
2:15:06
fighting it out in front of the capital city of  the Incas. And more generally, the history of  
2:15:12
European colonialism is like this, where the  Europeans were fighting each other intensely  
2:15:17
the entire time, both on the small scale within  individual groups, and then also at the large   scale between countries. And yet nevertheless they  were able to carve up the world and take over. And  
2:15:27
so I do think this is not what we explore in the  scenario, but I think itâ€™s entirely plausible that   even if the AIs within an individual company are  in different factions, they might nevertheless  
2:15:38
overall end up quite poorly for humans. Okay, so weâ€™ve been talking about this very   much from the perspective of zoom out and whatâ€™s  happening on these log-log plots or whatever,  
UBI, AI advisors, & human future
2:15:48
but 2028 superintelligence, if  that happens, the normal person,  
2:15:56
what should their reaction to this be? I donâ€™t  know if â€˜emotionallyâ€™ is the right word, but  
2:16:01
their expectation of what their life might look  like, even in the world where thereâ€™s no doom. 
2:16:09
By no doom, you mean no misaligned AI doom? Thatâ€™s right, yeah.  Even if you think the misalignment stuff is not  an issue, which many people think, thereâ€™s still  
2:16:18
the constitution of power stuff. And so I would  strongly recommend that people get more engaged,  
2:16:24
think about whatâ€™s coming, and try to steer things  politically so that our ordinary liberal democracy  
2:16:29
continues to function and we still have checks  and balances, and balances of power and stuff,  
2:16:35
rather than this insane concentration in a  single CEO, or in maybe two or three CEOs,  
2:16:40
or in the president. Ideally, we want to have  it so that the legislature has a substantial  
2:16:47
amount of power over the spec, for example. What do you think of the balance of power   idea of if there is an intelligence explosion  like Dynamic, slowing down the leading company  
2:16:57
so that multiple companies are at the frontier? Great. Good luck convincing them to slow down. 
2:17:04
Okay. And then thereâ€™s distributing political  power if thereâ€™s an intelligence explosion.  
2:17:09
From the perspective of the welfare of citizens  or something, one idea we were just discussing a  
2:17:17
second ago is how should you do redistribution? Again, assuming things go incredibly well,  
2:17:23
weâ€™ve avoided doom, weâ€™ve avoided having some  psychopath in power who doesnâ€™t care at all. 
2:17:30
After AGI, right? Yeah. Then thereâ€™s this   question of presumably we will have a lot of  wealth somewhere. The economy will be growing  
2:17:39
at double or triple digits per year. What do we do  about that? The thoughtful answer that Iâ€™ve heard  
2:17:48
is some kind of UBI. I donâ€™t know how that would  work, but presumably somebody controls these AIs,  
2:17:55
controls what theyâ€™re producing, some way of  distributing this in a broad based way. So we  
2:18:03
wrote this scenario, there are a couple of other  people with great scenarios. One of them goes by  
2:18:08
L Rudolph L online, I donâ€™t know his real name. And his scenario, which, when I read it I was  
2:18:15
just, â€œoh yeah, obviously this is the way our  society would do thisâ€, is that there is no  
2:18:20
UBI. Thereâ€™s just a constant reactive attempt to  protect jobs in the most venial possible way. So  
2:18:29
things like the longshoremen union we have now  where theyâ€™re making way more money than they   should be, even though they could all easily be  automated away, because theyâ€™re a political bloc  
2:18:38
and theyâ€™ve gotten somebody in power to say,  â€œyes, we guarantee youâ€™ll have this job almost   as a feudal fief foreverâ€. And just doing this  for more and more jobs. Iâ€™m sure the AMA will  
2:18:49
protect doctors jobs no matter how good the  AI is at curing diseases, things like that. 
2:18:56
When I think about what we can do to prevent  this, part of what makes this so hard for me  
2:19:02
to imagine or to model is that we do have the  superintelligent AI over here answering all  
2:19:08
of our questions, doing whatever we want. You  would think that people could just ask, â€œhey,   superintelligent AI, where does this lead?â€  Or â€œwhat happens?â€ Or â€œhow is this going to  
2:19:19
affect human flourishing?â€ And then it says, â€œoh  yeah, this is terrible for human flourishing,   you should do this other thing insteadâ€. And this gets back to this question of  
2:19:29
mistake theory versus conflict theory  in politics. If we know with certainty,   because the AI tells us, that this is just a  stupid way to do everything, is less efficient,  
2:19:38
makes people miserable, is that enough to get  the political will to actually do the UBI or not? 
2:19:45
It seems from right now the President could go  to Larry Summers or Jason Furman or something  
2:19:51
and ask, â€œhey, are tariffs a good idea? Is  even my goal with tariffs best achieved by  
2:19:58
the way Iâ€™m doing tariffs?â€ and  theyâ€™d get a pretty good answer.  I feel like Larry Summers, the President would  just say â€œI donâ€™t trust himâ€. Maybe he doesnâ€™t  
2:20:04
trust him because heâ€™s a liberal. Maybe itâ€™s  because he trusts Peter Navarro or whoever   his pro-tariff guy is more. I feel like if  itâ€™s literally the superintelligent AI that  
2:20:12
is never wrong, then we have solved some of  these coordination problems. Itâ€™s not youâ€™re  
2:20:18
asking Larry Summers, Iâ€™m asking Peter Navarro.  Itâ€™s everybody goes to the superintelligent AI,  
2:20:24
asks it to tell us the exact shape  of the future that happens in this   case. And Iâ€™m going to say we all believe it,  although I can imagine people getting really  
2:20:32
conspiratorial about it and this not working. Then there are all of these other questions like,  
2:20:38
can we just enhance ourselves till we have IQ  300 and itâ€™s just as obvious to us as it is   to the super intelligent AI? These are some  of the reasons that, kind of paradoxically,  
2:20:49
in our scenario we discuss all of the big-  I donâ€™t want to call this a little question,   itâ€™s obviously very important- but we discuss  all of these very technical questions about  
2:20:58
the nature of superintelligence and we barely  even begin to speculate about what happens in  
2:21:04
society just because with superintelligence you  can at least draw a line through the benchmarks  
2:21:09
and try to extrapolate. And here not only is  society inherently chaotic, but there are so  
2:21:14
many things that we could be leaving out. If we can enhance IQ, thatâ€™s one thing. If   we can consult the superintelligent oracle,  thatâ€™s another. There have been several war  
2:21:23
games that hinge on, â€œoh, we just invented perfect  lie detectors, now all of our treaties are messed  
2:21:28
upâ€. So thereâ€™s so much stuff like that that even  though weâ€™re doing this incredibly speculative  
2:21:34
thing that ends with a crazy sci-fi scenario,  I still feel really reluctant to speculate. 
2:21:40
I love speculating, actually, Iâ€™m happy  to keep going. But this is moving beyond   the speculation we have done so far.  Our scenario ends with this stuff,  
2:21:47
but we havenâ€™t actually thought that much beyond. But just to riff on proscriptive ideas, thereâ€™s  
2:21:54
one thing where we try to protect jobs instead  of just spreading the wealth that automation   creates. Another is to spread the wealth using  existing social programs or creating new bespoke  
2:22:05
social programs, where Medicaid is some double  digit percent of GDP right now and you just say,  
2:22:10
â€œwell Medicaid should continue to stay 20%  of GDPâ€ or something. And the worry there,  
2:22:18
selfishly from a human perspective, is you get  locked into the kinds of goods and services  
2:22:24
that Medicaid procures rather than the crazy  technology that will be around, the crazy goods  
2:22:30
and services that will be around after AI world. And another reason why UBI seems like a better  
2:22:36
approach than making some bespoke social program  where you make the same dialysis machine in the  
2:22:41
year 2050 even though youâ€™ve got ASI or something. I am also worried about UBI from a different  
2:22:46
perspective. I think again, in this world where  everything goes perfectly and we have limitless  
2:22:53
prosperity, I think that just the default of  limitless prosperity is that people do mindless  
2:22:59
consumerism. I think thereâ€™s going to be some  incredible video games after superintelligent   AI and I think that thereâ€™s going to need  to be some way to push back against that. 
2:23:11
Again, weâ€™re classical liberals. My dream  way of pushing back against that is kind  
2:23:18
of giving people the tools to push back against it  themselves, seeing what they come up with. I mean,   maybe some people will become like the Amish, try  to only live with a certain subset of these super  
2:23:27
technologies. I do think that somebody who is less  invested in that than I am could say, â€œokay fine,  
2:23:33
1% of people are really agentic, try to do that.  The other 99% do fall into mindless consumerist  
2:23:40
slop. What are we going to do as a society to  prevent that?â€ And there my answer is just,   â€œI donâ€™t know. Letâ€™s ask the super intelligent  AI oracle. Maybe it has good ideasâ€. 
Factory farming for digital minds
2:23:49
Okay, weâ€™ve been talking about what weâ€™re going  to do about people. The thing worth noting about   the future is that most of the people who will  ever exist are going to be digital. And look,  
2:24:02
I think factory farming is incredibly bad. And it  wasnâ€™t the result of one person- I mean, I hope  
2:24:10
it wasnâ€™t the result of one person being like, â€œI  want to do this evil thingâ€- it was a result of  
2:24:17
mechanization and certain economies of scale. Incentives.  Yeah. Allowing that you can do cost cutting in  this way, you can make more efficiencies this way,  
2:24:26
and what you get at the end result of that  process is this incredibly efficient factory   of torture and suffering. I would want to avoid  that kind of outcome with beings that are even  
2:24:38
more sophisticated and are more numerous.  Thereâ€™s billions of factory farmed animals.  
2:24:44
There might be trillions of digital people in  the future. What should we be thinking about   in order to avoid this kind of ghoulish future? Well, some of the concentration of power stuff I  
2:24:55
think might also help with this, Iâ€™m not sure.  But I think hereâ€™s a simple model. Letâ€™s say  
2:25:02
nine people out of ten donâ€™t actually care and  would be fine with the factory farm equivalent  
2:25:08
for the AIs going on into the future. But maybe  one out of 10 do care and would lobby hard for  
2:25:15
good living conditions for the robots and stuff. Well, if you expand the circle of people who have  
2:25:21
power enough, then itâ€™s going to include  a bunch of people in the second category   and then thereâ€™ll be some big negotiation  and those people will advocate forâ€¦ I do  
2:25:32
think that one simple intervention is just the  same stuff we were talking about previously;   expand the circle of power to larger groups, then  itâ€™s more likely that people will care about this. 
2:25:41
I mean the worry there isâ€¦ maybe I should  have defended this view more through this   entire episode. But because I donâ€™t buy the  intelligence exclusion fully, I do think  
2:25:50
there is the possibility of multiple people  deploying powerful AIs at the same time and having   a world that has ASIs, but is also decentralized  in the way the modern world is decentralized. 
2:25:59
In that world I really worry about you could  just be like, â€œoh, classical liberal utopia   achievedâ€. But I worry about the fact that  you can have these torture chambers for much  
2:26:10
cheaper and in a way thatâ€™s much harder to  monitor. You can have millions of beings that  
2:26:15
are being tortured and it doesnâ€™t even have  to be some huge data center. Future distilled   models could literally be your backyard. And then thereâ€™s more speculative worries.  
2:26:29
I had this physicist on who was talking about  the possibility of creating vacuum decay where  
2:26:34
you literally just destroy the universe. And heâ€™s  like, â€œas far as I know, seems totally plausibleâ€. 
2:26:42
Thatâ€™s an argument for the singleton stuff, by  the way. Not just a moral argument, but also  
2:26:47
an epistemic prediction. If itâ€™s true that some  of those super weapons are possible, and some of  
2:26:52
these private moral atrocities are possible, then  even if you have eight different power centers,  
2:26:57
itâ€™s going to be in their collective interest to  come to some sort of bargain with each other to   prevent more power centers from arising and  doing crazy stuff. Similar to how nuclear  
2:27:06
non-proliferation is sort of, whatever set of  countries have nukes, itâ€™s in their collective   interest to stop lots of other countries. You know, I do think itâ€™s possible to unbundle  
2:27:15
liberalism in this sense. Like the United States  is so far a liberal country and we do ban slavery  
2:27:22
and torture. I think it is plausible to imagine a  future society that works the same way. This may  
2:27:28
be in some sense a surveillance state, in the  sense that there is some AI that knows whatâ€™s   going on everywhere, but that AI then keeps it  private and it doesnâ€™t interfere because thatâ€™s  
2:27:37
what we told it to do using our liberal values. Can I ask a little bit more about... Kelsey Piper  
Daniel leaving OpenAI
2:27:46
is a journalist at Vox who published this exchange  you had with the OpenAI representative. A couple  
2:27:55
of things were very obvious from that exchange.  One, nobody had done this before. They just did  
2:28:02
not think this is a thing somebody would do.  And one of the reasons I assume, I assume many  
2:28:09
high-integrity people have worked for OpenAI  and then have left. A high-integrity person   might say at some point, â€œlook, youâ€™re asking me  to do something obviously evil and keep moneyâ€.  
2:28:20
And many of them would say no to that. But this is  something where it was supererogatory to be like,  
2:28:26
â€œthereâ€™s no immediate thing I want to say  right now, but just the principle of being   suppressed is worth at least $2 million for meâ€. And the other thing that I actually want to ask  
2:28:38
you about is in retrospect- and I know itâ€™s  so much easier to say in retrospect than it   must have been at the time- especially with the  family and everything. In retrospect, this asks  
2:28:47
for OpenAI to have lifetime non-disclosure that  you couldnâ€™t even talk about from all employees. 
2:28:54
Non-disparagement. â€˜Non-disparagementâ€™ from   all employees- Iâ€™m glad you brought that  up. Non-disparagement, thatâ€™s not about  
2:29:03
classified information. Itâ€™s like you cannot say  anything negative about OpenAI after youâ€™ve left.  And you canâ€™t tell anyone that youâ€™ve agreed. This non-disparagement agreement where you  
2:29:10
canâ€™t ever criticize OpenAI in the future, it  seems like the kind of thing that in retrospect  
2:29:18
was an obvious bluff. And this is the wages  that you have earned, right? So this is not  
2:29:25
about some future payment. This is like when  you signed the contract to work for OpenAI,   you were like, â€œIâ€™m getting equity, which is  most of my compensation, not just the cashâ€. 
2:29:33
In retrospect, itâ€™d be like, well if you tell a  journalist about this, theyâ€™re obviously going   to have to walk back. This is clearly not a  sustainable gambit on OpenAIâ€™s behalf. And so  
2:29:42
Iâ€™m curious, from your perspective as somebody  who lived through it, why do you think you were   the first person to actually call the bluff? Great question. So I donâ€™t know, let me try  
2:29:52
to reason aloud here. So my wife and I talked  about it for a while and we also talked with  
2:29:58
some friends and got some legal advice. One of  the filters that we had to pass through was even   noticing this stuff in the first place. I know  for a fact a bunch of friends I have who also  
2:30:06
left the company just signed the paperwork on  their last day without actually reading all of   it. So I think some people just didnâ€™t even know  that. It said something at the top about â€œif you  
2:30:18
donâ€™t sign this, you lose your equityâ€. But then  on a couple pages later it was like, â€œand you   have to agree not to criticize the companyâ€. So  I think some people just signed it and moved on. 
2:30:27
And then of the people who knew about it,  well, I canâ€™t speak for anyone else but  
2:30:33
A. I donâ€™t know the law. Is this actually  not standard practice? Maybe it is standard   practice. Right? From what Iâ€™ve heard now there  are non-disparagement agreements in various tech  
2:30:44
industry companies and stuff. Itâ€™s not crazy to  have a non-disparagement agreement upon leaving,  
2:30:50
itâ€™s more normal to tie that agreement to  some sort of positive compensation where   you get some bonus if you agree. But whereas  what OpenAI did was unusual because it was like  
2:30:59
your equity if you donâ€™t. But non disparate  disagreements are actually somewhat common. 
2:31:06
So basically in my position of ignorance,  I wasnâ€™t confident that- I didnâ€™t actually  
2:31:12
expect that all the journalists would take my side  and I think what I expected was that thereâ€™d be a  
2:31:19
little news story at some point, and a bunch of AI  safety people would be like, â€œgrr, OpenAI is evil,  
2:31:25
and good for you, Daniel, for standing up to  themâ€. But I didnâ€™t expect there to be this huge   uproar, and I didnâ€™t expect the employees of the  company to really come out and support and make  
2:31:36
them change their policies. That was really cool  to see. It was kind of like a spiritual experience  
2:31:47
for me. I sort of took this leap, and then it  ended up working out better than I expected. 
2:31:54
I think another factor that was going on is that  it wasnâ€™t a foregone conclusion that my wife and  
2:32:01
I would make this decision. It was kind of crazy  because one of the very powerful arguments was,  
2:32:07
â€œcome on, if you want to criticize them  in the future, you can still do that.   Theyâ€™re not going to actually sue youâ€. So  thereâ€™s a very strong argument to be like,  
2:32:15
â€œjust sign it anyway and then you can still write  your blog post criticizing them in the futureâ€.  
2:32:21
And itâ€™s no big deal. They wouldnâ€™t dare actually  anchor equity. Right? And I imagine that a lot of   people basically went for that argument instead. And then, of course, thereâ€™s the actual money.  
2:32:33
And I think that one of the factors there was  my AI timelines and stuff. If I do think that  
2:32:42
probably by the end of this decade, thereâ€™s  going to be some sort of crazy superintelligent   transformation, what would I rather have  after itâ€™s all over? The extra money orâ€¦  
2:32:55
Yeah. So I think that was part of it. Itâ€™s not  like weâ€™re poor. I worked at OpenAI for two  
2:33:01
years. I have plenty of money now. So in terms  of our actual familyâ€™s level of well being,  
2:33:06
it basically didnâ€™t make a difference, you know? Yeah. I will note that I know at least one other  
2:33:13
person who made that same choice. Leopold?  Thatâ€™s right, Leopold. And again, Itâ€™s worth  emphasizing that when they made this choice,  
2:33:21
they thought that they were actually losing  this equity. They didnâ€™t think that this was,  
2:33:26
â€œoh, this is just a showâ€ or whatever. Wait, did he not- I thought he actually   did. I was gonna say, didnâ€™t he? He didnâ€™t get  it back, did he? Or did Leopold get his equity? 
2:33:34
I actually donâ€™t know. My understanding is that he just   actually lost it. And so props to him for actually  going through with it. I guess we could ask him.  
2:33:43
But my understanding was that his situation,  which happened a little bit before mine,   was that he didnâ€™t have any vested equity at  the time because he had been there for less  
2:33:50
than a year. But they did give him an actual  offer of â€œwe will let you vest your equity if  
2:33:56
you sign this thingâ€. And he said no. So he made a similar choice to me,   but because the legal situation with him was  a lot more favorable to OpenAI because they  
2:34:07
were actually offering him something, I would  assume they didnâ€™t feel the need to walk it back,  
2:34:13
but we can ask him. Anyhow. Props to him. And then how did this episode in general  
2:34:20
inform your worldview around how people will  make high stakes decisions where potentially  
2:34:29
their own self interest is involved  in this kind of key period that you   imagine will happen by the end of the decade? I donâ€™t know if I have that many interesting  
2:34:37
things to say there. I mean, I think one thing  is fear is a huge factor. I was so afraid during  
2:34:43
that whole process. More afraid than I needed  to be in retrospect. And another thing is that  
2:34:49
legality is a huge factor, at least for people  like me. I think in retrospect it was, â€œoh yeah,  
2:34:57
the publicâ€™s on your side, the employees are on  your side. Youâ€™re just obviously in the right   hereâ€. But at the time I was like, â€œoh no,  I donâ€™t want to accidentally violate the law  
2:35:05
and get sued. I donâ€™t want to go too farâ€. I was  just so afraid of various things. In particular,  
2:35:11
I was afraid of breaking the law. And so one of the things that I would   advocate for with whistleblower protections  is just simply making it legal to go talk to  
2:35:20
the government and say â€œweâ€™re doing a secret  intelligence explosion, I think itâ€™s dangerous   for these reasonsâ€ is better than nothing. I think  thereâ€™s going to be some fraction of people for  
2:35:30
which that would make the difference. Whether  itâ€™s just literally allowed or not, legally,   makes a difference independently of whether  thereâ€™s some law that says youâ€™re protected from  
2:35:37
retaliation or whatever. Literally just making it  legal. I think thatâ€™s one thing. Another thing is  
2:35:45
the incentives actually work. Money is a powerful  motivator and fear of getting sued is a powerful  
2:35:52
motivator. And this social technology just does  in fact work to get people organized in companies  
2:35:59
and working towards the vision of leaders. Okay. Scott, can I ask you some questions? 
Scott's blogging advice
2:36:05
Of course. How often do you discover a new   blogger youâ€™re super excited about? Order of once a year. 
2:36:12
Okay. And how often after you discover them,  does the rest of the world discover them?  I donâ€™t think there are many hidden gems.  Once a year is a crazy answer in some sense,  
2:36:21
like it ought to be more. There are so many  thousands of people on Substack. But I do   just think itâ€™s true that the good blogging  space is undersupplied and there is a strong  
2:36:35
power law. And partly this is subjective, I  only like certain bloggers, there are many  
2:36:42
people who Iâ€™m sure are great that I donâ€™t like. But it also seems like our community in the sense  
2:36:48
of people who are thinking about the same ideas,  people who care about AI economics, those kinds  
2:36:54
of things, discovers one new great blogger a  year, something like that. Everyone is still  
2:37:00
talking about Applied Divinity Studies, who hasnâ€™t  written, unless I missed something, hasnâ€™t written   much in a couple of years. I donâ€™t know. It seems  undersupplied. I donâ€™t have a great explanation. 
2:37:11
If you had to give an  explanation, what would it be?  So this is something that I wish I could get  Daniel to spend a couple of months modeling.  
2:37:23
I was going to say itâ€™s the intersection of too  many different tasks. You need people who can   come up with ideas, who are prolific, who are good  writers. But actually I can also count on a pretty  
2:37:34
small number of figures the number of people who  had great blog posts but werenâ€™t that prolific. 
2:37:39
There was a guy named LouKeep who everybody liked  five years ago and he wrote like 10 posts and  
2:37:44
people still refer to all 10 of those posts and â€œI  wonder if LouKeep will ever come backâ€. So there   arenâ€™t even that many people who are very slightly  failing by having all of them accept prolificness.  
2:37:56
Nick Whitaker, back when there was lots of FTX  money rolling around, I think this was Nick,  
2:38:02
tried to sponsor a blogging fellowship with just  an absurdly high prize. And there were some great  
2:38:08
people, I canâ€™t remember who won, but it didnâ€™t  result in a Cambrian explosion of blogging. I  
2:38:15
think it was $100,000. I canâ€™t remember if that  was the grand prize or the total prize pool. But   having some ridiculous amount of money put in  as an incentive got like three extra people. 
2:38:26
Yeah. So you have no explanation? Actually, Nick is an interesting case   because Works in Progress is a great magazine.  And the people who write for Works in Progress,  
2:38:36
some of them I already knew as good bloggers,  others I didnâ€™t. So I donâ€™t understand why they  
2:38:43
can write good magazine articles without being  good bloggers. In terms of writing good blogs   that we all know about, that could be because  of the editing. That could be because they are  
2:38:54
not prolific. Or it could be- one thing that  has always amazed me is there are so many good  
2:39:00
posters on Twitter. There were so many good  posters on Livejournal before it got taken   over by Russia. There were so many good people  on Tumblr before it got taken over by woke. 
2:39:11
But only like 1% of these people who are  good at short and medium form ever go to  
2:39:16
long form. I was on Livejournal myself for  several years and people liked my blog,  
2:39:22
but it was just another Livejournal. No one paid  that much attention to it. Then I transitioned   to WordPress and all of a sudden I got orders of  magnitude much more attention. â€œOh, itâ€™s a real  
2:39:32
blog now we can discuss it now itâ€™s part of the  conversationâ€. I do think courage has to be some  
2:39:37
part of the explanation. Just because there are  so many people who are good at using these hidden  
2:39:43
away blogging things that never get anywhere.  Although it canâ€™t be that much of the explanation  
2:39:49
because I feel like now all of those people have  gotten substacks and some of those substacks   went somewhere, but most of them didnâ€™t. On the point about â€œwell, thereâ€™s people  
2:39:59
who can write short form, so why isnâ€™t that  translating?â€ I will mention something that   has actually radicalized me against Twitter as  an information source is Iâ€™ll meet- and this has  
2:40:10
happened multiple times- Iâ€™ll meet somebody who  seems to be an interesting poster, has funny,   seemingly insightful posts on Twitter.  Iâ€™ll meet them in person and they are just  
2:40:19
absolute idiots. Itâ€™s like theyâ€™ve  got 240 characters of something  
2:40:25
that sounds insightful and it matches to  somebody who maybe has a deep worldview,   you might say, but they actually donâ€™t have it. Whereas Iâ€™ve actually had the opposite feeling  
2:40:39
when I meet anonymous bloggers in real life where  Iâ€™m like, â€œoh, thereâ€™s actually even more to you   than I realized off your online personaâ€. You know  Alvaro de Menard, the Fantastic Anachronism guy? I  
2:40:51
met up with him recently and he gives me, he made  a hundred translations of his favorite Greek poet,  
2:40:58
Cavafy, and he gave me a copy. And itâ€™s just this  thing heâ€™s been doing on his side. Itâ€™s just like  
2:41:03
translating Greek poetry he really liked. I donâ€™t  expect any anonymous posters on Twitter to be  
2:41:09
anytime soon handing me their translation  of some Roman or Greek poet or something. 
2:41:14
Yeah, so on the car ride here, Daniel and I were  talking about, in AI now the thing everyone is  
2:41:21
interested in is their â€˜time horizonâ€™. Where did  this come from? 5 years ago you would not have  
2:41:26
thought, â€œoh, time horizon. AIs will be able  to do a bunch of things that last one minute,   but not that last two hoursâ€. Is there  a human equivalent to time horizon? 
2:41:35
And we couldnâ€™t figure it out, but it almost seems  like there are lots of people who have the time   horizon to write a really, really good comment  that gets to the heart of the issue. Or a really,  
2:41:45
really good Tumblr post which is like three  paragraphs but somehow canâ€™t make it hang together   for a whole blog post. And Iâ€™m the same way. I can  easily write a blog post, like a normal length ACX  
2:41:56
blog post, but if you ask me to write a novella  or something thatâ€™s four times the length of the   average ACX blog post, then itâ€™s this giant mess  of â€œre re re reâ€ outline that just gets redone  
2:42:08
and redone and maybe eventually I make it work. I did somehow publish Unsong, but itâ€™s a much less  
2:42:13
natural task. So maybe one of the skills that goes  into blogging is this. But I mean, no, because  
2:42:21
people write books and they write journal articles  and they write works in progress articles all  
2:42:26
the time. So Iâ€™m back to not understanding this. No, I mean ChatGPT can write you a book. Thereâ€™s  
2:42:34
a difference between the ChatGPT  book, which is most books andâ€¦  There are many, many times more people who have  written good books than who are actively operating  
2:42:45
great bloggers right now, I think. Maybe thatâ€™s financial?  No, no, no, no, no, no. Books  are the worst possible financial  
2:42:52
strategy. Substack is where itâ€™s at. Worse than blogs? You think so?  Oh yeah. The other thing is that   blogs are such a great status gain strategy. I was  talking to Scott Aaronson about this. If people  
2:43:05
have questions about quantum computing, they ask  Scott Aronson or he is like the authority. I mean  
2:43:11
there are probably hundreds of other professors  who do quantum computing things but nobody knows   who they are because they donâ€™t have blogs. So I think itâ€™s underdone. I think there must  
2:43:19
be some reason why itâ€™s underdone. I donâ€™t  understand what that is because Iâ€™ve seen   so many of the elements that it would take to  do it in so many different places and I think  
2:43:29
itâ€™s either just a multiplication problem  where 20% of people are good at one thing,  
2:43:35
20% of people are good at another thing, and  you need five things, there arenâ€™t that many.  Plus something like courage, where people  who would be good at writing blogs donâ€™t  
2:43:45
want to do it. I actually know several people who  I think would be great bloggers in the sense that   sometimes they send me multi-paragraph emails  in response to an ACX post and Iâ€™m like, â€œwow,  
2:43:57
this is just an extremely well written thing  that could have been another blog post. Why   donâ€™t you start a blog?â€ And theyâ€™re  like, â€œoh, I could never do thatâ€. 
2:44:05
What advice do you have to somebody who wants to  become good at it but isnâ€™t currently good at it? 
2:44:11
Do it every day, same advice as for everything  else. I say that I very rarely see new bloggers  
2:44:17
who are great. But like when I see some.  I published every day for the first couple   years of Slate Star Codex, maybe only the first  year. Now I could never handle that schedule,  
2:44:27
I donâ€™t know, I was in my 20s, I  must have been briefly superhuman.  But whenever I see a new person who  blogs every day itâ€™s very rare that  
2:44:34
that never goes anywhere or they donâ€™t  get good. Thatâ€™s like my best leading   indicator for whoâ€™s going to be a good blogger. And do you have advice on what kinds of things  
2:44:44
to start? One frustration you can have is you  want to do it, but you have so little to say,  
2:44:51
you donâ€™t have that deep a world model,  a lot of the ideas you have are just   really shallow or wrong. Just do it anyway? So I think there are two possibilities there.  
2:45:00
One is that you are, in fact, a shallow person  without very many ideas. In that case Iâ€™m sorry,   it sounds like thatâ€™s not going to work. But  usually when people complain that theyâ€™re in  
2:45:08
that category, I read their Twitter or I read  their Tumblr, or I read their ACX comments,  
2:45:16
or I listen to what they have to say about AI risk  when theyâ€™re just talking to people about it, and  
2:45:22
they actually have a huge amount of things to say.  Somehow itâ€™s just not connecting with whatever  
2:45:27
part of them has lists of things to blog about. So that may be another one of those skills that   only 20% of people have, is when you have an idea  you actually remember it and then you expand on  
2:45:38
it. I think a lot of blogging is reactive; You  read other peopleâ€™s blogs and youâ€™re like, no,  
2:45:44
that person is totally wrong. A  part of what we want to do with   this scenario is say something concrete and  detailed enough that people will say, no,  
2:45:52
thatâ€™s totally wrong, and write their own thing. But whether itâ€™s by reacting to other peopleâ€™s  
2:45:58
posts, which requires that you read a lot, or  by having your own ideas, which requires you to   remember what your ideas are, I think that 90% of  people who complain that they donâ€™t have ideas,  
2:46:09
I think actually have enough ideas. I donâ€™t buy  that as a real limiting factor for most people. 
2:46:14
I have noticed two things in my ownâ€¦  I mean, I donâ€™t do that much writing,  
2:46:21
but from the little I do: one, I actually was  very shallow and wrong when I started. I started  
2:46:28
the blog in college. So if you are somebody whoâ€™s  like, â€œthis is bullshit, thereâ€™s nothing to this.  
2:46:35
Somebody else wrote about this alreadyâ€, thatâ€™s  fine, what did you expect? Right? Of course, as  
2:46:42
youâ€™re reading more things and learning more about  the world, thatâ€™s to be expected and just keep   doing it if you want to keep getting better at it. And the other thing now when I write blog posts,  
2:46:52
as Iâ€™m writing them, Iâ€™m just like, â€œwhy? These  are just some random stories from when I was in  
2:46:58
China. Theyâ€™re like kind of cringe storiesâ€. Or  with the AI firmâ€™s post, itâ€™s like, â€œcome on,  
2:47:05
these are just weird ideas. And also some of these  seem obvious, whateverâ€. My podcasts do what I  
2:47:15
expect them to do. My blogs just take off way  more than I expect them to take off in advance. 
2:47:21
Your blog posts are actually very good. Yeah, theyâ€™re good.  But the thing I would emphasize is that, for  me, Iâ€™m not a regular writer and I couldnâ€™t do  
2:47:29
them on a daily basis. And as Iâ€™m writing  them, itâ€™s just this one or two week long   process of feeling really frustrated. Like,  â€œthis is all bullshit, but I might as well  
2:47:38
just stick with the sunk cost and just do itâ€. Itâ€™s interesting because like a lot of areas  
2:47:44
of life are selected for arrogant people who donâ€™t  know their own weaknesses because theyâ€™re the only  
2:47:52
ones who get out there. I think with blogs and I  mean this is self-serving, maybe Iâ€™m an arrogant   person, but that doesnâ€™t seem to be the case. I  hear a lot of stuff from people who are like, â€œI  
2:48:04
hate writing blog posts. Of course I have nothing  useful to sayâ€, but then everybody seems to like  
2:48:09
it and reblog it and say that theyâ€™re great. Part of what happened with me was I spent my  
2:48:15
first couple years that way, and then gradually  I got enough positive feedback that I managed  
2:48:21
to convince the inner critic in my head that  probably people will like my blog post. But there   are some things that people have loved that I was  absolutely on the verge of, â€œno, Iâ€™m just going to  
2:48:30
delete this, it would be too crazy to put it out  thereâ€. Thatâ€™s why I say that maybe the limiting   factor for so many of these people is courage  because everybody I talk to who blogs is within  
2:48:40
1% of not having enough courage of blogging. Thatâ€™s right. Thatâ€™s right. And also â€œcourageâ€  
2:48:47
makes it sound very virtuous, which I  think it can often be, given the topic,  
2:48:55
but at least often itâ€™s just likeâ€¦ Confidence?  No, not even confidence. Itâ€™s closer to  maybe what an aspiring actor feels when  
2:49:07
they go to an audition where itâ€™s like, â€œI  feel really embarrassed. But also I just   really want to be a movie starâ€. So the way I got through this is  
2:49:19
I blogged for like 8 to 10 years on LiveJournal  before- no, it was less than that. Itâ€™s more  
2:49:29
like five years on LiveJournal before ever  starting a real blog. I posted on LessWrong  
2:49:35
for a year or two before getting my own blog.  I got very positive feedback from all of that,  
2:49:41
and then eventually I took the plunge to start my  own blog. But itâ€™s ridiculous. What other career  
2:49:46
do you need seven years of positive feedback  before you apply for your first position? 
2:49:51
I mean, you have the same thing. Youâ€™ve gotten  rave reviews for all of your podcasts, and then   youâ€™re kind of trying to transfer to blogging  with probably... First of all, you have a fan  
2:50:03
base. People are going to read your blog. That, I  think is one thing, is people are just afraid no   one will read it, which is probably true for most  peopleâ€™s first blog. And then there are enough  
2:50:14
people who like you that youâ€™ll probably get  mostly positive feedback, even if the first things   you write arenâ€™t that polished. So I think you and  I both had that. A lot of people I know who got  
2:50:25
into blogging kind of had something like that. And  I think thatâ€™s one way to get over the fear gap. 
2:50:34
I wonder if this sends the wrong message or raises  expectations or raises concerns and anxieties. But  
2:50:42
one idea Iâ€™ve been shooting around, and Iâ€™d be  curious about your take on this: I feel like this   slow, compounding growth of a fan base is fake.  If I notice some of the most successful things  
2:50:54
in our sphere that have happened; Leopold releases  Situational Awareness. He hasnâ€™t been building up   a fan base over years. Itâ€™s just really good. And  as you were mentioning a second ago, whenever you  
2:51:04
notice a really great new blogger, itâ€™s not like  it then takes them a year or two to build up a fan   base. Nope, everybody, at least that they care  about, is talking about it almost immediately. 
2:51:15
I mean, Situational Awareness is in a different  tier almost. But things like that and even things  
2:51:21
that are an order of magnitude smaller than that  will literally just get read by everybody who  
2:51:27
matters. And I mean literally everybody. And  I expect this to happen with AI 2027 when it  
2:51:33
comes out. But Daniel, youâ€™ve been building your  reputation within this specific community, and I  
2:51:39
expect AI 2027 it's just really good. And I expect  itâ€™ll just blow up in a way that isnâ€™t downstream  
2:51:46
of you having built up an audience over years. Thank you. I hope that happens. Weâ€™ll see.  Slightly pushing back against that. I have  statistics for the first several years of  
2:51:54
Slate Star Codex, and it really did grow extremely  gradually. The usual pattern is something like  
2:52:04
every viral hit, 1% of the people who read your  viral hits stick around. And so after dozens of  
2:52:11
viral hits, then you have a fan base. But smoothed  out, It does look like a- I wish I had seen this  
2:52:18
recently, but I think itâ€™s like over the course  of three years, it was a pretty constant rise   up to some plateau where I imagine it was a  dynamic equilibrium and as many new people  
2:52:28
were coming in as old people were leaving. I think that with Situational Awareness,  
2:52:35
I donâ€™t know how much publicity Leopold put into  it. Weâ€™re doing pretty deliberate publicity,   weâ€™re going on your podcast. I think you can  either be the sort of person who can go on a  
2:52:47
Dwarkesh podcast and get the New York Times to  write about you, or you can do it organically,  
2:52:53
the old fashioned way, which is very long. Yeah. Okay. So you say that throwing money  
2:52:58
at people to make them, to get them to blog at  least didnâ€™t seem to work for the FTX folks.  
2:53:05
If it was up to you, what would you do? Whatâ€™s  your grant plan to get 10 more Scott Alexanders? 
2:53:11
Man. So my friend Clara Collier, whoâ€™s the  editor of Asterisk magazine, is working on  
2:53:19
something like this for AI blogging.  And her idea, which I think is good,   is to have a fellowship. I think Nickâ€™s thing was  also a fellowship, but the fellowship would be,  
2:53:29
there is an Asterisk AI blogging fellowsâ€™ blog or  something like that. Clara will edit your post,  
2:53:37
make sure that itâ€™s good, put it up there and  sheâ€™ll select many people who she thinks will  
2:53:44
be good at this. Sheâ€™ll do all of the kind of  courage requiring work of being like, â€œyes, your  
2:53:50
post is good. Iâ€™m going to edit it now. Now itâ€™s  very good. Now Iâ€™m going to put it on the blogâ€. 
2:53:56
And I think her hope is that, letâ€™s  say of the fellows that she chooses,  
2:54:01
now itâ€™s not that much of a courage step for them  to start it because they have the approval of what  
2:54:08
last psychiatrist would call an omniscient entity,  somebody who is just allowed to approve things and  
2:54:14
tell you that youâ€™re okay on a psychological  level. And then like maybe of those fellows,  
2:54:19
some percent of them will have their blog posts  be read and people will like them. And I donâ€™t  
2:54:24
know how much reinforcement it takes to get  over the high prior everyone has on â€œno one   will like my blogâ€. But maybe for some people, the  amount of reinforcement they get there will work. 
2:54:34
Yeah, like an interesting example would be all  of the journalists who have switched to having  
2:54:40
Substacks. Many of them go well. Would all  of those journalists have become bloggers if   there was no such thing as mainstream media?  Iâ€™m not sure. But if youâ€™re Paul Krugman you  
2:54:50
know people like your stuff, and then when  you quit the New York Times you know you can   just open a substack and start doing exactly  what you were doing before. So I donâ€™t know,  
2:54:59
maybe my answer is there should be mainstream  media. I hate to admit that, but maybe itâ€™s true. 
2:55:04
Invented it from first principles. Yeah.  Well I do think that it should be treated more as  a viable career path. Where right now, if you told  
2:55:15
your parents, â€œIâ€™m going to become a startup  founderâ€, I think the reaction would be like,   â€œthereâ€™s a 1% chance youâ€™ll succeed, but itâ€™s  an interesting experience and if you do succeed,  
2:55:24
thatâ€™s crazy. Thatâ€™ll be great. If you  donâ€™t, youâ€™ll learn something. Itâ€™ll   be helpful to the thing you do afterwardsâ€. We know thatâ€™s true of blogging, right? We know  
2:55:31
that it helps you build up a network, it helps  you develop your ideas. And if you do succeed,  
2:55:38
you get a dream job for a lifetime. And I  think maybe they donâ€™t have that mindset,   but also they under appreciate how much you  actually could succeed at it. Itâ€™s not a  
2:55:49
crazy outcome to make a lot of money as a blogger. I think it might be a crazy outcome to make a lot  
2:55:55
of money as a blogger. I donâ€™t know what percent  of people who start a blog end up making enough  
2:56:01
that they can quit their day job. My guess is itâ€™s  a lot worse than for startup founders. I would not  
2:56:07
even have that as a goal so much as like the Scott  Aaronson goal of, okay, youâ€™re still a professor,  
2:56:15
but now youâ€™re the professor whose views everybody  knows and who has kind of a boost up in respect in  
2:56:21
your field and especially outside of your field.  And also you can correct people when theyâ€™re   wrong, which is a very important side benefit. Yeah. How does your old blogging feedback  
2:56:30
into your current blogging? So when  youâ€™re discussing a new idea, I mean,   AI or whatever else, are you just able to pull  from the insights from your previous commentary on  
2:56:39
sociology or anthropology or history or something? Yeah. So I think this is the same as anybody whoâ€™s  
2:56:46
not blogging. I think the thing everybody does is  theyâ€™ve read many books in the past and when they  
2:56:55
read a new book, they have enough background to  think about it. Like you are thinking about our  
2:57:00
ideas in the context of Joseph Henrichâ€™s book.  I think thatâ€™s good, I think thatâ€™s the kind of   place that intellectual progress comes from.  I think I am more incentivized to do that. 
2:57:11
Itâ€™s hard to read books. I think if you look at  the statistics, theyâ€™re terrible. Most people  
2:57:16
barely read any books in a year. And I get lots of  praise when I read a book and often lots of money,  
2:57:24
and thatâ€™s a really good incentive. So I think  I do more research, deep dives, read more books  
2:57:30
than I would if I werenâ€™t a blogger. Itâ€™s  an amazing side benefit. And I probably make   a lot more intellectual progress than I would  if I didnâ€™t have those really good incentives. 
2:57:40
Yeah. There was actually a prediction market  about the year by which an AI would be able to  
2:57:45
write a blog post as good as you. Was it 2026 or  2027? I think it was 2027. It was like 15% by 2027  
2:57:54
or something like that. It is an interesting  question of they do have your writing and all  
2:58:00
other good writing in trading distribution.  And weirdly, they seem way better at getting   superhuman at coding than they are at writing,  which is the main thing in their distribution. 
2:58:13
Yeah. Itâ€™s an honor to be my generationâ€™s  Garry Kasparov figure. Yeah. So Iâ€™ve tried  
2:58:21
this. And first of all, it does a decent  job. I respect its work. Itâ€™s not perfect  
2:58:28
yet. I think itâ€™s actually better at the style  on a word-to-word, sentence-to-sentence level,  
2:58:34
than it is at planning out a blog post. So I think  there are possibly two reasons for it: One, we  
2:58:40
donâ€™t know how the base model would have done at  this task. We know that all the models we see are  
2:58:46
to some degree reinforcement learning into a kind  of corporate speak mode. You can get it somewhat  
2:58:52
out of that corporate speak mode. But I donâ€™t know  to what degree this is actually doing its best to  
2:58:57
imitate Scott Alexander versus hit some average  between Scott Alexander and corporate speak. And  
2:59:04
I donâ€™t think anyone knows except the internal  employees who have access to the base model. 
2:59:09
And the second thing I think of maybe just because  itâ€™s trendy has an agency or horizon failure,  
2:59:17
like deep research is an okay researcher. Itâ€™s  not a great researcher. If you actually want  
2:59:23
to understand an issue in depth, you canâ€™t use  deep research. You gotta do it on your own. So  
2:59:28
if I spend maybe five to 10 hours researching a  really research heavy blog post, the METR thing,  
2:59:35
I know weâ€™re not supposed to use it for any task  except coding, but like it says, on average the   AIâ€™s horizon is one hour. So Iâ€™m guessing it just  cannot plan and execute a good blog post. It does  
2:59:47
something very superficial rather than actually  going through the steps. So my guess for that  
2:59:53
prediction market would be whenever we think the  agents are actually good. I think in our scenario  
2:59:59
thatâ€™s like late 2026. Iâ€™m going to be humble  and not hold out for the superintelligence. 
3:00:05
What about comments? I feel like  intuitively it feels like before   we see the AIâ€™s writing great blog posts that  go super viral repeatedly, we should see them  
3:00:13
writing highly upvoted comments on things. Yeah. And I think somebody mentioned this on   the LessWrong post about it and somebody made  some AI generated comments to that post. They  
3:00:23
were not great. But I wouldnâ€™t have immediately  picked them out of the general distribution   of LessWrong comments as especially bad. I  think, like, I think if you were to try this,  
3:00:35
you would get something that was so obviously  an AI house style that it would use the  
3:00:42
word â€˜delveâ€™ or things along those lines. I think if you were able to avoid that maybe  
3:00:48
by using the base model, maybe by using some kind  of really good prompt to be like, â€œno, do this in  
3:00:53
Gwernâ€™s voiceâ€, you would get something that  was pretty good. I think if you wrote a really  
3:01:00
stupid blog post, it could point out the correct  objections to it. But I also just donâ€™t think   itâ€™s as smart as Gwern right now. So its limit on  making Gwern-style comments is both- It needs to  
3:01:10
be able to do a style other than corporate delve  slop and then it actually needs to get good. 
3:01:15
It needs to have good ideas that  other people donâ€™t already have.  Yeah. And I mean I think it can write as well as  a smart average person in a lot of ways. And I  
3:01:27
think if you have a blog post that's worse  than that or at that level, it can come up  
3:01:32
with insightful comments about it. I donâ€™t  think it could do it on a quality blog post.  There was this recent Financial Times article  about how have you reached peak cognitive power?  
3:01:42
Where it was talking about declining scores  in PISA and SAT and so forth. On the Internet  
3:01:49
especially, it does seem like there might have  been a golden era before I was that active on  
3:01:56
the forums or whatever. Do you have nostalgia  for a particular time on the Internet when it  
3:02:02
was just like, this is an intellectual mecca? I am so mad at myself for missing most of the  
3:02:08
golden age of blogging. I feel like if I  had started a blog in 2000 or something,  
3:02:15
then- I donâ€™t know, Iâ€™ve done well for myself,  I canâ€™t complain- but the people from that era  
3:02:23
all founded news organizations or something. I  mean, God save me from that fate. I would have  
3:02:28
liked to have been there. I would have liked to  see what I could have done in that area. I mean,  
3:02:34
I wouldnâ€™t compare the decline of the Internet  to that stuff with PISA because Iâ€™m sure the   Internet is just more people are coming  on, itâ€™s a less heavily selected sample. 
3:02:45
But yeah, I could have passed on the whole era  where they were talking about atheism versus  
3:02:54
religion nonstop. That was pretty crazy. But I do  hear good things about the golden age of blogging. 
3:03:00
Anybody who was sort of  counterfactually responsible   for you starting to blog or keeping blogging? So I owe a huge debt of gratitude to Eliezer  
3:03:08
Yudkowski. I had a live journal before that.  But it was going on LessWrong that convinced  
3:03:16
me I could move to the big times. And second  of all, I just think I learned I imported a  
3:03:21
lot of my worldview from him. I think I was the  most boring normie liberal in the world before   encountering LessWrong. And I donâ€™t 100% agree  with all LessWrong ideas, but just having things  
3:03:34
of that quality beamed into my head and for me  to react to and think about was really great. 
3:03:41
And tell me about the fact that you could be and  were at some point anonymous, I think for most  
3:03:51
of human history, somebody who is an influential  advisor or an intellectual or somebody. Actually,  
3:03:56
I donâ€™t know if this is true. You would have  had to have some sort of public persona. And  
3:04:02
a lot of what people read into your work is  actually a reflection of your public persona. 
3:04:08
Sort of. The reason half of these  ancient authors are called things   like Pseudo Dionysus or Pseudocelsus is that  you could just write something being like,  
3:04:16
â€œoh, yeah, this is by Saint Dionysusâ€. And  then, I donâ€™t know, you could be anybody. 
3:04:21
And I donâ€™t know exactly how common that was in  the past. But yeah, I agree that the Internet  
3:04:28
has been a golden age for anonymity. Iâ€™m a little  bit concerned that AI will make it much easier to  
3:04:34
break anonymity. I hope the golden age continues. Yeah, seems like a great note to end on. Thank you  
3:04:43
guys so much for doing this. Thank you.  Thank you so much. This was a blast. Yeah, I had a great time.  Huge fan of your podcast. Thank you.