# 51 Charts Explaining AI in 2026

Today we are looking at 51 charts that tell the story of artificial intelligence heading into next year. We are in the midst of end-of-year episodes, a combination of both looking back and looking forward. This episode sits right at that intersection. There are charts that tell us where AI is today and give us some idea of what we should be planning on heading into 2026. Given that there are 51 of these things, I am going to rip through them.

A quick note on the production of this: the charts were all sourced entirely by me. Part of my process for preparing this show is spending a ton of time on X/Twitter and using those bookmarks heavily. I have a folder where I actually keep these types of charts. Step one was going back and looking at the charts that I thought were most reflective of the current moment and had something to say about the year we're heading into. The second part of the process was outlining a somewhat rough organization of which charts I wanted to include. From there, I turned it over to Claude, ChatGPT, and Gemini to see how they would organize it. I liked Opus 4.5 best, so we went with that with a few tweaks. Then I handed that and the charts off to Gen Spark and Manis to put it all together. While Gen Spark looked much better, it made some really weird leaps in terms of how it was describing things and had some errors. Ultimately, we went with Manis, which was then exported to Google Drive for a final edit by me. I just think a lot of you are also interested in the operator and production side of AI, so I like telling you how these things get put together.

We've divided this into seven categories: capabilities, infrastructure, markets, economics, vibe coding, jobs, and politics.

## Capabilities

We kick off with capabilities. The first chart comes from Open Router and shows the reasoning versus non-reasoning token trends over time. Basically, at the beginning of 2025, reasoning models were not yet really a thing. OpenAI had announced O1 preview back in September, and it had finally become available at the very end of December, but we were just starting to get our hands on these things. That would change dramatically over the course of the last year, and by November of 2025, reasoning tokens represented meaningfully over 50%. This has brought with it new capabilities, new use cases, and new ways of thinking about how we scale.

Our next chart is the one that for much of this year held up the entire world. This is the chart from Metr that measures the time horizon of software engineering tasks that different LLMs can complete at 50% and 80% success rates. The task duration here is not how long the model works for independently—it's how long in human equivalent time a task can complete. Coming into this year, Metr had shown a doubling of capability roughly every seven months, but it had started to inch up to closer to four months, and this year reified that four-month doubling time. In these charts, you can see the seven-month doubling line in green and the five-month doubling line in red. At 50%, it hews really closely to the four-month line. At 80%, it's mostly on the four-month line with a few recent ones in between the four and the seven-month line. Whether it's seven months or four months, the point is capabilities have not plateaued. They continue to increase dramatically and quickly.

We are also seeing major efficiency gains. This chart shows the performance efficiency of Gemini 3 Flash, which is better performing than Gemini 2.5 Pro—which was state-of-the-art just a few months ago—for around a third of the cost. Especially as we move into a world where production workloads are getting bigger and bigger and we are consuming more tokens, the fact that it's not just capabilities but also efficiency and cost that are improving is a big deal.

Another measure of the efficiency gains came with GPT-5.2's performance on the ARC AGI1 exam. The ARC AGI benchmark folks noted that between a tweaked O3 model last year and GPT-5.2 this year, there was a 390% efficiency gain in a single year.

Now, what this all adds up to in terms of when we get AGI is kind of anyone's guess. As you can see from this chart, people are all over the place in terms of when they think we're actually going to get AGI. By the way, there's no common definition of AGI, and there are even plenty of folks out there who think that the term is getting more and more meaningless. One interesting note is that I think if anything, people's timelines actually got moved back slightly heading into 2026 from where they were heading into 2025, despite all these capability gains. Andrej Karpathy in particular, in a big interview he did, might have single-handedly set back the timeline a couple of years.

As we look at these new releases, they are not just incremental. In many cases, they are solving key challenges. The charts we have here are for a long context test that basically tests how an LLM's performance degrades the more context you give it. With GPT-5.1, which is like a month old at this point, you can see the performance on this test went from around 85 or 90% at 8K tokens to a little under 50% at 256K tokens. Whereas with GPT-5.2 thinking, it was at 100% to start and stayed very close all the way to the end. This makes that context window actually usable in a way that it just wasn't before, which is extremely powerful and opens up new use cases.

Still, AI capabilities, as much as they are evolving, are not evolving evenly. There are a bunch of different versions of this chart that have different size spikes depending on how good you think AI is. But the idea here is that AI progress is not uniform. It is instead jagged, where a model can be superhuman at certain tasks and unbelievably incompetent at basic things that a kid could do. This jaggedness is a key facet of AI and is part of the challenge in implementing it well.

Indeed, when it comes to what slows down AI, there are a set of three different charts I want to show you around capabilities. The first one is on the SWE bench, and it's what was called the unhobbling curve. The version of an LLM that you get out of the box, straight out of the box, has a bunch of impediments to its performance relative to how it might do if you do a bunch of modifications and hand-holding and additional architecture around it. What people have been finding as this problem set has gotten harder and harder is that there's actually still a huge amount of room for improvement just through more sophisticated implementation.

Another really interesting chart shows the difference in LLM performance when generating Python code. If you ask an LLM to write something in Python using a self-contained mode—meaning it's basically just iterating and it doesn't have the ability to test—you see a pretty big drop-off when you get from the simple problem solving to expert problem solving. In contrast, when you give the LLM the ability to actually execute Python and run tests against their solutions, expert problem solving gets really, really good. This tool use matters a lot.

I've also seen some really interesting charts looking at the challenges of reasoning models as tasks get harder and harder. The charts show that reasoning models can crush straightforward problems, but as problems become increasingly challenging—especially when you allow reasoning to go on indefinitely—it often just loops and doesn't necessarily get better. What SaKana was able to show was that when you combine better workflow around how the model is iterating with the scaling of reasoning, you get both accuracy and efficiency gains. These charts have collectively become known as the unhobbling charts, and the message has been twofold: capabilities are making big leaps forward and becoming more usable, but we're still bottlenecked by actually being able to effectively verify and test our processes.

## Infrastructure

Moving on to infrastructure. We kicked off this section with a chart that has gotten a lot of airtime—the planned hyperscale data center spend, which surged from around $75 billion in 2022 to well over $300 billion in 2025. These are incredibly big numbers and show the optimism and the energy that is driving the entire industry at the infrastructure layer.

Another big infrastructure story this year has been power constraints and whether there would be enough energy to satisfy these buildouts. The White House early this year pulled together what was basically a list of all the plants that were being brought online or recommissioned to help with data centers. All of these plans add up to somewhere in the neighborhood of 35 gigawatts. When you set those against the expected data center power demand, that 35 gigawatts—which seemed like an absolutely massive number when this list first came out earlier this year—is easily going to get soaked up by demand.

Also on the infrastructure side, we had a new benchmark that was released this year—let's call it not new, but more well-known this year—looking at data center efficiency. This chart looked at the FLOP per watt in data centers over time and noted the implications of new chip architectures. On the Blackwell chip from Nvidia, which came out this year, there is a four-times improvement in the operations per watt versus the previous generation H100.

Speaking of Nvidia, Nvidia had, to put it mildly, a pretty good year. This chart looks at their revenue divided by GPU and data center and other revenue, and data center revenue has just shot the moon over the last two and a half years. In fiscal year 2022, they did around $11 billion in data center revenue. In fiscal year 2025, they did just shy of $130 billion.

So what does all of this infrastructure spend actually get you? It's a mix. This is Anthropic's chart in particular, and it looks at a few different variables, looking at the level of spending on compute. The first one is this chart on the left, which is how much spending gets you based on how much of algorithmic efficiency gains you believe we're about to realize. The second is how much do you believe compute will continue to scale at its current rate. On the left-hand chart, assuming everything continues to double every five months and we get a 100x improvement in algorithmic efficiency, we could end up with a GPT-7 level model very quickly. In contrast, if we believe that capabilities are going to slow to doubling every two years and algorithmic efficiency improvements turn out to be modest, then our GPT-7 equivalent could take as much as seven years. So depending on which version of the world you believe we live in, things look very different.

Where we should probably actually expect to be is somewhere in between those two extremes. Assuming that compute scales at its current rate and that algorithmic efficiency gives us a 10x efficiency improvement, you're looking at about two to three years to be able to get to a GPT-7 level system, which is three model generations ahead.

Another interesting chart from Nathan Lambert looks at the two poles of spending on compute—whether that's for research and development or whether that is for inference. Where we are today is in an extremely weird place. Inference is really cheap on a per-query basis. That means that the big hyperscalers are all basically spending their money on R&D, or to put it another way, not monetizing chatbot usage while all of the startups are seeing inference as their main cost. So you have this mismatch where the hyperscalers are building better and better models, driving down the cost of serving queries. Startups are starting to get crushed because they have a much larger relative cost on a per-query basis.

## Markets

This brings us to markets and investment. If you take together all of the rounds that Anthropic, OpenAI, and xAI have raised, they total around $30 billion over the last two years. That is a massive amount of capital that's been put to work. And in Anthropic's case, we did two rounds in a very short period of time.

For early-stage investors, we saw in 2023 and the beginning of 2024 an incredible opening and exuberance around infrastructure plays. We've since started to see an important shift toward application layer plays. Q1 of 2024 was the peak for infrastructure spend. Since then, we have seen applications pick up while infrastructure has fallen off. My own guess is that we will see one more infrastructure surge, but that we're going to be in an application-defined period for the next couple of years.

We should also note that the top end of the market has been absolutely incredible. This chart looks at the top ten AI deals of 2024 in terms of valuation and shows valuations from around $2 billion up to around $157 billion for OpenAI. We have a number of companies that are in the $4 billion to $24 billion valuation range—Perplexity, Anthropic, Mistral, Glean, Databricks, CoreWeave, Crusoe, Scale AI. The difference between this and the late 2021 froth is that the actual usage and revenue metrics are real. These are companies that are seeing massive numbers in terms of users, revenue growth, and market share.

Speaking of usage, ChatGPT is at 300 million weekly active users at this point. That is roughly equivalent to the size of Twitter before it was bought by Elon. To get a sense of how other models rank in terms of usage, this chart looks at different chat models and the number of conversations or chats they did per month as of October of this year. ChatGPT did a billion, Claude did 150 million, Gemini did 125 million, and things drop off from there pretty dramatically.

Importantly, when it comes to what's actually driving people, there's been some debate. This chart, which is a proprietary tracking dataset of the different websites that Frontier models drive traffic to, shows that the overall volume of queries is going up. But if you break that down, the question is: is it going up because of an increased volume of chatbot users and increased usage frequency, or is it going up because of an absolute growth in the number of people using chatbots? Interestingly, in the wake of GPT-5 and Claude Sonnet 4, we saw absolute user growth accelerate for the first time in six months.

Also on the market side, competition is extremely fierce between the labs and between the models. This chart, which is a version of a chart that we use internally, looks at where different models have ranked over time. What's been interesting this year is that at the top of the leaderboard—which is designed to measure coding performance since that is the use case everyone cares about most—we have seen Google, OpenAI, and Anthropic trade places multiple times. Basically, every single model release by one of these labs is kicking the others off the top of the leaderboard. This is creating a very healthy and competitive environment with a lot of momentum and urgency driving releases.

So who's going to win? Polymarket had a market on which lab would release the best model at the end of the year. As of a couple of weeks ago, OpenAI was leading with around a 72% likelihood, DeepMind with around 20%, Anthropic with around 6%. It's worth noting that since then, things have changed pretty dramatically. Gemini 3 has come out, Claude Code has come out, DeepSeek V3 has come out. The market has gotten a lot more interesting.

At the same time, China is surging in terms of model quality. Just in the last couple of weeks, we've seen the releases of DeepSeek V3, which is an absolutely incredible model—an open-source model that is on par with the best proprietary frontier models at an absolute fraction of the cost to train. We also have the Qwen models, which have been getting better and better. This chart shows how quickly Chinese LLMs have been catching up to American LLMs. In fact, this entire section of China surging should probably get its own standalone 51 charts episode. There's a lot going on there.

## Economics

Let's talk about the economics. One of the most important things happening in AI right now is that we are really starting to see enterprise penetration. This chart looks at how many businesses said they are actively using AI in at least one business function. That number has gone up from around 55% in early 2023 to around 72% today. When you break that down by what they're getting out of it—what degree of cost decrease or revenue increase they're seeing from it—the numbers are pretty incredible. In the survey, 81% said they were seeing significant or moderate value. Of that, 42% said significant value. Only 8% said no value. That's a really healthy set of numbers that suggests that this is being used and is seeing real ROI.

There are, however, some real challenges around agent adoption. This chart, which is from Gartner, shows hype cycles. The hype cycle for agentic AI is basically at the peak of inflated expectations, with a trough of disillusionment ahead. There's a similar chart from a16z that basically notes that agents are the area where the most promise exists but also where the least maturity exists. This is one of the big narratives that we are going to carry into 2026: on one hand, AI coding works amazingly well—we can actually deploy AI for a lot of use cases—and yet there are still limitations and bottlenecks that we are going to have to work through to get it to work more effectively.

## Vibe Coding

All of which brings us to vibe coding, which is absolutely the story of this year. On SWE bench—a benchmark designed to test how coding models do when asked to solve real GitHub issues—we saw scores go from 4.8% at the beginning of 2024 to 50% at the end of the year. In less than 12 months, we saw the ability to solve real coding issues improve tenfold. That is absurd progress.

That progress brought with it a bunch of new products, a bunch of new businesses, and a whole ton of usage. This chart looks at the number of developers using different AI coding tools. GitHub Copilot at 2.3 million, Cursor at 126,000, Windsurf at 56,000, Replit at 43,000. And we saw multiple companies surge into the nine figures of revenue. Some like Cursor creep up on a billion dollars in ARR, and some like Claude Code blow past that. The combination of meaningful token cost and high consumption, plus implications for other use cases in LLMs, made coding-related performance become the industry's number one priority.

As we head into next year, however, engineering organizations are trying to figure out how to redesign themselves around AI coding. A chart that I've seen from Swyx, Shawn Wang, and others is this semi-async valley of death chart. It looks at agent autonomy and measures the experience or observed productivity at various autonomy levels. On one end of the spectrum, when coding agents are extremely responsive in very fast order, they can be extremely valuable for deep work focused on the hardest problems. On the other end of the spectrum, when they have a lot of autonomy, they can be great for simpler tasks that are handled in the background. The challenge is in the middle range, where as the chart puts it, it's not enough to delegate and it's not fun to wait.

Different organizations are handling this differently, and I think Swyx even has some questions around whether this is exactly the right way to think about things. But for our purposes, this chart represents not just the semi-async valley of death, but the broader set of questions that engineering organizations are going through heading into next year to redesign themselves around AI coding. The reason that this matters outside of software engineering is that I believe that they are the first department that will fully reorganize themselves around AI capabilities and in so doing set a template that other departments and functions can start to follow.

Another interesting chart showing the impact of vibe coding: after a long period of being flat in 2024 and 2025, we started to see the number of apps and games released to the App Store going back up. The jump in 2025 was particularly acute, going up 25% in a year. Some are attributing this to the rise of vibe coding, and I think that's right.

## Jobs and Politics

Now for our last two sections, we move into the society-level issues and the charts that will shape some of the big debates that we're about to have.

This chart has been absolutely everywhere. The idea of a K-shaped economy, where stocks and asset owners are doing great and everyone else is doing not so great, has become fairly standard belief at this point, and there are many who want to attribute it to the launch of ChatGPT. Now, there are a ton of other factors like the rate hiking cycle and the return to the mean after post-COVID over-hiring. But when it comes to politics and society-level conversations, narratives can often matter more than nuance. And there are some parts of the economic challenge for people, whether attributable to AI or not, that are undeniable.

For example, we have the highest youth unemployment rate we've had since about 2015, if you don't take into account the COVID spike. What's more, to the extent that we are seeing patterns that are maybe actually attributable to AI, it does look like early career folks are being hit the hardest. This is a chart of the headcount over time organized by different career sectors. You could see towards the end of 2022, there's a divergence between the mid and senior career folks, with early career really falling off. To the extent that AI is taking on all of the junior tasks, there is going to be a really interesting challenge for us around how people bridge from their early career to their mid-career.

Some folks are starting to think about where the job disruption is likely to come. There were about a million studies this year that were less studies and more predictions of which types of jobs are going to be most subject to disruption. One really valuable chart came out of Stanford, which divided tasks and roles based on where workers desired automation and based on where AI was actually capable of automation. Roles and tasks where workers desired automation and where AI was capable is what they called the green light zone. Tasks where automation desire was high but automation capability was low, they called the R&D opportunity zone. Tasks where capability was high but desire was low is what they called the red light zone. Unfortunately, some others looked there and found that a lot of, for example, Y Combinator startups were working in that red light zone, which I think more than anything reflects just the fact that we need to be having these conversations more around where we actually want automation.

As narratives take hold of AI labor disruption, some studies are also pointing out that counterfactually, that's not necessarily the only thing that's showing up. A recent study, for example, showed that in terms of both wage growth and overall job growth, occupations with high AI exposure, at least right now, are growing much more significantly than those with low exposure.

All of which sets us up for the politics conversation. Merriam-Webster's word of the year was slop. I had tweeted that I think that it tells you all you need to know about the difference of our perspective inside the AI industry than outside that it was slop and not something like vibe coding that was the word of the year. And yet, as much as it seems like AI is going to become an issue, at least for right now, most folks don't rate it super highly as something they care about. Only 7% of people polled had AI in their top five most important issues.

That said, they definitely don't want companies to have a free hand. Recent polling around the White House executive order to ban state-level regulation had pretty strong opposition: 55% opposed to just 18% supporting it, with 27% not sure. And while broadly the issue may not be clear, data center politics are starting to emerge as a local issue. It's still very nascent, but in a couple of elections we saw this year, it was a meaningful part of the discourse. I would expect to see a lot more of that heading into the midterms next year.

So there you have it, my friends: 51 charts that explain AI heading into 2026.

---

*Note: All preview content, advertising, and promotional material has been removed from this transcript. The content begins at its natural intellectual starting point.*
