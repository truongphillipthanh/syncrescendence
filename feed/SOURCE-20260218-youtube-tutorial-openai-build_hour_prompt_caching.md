# Build Hour: Prompt Caching

**Channel**: OpenAI
**Published**: 2026-02-18
**Duration**: 56m 4s
**URL**: https://www.youtube.com/watch?v=tECAkJAI_Vk

## Description (no transcript available)

Build faster, cheaper, and with lower latency using prompt caching. This Build Hour breaks down how prompt caching works and how to design your prompts to maximize cache hits. Learn whatâ€™s actually being cached, when caching applies, and how small changes in your prompts can have a big impact on cost and performance. 

Erika Kettleson (Solutions Engineer) covers:
â€¢ What prompt caching is and why it matters for real-world apps
â€¢ How cache hits work (prefixes, token thresholds, and continuity)
â€¢ Best practices like using the Responses API and prompt_cache_key
â€¢ How to measure cache hit rate, latency, and token savings
â€¢ Customer Spotlight: Warp (ttps://www.warp.dev/) led by Suraj Gupta (Team Lead) to explain the impact of prompt caching


ðŸ‘‰ Prompt Caching Docs: https://platform.openai.com/docs/guides/prompt-caching
ðŸ‘‰ Prompt Caching 101 Cookbook: https://developers.openai.com/cookbook/examples/prompt_caching101
ðŸ‘‰ Prompt Caching 201 Cookbook: https://developers.openai.com/cookbook/examples/prompt_caching_201
ðŸ‘‰ Follow along with the code repo: http://github.com/openai/build-hours
ðŸ‘‰ Sign up for upcoming live Build Hours: https://webinar.openai.com/buildhours

00:00 Introduction
02:37 Foundations, Mechanics, API Walkthrough
12:11 Demo: Batch Image Processing
16:55 Demo: Branching Chat
26:02 Demo: Long Running Compaction
32:39 Cache Discount Pricing Overview
36:03 Customer Spotlight: Warp
49:37 Q&A
