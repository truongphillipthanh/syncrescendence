# Large Language Model Reasoning Failures
Stanford and Caltech researchers just published the first comprehensive taxonomy of how llms fail at reasoning
not a list of cherry-picked gotchas. a 2-axis framework that finally lets you compare failure modes across tasks instead of treating each one as a random anecdote
the findings are uncomfortable
(Description: Research paper cover image showing the title "Large Language Model Reasoning Failures" published in Transactions on Machine Learning Research (01/2026). Authors: Peiyang Song (California Institute of Technology, Stanford University; psong@caltech.edu), Pengrui Han (Carleton College; barryhan@carleton.edu), Noah Goodman (Stanford University; ngoodman@stanford.edu). Reviewed on OpenReview: https://openreview.net/forum?id=vnX1Wt4Nmz. 
Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the **first comprehensive survey dedicated to reasoning failures in LLMs**. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning **failures** along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide easy access to this area.)
**Post metadata:**
- Posted: 9:45 AM Â· Feb 14, 2026
- Views: 120.1K
- Replies: 38
- Reposts: 371
- Likes: 1.7K
- Bookmarks: 1.7K