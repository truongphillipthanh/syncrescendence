{"atom_id": "ATOM-SOURCE-20260202-004-0001", "source_id": "SOURCE-20260202-004", "category": "concept", "content": "AI Engineering is a distinct discipline from traditional Machine Learning Engineering, primarily focused on building applications atop foundation models rather than training models from scratch.", "line_start": 6, "line_end": 8, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.7, 0.1, 0.1, 0.2, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0002", "source_id": "SOURCE-20260202-004", "category": "framework", "content": "The AI Engineering Roadmap 2026 outlines nine key areas: Understanding Foundation Models, Prompt Engineering, Retrieval Augmented Generation (RAG), Evaluation and Testing, Agents and Tool Use, Structured Outputs and Data Extraction, Guardrails and Safety, Observability and Monitoring, and AI System Architecture.", "line_start": 10, "line_end": 79, "chaperone": {"context_type": "method", "argument_role": "claim", "tension_vector": [0.4, 0.6, 0.1, 0.2, 0.7, 0.6], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0003", "source_id": "SOURCE-20260202-004", "category": "concept", "content": "Foundation models (e.g., GPT-5.2, Claude, Gemini, Llama) are pre-trained models that serve as the building blocks for modern AI applications, differing from traditional ML where models are trained from scratch.", "line_start": 12, "line_end": 15, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.2, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0004", "source_id": "SOURCE-20260202-004", "category": "praxis_hook", "content": "To understand foundation models, create a Python notebook to send the same 10 prompts to different models (e.g., Gemini API, Groq for Llama, OpenAI playground) and compare their responses for quality, speed, and style.", "line_start": 17, "line_end": 20, "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0005", "source_id": "SOURCE-20260202-004", "category": "claim", "content": "Prompt engineering, using techniques like few-shot learning, chain-of-thought, and structured outputs, can significantly improve AI application results without model training.", "line_start": 23, "line_end": 26, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.7, 0.1, 0.2, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0006", "source_id": "SOURCE-20260202-004", "category": "praxis_hook", "content": "To practice prompt engineering, pick a task (e.g., summarizing an article), write five different prompts (zero-shot, few-shot, chain-of-thought, persona-based, structured output), test each on 10 examples, and score results in a spreadsheet.", "line_start": 28, "line_end": 31, "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0007", "source_id": "SOURCE-20260202-004", "category": "claim", "content": "Retrieval Augmented Generation (RAG) is a common pattern for production AI applications that grounds LLMs in specific data, addressing knowledge cutoffs and hallucinations, and requires understanding chunking strategies, embedding models, vector databases, and retrieval metrics.", "line_start": 34, "line_end": 37, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0008", "source_id": "SOURCE-20260202-004", "category": "praxis_hook", "content": "Build a simple RAG app over 5-10 markdown notes or text files using an agentic framework with ChromaDB to split documents, embed them, and query with natural language.", "line_start": 39, "line_end": 42, "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0009", "source_id": "SOURCE-20260202-004", "category": "claim", "content": "Systematic evaluation and testing, including building eval datasets, choosing metrics (accuracy, helpfulness, safety), running A/B tests, and detecting regressions, are crucial for measuring and improving AI applications.", "line_start": 45, "line_end": 48, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0010", "source_id": "SOURCE-20260202-004", "category": "praxis_hook", "content": "Create a simple evaluation script by making a JSON file with 20 question-answer pairs, then write a Python script to run a prompt against each question, compare outputs to expected answers (or use an LLM-as-judge), and print a score.", "line_start": 50, "line_end": 53, "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0011", "source_id": "SOURCE-20260202-004", "category": "claim", "content": "Agents extend LLMs to perform actions like browsing the web, executing code, and querying databases, requiring an understanding of agent architectures (ReAct, function calling, planning), tool design, and failure modes.", "line_start": 56, "line_end": 59, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.7, 0.1, 0.2, 0.6, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0012", "source_id": "SOURCE-20260202-004", "category": "praxis_hook", "content": "Build a calculator agent using OpenAI function calling or an agentic framework, giving the LLM access to basic arithmetic functions to answer math questions.", "line_start": 61, "line_end": 64, "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0013", "source_id": "SOURCE-20260202-004", "category": "claim", "content": "Structured outputs and data extraction, using techniques like JSON mode, function calling, and constrained generation, are essential for integrating LLM outputs with downstream systems and bridging conversational AI with software engineering.", "line_start": 67, "line_end": 70, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.3, 0.7, 0.1, 0.2, 0.7, 0.8], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0014", "source_id": "SOURCE-20260202-004", "category": "praxis_hook", "content": "Create a recipe extractor that takes a recipe from a website and outputs structured JSON (ingredients, quantities, steps, times) using OpenAI's structured output feature.", "line_start": 72, "line_end": 75, "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0015", "source_id": "SOURCE-20260202-004", "category": "claim", "content": "Implementing input/output guardrails, PII detection, content filtering, and adversarial testing is crucial for production AI deployment to prevent jailbreaking, harmful content, or sensitive information leaks.", "line_start": 78, "line_end": 81, "chaperone": {"context_type": "consensus", "argument_role": "claim", "tension_vector": [0.2, 0.8, 0.1, 0.1, 0.6, 0.9], "opposes_atom_ids": []}, "extensions": {}}
{"atom_id": "ATOM-SOURCE-20260202-004-0016", "source_id": "SOURCE-20260202-004", "category": "praxis_hook", "content": "Wrap a chatbot with simple guards: check input for prompt injection attempts using keyword matching or a classifier, and check output for blocklisted words, logging flagged messages.", "line_start": 83, "line_end": 85, "chaperone": {"context_type": "method", "argument_role": "evidence", "tension_vector": [0.3, 0.6, 0.1, 0.2, 0.9, 0.7], "opposes_atom_ids": []}, "extensions": {}}
