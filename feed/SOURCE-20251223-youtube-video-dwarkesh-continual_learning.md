# What Are We Scaling? The Continual Learning Bottleneck

## Executive Summary

Provocative critique of short-timeline AGI predictions: if we're close to human-like learners, then current RL scaling approaches are fundamentally misguided. The core observation: "Human workers are valuable precisely because we don't need to build in the bespoke training loops for every single small part of their job." Models lack on-the-job learning, which is why economic impact lags capability benchmarks. Economic diffusion lag is "cope"â€”if these were true AGI, they'd integrate faster than human hires. Prediction: labs will make progress on continual learning by 2030 but won't automate all knowledge work.

## Key Insights

### The Continual Learning Paradox
"I'm confused why some people have super short timelines yet at the same time are bullish on scaling up reinforcement learning atop LLMs. If we're actually close to a human-like learner, then this whole approach of training on verifiable outcomes is doomed."

### The Macrophage Example
A biologist identifies macrophages in slides as part of her work. The AI researcher says this is "textbook deep learning." But: "It's not net productive to build a custom training pipeline to identify what macrophages look like given the specific way that this lab prepares slides, and then another training loop for the next lab-specific microtask."

### Economic Diffusion as Capability Test
"If these models actually were like humans on a server, they'd diffuse incredibly quickly. They could read your entire Slack and Drive within minutes. And they could immediately distill all the skills that your other AI employees have."

### Goalpost Shifting is Justified
"If you showed me Gemini 2.0 in 2020, I would have been certain that it could automate half of knowledge work. So we keep solving what we thought were the sufficient bottlenecks to AGI... And yet we still don't have AGI."

### The Broadly Deployed Intelligence Explosion
"The future might look like continual learning agents who are all going out and they're doing different jobs and they're generating value, and then they're bringing back all their learnings to the hive mind model which does some kind of batch distillation on all of these agents."

### RL Scaling Skepticism
Cites Toby Ord: "We need something like a million-x scale up in total RL compute to give a boost similar to a single GPT level."

## Quotable Passages

> "Models keep getting more impressive at the rate that the short timelines people predict, but more useful at the rate that the long timelines people predict."

> "The reason that labs are orders of magnitude off [tens of trillions in revenue] is that the models are nowhere near as capable as human knowledge workers."

> "Humans don't have to go through a special training phase where they need to rehearse every single piece of software that they might ever need to use on the job."

> "We're losing money on every sale, but we'll make it up in volume."

## Integration Notes

- **CANON-30000 (Intelligence Chain)**: Continual learning as key capability gap; economic value as capability measure
- **CANON-30400 (Agentic Architecture)**: Hive mind distillation architecture for continual learning
- **CANON-33100 (Efficacy)**: Economic diffusion speed as AGI test; knowledge work automation timeline
- Novel contribution: "Impressive vs. useful" rate distinction; macrophage problem as embodiment of generalization gap
