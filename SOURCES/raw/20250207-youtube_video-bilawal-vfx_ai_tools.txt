https://www.youtube.com/watch?v=WEsq6D0DkiA
7 AI Tools Revolutionizing Film & VFX Industry
98,364 views  Feb 7, 2025  #ainews #artificialintelligence #aitools
In this comprehensive roundup, 7 groundbreaking tools and research projects that are transforming filmmaking and VFX. Plus, a deep dive into where 3D and video creation tools are headed next. Subscribe for more in-depth analysis of AI and creative technology @bilawalsidhu 

Join My Newsletter: https://spatialintelligence.ai
Connect with me on X/Twitter here: https://x.com/bilawalsidhu
Everywhere else here: https://bilawal.ai
Business inquiries: team@metaversity.us 

00:00 AI Video Mega Update
00:33 Pikadditions & Pika 2.1
03:22 OmniHuman-1 (ByteDance)
06:35 VideoJAM, SAMURAI & Movie Gen (Meta AI)
11:38 MidJourney CEO on Future of 3D & AI Video
16:40 3D + AI Video Techniques
19:37 Topaz Starlight Upscaling
22:56 MoCapade 3.0 Motion Capture
25:50 Future of AI Video Creation
32:07 Unbundling & Rebundling

Links & Resources: 
Pikadditions & Pika 2.1: https://x.com/pika_labs/status/188754...
OmniHuman-1 (ByteDance): https://omnihuman-lab.github.io/
VideoJAM: https://hila-chefer.github.io/videoja...
SAMURAI (enhanced SAM2): https://x.com/bilawalsidhu/status/186...
Movie Gen (Meta AI): https://x.com/bilawalsidhu/status/184...
MidJourney CEO on Future of 3D & AI Video: https://x.com/bilawalsidhu/status/188...
Odyssey ML: https://x.com/bilawalsidhu/status/186...
Krea AI real-time: https://x.com/krea_ai/status/18823968...
Krea Image to 3D to image: https://x.com/OlivioSarikas/status/18...
3D + AI Video Techniques: https://x.com/bilawalsidhu/status/187...
Topaz Starlight Upscaling: https://x.com/topazlabs/status/188749...
MoCapade 3.0: https://x.com/meshcapade/status/18871...
Future of AI Video Creation: https://x.com/bilawalsidhu/status/182...

Bio: 
Bilawal Sidhu is a creator, engineer, and product builder obsessed with blending reality and imagination using art and science. Bilawal is the technology curator for TED Talks, and a venture scout for Andreessen Horowitz. With more than a decade of experience in the tech industry, he spent six years as a product manager at Google, where he worked on spatial computing and 3D maps. His work has been featured in major publications including Bloomberg, Forbes, BBC, CNBC, and Fortune, among others. Bilawal’s journey into computer graphics began at 11, when he fell in love with seamlessly blending 3D into real life footage. Since then, he's captivated over 1.5M subscribers, garnering more than 500M+ views across his platforms. Driven by a mission to empower the next generation of artists and entrepreneurs, Bilawal openly shares AI-assisted workflows and industry insights on social media. When he’s not working, you can find Bilawal expanding his collection of electric guitars.

TED: https://www.ted.com/speakers/bilawal_...
Google: https://io.google/2022/speakers/bilaw...

#ainews #aitools #artificialintelligence

--- 

AI Video Mega Update
0:00
so I'm back from spending a week in San Francisco at Adobe HQ in a16z and let me tell you A lot has happened while I was
0:07
gone P just dropped a new way to add visual effects to existing videos using just an image reference omnium gives us
0:13
the best Glimpse we've seen at image to character animation meta fixed the whole warbly physiology problem plaguing AI
0:20
video generation topaz dropped some crazy new video upscaling tech mesh capap dropped new video to motion
0:26
capture Tech and the mid Journey CEO's take on the future of AI video and 3D all that and more in this episode let's
0:32
get into it all right so P's already been doing some amazing stuff making it very easy to what I call edit reality
Pikadditions & Pika 2.1
0:39
very simple templatized effects that you can apply to any image and of course they've had really good upgrades in
0:45
quality but now they're taking things to the next level of control earlier today Pika dropped Peak editions basically you
0:52
can upload any real world or AI generated video that you've captured and then you give it a reference for an
0:58
object and character that you want to insert into it along with the text prompt and voila you get some phenomenal
1:04
results let's take a closer look at this stuff we're looking at the the flashlight over here the lighting interaction with the rabbit that you've
1:10
inserted is looking really good you got the lion pretty nicely lit jumping in and reacting on the couch you got our
1:17
favorite hippo popping out of a cup over here and it's interesting some of the reflections are pretty good they're not
1:23
perfect but pretty good the eagle lands perfectly on the kid's shoulder over here and of course you got a freaking
1:28
polar bear in your ref refrigerator all right so here's how it works uh you get started I'm just going to pick the sample video for now and then you can
1:35
give it a reference image of something that you want to insert now this is something you could just pull off of Google Bing image search create in
1:41
whatever other tool you like and then you can basically give a prompt to describe the addition that you're trying
1:47
to make of this object or character or whatever it is and then you hit generate and voila here's what you get this is
1:54
kind of wild right like very very simple and you get something that looks fairly impressive especially when you think
2:00
about it you could look at the transparency and translucency of the washing machine's door is being respected too this is kind of wild now
2:07
I'm not going to pretend like this is ready for Hollywood or something like that but Pika isn't targeting professionals they're targeting
2:13
consumers people who would never go through the pain and effort of learning Advanced visual effects techniques but
2:20
that is something I know a little bit about in case you don't know I actually have a second Channel called Billy effects both on YouTube and Tik Tok
2:26
where I have nearly 1 million subscribers and I made a bunch of the effect Laden content both the good oldfashioned way and with AI content in
2:33
the mix and let me tell you this stuff blows my mind so taking you back to this example from 2018 where I teleported
2:40
this TIE fighter in my backyard I had to do a bunch of stuff right like you got to do your 3D camera tracking you got to
2:46
do lighting estimation so the CG objects are lit believably you need to create 3D geometry of the scene so you can have
2:53
everything intersecting like the Tie fighters intersecting uh with these statues over here and add a little bit
2:58
of that depth of field build all of that and I mean all of it is now a video a
3:03
reference image and a prompt we're getting very close to this point of like the same guidance that you would give a
3:09
visual effects artist to create something you can do that with AI videos and what I find very cool about this is
3:14
they're making it super super accessible I am sure we'll see similar techniques popping up in more advanced tools that
3:20
give you that fine grain control that you desire the next thing I want to talk about is omnium this is a paper coming
OmniHuman-1 (ByteDance)
3:25
out of bite dance that's the parent company of Tik Tok cap cut the works and the results are fantastic let's take a
3:31
look at this example of Albert Einstein where you just take a still image of Albert and then pipe in an audio file
3:36
and here's what you get what would art be like without emotions it would be empty what would our lives be like
3:44
without emotions they would be empty of values so a famous classical poet said
3:51
we hate and we love can one tell me why catulus uh science does not answer why
3:59
question now that's amazing right because everything down to the breathing the enunciation uh the micro Expressions on
4:06
his face the hand gestures is all being modeled really really well here's another example emulating sort of the
4:11
Ted Talk form and I know about this because I I had the opportunity to stand on top of the Olympic Podium and feel
4:18
the weight of that gold medal around my neck and this is really really impressive to me like now if you really
4:23
really go up close and you start looking at some of the details of the face uh some of the clothing deformation you can
4:29
find signs that this is uh a generative AI video but the fact that you can do this with just an image and an audio
4:36
file is kind of wild speaking of Ted examples there are tools like hedra that let you do this but bite dance has shown
4:42
us an order of magnitude increase in quality and the ability to model not just the face itself but also upper body
4:49
movement very very realistically just like I'm moving my hands right now now here's what makes it clever instead of
4:55
having separate models for different tasks right like let's say one for Talking Heads another for full body
5:00
movement they created a single model that can handle different types of these
5:05
driving signals signals to drive of the final output whether that's a text prompt image prompt or even pose
5:11
estimation data the real Innovation is how they trained it rather than tossing out all the data that isn't perfect for
5:17
every scenario which is what most approaches do they've got this smart training hierarchy so if you look at all
5:23
these modalities right like text image audio and then pose estimation data they've got this hierarchy where they're
5:28
outlining the stuff that's like strongly motion related right like obviously full body pose would pop into that obviously
5:34
if you've got audio that is more related and there are other signals that are weekly related to motion for for example
5:40
just the image itself now of course the results are super super impressive because you can feed in just a single image and then along with text audio
5:48
maybe even video to condition the movement and get something fantastic and this example really puts it to the test
5:53
this is the actual footage and then here you see what happens if you drive that single image with just image plus audio
6:01
only on the bottom you see image plus Audio Plus body plus hand so you're seeing the pose of the real videoos now
6:07
conditioning the generation and maybe you pull out the body in just the hand so if you notice his torso isn't moving
6:12
exactly the same way but you've got the enunciation in the hands nicely encapsulated I think this paper has
6:18
solved one of the biggest biggest bottlenecks in this field which is the need for pristine label training data
6:25
and so by being sparter about how they use in quotes imperfect data they might have found a path to building far more
6:32
capable general purpose animation models now speaking of Animation a lot of people say but AI will never get good at
VideoJAM, SAMURAI & Movie Gen (Meta AI)
6:39
motion and physics while this paper out of meta AI called video Jam entered the chat and said hold my beer they
6:45
basically introduce motion priors and outperformed Runway Sora cling oh and it
6:50
works on pretty much any video model with minimal tweaks so I think we've all seen the crazy War physiology videos and
6:58
this fixes that problem significantly now let's get into the paper and itself this might look a little bit scary but
7:03
let me tell you basically what it means the key word is Jam joint appearance motion representation so look the
7:09
current crop of video models are really really good at making videos look good frame by frame but they really struggle
7:16
when things move naturally now you obviously seen those AI videos where people have way too many limbs doing a cartwheel or objects just like passing
7:22
right through each other Sora now this team of researchers led by lead author
7:28
Hillis shafir figured out something very very interesting the way we train these models actually makes them Focus too
7:34
much on making the individual frames look really pretty at the expense of
7:39
overall motion so they develop video Jam which basically teaches the model to consider both appearance and motion in
7:46
concert so the key Insight here is having the model learn this joint representation meaning it learns to
7:52
predict not just how the video should look but also how things should move at the same time they also added this neat
7:57
inner guidance system that helps keep the generation uh self-consistent and coherent what's also cool is that it's
8:04
very lightweight this is just a couple linear layers added to existing models but it makes a huge difference just look
8:09
at the left and right comparisons over here I mean this is the meme video we've seen of all the gymnasts and now it
8:16
looks a lot a lot better same thing with this dog jumping over this beam over here or the chef chopping up some sushi
8:22
I mean vo hits this quality already we don't know the exact insides of VO but my God this looks pretty fantastic and
8:29
so the fact that this can be applied this technique can be applied to any video model with minimal adaptions and
8:34
by the way no modification to the training data you're literally getting the model to consider both the
8:40
appearance and the motion at the same time means that most products and services out there could be implementing
8:46
this very very exciting for AI video you know it reminds me of this other paper that kind of went under the radar called
8:51
Samurai which basically takes meta segment anything model to and makes it way better at tracking objects and the
8:58
cool idea here is very very similar where you not only consider where an object is but where it's going right and
9:05
by just doing making this very simple change you don't even need retraining it runs in real time and you can see the
9:11
comparison right here it's staggering this is also a good point to mention that meta is actually accumulating quite
9:16
a lot of generative AI vide te and they're hopefully going to put it to work very soon in commercial products
9:22
and services I mean if you missed movie gen this is one of my favorite papers of
9:27
last year and what's cool about it is that it can handle all these complex visual effects tasks like replacing environments doing set extensions
9:34
realistic lighting interactions I mean let's look at these example videos they're absolutely wild so here you're
9:39
like okay I just want to swap the background with this text prompt and everything's perfect same thing you're doing a set extension you know what
9:45
would otherwise require adding CG objects and motion tracking stuff boom here you can add some like sparklers to
9:51
the hands maybe change the background to Aurora Borealis and you've got the realistic lighting interaction with the
9:57
skin and the face same here if you want to do a headset swap let me swap the headset and put the real face back in or
10:04
swap it with something else entirely and again you can see as the background lighting is changing with this like
10:09
projection mapping visuals you can see the added in visuals very nicely respond
10:14
to those environmental changes now I won't go into the specifics of how this model works but it's really really
10:20
clever how they scaled up synthetic training data but wow they've got quite a repertoire of capabilities and it'll
10:25
be interesting to see how they roll them out now Instagram CEO actually brought this up let's see if we can find it very
10:32
quickly we're working on some really exciting AI Tools For You video creators out there a lot of you make amazing
10:38
content that makes Instagram what it is and we want to give you more tools to help realize your ideas and you should
10:44
be able to do anything you want with your videos you should be able to change your outfit or change the context in
10:50
which you're sitting or add a chain whatever you can think of now all that
10:56
you're seeing here are some of the early research models that we've been exploring and we're hoping to bring some of these into the Instagram app next
11:03
year so keep an eye out more to come so this is really cool right like they're basically using movie Jens so you can do
11:08
all these selective edits that would have been very very painstaking to do in a classical workflow but honestly even
11:15
in a modern generative AI workflow right like you'd be rendering out isolating all these layers you'd be running it
11:20
through like this some comfy UI or video to video workflow then you bring it into after effects to composite it back in
11:26
match the lighting do all of this other stuff if you're doing a background replacement you might go into bble to do
11:33
like PBR estimation all this crazy stuff that you got to do it's going to be increasingly accessible so that's kind
MidJourney CEO on Future of 3D & AI Video
11:38
of cool and that's a nice segue into what the mid Journey CEO David Hull said about video and 3D on the mid Journey
11:45
road map and so one of the interesting quotes I'd like to open with is he said that V2 is not the upper Bound for video
11:52
quality this year it's actually the lower bound and that's kind of mind-blowing to me because of course I've had Early Access to VO2 I made made
11:59
a whole video about it and the quality and Fidelity is just I mean insane like I mean need I say more I mean this is
12:05
just absolutely real hilariously realistic I made this little skit about an alien getting a DUI it's super funny
12:11
according to David this is one of the biggest mental model updates that he's made which is that hey video has
12:17
actually crossed this threshold of usefulness because it's hit this Fidelity that's actually amazing and
12:23
it's probably going to continue to improve this year with the research papers we just went over being a
12:28
testament to this but it may not be the year for cost optimization now I'm not going to rule that out because we saw
12:33
this happen with deep seek while open AI was talking about building $500 billion data centers engineers at a Quant firm
12:40
deep seek said hold my beer I got a far more efficient way but it does put mid journey in interesting spot right like
12:45
obviously they've got this interesting product right now that's very beloved people love the mid Journey aesthetic but people aren't using it for anything
12:52
other than imagery and they haven't really launched anything they've talked a lot about it and what it sounds like
12:57
they're doing is experim exp menting with their own video models inhouse but now strongly evaluating these third
13:04
party models I mean there's a V2 API that's an early access that also includes image to video by the way so
13:10
expect that to drop in products you know and love in the coming weeks and months but that also gives a very interesting
13:15
approach to Mid journey to start launching an interesting video product and it might mirror what some other
13:20
folks are doing for example Korea where they're not actually training their own models they're using let's say something
13:26
like flux or ideogram for image generation and then using something like cling or Luma for video generation and
13:33
this is important for Mid Journey because in case you missed it they're minting like 200 million in AR every
13:39
single year and they're profitable which is very rare in the generative AI space and if they start focusing on these
13:45
video models inhouse that would be cost prohibitive and would absolutely cut into their margins so that either means
13:51
David and midney needs to decide hey maybe we're actually not profitable next year but let's train our own homegrown
13:57
models or they could just go partner this is where David talked about the interesting challenge that they're
14:02
facing which is sort of big companies have the raw talent in compute right if you think about a company like Google
14:08
they've got YouTube to train on they've got a lot of tpus to train on they've got worldclass talent in Google Deep
14:14
Mind and then on the other hand if you take a startup like Luma well funded really awesome but hey not all that data
14:20
not all that compute and so and of course this might be the reason why M Journey actually hasn't shipped anything
14:26
on these lines because what my buddies tell me is like they've been talking about this in all the office hours is that they're totally focused on image
14:33
generation and a lot of the folks internally in that team want to keep it that way rather than focusing on things
14:38
like 3D or video which could be viewed as a distraction and hey look I kind of get it so I think Tim is absolutely on
14:45
the money here it's like yeah you know they could do crappy video but there's no rush for them right but at the same time I have to imagine they're probably
14:52
turning users because all the other stuff is just getting so good even for image generation now it's interesting
14:57
the former pmma Photoshop John Knack who's currently working on Microsoft designer uh basically says that yeah you
15:03
know it's a pretty crowded Space video everyone's trying to do it right like you've got the higher end players vo and
15:09
Sora and so forth clling minmax Etc and then you've got sort of the consumer players like paa hetra vigle Etc and so
15:16
you know it's a pretty crowded space but I'd argue image generation is kind of too but there's so much to be done around image editing graphic composition
15:24
which is where the 3D point is quite interesting they've opened up a new San Francisco office where they're
15:29
presumably spatial 3D capture and 2D capture training now what they might be thinking about doing on the 3D end given
15:35
you know David Hull's own background starting Leap Motion back in the day is kind of what Odyssey is doing which is
15:40
basically turning 2D images into 3D worlds that you can control and explore right and they're doing this in an
15:46
interesting way because the team the founding team has a background in self-driving cars and geospatial 3D
15:52
mapping very similar to my background to be honest and they're using capture rigs like this one that my buddies over at
15:59
mosaic 360 make which is these like heavyduty Global shutter 360 camera rigs with really high-grade sensors to do
16:05
some amazing amazing stuff and so when you have data sets like this at ground level with such insane Fidelity where
16:12
you can like resolve the menu on the facad this is some prime training data
16:17
so from something like this you could basically create a image to 3D World model and that's really exciting right
16:24
because imagine you're sitting in mid journey and you're like wow I really really like this shot I just just want to get a slightly different angle of it
16:31
without everything else changing violently underneath your feet well you could do something exactly like that
16:37
using the type of approaches that they're investing in so that ability to kind of Kit bash something and reframe
3D + AI Video Techniques
16:42
something I think is really really powerful now a way that you can do that today using the tools you already have
16:48
access to is probably something like runway in Korea so in this case I went into Maya and basically modeled out this
16:55
rough scene and then I'm basically reskinning it using runways video to video functionality and the consistency
17:02
within a generation is pretty Immaculate right like so within this like 8sec clip
17:08
everything is very very self-consistent and we're getting everything down to the reflections the refractions all the
17:14
various materials and segmentation the Skybox detail including the Aurora Borealis is looking really really good
17:20
and I love this example of in where you don't even actually need a full 3d let's
17:25
say you just went into cap cut or Final Cut or adobe Prem whatever the heck you use and just like animated this like
17:32
cutout of a car going down uh this freaking heap of trash um and you throw
17:38
it into a videoo video model it'll kind of hallucinate the physics for you which is really really cool and so this like
17:44
two and a halfy compositing can take you a really far away here's another really good example of this let me just zoom in
17:49
on this where you can see it so you've got a bullet just going through this ice cream and you run through video to video Whatever video to video model you prefer
17:56
let's say Runway and you'll get these amazing amazing slow motion bullet swish effects right including the Tracer and
18:04
the trail of the bullet as it comes out on the other end so when you start taking these real world scan here's an example of me taking a neural Radiance
18:11
field or even a 3D gaussian Splat if that's what you prefer these days um running it through video to video again
18:16
gives you this like amazing control to reframe the camera exactly the way you
18:22
want without everything changing So Korea of course has taken this workflow and I mean heck I did a freaking Ted
18:27
Talk on this in 2023 but they productize it to make it very very easy so let's say you're trying to do this Joker thing
18:33
you do like you know text prompt to get a joker modeled and you do image to 3D model itself and then you fine-tune this
18:39
Laura on the likeness of the Joker so you can start very precisely framing up
18:46
exactly what you want versus I don't know spending many many many many many
18:52
minutes rrolling text prompts and hoping you hit the slot machine Jackpot now I
18:57
think Olivia here shows it best where if you're taking the scene where again composition matters even if you're
19:03
focused on imagery let's say mid Journey or you are focused on image generation this gives you so much more fine grain
19:09
control where it's like I want this exact angle of this Ferrari I want this exact angle or oh let me add a moon in
19:15
the backdrop over there and it should look kind of like this I want it to be positioned here not there and as you notice as Olivia is moving the moon
19:22
around right like the rest of the image isn't violently changing that's very very cool and of course when you take
19:28
that final image from LCM Laura because this is a fast real-time model you can run it through a slower model to go do
19:34
image to image and add in all that extra detail now speaking of detail topaz is
Topaz Starlight Upscaling
19:39
rolling out this very cool upscaling model called project Starlite now if you are not familiar with topaz they're ogs
19:47
in the space they've had gigapixel they've had video Ai and until very recently they weren't actually taking
19:53
advantage of sort of the creativity of diffusion models so you upscale and try to stay as true to the course material
19:59
as POS possible but then you can lean on sort of the World Knowledge of diffusion models to creatively upscale this image
20:06
so you know so like this isn't exactly me but you can see in my beard it's adding in all this fine grain detail
20:12
that just makes it look so so much better so freaking cool I love this I love this shot in particular now of
20:18
course the problem with all of this is sort of the temporal consistency problem we talked about with the meta models where these models are very good at
20:24
making every individual frame look good but what about changing is over time and especially what about when you're
20:30
upscaling stuff that has dust scratches you've got jittery artifacts of old
20:35
school footage that you want to upscale very very nicely they've done a really really great job of sort of finding that
20:41
middle point of using the creative upscaling of diffusion models and doing tried and tested Gan base upscaling to
20:48
get some very very cool results here's a great example Muhammad Ali fight back in the day looks really really good and so
20:55
they've got this new diffusion based architecture that uses a six billion parameter model which is actually pretty
21:00
small which also means you can run it locally on your machine which obviously matters for a lot of folks that don't
21:06
want to upload everything into the cloud and you can do some pretty insane quality restoration like all the feather
21:11
detail and this bird over here just Immaculate down to the details of the feathers the rocks in the background oh
21:18
this this looks absolutely fantastic I mean just this this detail is immaculate and so I'm super excited about this
21:24
right because I mean at least I've got a boatload of mini DV and high8 old school footage and I want to go upscale it and
21:30
honestly one of the things that I'm really excited about upscaling to be honest with you is my old school Shore reel oh get ready for some cringe over
21:37
here guys this is this this about to get very cringey folks oh yeah check that out yes I did in fact paint the entirety
21:44
of my wall blue as a kid so I could do these type of visual effects and what's funny is like I look at all of this I'm
21:49
like oh my God not only could I like remaster this stuff using something like uh the new topaz launch but I could
21:56
actually take it into something like Sora and video to video and do like a 10x better job of the kind of stuff
22:02
they're doing and so this is why the meta model is so crazy to me like so in this case I've got an explosion in the
22:07
background behind the glass I got a bullet hole and then I've got this like Star Wars vehicle Landing in the
22:13
background and that was like a whole ass day for me to make this shot um not to
22:19
mention sitting around rendering everything on my old ass machine back then the fact that with that Pika tool
22:24
you can just do it in one go still kind of blows my mind I don't think kids realize how good they have it these days with
22:30
that meta plugin doing this kind of explosion in the background with a nice light wrap you can go take that slide in
22:36
your Park and turn that into a freaking spaceship you can go do CG characters where you become the Terminator and
22:42
you're go duking it out with a bunch of other folks I mean I'm just super excited for what kids are going to do like the next James Cameron is sitting
22:49
in a basement somewhere and is going to use these tools to blow our minds and I literally can't wait in fact I kind of
22:55
want to enable that so now speaking of superpowers another one is motion capture so mesap launched there's a
MoCapade 3.0 Motion Capture
23:00
really really cool new model called mochap 3.0 basically you can input 2D videos and it will identify not just a
23:07
single Avatar but multiple people and essentially give you a rigged 3D
23:12
animation that you can toss into whatever 3D tool you like and be Off to the Races you don't know need those like
23:18
multi thousand T thousands of dollars worth of really crazy uh motion capture
23:24
suits that have those like military grade Imus in them just use freaking video especially for the kind of content
23:29
that you're doing and including not just capturing the animation of the person but also the camera move itself so
23:36
you're probably familiar with Wonder Dynamics now this thing came out like two years ago in fact I headed in my TED
23:41
Talk for 2023 and back then this was state-of-the-art where basically it could you know do full body pose
23:48
estimation in paint you and then swap out this character with very basic lighting estimation and it looks really
23:54
really cool now basically they pulled apart that capability and going back to the example I was showing you earlier
24:00
you can run these animations through a diffusion model and reskin it and so
24:06
this is also huge in terms of the slot machine problem which is hey instead of prompting to get just the right movement
24:13
why don't you take your freaking iPhone do the movement that you want get this sort of placeholder character and run it
24:20
through your video model of choice you can do some very very cool things that way now if you want to even bypass the
24:25
motion capture step right like you can make a frame of an image flux or mid journey and you can do what Jon's doing
24:31
here is take something into vigle and you could take things a step further is take that vigle animation that you get
24:36
as an output and composite it into liveaction footage you could take that image itself toss it into something like
24:42
magnific to relight yourself and splice all of that stuff together I mean so all The Primitives are kind of there to let
24:49
you do some very very cool things and so it's interesting like which is the team that's going to bring this all together
24:54
into one tool clearly Kaa is kind of more focused on the image generation aspect they do offer video generation
25:01
but they haven't really gone deep into it is this an opportunity for Mid Journey maybe some other company like Adobe to jump into the mix here and
25:08
create something that's more closer to an after effects and mayor blender replacement uh than it is a camera
25:16
replacement which is really how I think about most video generation tools like they're replacing the camera the camera
25:21
that's like recording me right now instead of having to record something you're just generating it so it's a
25:26
virtual camera in a sense and really all of these are kind of virtual production techniques but they're getting so democratized cuz again this workflow
25:34
that JN is showing completely bypasses the 3D pipeline right like mid's trained
25:39
on enough 3D tools that you can just generate this still image and then vle good enough at like full body swapping
25:45
that you can take that still image from one perspective and toss it in and get something very very compelling so if I
Future of AI Video Creation
25:51
had to talk about where all this is going next is I think there's a merger of AI and 3D video creation coming now
25:57
I've been wax poetic about this really for the last couple years ever since control net came out really uh but I
26:03
think this post that I did last August encapsulate the best where basically I'm
26:09
dreaming of this 3D creation tool where you've got this unified scene graph right like every time we make a video
26:14
it's like hey these are the three environments in my video These are the two characters in my video These are the
26:20
props in my video this is the time of day and lighting conditions and I would like these things to be respected please
26:26
across all of my Generations now of course you get this stuff for free inside of 3D tools like Maya blender
26:32
even Unreal Engine if you're doing more real-time stuff but I what I want is the ability to sort of gray box storyboard
26:37
out in 3D and get sort of the consistency and get the like fine grain control of this 3D scene graph toss it
26:44
into these massive context window models like Gemini is at what 1 million 2 million token context window that's like
26:50
two hours of video that's a lot of context and then you can use multimodal prompting to breede New Life into it so
26:55
you're kind of in director mode imagine you got your Vision Pro on your you're in like spectator mode on your phone or
27:01
your iPad and you're basically using if you've seen like chat GPT advanced voice like audio in audio out models and
27:08
basically like directing your virtual characters like you would real characters like so you're in that shot
27:14
and you're like hey I want you to walk up to the ramen bar and say this to the chef over there and the chef I want you
27:19
to lean in with this menacing look and say this back Etc and you're there kind of like they're looping the motion
27:25
you're recording exactly the takes and camera angles you want now what's cool about this you toss that all into a timeline editing view but you can still
27:32
go back and make those fine grain edits like hey everything looked good over here but I just want to tweak the camera oh so slightly and all of the techniques
27:39
we're talking about are Primitives or Lego bricks that could plug into this type of workflow so you take advantage
27:45
of 3D models where it matters right like the cause effect relationship physics Shader level control you know object ID
27:51
level control all the stuff that we're used to but then you take advantage of the creativity of diffusion models to take it all the way now imagine this
27:57
would be such a crazy Flow State right like rather than jumping between the 1520 you know the tyranny of this ill
28:05
tailor tapestry of tools you can kind of jump into something far more lightweight
28:11
where you can jump between that timeline level edit and then shot level edits and seamlessly pivot between them so I Scout
28:17
for Andre on horowits and I've had a few companies hit me up that are working on something exactly like this but it's not
28:22
quite there yet and I think the problem is everyone's taken this problem of like visual creation or audio visual creation
28:30
and unbundled it so much that they're solving very specific problems in the
28:35
space and so you're starting to see representations of tools that bring this stuff together kind of like Korea for
28:42
example or LTX so I'd argue that LTX Studio actually has the closest ux like
28:47
this but the models kind of suck uh but maybe they're just one update away from plugging in Google VO2 or something like
28:53
that and suddenly it's completely completely plausible because they have all of the these Concepts that M jour's
28:59
just been talking about they've actually been shipping um it's not as fancy as the techniques that I showed you it's
29:05
more like oh let me just use a depth map so I can like crudely tweak the camera movement and you get all these weird
29:11
stretchy artifacts but you're going to throw a diffusion pass over at anyway and that's kind of going to fix the rest right so I think we're seeing the
29:17
earliest signs of this but as of right now we are not quite there yet now other folks would argue that hey actually this
29:23
is maybe more like you know an llm type of environment if you look at what Luma is doing with their video generation
29:28
models you can kind of have this back and forth conversation ask it to generate different batches and to me
29:34
that's like kind of collapsing the conversation you might have with let's say a designer or like a motion graphics
29:39
artist uh over slack you're basically collapsing that into a back and forth with you and the AI without a human in
29:45
the loop right now others will argue that maybe it's more of like a figma style kind of node-based comfy UI meets
29:52
figma style interface sort of like this where you can lay out all the character animation stuff you can lay out all your
29:58
your character designs do some stencils then you're cranking out some custom variations and then you start doing some
30:03
video generation based on that right at least for video generation I really want some sort of an nle view like a let's
30:09
take the example of Unreal Engine sequencer right like where you can go have this like familiar video editing
30:15
environment then you can go back into more a traditional 3D content creation UI and make other changes but I think
30:21
what's exciting about all of this is in addition to all these visual creation models llms themselves are getting much
30:27
much better like in case you missed it Google rolled out a bunch of new Gemini models this past week including flash
30:33
thinking which is their own version of doing deep seek style Chain of Thought a really really fast Flash models and I
30:39
mean if you're not using AI Studio again 2 million context window you can throw in a lot of audio files video files text
30:46
and query over it but also deep seek itself I mean I use this from my previous video to package it up and like
30:51
gave it a bunch of thumbnails and ask it to think through a bunch of different stuff like it was really really cool now
30:56
between Google stuff deep seek and what opening ey is doing you've got all of these multimodal language models now um
31:04
that are going to be very good at being sort of the orchestration function or maybe even taking on specific personas
31:09
like you could have somebody be like a storyboarding artist and take wield these other tools that we've talked about whether it's using things like
31:16
computer use or direct API which we talked about in our previous video including stuff from bant like UI tars
31:22
we can literally take control of your computer and use Photoshop and premiere for you and then maybe a visual effects
31:28
agent that's very good at taking advantage of like the meta models that I showed you to replace after effects compositing or literally use after
31:35
effects for you these llms are going to be a very important orchestration layer and again like these context windows are
31:41
only going to expand their ability to understand multimodal context is only going to get better but they're also
31:47
capable of outputting multimodal data as well so these models natively will be able to create let's say that audio file
31:54
that then you toss into that bite dance model to animate your character and so you've got one model that has full
31:59
perview into the entirety of the shot list and the script that you have let's say for a short video that you're making
32:06
that's going to be very very exciting all right so to wrap things up given that everyone has unbundled this problem space of audio visual creation into all
Unbundling & Rebundling
32:13
these little Primitives we're heading towards a future where these Primitives are going to keep getting better but they're also going to start coming
32:20
together sort of like the T1000 V2 omnium UI tars mesap topaz the list goes
32:27
on all these tools will come together with a net result with you being able to orchestrate creation at a higher level
32:34
of abstraction you'll basically be like the conductor of the symphony yes you can go in and play the instruments that
32:39
you want to play but if you want to just sit back and orchestrate at a higher level you can do that too all right so that's it for this video thank you so
32:46
much for joining me I'm really happy that this channel has crossed 10K Subs your subscription matters most of you
32:51
are not subscribed based on the analytics so drop a like comment
32:57
subscribe and turn on that Bell icon the support means a lot to me and helps me as I'm growing this channel basically
33:02
from scratch and in case it's helpful for anyone I'll just leave my 2025 creation stack right here to screenshot
33:08
this and jump into a bunch of these tools there's a lot out there and if you want to see a video like this or you want me to see cover other topics that
33:15
you see that I'm covering on Twitter or LinkedIn and not over here let me know I'm still trying to figure out this
33:20
whole YouTube thing and your support and help goes a long way keep the comments coming I read every single one of them
33:26
video a 3D I really appreciate what you're saying this is my version of trying to plug and get you to subscribe
33:32
you're probably most of the folks probably haven't even made it to the end of the video but if you have I would
33:38
appreciate it and that's it for me bavo signing off and I'll see y'all in the next one cheers