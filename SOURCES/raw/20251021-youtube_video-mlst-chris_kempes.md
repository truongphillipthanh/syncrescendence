# The Universal Hierarchy of Life

**Participants:**  
- **Chris Kempes** – Professor at the Santa Fe Institute  
- **Keith Duggar** – Interviewer (Machine Learning Street Talk)

**Context:** A conversation exploring universal theories of life, convergent evolution, assembly theory, and the philosophical frameworks for understanding biological and non-biological complexity.

---

**Chris Kempes:** I'm a professor at the Santa Fe Institute, which is a massively interdisciplinary research institute trying to bring together minds from across the spectrum of academia to think about some of our hardest problems. My own background is that I was a physicist and then I became a biophysicist, and that's taken me into ecology and cities and human organizations and thinking about the whole history of life and also origins of life and astrobiology. That's really the SFI thing—to take a core expertise and use that as a nucleation to expand into all these different topics and areas.

**Keith Duggar:** It's so interesting that you work at the Santa Fe Institute. I spoke with David Krakauer recently, very inspiring gentleman. What you folks are really doing is, I guess you would call it a multidisciplinary lens on science. You were talking about three cultures in science. There's the variance culture where we look at diversity and deviation, the exactitude culture where we have quite a high resolution mapping of everything, and this coarse-grained abstract culture where we look for principles. Even in machine learning we have similar terms for this. We have the neats and the scruffies. How can you reconcile these different aspects of science?

**Chris Kempes:** Part of the inspiration for us in writing that paper was to say if we look at the history of physics, which has had enormous success—part of that amazing success is that it was such a set of easy questions in relative terms. Biology, the economy, intelligence—those are all much harder questions. But for physics, trying to answer relatively simple questions like gravity and planetary motion, they had what I call the magic loop, which is this observation leading to theory, theory leading back to observation. People took huge sets of observations that we had, tried to find regularities, tried to find simple equations that predicted those observations. Once they had those theories, they would explore them mathematically. Often that gave rise to new sorts of surprising predictions, and then experimentalists would go looking for those predictions. Then this loop just continued on and on as people uncovered most of the laws for most of the fundamental forces. Obviously there's still open questions in physics, but that was a very productive framework. 

When you try to do that in other fields, it becomes very difficult because you have much less data in most cases. Or you have the opposite problem—you have way more variables than your experimental manipulations can really encompass or way more things that are confounding each other. What we started thinking about is that there's a kind of natural way to think about different forms of science where some fields need lots and lots of data. They collect lots and lots of observations and they try to find regularities in those observations. This is one of the places machine learning is really helpful. Other fields are more exact and precise—they have very well-controlled circumstances. They have mathematically predictive theories. They're just missing a little bit here and there to fill things in. They might not need a lot of data; they might just need a very clever experiment to resolve a specific question between specific theories.

There's a third class which is more coarse-grained where people are trying to abstract away from all the details and get rid of the details and find organizing principles—something that holds regardless of species or regardless of organism or regardless of time period. Those three cultures each have a different product they're each after. You need to combine them eventually. This is what SFI calls a multiscale approach. You need to understand the high-resolution aspects of things to build intuition for how to do the coarse-graining. You need the coarse-graining to tell the high-resolution people where they should look for details that are going to really matter. The variance tells you what the scope of explanation should be—how general is the principle you're talking about, and where do you need additional principles.

**Keith Duggar:** What makes a good scientific theory?

**Chris Kempes:** That's a complicated question. There's lots of criteria that get brought up. One that Popper was very interested in is falsifiability. Are you making claims that could be shown to be wrong? That's one criterion. I think another is explanatory power. Are you actually getting at mechanisms? Do the mechanisms you're proposing actually make sense given what we know about the substrate—whether that substrate is cells or molecules or planets or whatever? Then there's the unification or generality of the theory. Does it apply to a bunch of things? This is the thing that really excites me—when you can come up with a simple framework that predicts lots of different phenomena, and especially if you're bringing together things that previously seemed disparate.

**Keith Duggar:** You've talked about a universal theory of life. Can you explain what that means?

**Chris Kempes:** The inspiration comes from astrobiology and thinking about life in the universe more generally. We have this one example of life—life on Earth—and it has a very particular chemistry. It has DNA, RNA, proteins, lipids, carbohydrates. We understand that quite well. The question is: if we were to find life elsewhere in the universe, would it have that same chemistry? I think the answer is almost certainly no, especially if it originated independently. It might have completely different molecules doing similar functional roles. But if that's the case, how do we even know what to look for? How do we recognize life when we see it if it doesn't share our biochemistry?

That motivates trying to find a universal theory of life—something that's not committed to the specific details of Earth biochemistry but can capture what makes something alive in a more general sense. The framework I like to think about has three levels. At the bottom level, you have materials—the physical stuff life is made of. This is where we expect enormous diversity across the universe. Life could be made of completely different chemistry, maybe even different physics in some sense. The middle level is constraints—these are the universal laws of physics that all life has to obey regardless of what it's made of. Things like thermodynamics, diffusion, the speed of light. At this level, different forms of life start to look more similar because they're all subject to the same physical constraints.

The top level is principles—these are the abstract optimization processes that drive living systems. Things like evolution through natural selection, learning, computation. These are the highest-level descriptions of what life does, and I think this is really what defines life. It's these computational or algorithmic or optimization principles that make something alive, not the particular substrate it's running on.

**Keith Duggar:** That's fascinating. So the material is at the bottom—that's where we see the most diversity. Then constraints narrow things down, and principles are universal?

**Chris Kempes:** Exactly. At the material level, we shouldn't be surprised if alien life uses completely different molecules. But at the constraint level, everyone has to deal with the same physics. If you're trying to move nutrients around, you have to deal with diffusion. If you're trying to process energy, you have to deal with thermodynamics. These constraints start to make different life forms converge on similar solutions even if they're made of different stuff.

The classic example is the eye. You might think the eye is so complicated that maybe it only gets discovered once and then everything that has an eye has a common ancestor. Not true. Eyes evolve many times. There are differences in the details of eyes—in terms of how exactly they see and how they're constructed and slightly what they're made of—but in some notion they're all doing the same sort of physics. They're all trying to focus light onto a surface that's sensitive to light in order to create an image. The physics of light provides a clear target, and evolution finds similar solutions to the problem of seeing even with different starting materials. That's convergence.

**Keith Duggar:** That convergence is really important. How does life diversify and then converge?

**Chris Kempes:** There's a hierarchy to how we think about this. At the material level, life diversifies enormously. Earth life uses thousands of different proteins, different metabolic pathways, different morphologies. But once you look at the constraint level, things start to converge. All cells have to move molecules around, and diffusion is diffusion regardless of what molecules you're talking about. All organisms have to extract energy from their environment, and thermodynamics constrains how efficiently you can do that.

At the principle level, things converge even more. Whether we're talking about biological evolution, cultural evolution, or even artificial intelligence systems, they're all using similar optimization strategies. They're all exploring possibility spaces, they're all dealing with tradeoffs, they're all subject to selection pressures. These are universal computational principles that apply regardless of substrate.

**Keith Duggar:** You mentioned cultural evolution and AI. Are you saying those are forms of life?

**Chris Kempes:** In the framework I'm describing, yes. If we define life by these abstract principles—evolution, learning, computation, information processing—then human culture looks like a living system. It evolves, it adapts, it processes information, it's subject to selection. It's not made of cells and DNA, but it's instantiated in a different substrate—in this case, human brains and social networks and written records. Same with artificial intelligence. If we build systems that can learn and adapt and optimize, those are exhibiting life-like properties even though they're not biological.

This is a very functionalist view. It's saying that what matters is the function, the process, not the physical substrate. A meme—an idea that spreads through a culture—is analogous to a gene. It has variations, it's subject to selection, the ones that are better at spreading become more common. That's evolution happening in a non-biological substrate. You can even build phylogenies of cultural artifacts the same way you build phylogenies of organisms, tracing descent with modification over time.

**Keith Duggar:** So we're looking at adaptive processes. Is that how you define life?

**Chris Kempes:** Adaptive processes are central. Life is characterized by adaptive information—information that's been shaped by selection to perform some function. Whether that's genetic information shaped by natural selection, learned information shaped by experience, or cultural information shaped by social selection, it's all fundamentally the same kind of process happening at different scales and in different substrates.

The key insight is that these processes have to deal with the same basic problems. They have to explore possibility spaces efficiently. They have to balance exploitation of known good solutions with exploration of potentially better solutions. They have to deal with noise and errors. These are computational problems, and different systems—biological, cultural, artificial—solve them in remarkably similar ways because there are only so many good solutions to these problems.

**Keith Duggar:** You mentioned convergence at multiple levels. Can you elaborate on that?

**Chris Kempes:** Sure. Convergence happens at every level of the hierarchy. At the material level, you see convergence because physics constrains what's possible. At the constraint level, you see convergence because the same physical laws apply everywhere. At the principle level, you see convergence because there are only certain computational strategies that work well for adaptive systems.

A beautiful example is flight. Flight has evolved independently multiple times—in insects, in birds, in bats, in extinct reptiles like pterosaurs. These are very different animals with very different evolutionary histories and different materials, but they all converge on similar solutions: wings with a certain shape, lightweight structures, efficient ways of generating lift. That's because the physics of flight constrains what works. You can't violate the laws of aerodynamics no matter what you're made of.

The same thing happens at more abstract levels. Problem-solving strategies that work well in biological evolution—things like modularity, hierarchy, redundancy—also work well in cultural evolution and in artificial systems. These are principles that emerge from the mathematics of optimization and adaptation, not from the specific details of the substrate.

**Keith Duggar:** Could we simulate life?

**Chris Kempes:** That's a deep question. In principle, yes. If life is fundamentally about computation and information processing, and we understand the principles well enough, we should be able to simulate it. The question is whether a simulation is the same thing as the real thing. This gets into philosophical territory—the question of substrate independence versus substrate dependence.

My view is that if you're capturing the right principles and the right causal structure, then yes, a simulation can be as real as the original. The key is whether the simulation is actually implementing the same computational processes. If you're just making a visual representation of life without actually implementing the underlying adaptive processes, that's not the same thing. But if your simulation genuinely evolves, genuinely processes information, genuinely adapts—then I think it's fair to call that a form of life, even though it exists in a computer rather than in molecules.

The interesting thing is that we're already seeing this with artificial life systems and evolutionary algorithms. These systems exhibit genuine evolutionary dynamics. They're not just metaphorically evolving; they're actually undergoing selection and adaptation. The fact that they're made of bits rather than atoms doesn't make them less real as evolutionary systems.

**Keith Duggar:** What about intelligence and parasitism? Are there spectrums here?

**Chris Kempes:** Absolutely. Both intelligence and parasitism exist on spectrums, and they're interesting to think about in terms of these universal principles. Intelligence is essentially the ability to learn and adapt on fast timescales. Evolution is a form of learning that happens over generations through selection. Individual learning happens within a lifetime. Intelligence is when you can adapt very quickly to new situations without having to wait for genetic evolution.

But there are lots of intermediate points. Bacteria can sense their environment and move toward favorable conditions or away from harmful ones. Is that intelligent? It's certainly adaptive. It's certainly information processing. Whether you call it intelligence depends on where you draw the line, but there's definitely a spectrum from simple reactive behaviors to complex cognition.

Parasitism is also interesting. A parasite is an organism that exploits the adaptive work of another organism. It's outsourcing some of its survival needs to a host. But again, there's a spectrum. Symbiotic relationships can be mutualistic, commensal, or parasitic, and the boundaries are often fuzzy. In fact, many of the most important evolutionary transitions involve what were once parasites becoming integrated into their hosts. Mitochondria, for example, were once free-living bacteria that became parasitic and then became essential organelles.

You can even think about ideas as parasites in some sense. A bad idea that spreads because it's good at spreading, not because it's true or useful—that's parasitizing human cognitive resources. But beneficial ideas also spread by using the same mechanisms. The distinction between symbiosis and parasitism can be context-dependent and can change over time.

**Keith Duggar:** You mentioned phase changes in evolution. What are those?

**Chris Kempes:** Phase changes are sudden, qualitative shifts in how a system is organized. In physics, water turning to ice is a phase transition—you go from one state with certain properties to a completely different state with different properties, and it happens relatively abruptly. I think evolution has similar transitions where you get sudden jumps in complexity or capability.

What we typically see is that you have some set of physical constraints that matter for a class of organisms. These constraints predict how things will change as the organism gets bigger or smaller, and we have nice theory predicting that. Those theories also tell you that at the boundary of those scaling relationships, you often get strong asymptotic behaviors where things go off to infinity or zero. There's some hard limit to a category of organization of organisms. That creates a wall, an evolutionary wall, and that is where a phase transition is going to happen. You need to do something to jump over the wall.

I'm not sure you always jump over the wall. It's not clear to me you always get these evolutionary transitions, but if you're going to get bigger than a certain scale, the theory says you need to invent a certain sort of architecture that gets you away from that constraint. That architecture is characteristically different. 

Going from bacteria to unicellular eukaryotes, you have to put one prokaryote inside another prokaryote. It looks like it was a bacterium inside an archaeon. That's a new sort of architecture. Now suddenly you have two genomes, you have more internal membranes—there's something really different happening there. When you go to multicellularity, true multicellularity where you're differentiating cell types and creating organs and tissues, not just living in a colony, that's another sort of organization that requires a different kind of regulation, communication, developmental programs. That's another phase transition.

**Keith Duggar:** When do these transitions happen?

**Chris Kempes:** Interestingly, when we look at when those jumps happen, they typically happen around huge shifts in Earth history—when the environment of the planet radically changes. We have a paper showing that Snowball Earth, which is this period in Earth history that happened a few times where the planet almost completely covers itself in ice, induced multicellularity. A bunch of physical conditions become just right then to get bigger and become complex multicellulars. You discover that, and then when the world thaws out again, that innovation is around and spreads and becomes bigger. You've found that solution. 

That's a case where there are these transitions, we understand what the wall on one side is, and then maybe sometimes you need an environmental inducement to jump the wall.

**Keith Duggar:** You invoked David Deutsch in your paper. He spoke about the importance of separating matter and logic. What did he mean by that?

**Chris Kempes:** I could look at a computer and just say it's a bunch of electrons moving around. It's some complicated network with a current running over it. A river is a complicated network with a current running over it. What's unique about a computer is that we've built it in a certain way to perform certain logical operations in a repeatable way so that we can give it inputs and outputs. In the case of generalized computers, we can reprogram it and have an arbitrary notion of what inputs and outputs we want—we can write software to it.

I think that's really the key—to say that in the crudest sense, the physical description leaves out a whole bunch of information. To understand certain architectures or why the architecture is there, you would need to bring in a logical or a software notion. That separation is about recognizing that there are multiple levels of description, and sometimes the higher-level logical description is actually more fundamental for understanding what something is doing than the lower-level physical description.

**Keith Duggar:** Can we talk about assembly theory? You've got a paper out about this. It's a way of quantifying complexity by looking at how something can be built step by step. Can you explain what that is?

**Chris Kempes:** Assembly theory is initially aimed at trying to search for life in the universe. A chemist, Lee Cronin, and a theoretical physicist, Sara Walker, and then a bunch of other of us who have gotten involved were interested in this idea: how do you fairly, without being committed to this past knowledge I was talking about before—the biochemistry we have—how do you fairly look for life in the universe?

Assembly theory simply says one way to do that is to look at the recursive use of parts and what the shortest path to build an object is. Here's the thing you're trying to get away from: I hand you a molecule and I say, is this molecule complex? I'm not allowed to tell you anything about the synthesis that we used to make it. I'm not allowed to tell you if a living system made it, if that living system shares our biochemistry or has different biochemistry. How do you decide if that molecule is complex or not?

What we want is something that bounds that complexity—an ultimate bound on that complexity. One way to do that is just to say when you have a synthesis pathway, say you have an evolving set of objects that are following some evolutionary lineage, you invent a set of parts and then the easiest thing to do is take those parts and reuse them in some way. If you invent a new part, that's a more complicated thing to do. With that sort of notion in mind, you could say, I give you an object and you're just trying to find the shortest path to build it where you build up a set of parts and then can recursively use those to make a next set of parts. You ask how often you have to build a new part or use an old part. Each of those counts as a step, and you're just trying to find the shortest path to get something.

That's definitely a bound. Certain processes could be much more complicated in building that up, but it lower-bounds the complexity. By comparing all these lower bounds of complexity, you can fairly compare how complex an object is in the best case, in this shortest-path process.

This gets away from things like if I hand you carbon-60, which has a large molecular weight, you could say that's a very big object. But it's actually not so complex in those terms—in this assembly theoretic perspective. Experimentally, it looks like there's a threshold where you can go from abiotic to biotic. This is work that Lee's done in his lab. Each step in assembly space is in a very rapidly growing space of combinatorial possibilities, so each step is a really big step. You expect a sharp cutoff there somewhere, and it looks like that happens in the experimental data, which is exciting.

**Keith Duggar:** Chris, this has been absolutely amazing. Thank you so much for joining us.

**Chris Kempes:** It's been a blast. Thank you so much.

---

*Note: All preview content, advertising, and promotional material has been removed from this transcript. The conversation begins at its natural starting point.*

**Verification Notes:**

[^1]: **Santa Fe Institute**: An independent research institute in Santa Fe, New Mexico, focused on complexity science and interdisciplinary research.

[^2]: **David Krakauer**: President of the Santa Fe Institute and frequent collaborator with Chris Kempes on papers about life, complexity, and evolution.

[^3]: **Assembly Theory**: A framework developed by Lee Cronin (University of Glasgow) and Sara Walker (Arizona State University, SFI External Professor) to quantify molecular complexity and identify biosignatures. Published in Nature (2023) with Chris Kempes as co-author.

[^4]: **David Deutsch**: British physicist and author of "The Beginning of Infinity" (2011), which explores explanatory knowledge, progress, and the relationship between physical laws and emergent phenomena.

[^5]: **Snowball Earth**: A hypothesis describing periods in Earth's history when the planet's surface was entirely or nearly entirely frozen. Multiple Snowball Earth events occurred between approximately 720 and 635 million years ago.

[^6]: **Carbon-60 (C60)**: Also known as buckminsterfullerene or "buckyball," a molecule consisting of 60 carbon atoms arranged in a soccer ball pattern. Despite its large size, it has relatively low assembly complexity.
