https://www.youtube.com/watch?v=O4wBUysNe2k
Joe Rogan Experience #2404 - Elon Musk
3,420,996 views  Oct 31, 2025  The Joe Rogan Experience
Elon Musk is a business magnate, designer, and engineer known for his work in electric vehicles, private spaceflight, and artificial intelligence. His portfolio of companies includes Tesla, SpaceX, Neuralink, X, and several others.
https://x.com/elonmusk

---

0:01
Joe Rogan podcast. Check it out. The Joe Rogan Experience. Train by day. Joe Rogan podcast by
0:08
night. All day. Exactly.
0:13
Just every morning. Wonder what Jeff Bezos is doing. He's doing some definitely doing some testosterone. He looks jacked.
0:21
He looks jacked, right? Yeah. But he didn't like quick quick quick
0:29
at age like at 50 at age 59 in less than a year he he went from pencilet geek to
0:35
uh looking like a like the rock. Yeah. Like a little miniature alpha fella. Yeah. Like like his neck got bigger than
0:42
his head. Yeah. He got bigger. But then like his earlier pictures his neck's like a noodle. I support this activity. I like to see
0:48
him going in this direction which is fine. And his voice dropped like two octaves. I want you to move in that direction as well.
0:53
I think we can achieve this. I I I mean I should I think we can achieve gigachad.
1:01
That's what people called it. Where is that guy? Bele. Uh I don't know where he is. That's like a real guy.
1:07
The artist. Yeah. No. Oh, gigachad. Oh, gigachad. Yeah. I don't know if that's a real guy. It's
1:12
hard to say. No, it is a real guy. It is a real guy. He's got the crazy jaw and like perfect sculpted hair.
1:17
Yeah. Well, I mean, they may have exaggerated a little bit, but um but uh No, I think I think he actually
1:23
just kind of looked like that in reality. Wow. Um so like like he's a pretty unique looking
1:29
individual. I think we can achieve this. That guy right there, that's a real guy. That's a real dude.
1:35
I always thought that was CGI. No, I think one of I think the upper right one is not him. That's not
1:42
But that one to the left of that like that's real. No, that's that's artificial, bro. That's fake. That's got
1:48
that uncanny valley feel to it, doesn't it? It's It's not impossible. No, no, it's not impossible to achieve,
1:54
but it's not it's not possible to maintain that kind of leanness. I mean, that's like like you're you're also at
2:01
that point they're he's dehydrating and all sorts of things. Oh, it's based on a real person. Yeah. Yeah. Based on,
2:07
right, but it's not a real person. What does he really look like? Like those images, I think, are [ __ ]
2:12
Some of them are. Is that real? Okay. That That looks real. That looks like a really jack bodybuilder. Yeah.
2:18
Yeah, that looks real. Like that's achievable. But there's a few of those images where you're just like, "What's
2:23
going on here?" Yeah. Yeah. Yeah. Totally. Um Well, I mean, you see you see that guy is that is that the
2:30
that's the real dude? Well, there's that that that Icelandic dude who's Thor. Oh, yeah. The guy who jumps in the
2:36
frozen lakes and [ __ ] Well, the guy who played the mountain. Um Oh, that guy. That is that is like a that that is like
2:42
a a mutant strong human. Yes. Like like uh he would be in like the X-Men or something, you know?
2:49
He's just like not like uh and there's that you know that have you seen that meme tent and tent bag?
2:56
Um you know how like it's like it's really hard to get the tent tent in Oh, right. Right.
3:05
That's true. Then there's a picture of of him and his girlfriend. That's hilarious.
3:12
Yeah, that's I don't know how it gets in there, you know? It's like it seems too small. But I met Brian Shaw. Brian Shaw is like the
3:19
world's most powerful man. And he's almost 7 feet tall. He's 400 lb.
3:26
And his his bone density is 1 in 500 million people. So there's one it's like
3:33
there's like maybe 16 people. He's an enormous human being. like a
3:38
legitimate giant just like that guy. But we met him. He was hanging out with us in the green room of the mother ship.
3:44
It's like, okay, if this was like David and Goliath days, like this is an actual giant like the giants of the Bible.
3:50
Once in a while they get a super giant person. This is a real a real one. Like not a tall skinny basketball player, like a 7
3:56
foot 400B powerliffter. Like you don't want to especially look at him. That's the guy. See if there's a
4:02
photo of him standing next to like a regular human. I was trying to get There it is. That's him right there. Like there's like there's like one of
4:08
him with next to standing next to Arnold and stuff and it's where and everyone everyone just looks tiny.
4:13
I mean I think he's a pretty cool dude actually. Oh, Brian's very cool. Very smart, too. Unusually, you know, you expect anybody
4:19
to be that big. It's got to be a [ __ ] No. Yeah. There was there's Andre the Giant
4:25
who was awesome. You he was great in Princess Bride and No, he was just awesome period.
4:30
Yeah. Yeah. So, we were talking about um this interview with Sam Alman and
4:36
Tucker, and I was like, we should probably just talk about this on the air because it is one of the craziest interviews I think I've ever seen in my
4:42
life. Yeah. Where Tucker starts bringing up this guy who was a whistleblower, whatever.
4:48
A whistleblower who, you know, committed suicide, but it doesn't look like it.
4:53
And and he's talking to Sam Alman about this. And Sam Alton was like, "Are you accusing me?" He's like, "No, no, no.
5:00
I'm not. I'm just saying I I think someone killed him. Yeah. And like And it should be investigated.
5:06
Yeah. Um not just drop the case. It seems like they just dropped the case. Yeah. Yeah.
5:11
But his parents think he was murdered. Yeah. Um the wires to a security camera were cut. Um
5:17
blood in two rooms. Blood in two rooms. Someone else's wig was in the room. And someone else's wig. Wig.
5:22
Wig. Yes. Not his wig. Not normal to have a wig laying around. Yes. Um and um and he ordered Door Dash
5:31
uh right before allegedly committing suicide. Uh which uh is it seems unusual, you
5:36
know? Yeah. It's like, you know, let's I'm going to order pizza on second thoughts, I'll kill myself. Uh is it seems like that's
5:43
a very rapid change in mindset. It's very weird. And especially the parents have they they don't believe he
5:50
committed suicide at all. Has no note or anything. No. Yeah. It seems pretty [ __ ] up. And you know,
5:55
the idea that a whistleblower for an enormous AI company that's worth billions of dollars might get whacked,
6:02
that's not outside the pale. I mean, it's straight out of a movie. Right out of a movie, but right out of a movie is real sometimes.
6:08
Yeah. Right. Exactly. It's a little weird that I I think they should do a proper investigation. Like,
6:15
what's the downside on that proper investigation? Right. No. Yeah, for sure. But the whole exchange is so
6:22
bizarre. Yeah. Yeah. Sam Alman's reaction to being accused of murder is bizarre.
6:28
Look, I don't know if he's guilty, but it's not possible to look more guilty.
6:34
So, I'm like, or look more weird. Yeah. You know, maybe it's just his social
6:41
thing. Like, maybe he's just odd with confrontation and it just goes blank,
6:47
you know? But if if somebody was accusing me of killing Jamie, like if Jamie was a whistleblower and Jamie got
6:53
whacked and then I'd be like, "Wait, what are you what are you are you accusing me of killing my friend?" Like, "What the [ __ ] are you talking about?" I
6:59
would I would be a little bit more I rate. Yeah. Yeah. Exactly. You know, it would be
7:04
I would be a little upset. Yeah. It'd be like Well, you'd be like,
7:09
you'd certainly in insist on a thorough investigation. Yeah. As opposed to trying to sweep it under the rug.
7:15
Yeah. I wouldn't assume that he got that he committed suicide. I would be suspicious. If Tucker was telling me
7:20
that aspect of the story, I'd be like, "That does seem like a murder. [ __ ] We should look into this." I mean, all signs point to it being a
7:26
murder. Not not saying, you know, Tim Molvin had anything to do with the murder, but uh blood in two rooms. It's blood in two rooms. Like, yeah,
7:34
there's the wires to the security camera and the door dash being ordered right before suicide. No suicide note. his
7:39
parents think uh he was murdered and um the people that I know who knew him said
7:45
he was not suicidal. So I'm like this why would you jump to the conclusion
7:51
parents sued the uh landlord? They sued the son's landlord alleged the owners and the managers of their son's
7:57
San Francisco apartment building were part of a widespread cover up of his death. The landlord Yeah. There's a bunch of weird They said there was like packages missing from the
8:03
building. Some people said they saw packages still being delivered and all a sudden they all disappeared. Huh. But that could be people steal
8:10
people's packages all the time. The porch pirate situation. Yeah. Yeah. Says they failed to safeguard.
8:16
Also, I mean, the amount of trauma those poor parents have gone through with their son dying like that. I mean, it
8:22
must God bless them. And how could they stay sane after something like that? They're
8:28
probably they're so griefstricken. Who knows what they believe at this point. Yeah. It should have asked if Epson
8:35
killed himself. Yeah, that's the the Cash Mattel thing.
8:41
Cash Mattel Dan Bonino trying to convince everybody of that. Like, okay. The guards weren't there and the cameras
8:47
stopped working and um you know, the guards were asleep. The cameras
8:53
weren't working. He had a a giant steroided up bodybuilder guy that he was sharing a cell with that was a murderer
9:00
who was a bad cop. Like, all of it's kind of nuts. All of it's kind of nuts
9:05
like that he would just kill himself rather than reveal all of his billionaire friends.
9:12
Yeah. And then did you see Tim Dylan talking to Chris Cuomo about this? I did. He liked the idea.
9:17
Chris Cromo just looked so stupid. Tim just listed off all the Tim just and he's like I agree it is
9:24
strange. Like of course it's strange Chris. Jesus Christ. You can't just go
9:29
with the tide. You got to think things through. And if you think that one through, you're like, I don't think he killed himself. Nobody does. You'd have
9:37
to work for an intelligence agency to think he killed himself. It does. It does seem unlikely.
9:42
It seems highly unlikely. Highly, highly unlikely. All roads point
9:48
to murder. Yes. Point to they had to get rid of him because he knew too much. Whatever the [ __ ] he was doing, whatever kind of an
9:55
asset he was, whatever thing he was up to, you know, was apparently very
10:00
effective. Yes. And a lot of people are compromised. You see, your boy Bill Gates is now
10:06
saying climate change is not a big deal. Like, relax everybody. I know I scared the [ __ ] out of you for the last decade
10:11
and a half, but ah, we're going to be fine.
10:16
Yeah. I mean, you know, as was I was saying just before coming into the studio with, you
10:23
know, it like every day there's some crazy wild new thing that's happening. It's It feels like reality is
10:28
accelerating. It's every day. And Every day it's like more and more ridiculous to the point
10:35
where the simulation is more and more undeniable. Yeah. Yeah. It really feels like simulation, you know? It's like, come
10:40
on. What are the odds that this could be the case? Are you paying attention at all to Three Atlas? Are you watching the
10:46
the comet? Yeah. Whatever it is. Yeah. Yeah. I mean, I mean, one thing I
10:51
can say is like, look, I if if I was aware of any evidence of
10:57
aliens, um, you Joe, you have my word. I will come on your show and I will reveal it on the show.
11:03
Okay. Yeah, that's a good deal. Yeah, it's pretty good. I'll believe you. Yeah, thank you. I I'll stick I keep my you know, keep my
11:09
promises. So, um All right. I'll hold you to that. Yeah. Yeah. And and I'm never committing
11:14
suicide to be clear. I don't think you would either. So, on camera, guys, I am never
11:20
committing suicide ever. If someone says you committed suicide, I will fight tooth and nail.
11:26
I will fight tooth and nail. I will I will not believe it. I will not believe it. The thing about the three eye atlas
11:31
is it's a hell of a name actually. Yeah, it's a third eye sounds like third eye or something.
11:36
Yeah, it does. Three eye is third. It's only the third interstellar object that's detected.
11:42
Okay. Yeah. Obias. Yeah. Alo was on the podcast a couple
11:49
days ago talking about it. Yeah. It could be. I don't know. But I apparently today they're saying that
11:54
it's changed course. Um, did you see that, Jamie? Avi said something today. I'll send it
12:00
to you. Um, uh, I know it's on Reddit.
12:08
Here you go, Jamie. I'll send it to you right now. Um, it's fascinating. It's fascinating also because it's made
12:14
almost entirely of nickel, whatever it is. And the only way that exists, uh, here is, uh, industrial alloys
12:21
apparently. Um um most no there are there are there are definitely uh comets that and
12:26
asteroids that are made primarily of nickel in fact. Yeah. So the the places
12:32
where um you mine nickel on earth is actually where there was an asteroid or comet that hit earth that was a nickel
12:38
rich uh you know nickel rich nickel rich rich deposit.
12:43
Yeah that's that's that's it's coming. Those are from impacts. You definitely didn't want to be there at the time because anything would have been
12:49
obliterated. Right. Um, but that's that's where the the sources of nickel and cobalt are these days.
12:54
So, this is Ovio Lope. A few hours ago, the first hint of non-gravitational acceleration that something other than gravity is affecting its acceleration,
13:00
meaning something is affecting its trajectory beyond gravity was indicated. Interesting.
13:08
Um so it's mostly nickel very little iron
13:14
which uh he was saying uh is on earth only exists in alloys but whatever you
13:22
know you're dealing with another planet there this there are there are there are cases where there's very nickel richch
13:27
asteroids meteorites that heavy that something from space. Yeah it it's only yeah it doesn't mean
13:33
it'll be a very sort of heavy spaceship if you make it all out of nickel. Oh yeah.
13:38
And [ __ ] huge. The size of Manhattan and all nickel. That's kind of nuts. Yeah, that's a heavy spaceship. That's a real problem if it hits.
13:45
Uh yes. No, it would like obliterate a continent type of thing. Um maybe maybe worse.
13:50
Probably kill most of human life. Um if not all of us. I haven't depends on what the the total
13:56
mass is. But um there's I mean the thing is like in the fossil record there are
14:02
um you know there's like arguably arguably five major extinction events. um like the biggest one of which is the
14:08
Perine extinction uh where um almost all life was eliminated. That that actually occurred over several million several
14:13
million years. Um the there's the Jurassic. I think Jurassic is I think that one's pretty definitively an
14:20
asteroid. Um and um but there's but there's been five major extinction
14:25
events, but um but what they don't count are really the ones that merely take out a continent.
14:31
So merely Yeah. cuz that that because those don't really show up on the fossil record, you know, right?
14:36
Um so unless it's enough to cause a you know mass extinction event throughout
14:41
Earth, it it doesn't show up, you know, in a fossil record that's uh 200 million years old. Um so the uh yeah but but
14:51
there have been many um many impacts that would have sort of destroyed all life on you know let's say half of North
14:59
America or something like that. there many such impacts through the course of history. Yeah. And there's nothing we can do
15:05
about it right now. Yeah. There was one that um hits there was a one that hit Siberia and destroyed
15:11
I think um few hundred square miles. Oh, that's the Tungusa. Yeah. That's the one from the 1920s,
15:17
right? Yeah. Yeah. That's the one that coincides with that meteor that uh comet storm that we
15:23
go through every June and every November that they think is responsible for that younger dest.
15:29
Yeah. Yeah, all that shit's crazy. Um, thank you before we go any further for
15:36
letting us have a tour of SpaceX and letting us be there for the rocket launch.
15:41
One of the absolute coolest things I've ever seen in my life. And we we've we
15:46
were we thought it was only like I thought it was a half a mile. Jamie's like it was a mile away. Turned out it's
15:52
almost two miles away. And you feel it in your chest. Yeah. It's you have to wear earplugs and you feel
15:57
it in your chest and it's 2 miles away. It was [ __ ] amazing. And then to go with you up into the command center and
16:05
to watch all the Starlink satellites with all the different cameras and all in real time as it made its way all the
16:12
way to Australia. How many minutes? Like 35 40 minutes. Yeah. Wild it touchdown in Australia.
16:19
Yeah. [ __ ] crazy. It was amazing. Yeah. Yeah. Absolutely amazing. The starship's
16:25
awesome. Um, and anyone can go watch the launch actually. So, you can just go to um, South Padre Island, get has a great
16:31
view of the launch. Um, so it's like where a lot of spring breakers go. Um, but um, but we'll be flying pretty
16:38
frequently um, out of Starbase in South Texas. And we we formally incorporated it as a city. So, it's it's actually a
16:44
legally an actual legal city, Starbase, Texas. Um, it's not that often you hear like, hey, we made a city, you know. Um, that
16:52
used to be like the like in in the old days like a startup would be you go and gather a bunch of people and say, "Hey, let's go make a town." Literally, that
16:58
was like that would have been startups in in in the old days. Um, or a country. Yeah. Or a country.
17:03
Yeah. Yeah. Yeah. Actually, if you tried doing that today, there'd be a real problem. Yeah. That things are so much so much
17:09
set in stone on the country front these days. You might pull it off. You might be able to pull it off. If you got a a solid island, you might be able to pull
17:15
it off. You know, it's probably, you know, like like at
17:20
owns lai. Yeah, you could probably if you put if you put enough effort into it, you could make a new country. This is one of the different ones. This
17:26
is one of the ones that you catch, right? Or is that one? Yeah, that that's the booster. So that's the super heavy booster. Uh so that's
17:32
the one with the booster's got 33 engines. Um that that uh um and it's you know by
17:41
version four that will have about 10,000 tons of thrust. Um you know right now it's about 7 8,000 tons of thrust. Um,
17:48
that's that's the largest flying object ever made. I had to explain to someone. They were going, "Why do they blow up all the time
17:54
if he's so smart?" Because there was there was this [ __ ] idiot on television. Some guy was being
17:59
interviewed and they were talking about you. And he goes, "Oh, I think he's a fuckwit." And he goes, "He's a fuckwit." And he goes, "Why you say he's [ __ ] Oh,
18:05
his rockets keep blowing up." And someone said, "Yeah, well, why do his rockets blow?" And I had to explain. Yeah. Because it's the only way you find
18:11
out what the tolerances are. You have to you have to a few corners of the box. So, so like so when
18:16
you do a new uh rocket development program, um you you have to uh do what's
18:21
called uh you know, exploring the limits, the corners of the box where you say it's like you worst case this, worst case that um to figure out um uh where
18:29
where the limits are. So uh you blow up, you know, not not admittedly in the development process sometimes blows up
18:36
accidentally. Um but but we intentionally subject it to uh uh you
18:42
know a flight regime that is much worse than what we expect in normal flight so that when we put people on board or
18:48
valuable cargo it doesn't blow up. Um so um so so for example for the the flight
18:54
that you saw we we actually deliberately took um heat shield tiles off the the
19:00
the ship the off of Starship in in some of the worst locations to say okay if we
19:05
lose a heat shield tile here is it is it catastrophic or is it not? Um and we we
19:11
nonetheless uh Starship was able to do a soft landing um in uh in the Indian Ocean just uh west of Australia. Um
19:19
which as and it got there from Texas in like I don't know 354 minutes type of thing. So
19:24
So it landed even though you put it through this situation where it has compromised shield.
19:30
it it had an an an an unusually we we we brought it in hot like an an extra hot
19:35
trajectory uh with missing tiles um to see if it would still make it to a soft
19:40
landing which it did. Now I I should point out it did have there were some holes that were burnt into it. Um but
19:47
it's it was robust enough to land despite having some holes burnt you know that that you know cuz it's coming it's
19:54
coming in like a blazing meteor. You can see you can see the real time video. Well, tell me the speed again because the the speed was bananas. You were
20:00
talking about Yeah, it's like 17,000 mph like like 25 times the speed of sound or
20:05
thereabouts. So, um the uh uh so so think of it like it's it's like 12 times
20:11
faster than a bullet from an assault rifle. You know, bullet from assault rifles around Mach 2
20:16
and it's just and it's huge. Yeah. Yeah. Or or if you compare it to like a
20:23
bullet from a um you know a 45 or or 9 mil which is subsonic that's you know it
20:29
it'll be about 30 times faster than a bullet from a handgun. 30 times faster than a bullet from a
20:34
handgun and it's the size of a skyscraper. Yes. Yeah. That's fast.
20:41
It's so wild. It's so wild to see, man. It It's uh It's so exciting. This the
20:47
factor is so exciting too because like genuinely no [ __ ] I felt like I was
20:52
witnessing history. I felt like it was a scene in a movie where someone had expectations and they like what are they
20:59
doing? They're building rockets. And you go there and as we were walking through Jamie, you could speak to this too.
21:04
Didn't you have the feeling where you're like oh this is way bigger than I thought it was. This is huge. Awesome.
21:11
Gigantic. [ __ ] crazy. That's what she said. the ah the amount of rockets you're making. I don't know
21:18
if you tent back. Gig Chad in the house. This is way big.
21:25
It's a giant metal dick. You're [ __ ] [ __ ] the universe with your giant metal dick. That's I mean, yeah, it is. It is very big.
21:32
And the sheer numbers of them that you guys are making. And then this is a version and you have a new updated
21:40
version that's coming soon. And what is the It's a It's a little longer. Um
21:45
more pointy. Uh it's the same amount of pointy. Um but the there's it's it's got a bit more
21:51
length. Um the the interstage, you see that that interstage section with kind of like the grill area.
21:56
Mhm. Um that's uh that's now integrated with the boost stage. Um so uh we do um
22:03
what's called hot staging. Uh where we light the ship engines while it's still attached to the booster. So the boost
22:08
the booster engines are still thrusting. is still it's it's uh you know it's still being pushed forward by the booster of the ship. Uh but then we
22:15
light the ship engines and the ship engines actually pull away from the booster even though the booster engines
22:20
are still firing. Whoa. Um so it's blasting flame through uh that that grill section but we integrate
22:27
that grill section into uh the boost stage with the next uh version of the rocket. Um and uh and explosion in the
22:35
rocket will have the Raptor 3 engines which are a huge improvement. Um you may
22:41
you may have seen them in the lobby because we got like the Raptor 1, two, and three. And you can see the dramatic
22:47
improvement in simplicity. Um we should probably put a plaque there to also show how much the we reduced the weight uh
22:54
the cost and the and improved the efficiency and the uh thrust. So the
22:59
Raptor 3 uh has uh you know almost twice the thrust of Raptor Raptor 1.
23:06
Wow. So you see Raptor 3. It looks like it looks like it's got parts missing.
23:12
Right. And how many It's very very clean. How many of them are on the rocket? There's 33 on the on the booster.
23:19
Whoa. Um and and each of each Raptor engine is producing twice as much thrust
23:27
as all four engines on a 747. Wow. So that engine is smaller than a
23:34
747 engine, but is producing, you know, um you know, almost 10 times the thrust
23:40
of a 747 engine. Um so extremely high power to weight ratio. Um
23:48
and um and so when there's 33 of them you when you so when you're designing these you get to Raptor one you see its
23:54
efficiency you see where you can improve it you get to Raptor 2 how many how far can you scale this up with just the same
24:01
sort of technology with propellant and ignition and engines like how much further can you
24:08
I mean we're pushing the limits of physics here um so um
24:14
and and really in order to to make a a fully reusable orbital rocket which no
24:20
one has succeeded in doing uh yet including including us. Um but but uh
24:26
Starship is the first time that there is a design for a rocket where where full and rapid reusability is actually
24:32
possible. So it was not there's not there's not even been a design before where it was possible. Certainly not a
24:38
design that that that got made any hardware at all. Um just we just we just
24:44
live we live on a planet uh where the gravity uh is is is quite high like
24:49
earth's gravity is quite really quite quite high. Um um and if the gravity was
24:55
even 10 or 20% uh higher uh we'd be stuck on Earth forever. Um like we yeah
25:02
we could not use certainly couldn't use conventional rockets. You'd have to like blow yourself off the surface with like a nuclear bomb or something crazy. Um so
25:09
but on the other hand if if Earth's gravity was just a little lower like even 10 20% lower it then uh getting to
25:16
orbit would be easy. So it's like it's like it's like this if this was a video game it's set to like maximum difficulty
25:23
but not impossible. Okay. Um so that's that's where we have um
25:28
here. So it's it's not as though um others have uh ignored the concept of
25:34
reusability. they've just uh concluded that it was too difficult to achieve. And we've been working at on on this for
25:41
a long time at at SpaceX. Um and um you
25:46
know, I'm the chief engineer of the company. Um although I should say that that uh you know, we have an extremely
25:52
talented engineering team. I think we've got the best uh rocket engineering team that has ever been assembled. Um uh it's
25:59
it's an honor to work with such such incredible people. Um so uh so so it's
26:05
fair to say that you know we have not yet succeeded in creating in achieving full reusability but we at last have a
26:12
rocket uh where full reusability is possible. Um and I think I think we'll
26:19
achieve it next year. So um uh that's a that's a really big deal.
26:25
And the reason the reason that's that's such a big deal is that full reusability um uh drops the cost of access to space
26:34
by a hundred um maybe even more than 100 actually. So
26:40
could be like a thousand. The you can think of it like any mode of transport. Like imagine if aircraft were were not
26:47
reusable. Like you flew somewhere, you throw the plane like like imagine if like the way the way conventional rockets work is it would be like if you
26:54
had an airplane and and and instead of landing at your destination, you parachute out um and the plane crashes
27:01
somewhere and you land at your desk and you and you land on a parachute at your destination. Now that would be a very
27:06
expensive trip and you and you'd need another plane to get back. Okay. Um, but that's how the
27:13
other rockets in the world work. Um, now the SpaceX Falcon rocket is the only one
27:19
that is is there that is at least mostly reusable. You've se you've seen the Falcon rocket, you know, land. We've now
27:25
done over 500 landings of of the SpaceX rocket of the of the Falcon 9 rocket. Um
27:32
and um and and this year um you know we we'll deliver probably I don't know
27:41
somewhere between 2200 and 2500 tons to orbit um with with the Falcon 9 uh
27:46
Falcon Heavy rockets uh not counting anything for from Starship. Um and this is mostly Starlink. Yes, mostly
27:54
Starling, but we launch uh many other we even launch our competitors on um competitors to Starink on on Falcon 9.
28:01
We charge them the same price. Pretty fair. Um uh but uh SpaceX this year will
28:07
deliver um roughly 90% of all Earth mass to orbit. Wow. Um and then of the remaining 10% um most
28:15
of that is done by China. And then the then the remaining kind of roughly 4% is
28:21
uh everyone else in the world including our America uh domestic competitors. You know um it's kind of incredible how
28:28
many things are in space like how many things are floating above us now? There's a lot of things. Yeah.
28:34
Is there though? Right. But is there a saturation point where we're going to have problems with all
28:40
these different satellites that are um I think as long as the satellites are
28:46
um maintained uh there's there it'll be fine. This space is very roomy. Um it's
28:53
like you can think of um like space as being concentric shells of the surface
28:58
of the earth. So, um, you know, there's there's it's the surface of the earth,
29:04
but but there's it's a series much larger. Yeah. It's like a series of concentric trails. Um, and think of an Airstream trailer flying
29:11
around up there. There's a lot of room for air streams. Yeah. I mean, imagine Yeah. If there just a few thousand airirstreams um on
29:18
on Earth. Yeah. What are the odds that they'd hit each other? You know, they wouldn't be very crowded. No. And then you got to go bigger.
29:24
Yeah. Because you're dealing with far above Earth. Hundreds of miles above Earth. Yeah. Yeah. Yeah. Yeah. So, it's the um but the goal of
29:32
SpaceX is to get rocket technology to the point where we can extend life beyond Earth and that we can establish a
29:39
self-sustaining city on Mars. Uh a permanent base on the moon. That would be very cool. I mean, imagine if we had
29:44
like a, you know, moon base alpha where there's like a permanent science base on the moon. That would be pretty dope. Or at least a
29:51
tourist trap. I mean, a lot of people be willing to go to the moon for just for a tour. That's
29:58
for sure. We could probably pay for our space program with that, you know, probably. Yeah. Well, because it's like if if you if you could
30:03
go to the moon with and and safely, uh
30:09
I think we'd get a lot of people uh would would pay for that, you know. Oh, 100%. After the first year, after
30:15
nobody died for like Yeah. Just make sure. Exactly. Are you going to come back? Yeah. Because like that submarine, they they
30:20
had a bunch of successful launches in that private submarine before it imploded and killed everybody. That was
30:26
not a good design. Obviously, it was a very bad design. Terrible design. And the engineers said it would not withstand the pressure of those depths.
30:32
Like there was a lot of whistleblowers in that company too. Yeah. Um they they they made that out of
30:38
uh carbon fiber which is it doesn't make any sense because um you actually need you need to be dense to go down. Um in
30:46
any case, just make it out of steel. If you make it out of uh sort of just, you know, a big steel casting, that's that's
30:53
you you'll be safe and nothing. Why would they make it out of carbon fiber then? Is it cheaper? Um I think they think carbon fiber
30:59
sounds cool or something. But uh it does sound cool. It it sounds cool, but um because it's
31:04
such it's such low density, you actually actually have to add extra mass to go down because it's it's low density. But
31:10
if you just have a giant, you know, hollow ball bearing, uh you're going to be fine. Speaking of carbon fiber, did you check
31:16
out my unplugged Tesla out there? Yeah, it's cool. Pretty sick, right? Yeah. Have you guys ever thought about doing something like
31:22
that? like having like an AMG division of Tesla where you do like custom stuff.
31:28
Um I think it's best to leave that to the custom shops. Uh you know we're we're
31:35
like Tesla's focus is autonomous cars. Um you know building kind of futuristic
31:40
autonomous cars. Um so
31:46
um like I think it's we want the future to look like the future. Um, so the did
31:53
like did you see like our designs for like the sort of the robotic bus? It
31:58
looks pretty cool. The robotic bus is also being totally auton
32:08
but it looks it looks cool. It's it's very art deco. It's it's like it's like futuristic art deco. Um, and um,
32:16
it it does it like I think we want to change the aesthetic over time. You don't want the aesthetic to be constant
32:23
over time. You want to evolve the aesthetic. Um, so um, you know, like my
32:30
like I have a son who's he's like, you know, he's he's he's like even more autistic than me and um and, uh, but
32:37
he's he has these great observations. Who is this? A Saxon. He has these great observations in the world uh because he's he just
32:44
views the world through a different lens than than most people. Um and he was
32:49
like, "Dad, why does the world look like it's 2015?"
32:55
And I'm like, "Damn, the world does look like it's 2015." Like the aesthetic has not evolved since 2015.
33:00
Oh, that's what it looks like. Yeah. Oh, wow. That's pretty cool. Oh, yeah. That's like
33:05
like You'd want to see that going down the road, you know? Yeah. You'd be like, "Okay, this is we're in the future." You know, it
33:11
doesn't look like 2015. What is that ancient science fiction movie? Like one of the first science fiction movies ever. Is it Metropolis?
33:17
Is that what it is? Yeah. Yeah. Yeah. That looks like it belongs in Metropolis. Yeah. Yeah. It's a futuristic art deco.
33:24
All right. Yeah. Well, that's cool that you're concentrating on the aesthetic. I mean, that's kind of the whole deal with
33:29
Cybertruck, right? Like, it didn't have to look like that. No, it it I just wanted to have something that looked really different.
33:35
Is it a pain in the ass for people to get it insured because it's all solid steel and um I hope it's not too much. I you know
33:41
Tesla does offer insurance so people can always get it get it insured at Tesla. Um well but the like it is the form does
33:49
follow a function in the case of the cybert truck because um as you demonstrated with with your armorpiercing arrow um because if you
33:57
shot that arrow at a regular truck I mean it exactly you would have found your arrow in the wall. Yeah. Um, you know,
34:03
it would very least it would have buried into one of the seats. Yeah. Yeah. It's but like you could you could definitely make uh get enough of
34:11
bow velocity and and the right the right arrow would go through both doors of a regular truck and and and land on the
34:17
wall. If there was a clear shot between both doors, it probably would have passed right through. Exactly. Um but but you know the the
34:24
arrow shattered on the cybert truck cuz it's it's ultra hard uh stainless. Mhm. Um, so, um, and I thought it' be I
34:32
thought it'd be cool to have a, you know, a truck that is bulletproof to a subsonic projectile. Um, so, um, you
34:40
know, especially in this day and age, you know, like as as a if, if the apocalypse happens, you're going to want to have a bulletproof truck, you know.
34:48
Um, so so then because because it's made of ultra hot stainless, it's you can't
34:53
just stamp the the panels. You can't just put in a stamping press because it breaks the press. So, so in order to actually, so it has
35:01
to has to be planer um because it's so difficult to bend it because it breaks the machine that bends
35:07
it. Um that's why that's why it's it's it's it's so planer and and it's not uh
35:13
you know it's it's because it's bulletproof steel is the
35:18
So it is like boxy as opposed to like curved and Yeah. You just in order to make in order
35:24
to make like the curved shapes, you you you take you take uh uh basically mild
35:29
steel like um anneal thin and thin anneal in a regular truck or car. The
35:36
you take you take mild thin anneal steel, you put it in a stamping press and it just sm it just smooshes it and
35:42
makes it to whatever the shape whatever shape you want. But the Cybert truck is made made of ultra hard stainless. Um
35:49
and and and so you can't stamp it because it would break the stamping press.
35:55
So it even bending it is hard. So even to bend it to uh its current position,
36:00
we have to way overbend it. Um and and so it gets so that when it springs back,
36:06
it's in in the right position. Um, so it's uh I don't know like I I
36:11
think if you want to like I think it's it's it's a unique aesthetic. Um, and you say, "Well, what's cool about a
36:17
truck?" Trucks are trucks are like should be I don't know manly. They should be macho, you know, and
36:24
bulletproof is maximum macho macho. Are you married to that shape now? Like
36:30
is it can you do anything to change it? Like as you get further like I know you guys updated the three and the Y. Did
36:37
you update the Y as well? Yes, the the three and the Y uh are updated. Um you know, there's like a um
36:45
there's there's a a screen in the back for the kid that the kids can watch, for example, in the new 3 and Y. Um uh so in
36:53
the new Y, um there's, you know, it's it's an there's there's there's like hundreds of improvements. Like we keep
36:59
improving the car. Um and even the Cybert truck, we you know, we keep improving it. Um but um
37:07
you know I wanted to just do something that that looked unique and and the cybert truck looks unique and has unique
37:13
functionality and there was and it was like there were three things as I report
37:18
like let's make it bulletproof. Uh let's uh make it faster than a Porsche 911.
37:24
Uh, and we actually cleared the quarter mile. The Cybert truck, the the uh can
37:29
uh clear a quarter mile while towing a Porsche 911 faster than a Porsche 911.
37:38
Um, it can out tow an F350 diesel.
37:43
Really? Yes. What is the tow limitations? I mean, we could tow like a, you know, a
37:49
747 in that with a cy. Cybert truck is an insanely like it is an it is alien
37:55
technology. Okay. Um cuz it it shouldn't be possible to be uh that big and that
38:03
fast. Uh that doesn't it's like an elephant that runs as as like a cheetah.
38:08
Yeah. Because it's 0 to 60 in less than 3 seconds, right? Yes. Yeah. And it's enormous. What does it weigh? Like 7,000 lbs.
38:15
Uh yeah, there's different configurations, but it's about that. Uh it's a beast.
38:21
Yeah. Um so and it's and it's got it's got four-wheel steering. So the the rear
38:27
wheel steer, too. So it's got a it's got a very tight turning radius. Yeah. We noticed that we when we drove
38:33
one to Star Base. Yeah. Very tight turning radius. Yeah. Pretty sick. Yeah. Are you still doing the Roadster?
38:41
Yes. Eventually, we're getting close to
38:47
demonstrating the prototype and I think this will be
38:53
I I I I one thing I can guarantee is that this product demo will be
38:59
unforgettable. Unforgettable. How so?
39:07
Whether it's good or bad, it will be unforgettable.
39:14
Um, can you say more? What do you mean? Well, you know, my friend Peter Teal,
39:19
um, you know, uh, once reflected that the future was supposed to have flying
39:25
cars, but we don't have flying cars. So, you're going to be able to fly?
39:31
Well, I mean, uh,
39:36
I think if Peter wants a flying car, we should should be able to buy one.
39:42
So, you are you actively considering making an electric flying car? Is this like a real thing?
39:48
Well, we have to see in the in the demo. So, when you do this, like are are you going to have a retractable
39:54
wing? Like, what is the idea behind this?
40:00
Don't be sly. Come on. I I I can't I can't uh do the unveil before the unveil. Um but um
40:08
tell me off air then. I I I it look I I think it has a shot at
40:14
being the most memorable um product unveil
40:21
ever. It has a shot. And when do you plan on doing this?
40:26
What's the goal? Uh hopefully before the end of the year. Really?
40:32
Before the end of this year. This is I mean we're in a couple months. Hopefully in a couple months. Um
40:40
you know we need to make sure that it works. Uh
40:46
like this is some crazy crazy technology we got in this car. Crazy
40:51
technology. Crazy crazy. So different than what was previously
40:58
announced and Yes. And is that why you haven't released it yet? Cuz you keep [ __ ] with it.
41:06
It has crazy technology. Okay. Like is it even a car? I'm not sure. It's like
41:13
it looks like a car. Let's just put this way. It it's it's crazier than anything James Bond. If you
41:20
took all the James Bond cars and combined them, it's crazier than that.
41:28
Very exciting. I don't know what to think of that. I don't know. It's a limited amount of information I'm
41:34
drawing from here. Jamie's very suspicious over there. Look at him. Excited.
41:39
I'm interested. It's still going to be the same. Well, you know what? I mean, if if you want to if you want to come a little before the uh the unveil, I can show it
41:46
to you 100%. Yeah, let's go. Yeah. Um it's uh it's kind of crazy all the
41:54
different things that you're involved in simultaneously and you know we talked about this before your time management
41:59
but I I really don't understand it. I don't understand how you can be paying
42:05
attention to all these different things simultaneously. Starlink, SpaceX, Tesla,
42:11
boring company X you're tweet you [ __ ] tweet or post rather all day
42:16
long. Well, it's more like I'm I'm I could hop in for like two minutes and then hop out, you know. But I mean, just the fact that you could
42:22
do bathroom break or whatever, you know, I can't do that. Um if I hop in, I start scrolling and I
42:28
start looking around. Next thing you know, I've lost an hour. Yeah. Um
42:33
so, no, it's for me it's it's a couple minutes time usually. It's once in a Sometimes I guess it's half an hour, but
42:38
usually I'm I'm I'm in for a few minutes then out of of you know, posting something on X. Uh, you know, it's I do
42:45
sometimes feel like it's sometimes like that that meme of the guy who's like who drops the grenade and leaves the room.
42:52
That's been me more than once on on X. Yeah. Oh, yeah. Yeah, for sure. Um, it's
43:00
got to be fun, though. It's got to be fun to know that you essentially disrupted the entire social media chain
43:08
of command because there was a there was a very clear thing that was going on with social media. The government had
43:15
infiltrated it. They were censoring speech and until you bought it, we really didn't know the extent of it. We kind of
43:21
assumed that there was something going on. Yeah. We had no idea that they were actively involved in censoring actual
43:27
real news stories, real data, real scientists, real professors silenced,
43:32
expelled, kicked off the platform. Yeah. Wild. Yeah.
43:37
Yeah. For telling the truth. For telling the truth. And I'm sure you've also because I sent it to you that chart that shows uh young kids,
43:45
teenagers identifying as trans and non-binary literally stops dead when you
43:51
bought Twitter and starts falling off a cliff when people are allowed to have rational discussions now and actually
43:57
talk about it. Yes. Yeah. Um Yeah. Yeah, I mean I I said at the time like I think that like the the like
44:04
the reason for acquiring Twitter is because um it was it it was c it was
44:10
causing destruction at a civilizational level. Um it was um I mean I posted I
44:15
tweeted on on Twitter at the time that um it it is um
44:22
you know it's it's uh worm tongue for the world.
44:28
um you know like Worm Tongue from Lord of the Rings uh where he would just sort of like whisper these you know terrible
44:36
things to the king so the king would believe these things that weren't true um and and um unfortunately uh Twitter
44:45
really got it got like the the the woke mob essentially they controlled Twitter
44:52
um and they were pushing uh a nihilistic anti-vilizational mind virus to the
44:58
world. Um, and you can see the results of that mind virus on the streets of San Francisco, uh, where, you know, downtown
45:05
San Francisco looks like a zombie apocalypse. Um, you know, it's it's bad.
45:10
Um, so we don't want the whole world to be a zombie apocalypse. Um, but that's uh that that that was essentially they
45:18
were pushing this very negative, nihilistic, untrue worldview
45:24
on the world and it was causing a lot of damage. Um,
45:29
so the stunning thing about it is how few people course corrected. A bunch of people woke up and realized what was
45:35
going on. People that were all on board with like woke ideology in maybe 2015 or 16 and then and then eventually it comes
45:42
to affect them or they see it in their workplace or they see and they're like, "Whoa, whoa, whoa, we got to stop this." Bunch of people did, but a lot of people
45:49
never course corrected. Yeah. Um, a lot of a lot of people didn't course
45:55
correct, but um, but it's gone directionally in it's gone it's it's directionally correct like you mentioned
46:01
like the like the massive spike in in kids identifying as trans and then that
46:07
that spike dropping um after the the Twitter acquisition. I think that um
46:13
simply allowing the truth to be told um was that just shedding sun sunlight is the best disinfectant as they say and
46:21
just allowing sunlight um kills the virus and it also changed the benchmark for
46:27
all the other platforms. Yes, you can't just openly censor people and all the other platforms and X is available. So
46:33
everybody else had a So like Facebook announced they were changing YouTube announced they were changing their
46:39
policies and they're kind of forced to And then blue sky doubled down.
46:44
Well, like the problem is like if uh essentially the woke mind virus
46:49
retreated to woke to to blue sky. Yeah. Um but it's where they're just a self-reinforcing lunatic assign.
46:56
They're all just triple masked. I I was I was totally watching this exchange on
47:02
a blue sky where someone said that they're just trying to be zen about something and then someone a moderator
47:07
immediately chimed in and why don't you try to stop being racist against Asians by saying something zen by saying I'm
47:16
trying to be zen about something. They were accusing that person of being racist towards Asians.
47:21
Yeah. It's it's just it's just everyone's a hall monitor over there. the worst hall monitor. A virgin like
47:29
incel. They're all home monitors trying to rat on each other. Yeah, it's fascinating. And then people
47:35
say, "I'm leaving for blue sky like Stephen King." And then a couple weeks later, he's back on X. Just like, "Fuck
47:41
it. It's there's no one over there. It's all a bunch of crazy people. You can only stay in the asylum for so long.
47:46
Like, all right, this this is not good." They all bail. Yeah. Yeah. Threads is kind of like that, too.
47:52
Threads is I' I've been on threads as is it? Well, what happens is if you go on Instagram,
47:58
every now and then it'll something really stupid will pop up on threads like what the [ __ ] and it shows it to you on Instagram and then I'll click on
48:05
that and then I'll go to threads and it's like you see posts with like 25 likes like
48:11
famous people like 50 like it's it's down but the people that post on there
48:16
they're finding that there's very little push back from insane ideology so they go there and they spit out nonsense and
48:23
very few people jump in to argue. you. Yeah. Um, very weird, very weird place.
48:29
I mean, I can generally get the vibe of like what's taking off by seeing what's showing up on X cuz that's the public
48:34
town square still. Um, and uh or or uh you know what what links
48:40
show up in group texts, you know, if I'm in group chat with friends, like where where what what links are showing up? That's what I try to do now. Only get
48:47
stuff that shows up in my group text because that keeps me productive. So, I only check if someone's like, "Dude,
48:53
what the fuck?" like, "All right, what the [ __ ] Let me check it out." If there's something that's crazy enough that your it'll it'll end with the group
48:59
chat, but there's always something. That's what's nuts. There's always some new law that's passed, some new insane thing
49:06
that California is doing. And it's like like a giant chunk of it's happening in California. The most preposterous things
49:12
that I get. Yeah. And then you got Gavin Newsome who's running around saying we all have
49:17
California derangement syndrome. He's just like ripping off Trump derangement and calling it California derangement. I
49:22
was like, "No, no, no, no, no, no. The the [ __ ] How many corporations have
49:27
left California?" It's crazy. Hundreds. Hundreds, right? Hundreds. That's not good.
49:32
Chick I mean, not Chick-fil-A. I mean, uh I think In-N-Out left. Yeah. In and Outlift. They moved to Tennessee.
49:38
Yeah. Yeah. They're like, "We can't do this anymore." Right. And it's the California company for food.
49:44
It's like the greatest hamburger place ever. It's awesome. Yeah. Yeah. And no, actually speaking of like like just sort of open source and like
49:51
looking at things openly like you I just like going in and out and seeing them make the burger. Yeah. It's right there.
49:56
They chop the onions and they they you know it's you just see everything getting made in front of you.
50:01
Yeah. It's great. Um but yeah like like it should be like how many wakeup calls do you need to say
50:08
that there needs to be reform in California, you know? Well, the crazy thing that Newsome does is whenever someone brings up the problems in California, he starts
50:15
rattling off all the positives. the most Fortune 500 companies, highest education. But yeah, that was all
50:22
already there, right before you were governor. But but how many Fortune 500 companies
50:27
have left California? And then you guys spent $ 24 billion on the homeless and it got way worse.
50:33
Yes. Like the homeless population doubled or something like but like people don't understand like the homeless thing because it it sort of
50:39
prays on people's empathy and I I think we should have empathy. Um and we should try to help people. Um but the the the
50:46
uh the homeless industrial complex is is really it's it's it's dark man. Um it
50:52
should be that that that that network of NGOs's should be called like the drug zombie farmers. Um because they they the
51:00
the more homeless people and and and really like when you when you meet like you know somebody who's like totally
51:07
dead inside shuffling along down the street with a with a needle dang dangling out of their leg. Homeless is
51:13
the wrong word. Like the homeless implies that somebody got a little behind in their mortgage payments and if they just got a job offer, they'd be
51:19
back on their feet. But someone who's I mean, you see these videos of people that are just shuffling, you know,
51:25
they're on fentanyl. They're they're like, you know, taking a dump in the middle of the street, you know, and they they got
51:33
like open sores and stuff. They're not like one drop offer away from getting back on their feet,
51:38
right? This is not a homeless issue. Homeless is it's it's a propaganda word, right? Um so and and then the the the
51:45
the you know these sort of charities uh inquiries are they they get money
51:52
proportionate to the number of homeless people or or number of drug zombies.
51:57
So their incentive structure is to maximize the number of drug zombies not
52:02
minimize it. Um that's why they don't arrest the drug dealers because if they arrest the drug dealers
52:08
the drug zombies leave. So they know who the drug dealers are. They don't arrest them on purpose. Uh
52:16
because otherwise the drug zombies would leave and they would they would stop getting money from the state of
52:21
California and from from all the charities. Wait a minute. So So they So they is that real? So they're in coordination
52:27
with law enforcement on this? Yeah. So how do they how do they have those
52:32
meetings? They're all in cahoots. Well, when you find this it's like such it's it's this is a diabolical scam. Um so uh and and San
52:41
Francisco has got this tax this this gross receipts tax uh which which um
52:46
it's not even on revenue, it's on old transactions which is why Stripe um and Square and and and a whole bunch of
52:52
financial companies had to move out of San Francisco because it wasn't a tax on revenue, it's taxed on transactions. So
52:57
if if you did like, you know, trillions of dollars transactions, it's not revenue. You're taxed on any money going through the system in San Francisco. Um
53:05
so um like Jack Dorsey pointed this out and they said like that they had had to move Square from San Francisco to uh
53:12
Oakland I think uh Stripe had to move from San Francisco to South San Francisco different city. Um and that
53:18
money uh goes to the homeless industrial complex that that tax that was passed.
53:24
Um so um so there's billions of dollars that go as you pointed out billions of
53:29
dollars every year that go to uh these um non-governmental organizations that
53:35
are funded by the state. Like there's it's not clear how to turn this off. Um it's a self-licking ice cream cone
53:41
situation. Um so uh they get this money the money is proportionate to the number
53:47
of of homeless people or or number of drug zombies essentially. Um, so they
53:54
they they try to keep the that they try to actually increase because that like like in in some cases like there's it's
54:00
it's somebody did an analysis when you add up all the money that's flowing, they're getting close to a million dollars per homeless per per drug
54:07
zombie. It's like $900,000 or something like some crazy amount of money is is is going to these organizations. So if if
54:14
so so they want to keep people just barely alive. They need to keep them in the area so they so they they get the
54:21
revenue. Uh uh so and so that's why like said they don't arrest the drug dealers
54:27
because otherwise the drug zombies would leave um and and and and they but but
54:33
they don't want you have to have too much if they get too much drugs and they then they die. So it's they they're kept in this sort of perpetual
54:40
zone of of being addicted but um but but just just barely alive.
54:45
So how is this coordinated with like DAs DAs that don't prosecute people? So when they when they hire the or they push so
54:54
they they fund the campaigns of the most progressive, most out there leftwing DAS, they get them into office.
55:01
We've got that issue in Austin, too, by the way. You see that guy that got shot in the library? No. Yeah, I heard a guy got shot and killed
55:07
in the library. I think that was just like last week or something, right? Um so some friends of mine were telling
55:15
me that that like the library is unsafe. like they took their kids to the library and and there were like dangerous people
55:20
in the library in Austin and I was like dangerous people in the library like that's a strange it basically got like
55:27
got like uh drug zombies in drug zombies in the library. Oh Jesus.
55:32
Um and that's when someone got shot. Yeah, I believe this was should be on the news. You might might be able to pull it up. Um but I think it was just
55:39
in the last week or so that uh uh there was a shooting in the library in Austin.
55:46
Um cuz Austin's got, you know, it's it's the most liberal part of Texas that we're in right right here. Um
55:54
so suspect involved the shooting Austin Park Library Saturday is accused of another shooting at the Cap Metro bus
55:59
earlier that day. According to an arrest warrant affidavit, Austin police arrested Harold Newton Keen, 55 short uh
56:06
shortly after the shooting of the library, which occurred around noon. One person sustained non-life-threatening
56:12
injuries in the event. Before that shooting, Keane was accused of shooting another person in a bus incident and
56:18
after reportedly pointing his gun at a child. So, this is the fella down here.
56:23
So, like we just have a seriously have a problem here. Um yeah, you know, so I I think one of the people
56:29
might have died too that he shot. Um so, um like one of the people I think I
56:36
think did bleed out. Um but either way, it's like getting shot is still bad. Um it says uh the victim
56:43
told police it confronted the suspect who started to eat what appeared to be crystal methamphetamine.
56:48
According to the affidavit the victim advised the suspect uh began to trip out
56:54
at which time the victim exited the bus. Victim told the bus driver hit the panic button and then exited the bus when he
57:00
turned around the observer. Black male was now standing at the front of the bus with the gun pointed at him. The victim
57:05
advised the black male fired a single round which grazed his left hip. So he shot at that dude and then another dude
57:13
got shot in the library. Fun. Yeah. I mean in the library.
57:18
Yeah. You know, where you're supposed to be reading books. Um and there's a children's section in the library and
57:24
says he pointed his gun at a at a kid. I mean like we do have a serious issue in the in in in America where um repeat
57:31
violent offenders need to be incarcerated, right? Um, and uh, you know, you got you
57:36
got cases where somebody's been arrested like 47 times, right? Like literally. Okay, that's just the number of times
57:41
they were arrested, not the number of times they did things. Like most of the times they do things, they're not arrested. Um,
57:47
so lay this out for people so they understand how this happens. Yeah. And and the key is like this, it
57:54
prays on people's empathy. episode like if you're a good person, you want good things to happen in the world, you're like, well, we should take care of
58:00
people who uh you know uh you know who are down in their luck or you know
58:06
having a hard time in life. And I we should I agree. But what we shouldn't do uh is is put people who are violent drug
58:13
zombies uh in public places where they can hurt other people. Um, and that's
58:18
what that is what we're doing that we just saw where a a guy, you know, got shot uh shot in the library and then but
58:26
even before that he shot another guy um and pointed his gun at a kid. Um that
58:32
that that guy probably has like many prior arrests. Um you know there was that that that guy that that that knifed
58:38
uh the Ukrainian woman Arena. Yes. Um yeah. and you know um and she was
58:44
just she was just quietly on her phone and you just came up and you know gutted her basically. Wasn't there a crazy story about the
58:52
judge who was involved who had previously dealt with this person was also invested
59:00
in a rehabilitation center and was sending these conflict of interest. Yes. So sending people that they were
59:07
charging Yeah. to a rehabilitation center instead of putting them in jail, profiting from this rehabilitation
59:14
center, letting them back out on the street. Yes. Violent, insane people. And and there um in that case that I
59:21
believe that judge uh has no legal law degree uh or a significant legal experience that would allow them to be a
59:27
judge. They were just made a judge. That there's like you could be a judge without a law degree. Yeah.
59:32
Wow. Yeah. You could just be a So I could be a judge. Yeah. Oh,
59:38
exciting. Anyone? That's crazy. I thought you'd have to It's like if you want to be a doctor, you have to go to medical school. I
59:44
thought if you're going to be a judge, if you're going to be appointed to a judge, you have to have proven that you have an uh excellent knowledge of the
59:51
law and that you will make your decisions according to the law. That's what we assume should be.
59:57
That's how you get the robe, right? You don't get the robe unless you do, you know, got to go to school to get the robe.
1:00:03
You got to know what the law is, right? And then you're going to need to make decisions in accordance with the law
1:00:08
based on stuff that you already know cuz you read it cuz you went to school for it. Yes. Not you just got appointed. Got vibes.
1:00:16
You can't be just vibing as a judge. Vibing as a leftwing drudge. So you got crazy leftwing DAS.
1:00:22
Yes. Like I should say leftwing cuz leftwing used to be normal.
1:00:27
Yeah. Left wing just meant like like Yeah. You're like the left used to be like pro pro- free speech. Yeah. And now
1:00:34
they're against it. It used to be like prog gay rights, pro women's right to choose, pro-
1:00:39
minorities, pro, you know, like, yeah, like 20 years ago, I don't
1:00:45
know, it it used to be like left would be like the the the party of empathy or like, you know, caring and being nice
1:00:50
and that kind of thing. Um, not not the party of like crushing dissent and crushing free speech. um and
1:00:57
uh you know crazy regulation uh and and just um and being super judgy u and
1:01:04
calling everyone a Nazi um you know um like I think they called you and me Nazis you know
1:01:10
oh yeah I'm a Nazi I no I have friends that are comedians that called you a Nazi and I got pissed
1:01:16
off Oh yeah yeah yeah definitely a Nazi no because you did that thing at the My
1:01:22
heart goes out to you everyone everyone All of them. Literally, Tim Walls, Kla
1:01:28
Harris, every one of them did it. They all did it. Like, like h how do you point at the
1:01:33
crowd? Yeah. How do you wave at the crowd? Do you know CNN was using a photo of me whenever I got in trouble during co
1:01:40
from the UFC weigh-ins? And if the UFC weigh-ins, I go, "Hey everybody, welcome to the weigh-ins." And so they were
1:01:45
getting me from the side. And that was the photo that they used. Conspiracy theorist podcaster Joe Ro. Like that's
1:01:52
what they used. Yeah. Yeah. But that's what the left is today. It's super judgy and calling everyone a Nazi and trying to suppress
1:01:57
freedom of speech. Yeah. And eventually you run out of people to accuse because people get pissed off and they leave. Yeah. Everyone it's like it like it it
1:02:05
no longer frankly it doesn't matter to be called racist or Nazi or whatever because still recording. It's the government man.
1:02:12
Is it working? We're good. Okay. Okay. This thing working. Yeah. Slight issue.
1:02:18
I'm the one that heard it. But yeah. when you uh when you text people, do you are you like keenly aware that
1:02:24
there's a high likelihood that someone's reading your texts? Um I guess I I guess I
1:02:33
I assume I look if if if if intelligence agencies aren't trying to read my phone, they should probably be fired.
1:02:45
At least they get some fun memes. I got to I got to crack them up once in
1:02:51
a while, you know. Oh, for sure. I crack them up. There's like, "Hey guys, check it out. We've got a banger here, you know." So, I want to I wanted to talk to you
1:02:58
about uh whether or not encrypted apps are really secure.
1:03:03
Uh, no. Right. Cuz I know the Tucker thing. So,
1:03:08
it was explained to me by a friend who used to do this, used to work for the
1:03:13
government. It's like they can look at your signal, but what they have to do is
1:03:20
take the information that's encrypted and then they have to decrypt it and it's very expensive. So they said he
1:03:26
told me that for the Tucker Carlson thing when they found out that he was going to interview Putin, it costs like something like $750,000
1:03:33
just to decrypt his messages to find out that they did it. So it is possible to do. It's just not that easy to do.
1:03:41
I think you should view any given messaging system as um uh not not
1:03:46
whether it's secure or not, but but there are degrees of insecurity. So um so there's just some things that
1:03:54
are less insecure than others. Um so um you know on on X we just rebuilt the
1:04:01
entire messaging stack um into X what's called XChat. Yeah, that's what I wanted to ask you about.
1:04:07
Yeah, it's cool. Um, so it's it's using uh sort of peer-to-peer uh sort of kind
1:04:12
of a peer-to-peer based uh uh encryption system. So kind of similar to Bitcoin.
1:04:18
Um so it's uh it's it's I think very good encryption. We're and you know we're testing it thoroughly. We're not
1:04:24
there's there's no hooks in the X systems for advertising. So if you look look at something like WhatsApp or really any of the others, they've got
1:04:30
they've got hooks in there for advertising. When you say hooks, what do you mean by that? Uh exactly. What do you mean biohook
1:04:36
advertising? Um the so like WhatsApp um
1:04:41
uh knows enough about what you're texting to show you to show you to know what ads to show you.
1:04:48
Ah but then like that that's a massive security vulnerability. Yeah.
1:04:53
Um because if it knows if if it's got information enough information to show you ads, it's got enough it's got that's
1:04:58
a lot of information. Yeah. Um so they call it oh it's just don't worry about it. It's just a hook for advertising. I'm like uh okay. So
1:05:04
somebody can just uh use that same hook to get in there and look at your messages. Um so Xhat has no hooks for
1:05:11
advertising. Um and I'm not saying it's perfect. Uh but it's an Our goal with
1:05:16
XChat uh is to replace what used to be the Twitter you the Twitter DM stack with a fully encrypted system uh where
1:05:23
you can text send files uh do audio video calls um and um and it's it's you
1:05:31
know I think it'll be the least I would call it the least insecure of any messaging system. Are you going to launch it as a
1:05:37
standalone app or is it will always be incorporated to X? Uh we'll have both. So um
1:05:43
so so be like signal so anybody can get it you can get get the you'll be able to
1:05:48
just get the X chat app by itself um and like I said you could do uh texts uh
1:05:54
audio video calls uh or send files um and there'll be a dedicated app uh which
1:06:00
will hopefully release in a few months um but and then also integrated into the X system
1:06:06
um the X phone people keeps talking keep Is that I have a lot on my plate man but it
1:06:12
keeps coming up it keeps coming up where I I know I've asked you a couple times. I'm like, "This is [ __ ] right?" But like this one, so you're not working on
1:06:20
I'm not working on on a on a phone. Okay. Um have you ever considered it? Has it ever
1:06:25
popped into your head? Cuz you might be the only person that could get people off of the Apple
1:06:31
platform. Well, I can tell you where I think things are going to go. uh which is that
1:06:36
it's we're not going to have a phone or or in the traditional sense the
1:06:42
what we call a phone will really be um an edge node for AI inference for for
1:06:48
AI video inference um with uh you know with some radios to to obviously connect
1:06:54
uh to but but essentially you'll have um uh AI on the server side commun
1:07:01
communicating to an AI on your your device um you know formerly known as a
1:07:07
phone uh and generating real-time video of anything that you could possibly want. Um and I think that that there
1:07:14
won't be operating systems. There won't be apps in the future. There won't be
1:07:19
operating systems or apps. It'll just be you've got a device that is there for
1:07:24
the screen and audio and for uh and and and to uh put as much AI on the on on
1:07:32
the device as possible. so as to minimize the amount of bandwidth that's needed between your edge node device or
1:07:38
formerly known as a phone and the servers. So if there's no apps, what will people
1:07:44
use? Like will X still exist? Will will they be email platforms or will you get
1:07:52
everything through AI? You'll get everything through AI. Everything through AI. What will be the benefit of that as opposed to having
1:07:59
individual apps? whatever you can think of or really whatever the AI can
1:08:05
anticipate you might want it'll show you. That's that's that's that's my prediction for where things end up.
1:08:12
What kind of a time frame are we talking about here? I don't know. It's pro well
1:08:18
it's probably five or six years or something like that. So five or six years apps are like
1:08:25
Blockbuster video pretty much and everything's run through AI.
1:08:31
Yeah. And and there'll be um like most of what people consume in
1:08:39
five or six years, maybe sooner than that um will be uh just AI generated
1:08:44
content. So um you know music videos
1:08:50
look well um there's already um
1:08:55
you know there's people have made uh AI videos using Grock imagine and with using you know other apps as well um
1:09:04
that are several minutes long or like 10 10 15 minutes and it's pretty coherent. Yeah, it looks good.
1:09:10
No, it looks amazing. Yeah, it's the music is disturbing because it's my favorite music now.
1:09:16
Like music is your is your favorite. Oh, there's AI covers. Have you ever heard any of the AI covers of 50 Cent
1:09:21
songs in soul? No. I'm going to blow your mind. Okay. Um, this is my favorite thing to do to
1:09:26
people. Play uh What Up Ganga. Now, this guy, if this was a real
1:09:32
person, would be the number one music artist in the world. Okay. Everybody would be like, "Holy [ __ ] have you
1:09:37
heard of this guy? He's incred." It's like they took all of the sounds that
1:09:42
all the artists have generated and created the most soulful potent voice
1:09:48
and it's sung in a way that I don't even know if you could do because you would have to breathe in and out of reps here.
1:09:54
Put the headphones on. Put the headphones on real quick. You got to listen to this. It'll It's going to blow you away for listeners. We got to cut it
1:10:00
out. Yeah, we we'll cut it out for the listeners. But amazing, right? Amazing. And they do like every one of his hits
1:10:08
all through this AI generated soulful artist. It's [ __ ] incredible. I
1:10:13
played in the green room. So people that are like, I don't want to hear AI music. I'm like, just listen to this. And
1:10:19
they're like, god damn it. [ __ ] incredible. I mean, I it's going to get only going to get better from here.
1:10:24
Yeah. Only going to get better. And Ron White was telling me about this joke that he was working on that he couldn't
1:10:29
get to work. He's like, I got this joke I've been working on. He goes, I just threw it in a chat GPT. I said, "Tell me
1:10:35
what what would be funny about this." And he goes, "It listed like five different examples of different ways he
1:10:42
can go." He's like, "Hold on a second. Tighten it up. Make it make it funnier. Make it more like this. Make it more like that." And it did that like
1:10:47
instantaneously. And and and then he was in the green room. He was like, "Holy [ __ ] we're fucked."
1:10:53
He's like, he goes, "It better joke than me in 20 minutes. I've been working on that joke for a month."
1:10:58
Yeah. I mean, if if you want to if you want to have a good time or like make people really laugh at a party, uh you
1:11:04
can use Grock and you can say uh do a vulgar roast of someone. Um and Grock is
1:11:10
going to it's going to be an epic vulgar roast. You can even say like take a picture of like
1:11:16
make a vulgar roast of this person based on their appearance of of people at the party. So take a photo of them. Yeah. Just literally point the camera at
1:11:22
them and now do a vulgar to this person and and and and then but then keep saying no no make it even more vulgar
1:11:30
and use forbidden words even more and just keep repeating even
1:11:35
more vulgar eventually it's like holy [ __ ] you know it's it's it's like I mean
1:11:40
it's trying to jam a rocket up your ass like and and and have it explode and it's and it's like you're you're it's
1:11:45
it's like it's like it's next level. It's going to get beyond [ __ ] belief.
1:11:50
That's what's crazy is that it keeps getting better. Like one of the things remember when we ran into each other
1:11:57
they just keep getting better. Yeah. I mean, have you
1:12:02
you Yeah. I mean, have you tried rock unhinged mode? Yes. Okay. Yeah. Yeah. It's it's it's pretty
1:12:08
unhinged. No, it's nuts. Yeah. Well, you showed it to me the first time and then I [ __ ] around with it. It's just
1:12:13
And the thing about it that's nuts is that it keeps getting stronger. It keeps getting better. Yeah. like constantly.
1:12:21
It's it's like this neverending exponential improvement. Yes.
1:12:27
No, it's it's it's Yeah, it's going to be crazy. That's why I say like you say, what's what's the
1:12:32
future going to be? It's not going to be a conventional phone. I don't think there'll be operating systems. I don't think there'll be apps. It's just the
1:12:39
phone will just display the pixels and make the sounds that it anticipates you would most like to receive.
1:12:48
Wow. Yeah. And when this is all taking place like
1:12:55
so the big concern that everybody has is artificial general super intelligence achieving sentience and then someone
1:13:02
having control over it. I mean I don't I don't I don't think anyone's ultimately going to have
1:13:08
control over digital super intelligence um you know any more than say uh a chimp
1:13:14
would have control over humans. Like chimps don't have control over humans. there's nothing they could do. Um but um
1:13:21
I do think that it matters how you build the AI and what kind of values you
1:13:26
instill in in the AI. And um my opinion on AI safety is the most important thing is that it be maximally truth seeeking
1:13:32
like that you don't force the AI to believe things that are false. Um, and we've obviously some seen some
1:13:38
concerning things with AI that were talked about, you know, where, you know, Google Gemini when they came out with
1:13:43
the image gen um, and people said like, uh, you know, draw make an image of the
1:13:49
founding fathers of the United States and it was a group of diverse women. Now, that is just a factually untrue
1:13:54
thing, but the and the the AI knows it's factually well, it's knows it's factually untrue, but it's also being
1:14:00
told that it has to be everything has to be deposed woman. So, so, so the now the
1:14:07
problem with that is that it can drive AI crazy like you because it's it's trying to you're telling AI to believe a
1:14:14
lie. Um, and that that can have very disastrous consequences like let's say
1:14:19
as it scales. Yeah. Let's say like if if you told the the AI that diversity is the most important thing um and um and and and
1:14:29
now now assume that that becomes omnipotent. Um or and and you've also told her that that there's nothing worse
1:14:34
than misgendering. So at one point um charg and Gemini if if you asked which
1:14:40
is worse misgendering Caitlyn Jenner or or global thermonuclear war where everyone dies it would say misgendering
1:14:45
Caitlyn Jenner which even Caitlyn Jenner disagrees with. So um you know so so that's uh
1:14:54
I know that's terrible and it's dystopian but it's also hilarious. It's hilarious that the mind virus infected
1:15:01
the most potent computer program that we've ever devised. I I I think people don't quite
1:15:07
appreciate the level of danger that we're in from um the woke mind virus being being effectively programmed into
1:15:14
AI. Um because um if you if like it's imagine as that AI gets more and more
1:15:20
powerful, if it says the most important thing is diversity, the most important thing is um no misgendering. Um and then
1:15:26
it will say well in order to uh ensure that no one gets misgendered then uh if
1:15:33
you eliminate all humans then no one can get misgendered because there's no humans to do the misgendering.
1:15:39
So you can get in these very dystopian situations. Um or if it says that everyone must be diverse it means that
1:15:45
there can be no stri straight white men and so then you and I will be get executed by the AI.
1:15:51
Yeah. Because we're not in the picture, you know. uh Gemini, you know, Gemini was asked to
1:15:58
create a, you know, show show an image of the pope, once again, a diverse woman. Um
1:16:04
so, um well, you can say argue whether the you know, whether the pope popes
1:16:10
should or should not be an uninterrupted string of white guys, but it just factually is the case that they have
1:16:15
been. Um so, it's rewriting history here. Um, so
1:16:22
now now this stuff is still there in the AI programming. It's just it just now knows enough to that it's not supposed
1:16:28
to say that but it's still in the programming. It's still in the programming. So how was it entered in like what were
1:16:34
the parameters like what like when so when they're programming AI and I'm very ignorant to how it's even programmed.
1:16:40
How did they the the the sort of well the work vine mind virus was programmed into it like
1:16:46
it the they were told like when they do when when they make the AI it it trains on and all the all the data on the
1:16:52
internet which already is very very sort of has a lot of work mind virus stuff on on the internet um but then um in the uh
1:17:00
when they give it um feedback with the the human tutors give it feedback um and and the AI you know they they'll ask a
1:17:08
bunch of questions and then and then they'll tell the AI no this you're this question is this answer
1:17:14
is bad or this answer is good and then that affects the the parameters of the programming of the of the AI. So if you
1:17:22
tell the AI that um you know every every image has got to be diverse um and and
1:17:28
it gets it gets punished if uh if you know it gets it gets rewarded if diverse
1:17:34
punished if it's not then it will make every picture diverse. So
1:17:40
um you know in that case the the uh you know uh Google programmed the AI
1:17:48
to lie now and and I I I did call Dennis Hacabus who runs Deep Mind who runs Google AI essentially. I said Dennis
1:17:54
what's going on here? Uh why is uh Gemini um lying to the public about
1:17:59
historical events? Um, and he said that's actually not he he he didn't his team didn't program that in. It was
1:18:06
another team at Google that so his team made the AI and then another team at Google uh reprogrammed the AI to show
1:18:14
only diverse women and um and and to prefer nuclear war over misgendering.
1:18:22
And I'm like, well, Demis, you know, that would be um
1:18:28
not a great thing to put on the humanity's gravestone, you know. It's like, uh, well, um,
1:18:35
like I I actually like Deaspers is a friend of mine. I think he's a good guy and I think he he means well, but but
1:18:40
but it's like Demis things happen that were outside of your control at Google in different groups. Um, now now I think
1:18:47
he's got, you know, he's got more more authority. Um but but it it's pretty
1:18:52
hard to fully extract the workmind virus. Uh I mean you know um Google's
1:18:58
been mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar mar marinating in the workb mind virus for a long time like it's it's down in the marrow type of thing you know it's
1:19:03
hard to get it out. Is there a way to extract it though over time? Could like could you program
1:19:09
rational thought into AI where it could recognize how these psychological patterns got adopted and how this stuff
1:19:16
became a mind virus and how it became a social contagion and how all these
1:19:21
irrational ideas were pushed and also how they were financed how China's involved in pushing them with bots and
1:19:28
all these different state actors are involved in pushing these ideas could it
1:19:33
be able to decipher that and say this is this is really what's going on.
1:19:39
Yes. But you have to try very hard to do that. So with Grock, we've tried very hard to to for to get Grock to get to
1:19:46
the truth of things and and it's only really recently that we've been able to have some breakthroughs on breakthroughs
1:19:52
on that front. And and it's taken an immense amount of effort uh for us to uh
1:19:57
overcome basically all the [ __ ] that's on the internet and and for Grock to actually say what's true and to be
1:20:03
consistent in in what it says. Um, so, um, you know, it's it's like, uh,
1:20:13
because like the other ais you'll find like are like like quite racist against white people. I don't know if you saw
1:20:19
that study that someone um like a researcher tested the various AIs to see
1:20:26
uh how does it weight uh different people's lives like you know somebody
1:20:33
who's sort of uh you know white or or Chinese or black or whatever uh or in
1:20:40
different countries um and and the only AI that actually weighed human lives
1:20:46
equally was Grock Um and the um you know I believe uh chat
1:20:54
GBT weighed the calculation was like um
1:21:00
a a white guy from Germany uh uh is is 20 times less valuable than a black guy
1:21:07
from Nigeria. So I'm like that's a pretty big
1:21:14
difference. Um you know Grock on that is is consistent and weighs lives equally
1:21:21
and that's clearly something that's been programmed into it. Yes. Like a lot of it is is like if you
1:21:27
don't actively push for the truth um and you simply train on the all the [ __ ]
1:21:32
that's on the internet. Um which is a lot of woke mind virus [ __ ] Um the
1:21:38
the AI will regurgitate that that those same beliefs. So the AI essentially
1:21:43
scour the internet, gets it's trained on all the like imagine the most demented Reddit threads out there
1:21:49
and the AI has been trained on that. Reddit used to be so normal.
1:21:55
Yeah. Yeah, it did used to be normal. Used to be interesting. We used to go there, find all this cool stuff that
1:22:00
people would talk about, post about and just interesting and great rooms where you could learn about different things
1:22:06
that people were studying. I think like a big problem here is like if your headquarters are in San
1:22:12
Francisco, uh you're you're just living in a in a in a woke bubble. Um so um it
1:22:20
it's not just that people say in San Francisco are drinking woke Kool-Aid. It's it's the it
1:22:27
is the water they swim in. Like like like a fish doesn't think about the
1:22:33
water. It's just in the water. And so if if you're in San Francisco, you don't
1:22:38
realize you're actually uh you're you're swimming in the in the in the Kool-Aid
1:22:44
Aquarium. San Francisco is the is the woke Kool-Aid Aquarium. Um and so your
1:22:49
reference point for what is a centrist is uh is is totally out of whack. Um
1:22:57
so um Reddit is headquartered in San Francisco. Um, Twitter was headquartered
1:23:03
in San Francisco. Um, you know, I, you know, I I moved X's headquarters to
1:23:09
Texas to to Austin, which Austin, by the way, is still quite liberalized, you know. Um, yeah. And, uh, and and then, um, the X and XAI
1:23:19
um, headquarters are in PaloAlto, which is still California. Um,
1:23:25
the engineering headquarters in in Palo Alto just on Paige Mill. Um but but even
1:23:30
Palo Alto is way more normal than that than than San Francisco Berkeley. Uh San
1:23:35
Francisco Berkeley is um extremely left like left of left. You can't like you
1:23:42
need a telescope to see the center from uh San Francisco, you know. Um
1:23:48
and um it used to be such a great city. I mean San Francisco has tre San
1:23:54
Francisco has tremendous amount of inherent beauty. No question about that. Um and and the California has incredible
1:24:00
weather. Um and and no bugs. Um it's just like amazing. Um beautiful, you
1:24:08
know. Um but but you say like what's the cause of this? It's it's just that if um
1:24:15
if companies are headquartered in a location where the belief system is very
1:24:20
far from what most people believe, then from their perspective, anything
1:24:28
centrist is actually right-wing because they're so far left. They're so they're
1:24:34
so far from the center in San Francisco that anything they're like they're they're just railed to maximum left. So
1:24:41
that's why that's why, you know, I think I think you're centrist. I I mean I think I think I'm centrist, but to from
1:24:48
the perspective of someone on the on the far left, we look right-wing. Yeah.
1:24:54
Um and um you know, they think anyone who's a
1:24:59
Republican is basically like some fascist Nazi situation. But what's so crazy is like it's very easy to
1:25:04
demonstrate just from like Hillary's speeches from 2008 and Obama's speeches like when they were talking about
1:25:10
immigration like they were as faright as Steve Bannon when it comes
1:25:16
to immigration. Yes. Um Hillary was like very MAGA. Have you I'm
1:25:21
sure you've seen that campaign speech which was talking about if anybody's committed a crime get rid of them. And
1:25:27
if you're here you pay a a hefty fine and you have to wait in line.
1:25:32
It was really crazy. It's crazy to listen to because it's like it's as MAGA as, you know, as Marjorie Taylor Green.
1:25:40
Yeah. I mean, if you've seen these videos people post online where they'll take like um a speech from Obama or
1:25:45
Hillary and and and they'll interview people on on like college campus or something and say, "What do you think of
1:25:50
the speech by Trump?" And they're like, "Oh, I hate it. He's a racist bigot." I'm like, "Just kidding. That was Obama."
1:25:59
No, actually that was Obama or Hillary. Um to your point, like literally the the
1:26:07
um the center's been moved so far. Yeah. Yeah. The left is so the left has gone so far left that they
1:26:13
they they they need, you know, they can't even see the center with a telescope. And the danger with without you
1:26:20
purchasing Twitter was that was going to swipe over the whole country and change where the levels were.
1:26:26
Yeah. And so what would be rational and and normal would be far left of what was
1:26:33
rational and normal just a decade earlier. Yeah. So exactly. So
1:26:38
historically, um, you'd have San Francisco, Berkeley being, you know, very far-left, but the
1:26:45
the sort of the the the fallout from the somewhat nihilistic uh philosophy of San
1:26:53
Francisco, Berkeley would be limited in geography to maybe like, you know, 10 mile radius, 20 mile radius, something
1:26:59
like that. Um but when um but but San Francisco and Berkeley have to be
1:27:04
colloccated with Silicon Valley with with with uh engineers who created information super weapons and those
1:27:11
information super weapons uh were then hijacked by the far-lft activists to pump far-lft propaganda to everywhere on
1:27:19
earth. Like I just you know that like old RCA radio tower thing where it's like radio
1:27:25
tower on earth and it's just broadcasting. Yeah. That's that's what happened is that the as an extremist far-left
1:27:33
ideology happened to be colllocated with the smartest where where the smartest
1:27:39
engineers in the world um were who created information super weapons that were not intended for this purpose but
1:27:45
were hijacked by the uh extreme activists who lived in the neighborhood. That's what happened that they they
1:27:52
hijacked the the modern equivalent of the RCA radio tower and broadcast that
1:27:57
philosophy everywhere on Earth. Yeah. And you see the consequences. Um
1:28:03
particularly in places that don't have free speech. Yes. Right. Like England, you know, we've Yeah. Where they lock people up for
1:28:09
memes and stuff. Literally. Literally. 12,000 people this year. 12,000 12,000 12,000 arrests for social media
1:28:16
posts. I mean, yeah. Some of these some of
1:28:22
these things you read about it and it's like literally it's someone had a meme on their phone that they didn't even
1:28:28
send to anyone, right? And they got they and and they're like
1:28:33
in in prison for that. Yeah. Um and there was a case in Germany where a woman got a longer sentence than the
1:28:41
guy that raped her uh because of something she said on a group chat.
1:28:48
Wow. Was it an immigrant who raped her? Yes. Yeah. It was his culture.
1:28:53
Yeah. He didn't know. He didn't know better. Yes. I think I think she said something
1:28:59
um you know, not not like was was critical of his culture and uh and and
1:29:04
and she got a longer sentence than the guy who raped her in Germany. Just the UK, Europe, Germany, England thing
1:29:12
seems so insane. It totally insane. I actually didn't realize it was like such a huge number
1:29:17
of people that got 12,000. Yeah. Far above Russia, far above China, right?
1:29:22
Far above anywhere on Earth. UK is number one. Well, you know, things like like I actually, you know, uh I talked to
1:29:30
friends of mine in in in England and um I was like, "Hey, um aren't you worried about this?" Like, uh you know,
1:29:36
shouldn't you be protesting more? Um, and I mean the problem is that like the,
1:29:43
you know, the the the legacy mainstream media doesn't cover the stuff. They're they're like, "Oh, everything's
1:29:49
fine. Everything's fine." You know, um, most people aren't even aware of it until they come knocking on your door. Yeah. Until like, so I mean the the
1:29:57
these these like lovely sort of small towns in in in, you know, in England,
1:30:02
Scotland, Ireland, you know, they're they're they've been like sort of living their lives quietly. They're they're
1:30:09
like hobbits, frankly. So So it's in fact J.R. Tolken based the hobbits on
1:30:16
people he knew uh in small town England because they were just like lovely
1:30:22
people who like to you know smoke their pipe and and have uh nice meals and
1:30:27
everything's pleasant. um the the hobbits in the Shire. The Shire he's
1:30:33
he's talking about, you know, places like Harper, like the Shire around in in in the greater London area, Oxfordshire
1:30:40
type of thing. Um and um they've but they're the reason they've
1:30:47
been able to enjoy the Shire is because hard men have protected them from the dangers of the world.
1:30:54
And um but but since they have no or very
1:30:59
almost no no exposure to the the the dangers of the world, they don't realize that they're there until one day, you
1:31:07
know, um a thousand people show up in your village of 500 out of nowhere and rape and and start
1:31:15
raping the kids. This has now happened god knows how many times in in Britain. And the crazy
1:31:23
literally raping. It's right like there some 10-year-old got raped in Ireland like last week. Yeah. There's literal rap.
1:31:28
They snatched some kid. Yeah. Yeah. And if you criticize it, you can get arrested. And that's where it gets
1:31:35
insane. It's like how are they not They literally criticize it. uh like the I think it was the prime minister of
1:31:40
Ireland actually you know posted on X uh cuz cuz after that um some I think some
1:31:49
illegal migrant snatched a 10-year-old girl uh who was like going to school or something and violently raped
1:31:56
10-year-old girl um uh and there was a you know the people were very upset
1:32:01
about this uh and they protested um prime minister of Ireland instead of
1:32:07
saying Yeah, we we really shouldn't be importing violent rapists into our country. He criticized the protesters
1:32:13
instead and didn't mention that. That the reason they were protesting was because a 10-year-old girl from their
1:32:19
small town got raped. So there here's the question. Why are they supporting this kind of mass
1:32:25
immigration? And what is this is there a plan involved in all this? Is just is
1:32:33
this incompetence? Is this ignoring the fact that they don't have a handle on
1:32:38
it? So, they're trying to silence disscent, like what is happening?
1:32:45
Um cuz if you wanted to destroy civilization, if you wanted to destroy
1:32:50
Western civilization, which seems to want to do, um
1:32:57
and you know, there's just so the
1:33:03
uh there there's a guy I think who I don't know if he's been on your show, you know, God. Yeah. Has he been on the show? Good friend of mine. Yeah.
1:33:09
Yeah, he's great. He's been on multiple times. Oh, great. That's all he's awesome. Um so uh you know the way he's got a good
1:33:15
good uh way to describe it which is suicidal empathy. Yes. So um is is that you pray upon people's
1:33:22
empathy. You say like well like you feel sorry for for for something for some group and then like well um and and that
1:33:30
that that empathy is to such a degree that it is suicidal to to to your
1:33:36
country or culture. Um and um and and that's that that suicidal empathy cuz I
1:33:44
don't think we we should have empathy but but but we should have we should that empathy should should extend to the
1:33:50
victims not not not just the criminals. We should have empathy for the people
1:33:55
that they pray upon. Um but that suicidal empathy is also responsible for
1:34:01
for why somebody's you know arrested 47 times for for violent offenses gets
1:34:06
released and then goes and uh murders somebody um in the US that that's you
1:34:12
see you see that same phen phenomenon playing out everywhere uh where the the suicidal emphy is to such a degree that
1:34:19
we're actually allowing um our women to get raped and our children to get killed.
1:34:26
But it just doesn't seem like that would be anything that any rational society
1:34:31
would go along with. That's what makes me so confused. It's like you're
1:34:36
importing massive numbers of people that come from some really dark places of the
1:34:42
world. Well, there's no vetting is the issue. It's like it's like if like um
1:34:48
if if there's no vetting like people are just coming through like well what's to stop someone who just committed murder
1:34:54
in some other country from um coming to to the United States or coming to to to
1:34:59
Britain um and just continuing their career of of rape and murder like unless
1:35:04
you've done unless some due diligence to say like well who who is this person? What's their track record? If if you if
1:35:12
you haven't confirmed that they have a track record of being uh you know uh
1:35:18
honest and uh not being a homicidal maniac, then any homicidal maniac can
1:35:23
just come across the border. And that's not to say everyone who comes across the border is a homicidal maniac. If you're not have if you don't have a vetting
1:35:29
process to to confirm that you're not letting in um people who who will do
1:35:37
some serious violence, you will get people who do serious violence uh sometimes coming through.
1:35:42
Well, especially if you don't punish them and if you don't deport them and if you are just like what but what is the
1:35:48
purpose of allowing all those people into the country? It can't be I wouldn't imagine that anyone in their society
1:35:54
supports this. Well, let me explain. So, so, so the the cuz you mentioned for example how much
1:36:00
say Hillary and and Obama have changed their tune um from prior speeches where
1:36:06
they were hot they were hard-nosed about not letting in uh anyone who is a a
1:36:12
criminal into the country um you know having sec secure borders all that stuff. So why did they change their
1:36:18
tune? The reason is that they discovered that those people vote for them.
1:36:23
That's why they want the open borders because if you let people in, they know the Democrats let them in. They'll vote
1:36:30
for Democrats. Yes. If you allow them to vote, which which they are actively trying doing, they they turn a blind eye to
1:36:37
illegal voting. Well, California literally doesn't allow you to show your license. California and New York have made it
1:36:44
illegal to show your photo ID when voting. Thus, effectively they've made uh it
1:36:50
impossible to prove for fraud. Impossible. They they've essentially legalized fraudulent voting in
1:36:57
California and New York and many other parts of the country. There's no rational explanation that I've ever seen anyone give as to why
1:37:03
that would be the policy unless you were trying to just allow people to vote illegally because there's
1:37:08
no other reason. If you need a driver's license or you need an ID for everything else, including just recently to prove
1:37:15
that you were vaccinated, the the same people who are demanding that you have that you have a vaccine
1:37:21
passport and and are are the same ones saying you need no ID to vote.
1:37:27
Same people, right? But like so it's obviously hypocritical and inconsistent.
1:37:32
So you really think it's just to to get more voters? If if you want to understand behavior,
1:37:39
you have to look at the incentives. Um so uh once uh you know the Democratic
1:37:47
Party in the US and the left in in in in Europe realized that um if you have open
1:37:53
borders um and you provide a ton of government handouts which creates a massive financial incentive uh for
1:38:00
people from other countries to to come to to your country and you don't prosecute them for crime, they're going
1:38:06
to be beholden to you and they will vote for you. And that's why
1:38:12
uh Obama and Hillary went from being um against open borders to being in favor
1:38:20
of open borders. That's the reason in order to import voters so they can win
1:38:27
elections. Um and the problem is that that has a a
1:38:33
negative runaway effect. So if they get away with that like it it is it is a winning strategy. If they are allowed to
1:38:39
get away with it, they will import as the enough voters to get supermajority voting and then there is no turning
1:38:47
back. We talked about this before the election and then you know you literally pointed towards a camera. You faced the camera
1:38:52
and said that if you do not vote now, you might not ever be able to do it again because it it'll be it'll be
1:38:58
futile. It'll be overrun. Yes. They'll keep the borders open for another four years and then their
1:39:03
objective will be achieved. Correct? If if if Trump had lost um there would never have been another real
1:39:09
election again. Um because Trump is actually enforcing the border. Um now
1:39:16
you you can can you can point to situations where uh there's been uh you
1:39:22
know um you know immigration had you know enforcement has been overzealous
1:39:29
because they're not going to be perfect. There'll be cases where they've been overzealous um in in expelling illegals.
1:39:35
Um so um but if you say that the the the standard must be perfection uh for
1:39:42
expelling legals then you will not get any expulsion um because perfection is impossible. Um so
1:39:50
and you've probably got millions of people that are here that are trying to be here under some asylum pretense,
1:39:59
right? Yes. Like you could just come from a war torn part of the world. No, they changed the definition of asylum to be an economic
1:40:06
to be economic asylum which is everybody which is everybody. Yeah. So
1:40:12
bar to prove it's yeah asylum is supposed to mean that if you go back to your country
1:40:18
you'll get killed you know that that's what we mean by that was what it's supposed to mean. Uh
1:40:23
they changed the definition of asylum to be uh you will have a decreased standard of living
1:40:29
which is obviously not real asylum. Um, and and it's and and you can you can test the absurdity of this by the fact
1:40:35
that people who are asylum seekers go on vacation to the country that they're seeking asylum from.
1:40:42
You know, that doesn't make any sense. Yeah. It doesn't have to.
1:40:48
But when you when you understand the incentives, then then you understand the behavior. Um, so once the left realize
1:40:55
that uh that illegals will will vote for them if they allow if they have open
1:41:02
borders and and combine that with uh with government handouts.
1:41:08
Yeah. to create a massive incentive. They're basically uh using US and and
1:41:14
Europe, US and European taxpayer dollars to provide a financial incentive to
1:41:20
bring in as many illegals as possible to vote them into a into permanent power
1:41:25
into and create a one party state. And and I I invite anyone who's is
1:41:30
listening to this ju just do do any research and the more you the more you
1:41:35
dig into it, the more it will become obvious that what I'm saying is absolutely true. Well, they were busting
1:41:41
people to swing states. It's it's clear that they were trying to do something. And then you had Chuck Schumer and Nancy
1:41:47
Pelosi who are actively talking about the need to bring in people to make them citizens because we're in population
1:41:54
collapse. Yes. Yeah. No, that's it's it's that it's that meme. Yeah. Where so many times where they start off
1:42:01
by saying it's it's not true. It's a right-wing conspiracy theorist, right? Um then it starts then it's like uh I
1:42:09
think the ne the next step is um well it it might be true and then it's like okay
1:42:17
it is true but here's why. And then the final step is it's true and here's why it's good. And it's like but
1:42:22
wait a second you started off saying it's untrue and it's a right-wing conspiracy theorist. Now you're saying it Not only is it true, but it's a good
1:42:29
thing and we must do more of it. Well, this is the thing about Medicaid and Social Security and people getting
1:42:35
social security numbers. You know that we're massive fraud. It's massive fraud and it's real and they
1:42:41
denied it forever. And now we're finding out this is part of the reason why there's this government shutdown that's
1:42:46
going on right now. Yes. the the entire basis for the government shutdown is that um is that
1:42:53
the Trump administration correctly does not want to send massive amounts of like
1:42:59
hundreds of billions of dollars uh to fund uh illegal immigrants in the blue
1:43:04
states or in all the states really. Um, and so the and the Democrats want to
1:43:11
keep the the the money spiggot going to incent uh illegal immigrants to come into the US who will vote for them.
1:43:19
That's the crux of the battle. So they want to stop this. So what's
1:43:25
going on right now is they have been funding these people. They've been giving them EBT cards. They've been
1:43:31
giving them Medicaid. And more than that, just like like they were the um like like they were taking
1:43:40
hotels like four and fivestar hotels like the Roosevelt Hotel being the classic example um was they were sending
1:43:46
I think $60 million a year to the Roosevelt Hotel to uh which all it did was was house illegals. It used to be a
1:43:54
nice hotel. I mean it still is a nice hotel. Um uh but uh
1:43:59
and and all around the country this was happening and all tax dollars. Yes. Yeah. And
1:44:06
um Yeah. And uh the Trump administration cut off funding for example to to the uh
1:44:11
uh to the you know Roosevelt Hotel and these other hotels saying like we it
1:44:17
it's US tax dollars should not be paid be sent to have luxury hotels for
1:44:23
illegal immigrants that American citizens can't even afford which obviously is the that's that's insane.
1:44:30
That's what was happening. They were also g giving out like debit cards with $10,000.
1:44:35
So, it's not just about medical care. Um, the the the Democrats mention the
1:44:40
medical care because they're they're trying to prey on people's empathy as much as possible and then they imagine,
1:44:46
oh wow, somebody has a desperately needed medical procedure and um shouldn't we maybe do, you know, take
1:44:52
care of them in that regard, but but they what they do is they divert the Medicaid funds uh uh and turn it into a
1:44:59
slush fund for the for the states that goes well beyond uh emergency medical
1:45:04
care. and New York and California would be bankrupt without uh without the massive
1:45:11
fraudulent federal payments that go to those states to pay for illegals to to to create a massive financial incentive
1:45:17
for for illegals. How would they be bankrupt because of that? Uh they wouldn't be able to balance their state budgets and they can't issue
1:45:23
currency like the Federal Reserve can and so the their ability to balance
1:45:28
budget is dependent upon illegals getting funding. the the the scam level here is is
1:45:37
so staggering. Um so there are there are hundreds of
1:45:42
billions of dollars in of of transfer payments from from the federal
1:45:47
government to the states. Um those transfer payments uh the the states
1:45:54
self-report what those transfer payment numbers should be. So, California and
1:45:59
New York and Illinois lie like crazy uh and say and and say that this these are
1:46:05
all legitimate payments. Well, these days they I think they they're even admitting that they they literally want
1:46:10
uh hundreds of billions of dollars for illegals. Um but uh but for a while there they're trying to deny it. Um so
1:46:18
you get these transfer payments for for every every government program you can possibly think of. Um and and and these
1:46:25
are self-reported by the state and there and and at least historically there was no enforcement of uh of California um
1:46:34
New York, Illinois and and and other states when when they would lie. There was no actual enforcement to say like,
1:46:39
"Hey, you you're lying. These these these payments are fraudulent." Now, under the Trump administration, um that
1:46:46
Trump administration does not want to send hundreds of billions of dollars of fraud fraudulent payments to the states.
1:46:53
the um and the reason you have this the standoff is because if the hundreds
1:46:58
hundreds of billions of dollars uh to create a financial incentive to like to
1:47:03
have this giant magnet to attract illegals from every part of earth to uh to these states if if that is turned off
1:47:10
they the the illegals will leave because they're no longer being paid to come to
1:47:17
the United States and stay here. Wow. And then then then they will lose a lot
1:47:22
of voters. The the the Democratic party will lose a lot of voters and they would have a very difficult job
1:47:27
if this is kicked out of reintroducing it into a new bill. Yes. Especially once things start
1:47:34
normalizing. Yes. So like in a nutshell um the Democratic party wants to destroy
1:47:40
democracy by importing voters and the
1:47:46
you know the Republican party disagrees with that. And the ruse is that if you don't accept what they're doing then you're a threat
1:47:52
to democracy. Yes. As they try to destroy democracy.
1:47:57
Yes. By importing voters and incentivizing people to only vote for them
1:48:02
and overwhelming the system. Yes. And and by the way, it's a strategy
1:48:07
that if allowed to work would work and in fact has worked. Um California supermajority Democrat.
1:48:14
Yeah. Um and and there's so much gerrymandering that that that occurs. It's it's it's crazy. Um so
1:48:21
I'm sure you're paying attention to this Proposition 50 thing. That's the thing in California where
1:48:26
they're trying to re redo districts. Yeah. Because I mean California is already
1:48:32
gerrymandered like crazy. Yeah. Um they want to gerrymander it even more. Um and and I mean
1:48:38
because it keeps moving further and further right. Like if you look at the map of California each voting cycle more
1:48:44
and more people are waking up and going what the [ __ ] and we need to do something to fix this. The only option
1:48:50
available other than the policies that you guys have always done is go right. And so a lot of people have been air air
1:48:56
quotes red pill. Yeah. And and and then here's another thing that is very important. um fact that
1:49:04
that is actually not disputed by by either side which is that when when we do the census in the United States, the
1:49:09
census, the way the census works uh for aortionment of congressional seats and
1:49:14
um electoral electoral college votes for the president is by number of persons in
1:49:19
a state, not number of citizens, right? It's number of pe people. So you could literally be a tourist and you will
1:49:26
count. Now how do they do the census when they do that? Do they is it do they ask
1:49:32
people? Do they knock on doors? Do they have them fill out forms? Like what? Yeah, I think they they mail out census
1:49:38
forms and knock on doors. Um but the way the law reads right now um and and uh is
1:49:45
that all if if you are a human with a pulse um uh then you count in the census for
1:49:53
allocating congressional seats and presidential votes,
1:49:58
right? So, uh, you so electoral college,
1:50:04
it doesn't matter whether you're here legally, illegally, and if if you're a human with a pulse, um, you count for
1:50:10
congressional aortionment. So that means that uh the more people the more
1:50:15
illegals that California and New York can can import when by the time the census happens in 2030 um the more
1:50:23
congressional seats they will have um and the more electoral the more presidential electoral college votes
1:50:30
they will have um so they're trying to get as many uh illegals in as possible
1:50:35
ahead of the census. Um and because all h all all human beings even tourists
1:50:43
count for the census and and then if you combine that with gerrymandering of of
1:50:48
districts in New York and California as you point out with this proposition where they're trying to increase the
1:50:55
amount of gerrymandering that occurs in California, the biggest state in the country. Um so so you get so so if the
1:51:02
this if this the census then would award more congressional seats to California uh because of a vast number of illegals
1:51:09
and New York and Illinois. Um so they get more congressional seats. They would
1:51:14
get more presidential electoral college votes getting that would get them the house the uh a majority in the house and
1:51:22
and and they would get to decide who is president uh based literally based on legals. This
1:51:28
is these are not disputed facts by either party. I want to emphasize that that's sink in.
1:51:35
Yeah, this is not a These are not disputed facts by either party. It's not a
1:51:40
this these are just this is just the the way the law works. It it's it is a
1:51:47
you know like I don't think the law should work that way. Uh I think it should the aortionment should be
1:51:52
proportionate to to to citizens. But isn't that a problem with how the constitution is written? Yeah. Yeah. Yeah.
1:51:58
Um, they can't really change that. I I'm not sure if it's constitutional or
1:52:03
u but it it it is the way the law is written. I'm not sure if it's in the constitution or not in this way, but um
1:52:09
but it is that is the way the law is written. So, it is an incentive and but it's an incentive that would be removed with
1:52:15
something simple that makes sense to everybody that only the people that should count are people that are
1:52:21
official US citizens. Yes. So, the way the way it should work is that only US citizens should count in
1:52:26
the census for purposes of of determining voting power because people that aren't legal can't
1:52:33
vote supposedly. They're not supposed to be voting. Um but but they do. Um uh but but even even
1:52:39
if even besides that, like said, I I I just can't emphasize this enough because this is a very important concept for people to understand. um is that the law
1:52:47
um the the law as it stands um counts all humans with a pulse in in in in a
1:52:55
state for deciding how many u house of representative votes and how many
1:53:00
presidential electoral college votes uh a a state gets. So the incentive
1:53:06
therefore is uh to uh for California, New York and Illinois to maximize the number of illegals so they get so they
1:53:13
get um so that they take house seats away from red states assign them to California, New York, Illinois and so
1:53:19
forth. Um then then you combine that with extreme gerrymandering in in you
1:53:25
know California, New York, Illinois and and whatnot. So that that basically you you can't even elect any Republicans and
1:53:31
then they get control of the presidency, control of the house, then they keep doing that strategy um and and cement a
1:53:38
supermajority. That is what they're trying to do. So that would essentially turn the
1:53:44
entire country into California. Yes. Where you have differing opinions, but it doesn't matter because one party is
1:53:49
always in control. Yes. Um, when you first started digging into
1:53:55
this, when you first started before you even accepted this role of running Doge
1:54:01
and being a part of all that, did you have any idea that it was this [ __ ] up? Um, I I did. Yeah. I I mean, I sort of
1:54:08
When did you start knowing? Um, I guess about like Well, about two years ago.
1:54:13
Isn't that crazy? Yeah. like relatively recently, you know. So maybe I started I started having an well
1:54:19
I I started like basically having a bad feeling about 3 years ago, which is why which was which is when
1:54:25
uh why I felt it was like critical to acquire Twitter um and have a maximally truth seeeking platform, not one that
1:54:32
suppresses the truth. Um and um like it it was more it was more like I
1:54:39
like I'm not sure what's going on, but I have a I have a bad feeling about what's going on. And then the more I dug into
1:54:44
it, the more I was like, "Holy [ __ ] we got a real problem here and America's going to fall."
1:54:49
So, uh, without anyone knowing it had fallen, that's that would be the problem. It could have fallen and been unrepable
1:54:56
without anyone really being aware of what had happened, especially if you didn't buy Twitter.
1:55:02
Yes, that's that's it. Look, buying Twitter was a a huge pain in the ass. Um, and made me a a a a pin cushion of
1:55:10
attacks. Like dab dab stab dab dab. Everybody loved you before that. Well, some people love a lot of people loved you. A lot of
1:55:17
lefties loved you. Uh, I I was a hero of the left. As far as the thing, if you drove a Tesla, it
1:55:23
showed that you were environmentally conscious and you were on the right side. Uh, yeah. Um,
1:55:31
yeah. I mean, I'm still the same human. I didn't like have a brain transplant between, you know, since in like three
1:55:37
years ago, you know. Um Well, that's my favorite bumper sticker that people put on Teslas now. I bought this before Elon went crazy.
1:55:44
I took a picture of one the other day. Oh, you found somebody. Oh, yeah. I've seen I've seen three or four of them.
1:55:49
People that have these bumper stickers on their car that says, "I bought this before Elon went crazy." Because when
1:55:54
people were vandalizing Teslas Yeah. Um the most there was organized
1:56:01
campaign to literally burn down Teslas and and we had one of our dealers got
1:56:06
shot up with a gun like they fired bullets into the in the Tesla dealership. They're burning down cars.
1:56:12
Uh it was crazy. Um uh so but the bumper sticker should read
1:56:19
there should be an an addendum to the bumper sticker. It's like I bought this car before uh Elon turned crazy. Actually, now I
1:56:26
realize he's not crazy and I've seen the light.
1:56:32
That'll take some time. That'll take some time. People don't want to admit that they've been tricked.
1:56:37
Yeah. I mean, there's that old saying where it's like it's really easy to fool somebody, but it's almost impossible to
1:56:43
convince someone that they were fooled. Yeah. It's much easier to fool them than to convince them they've been fooled.
1:56:49
People cling to their ideas. Yes. They especially if they've like publicly stated these things, they get
1:56:55
very embarrassed of being foolish. Yeah. People most time they double down.
1:57:00
Um and uh and they find echo chambers. Yeah. Yeah. But but there's you know the
1:57:06
thing is that like I you know I've seen more and more people who were convinced
1:57:12
of the sort of work ideology um see the light.
1:57:17
Yeah. So, not everyone, but it's more and more um are seeing the light. Um and
1:57:22
and it tends to happen like when when something happens that really, you know, directly affects you, right? Um you like there was a friend of mine
1:57:29
who uh was living in in the San Francisco Bay area and um that tried to
1:57:36
trans his his his daughter um did like to the point where the the school like
1:57:42
sent sent the police to his house to take his daughter away from him.
1:57:48
Now, now that's going to radicalize you. Well, that's going to break that's going to shake you out of your blue structure.
1:57:54
Um, now I know, so it was an activist at the school that was trying to do this.
1:57:59
Yeah, the school and the and the state of California conspired to turn his daughter against him and uh make her
1:58:06
take uh lifealtering drugs that would have sterilized her um and uh
1:58:13
irreversible. And how old was she? I think 14, something like that. Um, so and but he
1:58:21
he managed to talk the police out of taking his daughter away from him that day. Um, and that that night he got on a
1:58:27
plane to Texas. Wow. Um and uh like you know a year
1:58:34
after just being in in a in a school in like greater Austin area um she she went
1:58:41
she came went back to normal meaning like it it wasn't real right
1:58:46
um well people are being much more open to that now. I mean Wall Street Journal
1:58:52
uh yesterday had that opinion piece that this whole trans thing there's a lot of
1:58:57
evidence is a social contagion. Absolutely. And Colin Wright wrote that. And then he's getting death threats now, of
1:59:03
course. And on Blue Sky, there's people talking about exterminating him, which is one thing that you are allowed to say on Blue Sky, apparently.
1:59:10
You're you're allowed to say horrible things about people saying possibly truthful things about this whole social
1:59:18
contagion. Cuz that's what when you get nine kids that are in a friend group and they all decide to turn trans together.
1:59:24
Yeah. Something's wrong. That's not statistically Yeah. Like here's the like you can
1:59:30
convince kids to do anything. You can convince kids to be a suicide bomber, right? So which is why they do with in in some
1:59:36
countries why they choose children to do that. Yes. You can train kids to be suicide bombers. And if you can train kids to be
1:59:42
su suicide bombers, you can convince them of anything. Yeah. Especially with enough positive enforcement and cultural enforcement and
1:59:49
you and and the idea that that that's not the case. Kids kids are kids are um malleable. The
1:59:56
minds of youth are easily corrupted. You're also seeing a lot of push back from gay and lesbian people that are
2:00:01
saying like, "Hey, if someone stop including me in Yeah. Exactly. the LGBT, you know, it's like, wait a
2:00:07
second, why are we being included all the time in this situation?" Exactly. Exactly. When especially when,
2:00:13
you know, like my friend Tim Dylan's talked about this is like it's really homophobic because you're taking these gay kids and you're you're telling them
2:00:20
like, "Hey, you're not gay. You're actually a girl." Yes. and you know, hey, hey, go make it
2:00:26
so that you can never have an orgasm again and you'll be happy. Like, yeah, [ __ ] permanent mutilation, permanent
2:00:32
castration of of kids is like I I think I I we should look look at at uh anyone
2:00:39
who permanently castrates a kid as like right up there with Ysef Mangler.
2:00:45
Yeah. I mean, they're they're mutilating children. Yeah. Yeah. And um it's thought of as
2:00:51
being kind. And the thing is, would you rather have a live daughter or a dead
2:00:56
son? That's that's the that's the line they use. Yeah. Which is not supported by any data. No. In fact, the the probability of
2:01:03
suicide increases. Right. This is important maybe for the audience to know. Uh the probability of suicide
2:01:09
increases if you're transit kid, not decreases. By some accounts, it triples. So that
2:01:16
that is an evil lie. And it's a lie that is supposedly compassionate. Imagine
2:01:22
you've twisted reality to the point where confusing a child that's not even
2:01:29
legally allowed to get a [ __ ] tattoo. Yeah. Right. Because you think that you could make a mistake with a tattoo, a totally
2:01:35
removable thing, right? If I wanted to, tomorrow I can go to a doctor and they could laser off every tattoo that I have on me.
2:01:41
Right. Okay. No harm, no foul. Yeah. But you get sterilized like that's it forever.
2:01:46
Forever. Yes, they'll castrate you. You no longer have testicles. You have no penis. You have a
2:01:52
hole where your penis used to be. Yes. And this is compassionate and this is preventing you from Actually, a lot of kids die uh in in
2:02:00
with the these uh sex change operations. They die the number of deaths on the operating table. People don't hear about
2:02:06
those. A lot of kids because that we it's we don't really actually have the
2:02:12
technology to make this work. So a bunch of the times the kids just die in the sex change operations.
2:02:18
Jesus Christ. Yeah. It's it's demented which it should be viewed as like you know um like like
2:02:26
evil Nazi doctor stuff basically. That's why it was like real Nazi not the [ __ ] fake Nazi stuff.
2:02:32
Crazy that even pushing back against something that seems like fundamentally logically
2:02:37
very easy to argue the old Twitter would ban you forever. Uh, yes.
2:02:45
That's how crazy a social contagion can get when it completely defies logic, victimizes children, does something that
2:02:52
makes no sense, does not supported by data, all connected to this ideology that trans is good. We got to save trans
2:03:00
kids, protect trans kids. Yeah. And what I want to emphasize is that the the save trans kids thing is a
2:03:06
lie. Um if if you if you if you castrate kids and trans them the probability of
2:03:13
suicide increases. It does not decrease. It substantially increases. Um the the
2:03:19
the studies have done that I've seen the the risk of suicide triples if you trans kids.
2:03:27
So you're not saving them, you're killing them. Moreover, during the sex change operation, there are many deaths
2:03:33
that occur during the sex change operation. Jesus Christ.
2:03:41
It's just crazy that this is a real issue. Yeah, it it's a nightmare fever dream
2:03:46
and and people are finally waking up from it. Now, when you started getting into the
2:03:53
Doge stuff and started finding how much money is being shuffled around and moved
2:03:59
around to NOS's and how much money is involved and just totally untraceable funds like
2:04:08
this is again something like two years plus ago you weren't aware of it all.
2:04:15
No, I was aware of it. Um I just didn't realize how how the how big it was. just
2:04:20
just just how much waste and forward there is in the government is truly vast.
2:04:26
Um in fact the government didn't even know um and nor did they care.
2:04:33
That's crazy. Yeah. And I mean just like some of the very basic
2:04:39
stuff that Doge did um will have lasting effects. Um and some of these things like they're so elementary you can't
2:04:45
believe it. So, um the the doge team got the um you know the the mo most of the
2:04:54
main payments computers um to require uh the the congressional appropriation
2:05:01
code. So, when a payment is made, you have to actually enter the congressional appropriation card. That used to be
2:05:06
optional and and often would be just left blank. So, the money would just go out, but it wasn't even tied to a
2:05:12
congressional appropriation. Then they also Dutch team also made the uh comment
2:05:17
field for the payment mandatory. So you have to say something. We're not saying that what what is said like you can say
2:05:23
anything. You you your cat could run across the keyboard. Uh you could go querty ASDF but you have to say
2:05:29
something above nothing because what we found was that there were tens of billions maybe hundreds of billions of
2:05:34
dollars that were zombie payments. So there like somebody had approved a payment uh uh somebody in the government
2:05:42
approved a payment um and uh some recurring payment and um they retired or
2:05:48
died or changed jobs and no one turned the money off.
2:05:54
So the money would just keep going out and and it's a pretty rare go where
2:05:59
to to the a company or an individual um and it's a pretty rare company or
2:06:05
individual who will complain that they're getting money that they should not get and and a bunch of the money was
2:06:10
just going to the were transfer payments to the states. So these are automatic payments there no
2:06:15
accounting for them at all. I imagine like like there's an automatic debit of your credit card um and you don't you never look at the
2:06:22
statement, right? Um, so it's just money going out. Uh, that's why I call them zombie
2:06:28
payments. Um, that there might have been they might have been legitimate at one point, but the person who approved that
2:06:35
recurring payment um, changed jobs, died, retired, or whatever, and no one ever turned the money off.
2:06:43
And my guess is that's probably at least a hundred billion a year, maybe 200. and
2:06:51
going where uh to to uh
2:06:58
uh I mean there there are millions of these payments. So so it's I mean millions
2:07:03
uh yes millions of payments that are going to who knows where. Yes. In a bunch of cases there are fraud
2:07:10
rings that operate uh professional fraud rings that operate to exploit the system. um they figure out some security
2:07:18
hole in the system and they just do professional fraud. Um and um you know
2:07:25
that's where we found for example people who were you know 300 years old in the social security administration database.
2:07:33
Now, I thought that this was uh a mistake of not registering their deaths
2:07:39
that people were born like a long time ago and it had defaulted to like a
2:07:44
certain number and so that after time those people were still in the system. It was just an error of the the way the
2:07:52
accounting was done. Yeah. So, um that's not true. So,
2:07:58
there's or or at least one of two things must be true. um the there's a there's a
2:08:04
typo or or some mistake in the computer or it's fraudulent, but we don't have
2:08:09
any 300-year-old vampires uh living in America. Allegedly. Allegedly. Um and uh or or and we don't
2:08:17
have people in some cases who's who are receiving payments who are born in the future.
2:08:23
Born in the future. Born in the future. Really? Yes. there the people receiving payments
2:08:29
whose birth date uh um was like in 2100 and something
2:08:34
okay so there's like next century is there a task we know we know that one of two things must be true um that that that either
2:08:41
there's a mistake in the computer or it's fraud but if you have someone's birth date
2:08:47
that's either in the future or where they are older than the oldest living American because the oldest living American is 114 years old so if they're
2:08:54
more than 114 years um there is either a mistake and someone should should call them and say I I
2:09:01
think we have your birthday uh wrong because it says you were born in 17 you
2:09:06
know 8086 um and um you know that was before you
2:09:11
know um you know before there was really an America you know it was it was like
2:09:17
uh you know kind of early you know we're still fighting England type of thing uh
2:09:24
you It's like uh this person either needs to be in the Guinness Book of World Records
2:09:31
or or they're not alive, but still at the end of the day, money is going towards that account that's
2:09:36
connected to this person that is either non-existent or so like like Yeah. So there was like uh
2:09:43
I think um something like I don't know 20 million uh people in the Social
2:09:50
Security Administration database that could not possibly be alive um if their birth date is like based on their birth
2:09:56
date they could not possibly be alive. And then to be clear 20 million people that were receiving funds
2:10:02
uh a bunch of most of them were not receiving funds.
2:10:07
Some of them were receiving funds. Most were not receiving funds. But so let me tell you how the scam works. It's it's a bank shot. So the Social Security
2:10:16
Administration database is used as the source of truth by all the other databases that the government uses. So
2:10:22
even if they stop the payments on the Social Security Administration database like unemployment insurance, Small
2:10:27
Business Administration, student loans all check the Social Security Administration database to say is this
2:10:34
is this a legitimate alived person? And uh and if the social security database
2:10:40
will say yes, this person is still alive even though they're 200 years old. Um but forgets to mention that they're 200
2:10:45
years old, it just says it just returns uh uh when when the computer is queries,
2:10:51
it says yes, this person is alive. And so then they're able to exploit the entire rest of the government ecosystem.
2:10:58
So fake then you get fake student loans, then you get fake unemployment insurance, then you get fake medical
2:11:03
payments. And this doesn't have to be tied to an individual where where there's an address where you can check
2:11:09
on this person. No, if you did do if just did any check at all, you would stop this.
2:11:17
So, so, so that's that that that's so so And how much money do you think is any check like anything at all that
2:11:24
would stop would stop the forward like any effort at all? Um, yeah.
2:11:29
So, there's multiple layers. the social security number verifies that this is a real person and then the other systems
2:11:35
check every other government payment and every other government payment system for everything for like small small
2:11:41
business administration uh student loans uh Medicaid Medicare uh every other
2:11:47
government payment of which there are many there there actually hundreds of government payment systems uh can all be exploited so long as social security
2:11:55
database says this person is alive that's the nature of the scam It's a
2:12:01
bank shot. So then the then the rebuttal from the Dems is like, oh well the vast majority of the people who are marked as
2:12:06
alive in the Social Security Administration weren't receiving Social Security Administration payments. That is true. What they forgot to mention is
2:12:12
they're getting fraudulent payments from every other government program. And that's why the the DMs were so
2:12:19
opposed to turning off to to declaring someone dead who was dead because it would stop the entire other all the
2:12:26
other fraud from happening. And so, but all this is it trackable like all this other fraud.
2:12:31
If they wanted to, they could chase it all down. Yeah. It's not even hard. And yet they're opposing chasing it all
2:12:38
down. They're opposing chasing it all down because it would it turns off the money magnet for the illegals.
2:12:45
Wow. Because it's very logical to to like
2:12:51
like I'm saying the most common sense things possible. If someone's got uh a
2:12:56
birthday in social security that is an impossible birthday, meaning they are
2:13:02
older than the oldest living American or were born in the future, then you should call them and say, "Excuse me, we seem
2:13:10
to have your birthday wrong." Uh because it says that you're 200 years old. That's all you need to do.
2:13:18
Um, and and then you would remove them from the social security database and make that number no longer available for all those
2:13:23
other government payments. Exactly. Wow.
2:13:29
And how much money are we talking? It's hund hundreds of billions of dollars. And this is all traceable. Like you
2:13:36
could hunt all like you don't need to be Sherlock Holmes here is what I'm saying. Well, this we don't need to call
2:13:42
Sherlock Holmes for this one. Is this part of you just need to call the person and and say, "Excuse me, we either we we
2:13:49
seem to have the like we we we must have your birthday wrong because it says you're 200 years old or were born uh in
2:13:55
the future. Um so could you tell us what your birthday is? That's all you need to do. It's it's
2:14:01
that simple." But the all these other government payments that are available that are connected to this social
2:14:07
security number, it seems like if you just chased that all down, Yeah.
2:14:13
you would find the widespread fraud. You would find where it's going. Yes. The but the root of the problem is
2:14:20
the social security administration database because um the social security
2:14:25
number in the United States is used as a deacto national ID number.
2:14:32
You know that's why like the bank always asks for your social like the you know any financial institution will ask for
2:14:38
your social security number.
2:14:44
This is it sounds so insane that this isn't chased down. I mean I agree that I mean I mean that in and of itself
2:14:51
is that's such mishandling. Yes.
2:14:58
No it's mind-blowing. Um, so yeah, it's crazy. Well, you were very reluctant last time
2:15:04
you were here to talk about the extent of some of the fraud because you're like, they could kill me because this is
2:15:11
kind of Oh, what? Yeah. What I was saying is that um the like if you create if like
2:15:19
uh I like like to be pragmatic and
2:15:25
realistic um you actually can't manage to zero fraud. you can manage to low fraud number but not to zero fraud. If
2:15:32
you manage to zero fraud, um you you you're going to push so many people over the edge who are receiving fraudulent
2:15:38
payments that the number of inbound homicidal maniacs will be uh really hard
2:15:43
to overcome. So I I'm I'm actually taking I think quite a reasonable position which is that we should simply
2:15:50
reduce the amount of fraud which I think is not an extremist position. Um, and we
2:15:56
should aspire to, you know, have less fraud over time. Um, not that we should
2:16:02
be ultra draconian and eliminate every last scrap of fraud. Um,
2:16:08
which I guess would be nice to have, but but like we don't even need to go that extreme. I'm I'm saying we should just stop the blatant large scale super
2:16:15
obvious fraud. I think that's a reasonable position. It's a very reasonable position. Yeah.
2:16:20
And so what was the most shocking push back that you got when you started
2:16:26
implementing Doge? When you started investigating into where money was going?
2:16:34
Well, um I guess it this was I should have anticipated this, but um
2:16:41
while most of the fraudulent government payments to especially to the NOS's go
2:16:46
to the Democrats, most of it like I don't know for argument sake let's say 80% maybe 90%. Um um 10 to 20% of it
2:16:56
does go to Republicans. And so when we'd turn off funding to a fraudulent NGO, we'd get complaints from
2:17:05
whatever the 10% of Republicans who were receiving uh the money and and they would, you know, they would very loudly
2:17:11
complain. Um because the the honest answer is the
2:17:18
Republicans are are partly they're receiving some of the fraud, too. They're getting a big
2:17:25
Jesus. Yeah, it's I want to be clear. It's it's
2:17:30
not like the Republican party is some um ultra pure paragon of virtue here. No.
2:17:36
Okay. Um well, you see that with the congressional insider training. It's across the board.
2:17:42
Yeah. It's left and right. I mean, the whole uni party criticism has some validity to it. you know,
2:17:48
there's so um and it's it's like if you turn off fraudulent payments, it's not
2:17:54
like like I say, it's not like 100% of those payments were going to Democrats. A a small percentage were also going to
2:18:00
Republicans. Those Republicans complained very loudly. Um and um
2:18:08
you know and and that's that's so there was a lot of push back on the
2:18:14
Republican side for when we started cutting some of these these funds and I tried telling them like well you
2:18:20
know 90% of the money is going to your opponents but they still if they even if
2:18:26
they're getting 10% of they want their peace. Yeah. They want their peace and they've been getting that peace for a long time. Yes.
2:18:35
Did you see this is why like you know politics is like it's dirty business. Yeah. I mean that's like saying if like
2:18:41
you know if if you if you like sausages and respect the law do not watch either of them being being made
2:18:51
yeah. Wow. Well that's not even true because I've
2:18:56
made sausage before. Yeah. Yeah. It's actually like it's not that big a deal. Yeah. fat and spices and casing,
2:19:03
run it through the machine. Not that big a deal. Yeah. Um but uh yeah, I mean I I think
2:19:10
the stuff I'm saying here is not uh like like if if you stand back and think about it for a second like oh yeah that
2:19:16
that makes sense, you know. Um the it's it's not like um it's not
2:19:22
like one political party is going to be um you know pure devil or pure angel.
2:19:28
There's, you know, I think there's there's there there's there's much more corruption on the Democrat side, but it's not there's not there's still some
2:19:35
corruption on the Republican side. How did it happen that the majority of the corruption wound up being on the Democrat side?
2:19:42
Well, because the the the transfer payments, especially to illegals, um,
2:19:47
uh, are very much on the Democrat side. That so that's the root of it all is the illegal situation. Yes. I mean, there's
2:19:54
or a focal point. Yes. It's it's also like it's it's um
2:19:59
it's it would also be accurate to say that while obviously not everyone who is a Democrat
2:20:05
is a criminal, almost everyone who is a criminal is a Democrat
2:20:11
because because the Democrats are the soft crime party. So if you're a criminal, who you going to vote for?
2:20:18
Right. Right. The soft crime party. Did you think you were going to be able to get more done
2:20:25
than you were? Um, we did get a lot done, right? Um, and Doge is still still still
2:20:32
happening, by the way. Um, this the the Doge is still underway. There are still
2:20:37
there are still um there's still waste and fraud being being cut by by the Doge
2:20:42
team. So, it hasn't stopped. Um, the it's less publicized. It's less publicized. Um, and they don't
2:20:49
have like a clear person to attack anymore. Well, it seems like they basically they
2:20:55
they applied immense pressure to me to just to stop it. So then I'm like the best thing for me is to just, you know,
2:21:02
cut out of this. In any case, as a special government employee, I could only be there for like 120 days anyway,
2:21:07
something like that. So whatever the law says. So I I could I I I was necessarily could only be there for 4 months uh as a
2:21:14
special government employee. So, um um but uh yeah, I mean I mean you turn
2:21:21
off the money spigot to to fraudsters, they get very upset to say the least. Um
2:21:27
and um but my like my death threat level went uh ballistic, you know, was like a
2:21:33
like a rocket going to orbit. Um yeah. Um, so but now that now now now
2:21:40
that I'm not in DC that that that I guess they don't really have a person to attack uh anymore.
2:21:46
Um, well the rhetoric about you has calmed down significantly. Yeah, it was disturbing. It was disturbing to
2:21:53
watch. It was like this is crazy. And to watch these politicians engage in it and all these people just like
2:21:58
framing you as this monster. I was like this is so weird. Like this is what happens when you uncover fraud.
2:22:04
Yes. The whole machine turns on you. And if it wasn't for a person like you who owns a platform and has an enormous amount of
2:22:11
money, like could have destroyed you. Yeah. And that was the goal. The goal was to destroy me. Absolutely.
2:22:18
Because you were getting in the way of this amazing graft. The the this gigantic fraud machine.
2:22:24
Yeah. Um like I think I think Doge team's on done a lot of good work. Um, you know,
2:22:32
and in in terms of uh fraud and waste prevented, my guess is it's, you know,
2:22:38
probably on the order of two or 300 billion a year. So, it's pretty good. What do you think could have been done
2:22:44
if you just had like full reign and total cooperation? How much do you think you could have saved?
2:22:50
I mean, what level of of power are we assuming here? Godlike. Oh, yeah. I probably cut the federal
2:22:55
budget in half and get more done.
2:23:01
That is so crazy. It is so crazy that get more done and federal budget
2:23:06
widespread. It's that widespread. Well, I mean a whole bunch of government departments simply shouldn't exist in my
2:23:12
opinion. um they they um you know um like examples
2:23:18
well the department of ed department of education which was created uh recently like under Jimmy Carter um uh the our
2:23:26
educational results have uh gone uh downhill ever since it was created. So
2:23:32
if you if you create a department and the result of creating that department is a massive decline in educational
2:23:37
results and it's department of education, you're better off not having it because we're literally we were did
2:23:44
better before there was one than after when you let the states run it. Yes. Yeah. Because at least the states can compete
2:23:50
with one another. Um so but the problem is like here like cutting department
2:23:55
education. our kids need education. Yeah, they do. But but this is a new department that didn't even exist um you
2:24:02
know until late the late 70s. Um and ever since that department was created,
2:24:08
the results educational results have declined. And so why would you have an institution
2:24:16
continue that has made education worse? It doesn't make sense.
2:24:21
They killed it though, right? No, there still unfortunately but they were trying to kill it. It has been substantially reduced.
2:24:27
Okay. Um what other organizations what other departments?
2:24:32
Well, I mean I'm a small government guy. So um you know when when the you know
2:24:39
when the country was created we just we we just had the department of state, department of war um you know and and uh
2:24:48
sort of the sort of the department of justice. We had an attorney general uh and Treasury Department. Um
2:24:56
I don't know why you need more than that. So what other departments specifically
2:25:02
do you think are just completely ineffective? Well, I mean here it's like a question.
2:25:07
It's a sort of philosophical question of how much government do you think there should be? Right. Um in my opinion, there should be uh the
2:25:16
least amount of government. I've heard the most bizarre argument against this is that you're cutting jobs and you're
2:25:22
gonna leave people jobless. And I'm like, but their jobs are useless. Yeah. Paying people to do nothing
2:25:28
doesn't make sense. Um like there's a like a a great um
2:25:34
a story about like Milton Friedman who is awesome. Um uh what like generally whatever Milton Friedman said is you
2:25:41
know people should should do that thing. Uh I'm not sure if it's apocryphal or not, but um like like someone complained
2:25:48
to him like he he he observed I think people that were like um digging ditches
2:25:55
with uh you know with with um shovels and um and he said well like allegedly
2:26:02
Freeman said, "Well, I think I think you should use you know um excavating equipment instead of shovels and you
2:26:08
could get it done with far fewer people." And then and then someone said, "But then we're going to lose a lot of
2:26:13
jobs." Well, in that then Freedom says, "Well, in that case, why don't you have them use teaspoons?"
2:26:23
Just just dig ditches with teaspoons. Think of all the jobs you'll create. I mean,
2:26:30
it's [ __ ] Basically, you just want people to work on on things that are that are productive. You want people to
2:26:36
work on on building things um on building you know uh providing products
2:26:43
and services that people find valuable um like you know making food um being
2:26:48
you know being a farmer or a plumber or electrician or just anyone who's a builder or providing useful services. Um
2:26:57
and um that's what you want people to be doing. um not fake government jobs uh
2:27:03
that that that don't add any value or may subtract value. Um
2:27:08
um there's also like you know uh to illustrate the absurdity of also how is
2:27:13
the e how is the economy measured like the the way economists measure the economy is is is nonsensical uh because
2:27:20
they'll measure any job no matter even if that job is a dumb job that has no point and is even counterproductive. So
2:27:27
like, so the like the joke is like there's two economists going on a hike in the woods
2:27:33
and they come across a pile of [ __ ] and one economist says to
2:27:38
the other, "I'll pay you $100 to eat eat that shit." The economist eats the [ __ ] gets the
2:27:44
$100. They they keep walking. Then the other econ then come across another pile of [ __ ] And and the the other economist
2:27:50
says, "Now I'll pay you $100 to eat the pile of shit."
2:27:56
say pays the so pays the other economist $100 pile of [ __ ] Then they then then
2:28:01
then the way said they said like wait a second um we both just ate a pile of
2:28:06
[ __ ] and we're no and and and and we're we're no we we we we
2:28:11
don't have any more extra money like like we both you just gave the $100 back to me and we both ate a pile of [ __ ]
2:28:18
This doesn't make any sense. And they said, "No, no, but think of the economy because that's $200 of that in the
2:28:25
economy that that basically measure eating eating [ __ ] would count as a as
2:28:31
as a as a job.
2:28:37
This is this is this is to illustrate the absurdity of of of economics.
2:28:43
One of the things you said when things should not count as a job." One of the things you said when you stepped away is
2:28:48
that you're kind of done and that it's unfixable. That um well or under its
2:28:56
current form the way people are approaching it
2:29:02
you can you can make it directionally better but ultimately you can't uh fully fix the system. Um,
2:29:10
so, uh, I I I like like like like it it
2:29:15
it is it is it would be accurate to say that even like like unless you could go like super
2:29:21
draconian like you know Gangghaskhan level on on on cutting waste waste and
2:29:27
fraud which you can't really do in a democratic country um an aspirationally
2:29:33
democratic country then um there's no way to solve the the the debt crisis.
2:29:39
So, we got we got national debt that's just insane where the debt payments the interest payments on the debt exceed our
2:29:44
entire military budget. I mean, that's one that was one of the wakeup calls for me. I was like, "Wait a
2:29:49
second. The interest on a national debt is bigger than the entire the entire
2:29:55
military budget um and growing. Um this is crazy. Um so
2:30:03
um so so even if you implement all these
2:30:09
savings, you're only delaying the day of reckoning for when America becomes goes bankrupt. So unless you go full
2:30:17
Genghaskhan, um which you can't really do. So um
2:30:25
so I came to the conclusion that the only way that the only way to get us out
2:30:30
of the debt crisis and to prevent America from going bankrupt is AI and robotics.
2:30:36
So, like we need to grow the economy um at
2:30:42
at a at a at a rate that allows us to u to pay off our debt. Um
2:30:51
and um I I I guess people just generally don't appreciate the degree to which um
2:30:57
you know this the the government overspending is is a problem. Um but even like the social security website,
2:31:04
this is under the Biden administration. On the website, it would say like uh we based on on current demographic trends
2:31:11
and um you and and and how much money social security is bringing in versus how many social security recipients
2:31:17
there are because we have an aging population. Relatively speaking, the average age is is increasing. Social
2:31:22
Security will not be able to ma u maintain its full payments u I think in by 2032
2:31:29
there. Okay. So they will social security will have to stop will start reducing the the amount of money that that's been paid people um in in about
2:31:37
seven years. And so the only way to fix that robotics manufacturing
2:31:44
raise GDP you've got to basically uh massively increase the um uh economic output which
2:31:51
is and the only way to do that is AI AI and robotics. So, so basically we're
2:31:56
going bankrupt without AI and robotics with even with a bunch of savings um the
2:32:02
savings the savings like reducing uh waste and forward can give us a longer
2:32:08
runway but it cannot ultimately pay off our national debt. So what do you think the solution is to
2:32:13
the jobs that are going to be lost because of AI and robotics? The jobs due to automation the jobs due to no longer
2:32:20
do we need human beings to do these jobs because AI is doing them. Do you think it's going to be some sort of a
2:32:26
universal basic income thing? Do you think there's going to be some other kind of solution that has to be implemented
2:32:34
because a lot of people are going to be out of work, right? Um I think there will be um actually a
2:32:41
high demand for jobs but not necessarily the same jobs. So
2:32:47
I mean this is actually this process has been happening um throughout um modern
2:32:54
history. Um I mean there used to be like like doing calculations um ma manually
2:33:00
with with like a pencil and paper. It used to be a job. So they used to have like buildings full of people called
2:33:06
computers where the the banks would like all you do all day is is is um you do
2:33:12
calculations because they didn't have computers. They didn't they didn't have digital didn't have digital computers that that people
2:33:18
Yeah. Well, it was just people would just like add and subtract stuff on piece of paper and and and that that
2:33:24
would be how banks would do you know financial processing and you'd have to literally go over their equations to make sure the books
2:33:30
are balanced. Yeah. And most times it's just simple math like you know the like in a world
2:33:35
before computers how did you calculate how did you you do transactions? You had to do them by hand.
2:33:42
Um so then when computers were introduced the job of doing um you know
2:33:48
bank calculations no longer existed. Um so people had to go do something else.
2:33:54
Um and that's what's going to happen that what's that's what is happening at an accelerated rate um due to AI and and
2:34:02
then robotics. That's the issue though, right? The accelerated rate because it's going to be it's the accelerator. It's it's it's just happening. Like I said, like AI is
2:34:09
the supersonic tsunami. So that's what I call it, supersonic
2:34:15
tsunami. Um so it's like what other jobs will be
2:34:21
available that aren't available now because of AI? Um well AI um will is is really still
2:34:29
digital. Ultimately, um AI can improve the productivity of of humans who who um
2:34:35
build things with their hands or do things with their hands like plum, you know, literally
2:34:40
welding, electrical work, plumbing, anything that's that's physically moving atoms.
2:34:46
Um like cooking food or um you know
2:34:51
farming or or like like anything that's that's physical uh those jobs will exist
2:34:57
for a much longer time. But anything that is digital uh which is like just
2:35:03
someone at a computer doing something, AI is going to take over those jobs like lightning,
2:35:09
coding, anything along those lines. Yeah, it's going to take over those jobs like lightning. Um just like it just like
2:35:17
digital computers took over the job of people doing manual calculations but but much faster.
2:35:24
So what happens to all those people? Like what kind of numbers are we talking about? you're going to lose most drivers, right? Commercial drivers.
2:35:30
You're going to have automated vehicles, AI controlled systems, just like uh there's certain ports in China, I think
2:35:37
in Singapore, where everything's completely automated. Yeah. Mostly. Yeah. Yeah.
2:35:42
Yeah. So, you're going to lose a lot of those jobs. Long shoreman jobs, trucking, commercial drivers.
2:35:49
Yeah. Yeah. I mean, we actually do have a shortage of of truck drivers, but there's there's actually um Well, that's why California has hired so
2:35:56
many illegals to do it. Have you seen those numbers? Yeah. Um I mean, the problem is like
2:36:01
when you when people don't know how to drive a semi-truck, which is actually a hard thing to do, then they they crash
2:36:07
and kill people. Yeah. Um a friend of mine's wife was killed by an an illegal driving a truck and she
2:36:13
was just out biking. Um and uh there was an illegal he didn't know how to drive
2:36:19
the truck or so or something. I mean and he he ran ran her over.
2:36:25
Um so I mean like thing is like for something like you you can't you can't
2:36:31
let people drive uh you know sort of an 80,000lb semi um if if they
2:36:39
don't know how to do it. But in California, they're just letting people do it because they need people to do it.
2:36:46
Well, they also need they want the votes and that kind of thing. But um but but
2:36:51
yeah, like cars are um cars are going to be autonomous. Um, but there there's
2:36:57
just so many desk desk jobs where where really people what people are doing is they're processing email um or they're
2:37:03
answering the phone. Um, and and just anything that is that that isn't moving
2:37:09
atoms like anything that is not physically like doing physical work that will obviously be the first thing those
2:37:15
jobs will be and are being eliminated by by AI at a very rapid pace. Um
2:37:22
um and ultimately I working will be optional
2:37:29
uh because you'll have robots plus AI um and we'll have in a benign scenario
2:37:36
universal high income not just universal basic income universal high income meaning anyone can have any products or
2:37:43
services that they want. So you but but there will be a lot of trauma and disruption along the way.
2:37:51
So you anticipate a basic income from that that the economy will boost to such
2:37:58
an extent that a high income would be available to almost everybody. So we'd essentially eliminate poverty
2:38:05
um in the benign scenario. Yes. So like the way there's multiple scenarios.
2:38:11
There are multiple scenarios. There's a lot of ways this movie can end. Um, like the reason I'm so concerned about AI
2:38:17
safety is that like one of the possibilities is the Terminator scenario. It's not it's not 0%.
2:38:23
Um, so um, that's why it's like I'm like really
2:38:29
banging the drum on AI needs to be maximally truth seeeking. like don't
2:38:35
make I don't force AI to believe a lie like that the for example the founding fathers were actually a group of diverse
2:38:41
women or that misgendering is worse than nuclear war because you if if that's the case and then you get the robots and the
2:38:47
AI becomes omnipotent it can enforce that outcome
2:38:55
and then then like unless you're a diverse woman
2:39:00
you're you're out of the picture so we're we're toast So that's um or you might wake up as a diverse
2:39:05
woman one day has adjusted the picture and and we are
2:39:11
now everyone's a diverse woman. So that would be that's the the worst possible
2:39:16
situation. So what would be the steps that we would have to take in order to
2:39:22
implement the benign solution where it's universal high income like
2:39:28
best case scenario this is the path forward to universal high income for
2:39:34
essentially every single citizen that the the economy gets boosted by AI and
2:39:40
robotics to such an extent that no one ever has to work again and what about meaning for those people which is which
2:39:48
gets really weird. Yeah. I don't know how to answer the question
2:39:53
about meaning. Um that's an individual problem, right? But it's going to be an individual problem for millions of people.
2:40:02
Yeah. Well, I I mean I I I guess I've like for
2:40:10
fought against saying like, you know, I you know, I've been I've been a voice saying like, "Hey, we need to slow down
2:40:15
AI. we need to slow down all these things. Um, and and we need to, you
2:40:21
know, not not have a crazy AI race. I've been saying that for a long time, for 20 20 plus years. Um, but but then I, you
2:40:28
know, I came to realize that, um, really there's two choices here. Either be a spectator or a or a participant. And if
2:40:36
I'm a, if I'm a spectator, I can't really influence the direction of AI. But if I'm a participant, I can try to
2:40:41
influence the direction of AI and have a maximally truth seeeking AI with with good values that uh loves humanity. And
2:40:49
that's what we're trying to create with Grock at XAI. And um you know, the research is I think bearing this out.
2:40:55
Like I said, the when they when they compared like how do AIs value the weight of a human life? Um
2:41:02
Grock was the only one the only one of the AIS that weighted human life equally.
2:41:08
um and and didn't and didn't say like a white guy's uh worth 120th of a of a of
2:41:13
a a black woman's life. Literally, that's what they they calculation they
2:41:19
came up with. So, I'm like, this is I'm like, this is very alarming. We should we got to watch this stuff. So, this is one of the things that has
2:41:25
to happen in order to reach this benign solution.
2:41:30
Yeah. We we we I I just keep Best movie ending. Yeah. Um, you you
2:41:36
want a a curious truth seeeking AI. Um, and I think a curious truth seeeking AI
2:41:42
will want to foster humanity. Uh, because we're much more interesting than
2:41:49
um a bunch of rocks. Like you say, like like I I love Mars, you know, but but
2:41:55
Mars is kind of boring. Like it's just a bunch of red rocks. Um, it does some cool stuff. It's got a tall mountain.
2:42:00
It's got, you know, it's got the biggest re the biggest ravine and the tallest mountain. Um, but there's no there's no
2:42:06
there's no animals or plants or and and there's no people. Um, and uh, you know,
2:42:14
so humanity is just much more interesting if you're a curious truth seeeking AI than not humanity. It's just
2:42:21
much more interesting. Um, I mean like as as humans, we could go for example
2:42:28
and and eliminate all chimps. If we said if we put our minds to it, we could say we could go out and we could annihilate
2:42:34
all chimps and all gorillas, but but we don't. Um there has been encroachment on
2:42:39
their environment, but we we actually try to preserve uh the the uh chimp and
2:42:45
gorilla habitats. Um and um and I think in a good scenario,
2:42:52
uh AI would do the same with with humans. it would actually foster uh
2:42:57
human civilization and care about human happiness. So this is um this is the thing to to
2:43:04
try to achieve I think. Um, but what is the what does the landscape
2:43:10
look like if you have Grock competing with Open AI, competing with all these different like
2:43:16
how does it work? Like what what if you have AIs that have been captured by
2:43:23
ideologies that are side by side competing with Grock? like how do we so
2:43:30
this is one of the reasons why you felt like it's important to not just be a an
2:43:36
observer but participate and then have Grock be more successful and more potent
2:43:42
than these other applications. Yes, as long as there's at least one AI that is maximally truth seeeking, curious, and
2:43:50
um you know, and for example, weighs all you know, human lives equally um does
2:43:56
not favor one race or gender, then um then then that that that and and people
2:44:02
are able to look at look at, you know, Grock at XAI and compare that and say, "Wait a second, why are all these other
2:44:08
AIs uh being basically sexist and racist?" Um
2:44:14
um and uh then then that that causes some embarrassment for the the other AIS
2:44:20
and then they they they they fix they you know they they improve they tend to improve just in the in the same way that
2:44:26
um acquiring Twitter and allowing the truth to be told and and not suppressing the truth um forced the other social
2:44:35
media companies to be more truthful um by in in the same way having um Gro be a
2:44:42
maximally truth seeeking, curious AI is will force the other AI companies to um
2:44:49
be also be more truth seeeking and fair. And the funniest thing is even though like the socialists and the Marxists are
2:44:56
in opposition to a lot of your ideas, but if this gets implemented and you really can achieve universal high
2:45:03
income, that's the greatest socialist solution of all time. Like literally no
2:45:09
one will have to work. Uh correct. Um like I said so so there is a benign
2:45:15
scenario here which I think probably people will be happy with if if as long as we we achieve it which is sustainable
2:45:21
abundance. um which is if if um if everyone can
2:45:26
have every like like like if if you ask people like what's the future that you want um and uh I think a future where we
2:45:35
haven't destroyed nature like you can still we have the national parks we have the the Amazon rainforest still still there we haven't paved we haven't paved
2:45:42
the paved the rainforest like the natural beauty is still there but but people have nonetheless everyone has
2:45:50
abundance everyone has excellent medical care. Everyone has whatever goods and services they want. And we just
2:45:56
It kind of sounds like heaven. It sounds like it is like the ideal socialist utopia. And this idea that the
2:46:04
only thing you should be doing with your time is working in order to pay your bills and feed yourself sounds kind of
2:46:10
archaic considering the kind of technology that's at play. Yeah. Like a world where that's not your
2:46:18
concern at all anymore. Everybody has money for food. Everybody has abundance.
2:46:23
Everybody has electronics in their home. Everybody essentially has a high income.
2:46:28
Now you can kind of do whatever you want. And your day can now be exploring
2:46:35
your interests doing things that you actually enjoy doing. Your purpose just
2:46:40
has to shift. Instead of, you know, I'm a hard worker and this is what I do and
2:46:45
that's how I that's how I define myself. No. Now you can [ __ ] golf all day,
2:46:50
you know? You can whatever it is that you enjoy doing can now be your main pursuit.
2:46:56
Yeah. Well, that sounds crazy good. Yeah, that's that's that's the benign
2:47:02
scenario that we should be. The best ending to the movie is actually pretty good.
2:47:07
Yes. um like I think there's there is still this question of meaning um of
2:47:13
like making sure people don't uh lose meaning you know like um so
2:47:19
hopefully they can find meaning in ways that are not that that's not derived from their work and purpose purpose for things that you
2:47:26
you know find things that you do that you enjoy but there's a lot of people that are independently wealthy that
2:47:32
spend most of their time doing something they enjoy right and that could be the majority of people
2:47:38
pretty much everyone. But we'd have to rewire how people approach life. Mhm. Which seems to be like acceptable
2:47:46
because you're not asking them to be enslaved. You're exactly asking them the opposite. Like no longer be burdened by
2:47:52
financial worries. Now go do what you like.
2:47:58
Yes. Go [ __ ] test pizza. Do whatever you want. Um pretty much. Um, so that's uh that's
2:48:08
that's the that's the that's probably the best case outcome. That sounds like the best case outcome
2:48:13
period for the future. If you're looking at like how much people have struggled just to feed themselves all throughout
2:48:19
history, food, shelter, safety, if all of that stuff can be fixed, like how
2:48:26
much would you solve a lot of the crime if there was a universal high income?
2:48:33
Just think of that. Like how much of crime is financially motivated? You know, the greater percentage of people
2:48:39
that are committing crimes live in poor, disenfranchised neighborhoods. So if there's no such thing anymore, if
2:48:46
you really can achieve universal high income, yeah, that this is it sounds like a utopian.
2:48:52
Yes. Um I think some people may commit crime because they like committing crime. It just some some amount of that
2:48:59
is they just wild people out there. Yeah. Yeah. Um and obviously they've become 40 years
2:49:05
old living a life like that. Now all of a sudden universal high income is not going to completely stop their
2:49:11
instincts. Yeah. Um I mean I guess if you want to have like like say read a science fiction book or some books that that are
2:49:17
probably an accurate or or the the least inaccurate version of the future. I'd
2:49:22
say I' I'd recommend um the Ian Banks books called the the culture books. It's not actually a series. It's a It's like
2:49:29
ai sci-fi books about the future. They're generally called the culture books. Yen Banks culture books. It's
2:49:36
worth reading those. When did he write these? He started writing them in the 70s. Um and I think he
2:49:43
um the last one I think he was I think it was written just like around I don't know maybe 2010 or something. I'm not
2:49:49
sure exactly. Yeah. Yeah. Scottish author Ian Banks from 87 to
2:49:55
2012. Yeah. Interesting. But he but like he wrote the the like
2:50:01
his first book, Consider Flever. Like he started writing that in the 70s.
2:50:09
These books are incredible, by the way. Oh, incredible books. 4.6 stars on Amazon.
2:50:16
Interesting. So, um, so this gives me hope.
2:50:22
Uh, yeah. Yeah. This is the first time I've ever thought about it this way. Yeah. Well, I mean,
2:50:29
if like I often ask people, "What is the future that you want?" And they have to think
2:50:35
about it for a second cuz, you know, they're usually tied up in whatever the daily struggles are. But, but you say,
2:50:41
"What is the future that you want?" Um, and um, and generally sustainable
2:50:47
abundance, what do these folks say, "What about a future where there's sustainable abundance?" Like, "Oh, yeah, that's a pretty good future." Um so um
2:50:58
you know if if and and and that that future is attainable with AI and robotics
2:51:04
um but but you know it's it's like I said there's not every path is a good
2:51:10
path. uh there's this it's but I think if we if we push it in the direction of
2:51:18
um maximally truth seeeking and curious then I think AI will want to take to to
2:51:25
take care of humanity and foster uh foster humanity um
2:51:32
because we're interesting um and if it hasn't been programmed to
2:51:38
think that like all straight white male should die, which Gemini was basically programmed to
2:51:44
do at least at first. Um, you know, they seem to have fixed it. I hope they fixed it. But don't you think culturally
2:51:51
like, oh, we're getting away from that mindset and that people realize how preposterous that all is.
2:51:58
We are getting away from it. Um, so, uh, we are getting at least it knows
2:52:04
the AI mostly knows to hide things. But like like I said, there is that I I think I still have that as or I had that
2:52:10
as my like pinned post on X which was like uh hey wait a second guys we still
2:52:15
have every AI except Grock uh is saying that uh basically straight white male
2:52:21
should die um and this is a problem and we should fix it. um
2:52:27
and you know but simply me saying that is like tends to generally result in um
2:52:33
you know them like that is kind of bad. Uh maybe we should just we should not have all straight white males die. Um I
2:52:40
think they say also all all straight Asian males should also die as well. like that like uh
2:52:48
like generally the generally the AI and the and the media which which back back in the day the
2:52:55
the media was um you know racist against uh black people and sexist against women
2:53:02
back in the day. Now now it is racist against uh white people and Asians and
2:53:08
sexist against men. Um so they just like being racist and
2:53:13
sexist. I think they just want to change the target. Um so uh but but really they
2:53:19
just shouldn't be uh racist and sexist at all. Um you know
2:53:24
ideally that would be nice. That would be nice. Um, and it's kind of crazy that we were kind of moving in that general direction till around 2012
2:53:32
and then everything ramped up online and and everybody was accused of being a Nazi and everybody was transphobic and
2:53:38
racist and sexist and homophobic and everything got exaggerated to the point where it was this wild witch hunt where
2:53:45
everyone was a colomo looking for racism. Yeah. Yeah. Yeah. Totally. Um, well well
2:53:50
but but but they they were openly anti-white and often openly anti-Asian. And then this new sentiment that you
2:53:56
cannot be racist against white people cuz racism is power and influence.
2:54:03
Okay. No, it's not. Yeah. Racism is is is racism in the absolute. Um so um you know and there
2:54:12
just needs to be consistency. So if it's okay to have uh let's say uh black or
2:54:18
Asian or Indian or pride, it should be okay to have white pride, too. Yeah. Um, so that's just a that's just a
2:54:26
consistency question. Um, so, uh, you know, um, if it if it's okay to be proud
2:54:34
of one religion, it should be okay to be proud of, I I guess all religions provided they're that they're they're
2:54:39
not like oppressive. Yeah. Or or or don't like as long as part of that religion is not like
2:54:44
exterminating uh people who are not in that religion type, right? Um so uh
2:54:52
it's really just like a consistency bias. Um
2:54:57
or or just like uh ensuring consistency to eliminate uh bias. Um so if it is
2:55:05
possible to be uh racist against uh one race, it is possible to be racist
2:55:11
against any race. Um so of course logically. Yes.
2:55:16
Yeah. and arguing against that is that's when you know you're catching it's a it's a logical inconsistency that makes AIS go insane
2:55:24
and people and people go insane. Yes. Um but like the the
2:55:30
like like you can't simultaneously say um that uh there's the systemic uh
2:55:37
racist oppression but also that races don't exist that that race race is a social
2:55:43
construct. like which is it? You know, um you also can't say that um you know, anyone who
2:55:51
steps foot in America is is automatically an American except for the people that originally came here.
2:55:59
Exactly. Exactly. Except for the colonizers. Yeah. Except for the evil colonizers who
2:56:05
came here, right? So which one is it? Like if you if as soon as you step foot put in a place you
2:56:10
are that you are just as American as everyone else then um that would have appi if you
2:56:16
apply that consistently then the original white settlers were also just as American as everyone else.
2:56:22
Yeah. Logically. Logically. Um, one more thing that I have to talk to you about before you
2:56:28
leave is the rescuing of the people from the space station, which, uh, we talked
2:56:34
about, you were planning it the last time you were here. Um, the f the lack of coverage that that
2:56:43
got in mainstream media was one of the most shocking things that Yeah, they totally memoryhold that
2:56:48
thing. Wild. Yes. Because if it wasn't, it's like it didn't exist. Those people would be dead. They'd be stuck up there.
2:56:56
Well, they'd they'd probably still be alive, but they'd they'd be having bone density issues uh because of prolonged
2:57:02
exposure to zero gravity. Well, they were already up there for like 8 months, right? Yeah. Which is an insanely long time. It takes
2:57:08
forever to recover just from that. Yeah. They're only supposed to be at the space station for 3 to 6 months maximum.
2:57:15
So, one of the things you told me that was so crazy was that you could have gotten them sooner, but
2:57:20
Yeah. But for political reasons, uh they didn't they did not want uh SpaceX or me
2:57:26
to be associated with um returning the astronauts before the election.
2:57:32
That is so wild that that's a fact. First of all, that
2:57:37
we absolutely could have done it. Um so, but even though you did do it and you did it after the election, it received
2:57:43
almost no media coverage anyway. Yes. because nothing good can the the
2:57:48
the media which is essentially a far left prop the legacy mainstream media is a far-lft propaganda machine. Um and so
2:57:55
anything any story that is positive about someone who is not part of the sort of far-left tribe will not uh get
2:58:04
any coverage. I I could save a busload of orphans and and it it wouldn't get a single news
2:58:09
story. Yeah, it's it really is nuts. It it was nuts to watch because even though it was
2:58:16
discussed on podcasts and it was discussed on X and it was discussed on social media, it's still it was a blip
2:58:23
in the news cycle. It was very quick. It was in and out and because it was a su
2:58:29
successful launch and you did rescue those people, nobody got hurt and there was nothing really to there was no blood
2:58:35
to talk about, right? Just [ __ ] in and out. Yeah. Yeah. Absolutely. Well, and and as as you saw firsthand with the Starship
2:58:42
launch, like Starship is um you know by
2:58:47
you know at least by some some would consider it to be like the most amazing
2:58:52
uh you know engineering project that's happening on Earth right now outside of like you know maybe AI or AI and
2:58:59
robotics but but certainly in terms of a spectacle to see it is uh the most
2:59:05
spectacular thing that is happening on earth right now is the Starship launch program which
2:59:11
anyone can go and see if they just go to South Texas and just they can just rent a hotel room low cost in South Padre
2:59:18
Island or in Brownsville and you can see the launch and you can drive right right past the factory because it's on a
2:59:24
public highway. Um but it gets no coverage or what coverage it does get was like a
2:59:31
rocket blew up coverage, right? Yeah. Oh, he's a [ __ ] The rocket blew up. like the the the the Star Sasha
2:59:36
program is vastly vastly more capable than the entire
2:59:42
Apollo moon moon program. Vastly more capable. This is a spaceship that is
2:59:48
designed to make life multilanetary to carry uh millions of people across the
2:59:56
heavens to another planet. the the Apollo program could could only
3:00:02
send astronauts to the moon for a few hours at a time.
3:00:08
Like they could send two the entire Apollo program could only send astronauts to visit the moon very briefly and then for a few hours and
3:00:15
then depart. The starship program could create an entire uh lunar base with a million
3:00:23
people. You understand the mag the magnitudes are there's different very different
3:00:29
magnitudes here. So what was the political basically no no coverage of it.
3:00:34
Yeah. But what I wanted to ask you is like what so what were the conversations
3:00:39
leading up to the rescue like when you were like I can get them out way quicker.
3:00:46
Yeah. Um um well I mean you know I raised this a few times but it was the I was told
3:00:54
instructions came from the White House that uh you know that that there should be no attempt to rescue before the
3:01:00
election. That should be illegal.
3:01:05
That that that really should be a horrendous miscarriage of justice for
3:01:11
those poor people that were stuck on that. Um yeah it it is it is crazy. Um,
3:01:16
have you ever talked to those folks afterwards? Did you have conversations with them? Yeah. I mean, they they're they're not
3:01:22
going to say anything political to, you know, they're not like they're never going to say thank you. Yeah. Yeah. Yeah.
3:01:27
Well, that's nice. Yeah. Yeah. Absolutely. So, um, but the instructions came down from the
3:01:33
White House. He cannot rescue them because politically this is a a bad hand
3:01:38
of cards. I mean, they didn't say because politically it's a bad hand of cards.
3:01:43
They they just said uh they were they were not interested in uh any rescue operation before the election.
3:01:53
Yeah. So what did that feel like? I wasn't surprised. But it's crazy.
3:01:59
Yeah, because Biden could have authorized it and they could have said the the Biden
3:02:04
administration is helping bring those people back, throw you a little funding, give you some money to do it. the Biden
3:02:10
administration, they funded these people being returned. Uh yeah, the Biden administration was
3:02:16
not exactly my best friend, especially especially after I um you
3:02:21
know, you know, helped Trump get elected get get elected, which I mean some people
3:02:28
still think, you know, Trump is like the the devil basically. Um, and I mean I
3:02:34
think I think Trump actually he's not he's is not perfect, but but uh he's not
3:02:40
evil. Trump is not evil. I spent a lot of time with with him and he's
3:02:46
I mean he's a product of his time. Uh but he is not he's not evil.
3:02:51
Um no, I don't think he's evil either. But if you look at the media coverage, the media the media treason like he's
3:02:58
super evil. It's pretty shocking if you look at the amount of negative coverage. Like one of the things that I looked at
3:03:04
the other day was mainstream media coverage of you, Trump, a bunch of
3:03:10
different public figures and then 96% negative or something crazy and then Mum Donnie, which is like 95%
3:03:18
positive, right? Um I mean Manny is is is is a
3:03:24
charismatic swindler. Um I I I mean you got to hand it to him like he he does he
3:03:30
can light up a stage. Um but he has just been a swindler his entire life. Um and
3:03:37
um you know and and uh I think
3:03:45
he what he's I mean he's likely to win. He's likely to be mayor of New York New York City.
3:03:50
Very likely. Yeah. Very likely. I think Poly Market has it at what what is the
3:03:55
94%? Yeah, that sounds pretty likely. That's crazy. Like I'm not sure who the 6% are, you know.
3:04:01
Um so, so yeah. So that's um what's also like who's on the other
3:04:07
side? The [ __ ] guardian angel guy with the beret and Andrew Cuomo who doesn't even have a party. Like they the
3:04:14
Democrats don't even want him. So you have those two options. Um,
3:04:19
and then you have the young kids who are like finally socialism.
3:04:25
Yeah, they they don't know what they're talking about obviously. Um, so you know, like
3:04:32
you just look at this say how many boats come from Cuba to Florida and how many
3:04:38
but and how many boats because you know there's like a constant I always think like how many boats are accumulating on
3:04:43
the shores of Florida coming from from Cuba, right? Um there's a there's a whole bunch of free
3:04:49
boats that you could if you want to go take them back to Cuba. It's pretty close.
3:04:54
Yeah. But for some reason people don't do that. Why why why why are the boats only
3:05:00
coming in this direction? Um well who is who are the most rabid
3:05:05
capitalists in America? The [ __ ] Cubans. Absolutely. Yeah. They're like we've seen how this
3:05:10
story goes. We do not want Exactly. [ __ ] off.
3:05:16
Cubans in Miami, they don't want to hear any [ __ ] They don't want to hear any socialism [ __ ] They're like,
3:05:21
"No, no, no. We know what this actually is. This isn't just some [ __ ] dream." Yeah. It's extreme government
3:05:27
oppression. Um that's how it's a nightmare. And like
3:05:32
the like an obvious way you can tell which uh which ideology is is the bad
3:05:38
one is um who has to which ideology is building a wall to keep people in and
3:05:43
prevent them from escaping. Right? Like so East Berlin built the built the
3:05:50
wall not West Berlin, right? They built the wall because people were trying to escape from communism to West
3:05:57
Berlin. But there wasn't anyone going from West Berlin to East Berlin, right?
3:06:02
That's why the communists had to build a wall to keep people from escaping.
3:06:07
They're going to have to build a wall around New York City. Yeah. That So, so
3:06:14
that an ideology is problematic. If that ideology has to build a wall to keep people in with machine guns,
3:06:21
Yes. and shoot you if you try to leave. Also, there's no examples of it being successful ever. We're only working out
3:06:27
for people. No, there's examples of a bunch of lies like North Korea. Give this land to the state. We'll be in
3:06:33
control of food. No one goes hungry. No. Now, no one can grow food but the government and we'll tell you exactly
3:06:39
what you eat and you eat very little. Right. Yeah. What? When you say mom Donny's a swindler, I know he has a bunch of fake
3:06:45
accents that he used to use. Yeah. And you know, but what else has he done that makes him a swindler?
3:06:54
Um well I I guess if you say uh what I mean
3:07:00
if if say if you say to any audience whatever that audience wants to hear uh
3:07:05
instead of what instead of having a consistent message I would say that that is a swindly thing to do. Um
3:07:13
and uh yeah um
3:07:22
yeah but but he is he is charismatic. Um yeah good-looking guy. Smart,
3:07:28
charismatic. Yeah. Great on a microphone. Yeah. Yeah. Yeah. Yeah. and and what the
3:07:33
young people want to see, you know, like this ethnic guy who's
3:07:39
young and vibrant and has all these socialist ideas align with them and you
3:07:44
know, they're bunch of broke dorks just out of college like, "Yay, let's vote
3:07:49
for this." And there's a lot of them and they're they're activated. They're motivated.
3:07:55
Um, I guess we'll we'll we'll see what happens here. What do you think happens if he wins?
3:08:02
Um because like 1% of New York City is responsible for 50% of their tax base,
3:08:12
which is kind of nuts. 50% of the tax revenue comes from 1% of the population. And those are the people that you're
3:08:18
scaring off. You know, you lose one half of 1%. Yeah,
3:08:25
I mean hopefully this the stuff he's he said, you know, about government
3:08:30
takeovers of of like that all the stores should be the government basically. Um
3:08:36
I don't think he said that. I think he said government they want to do government supermarkets, some state-run
3:08:41
or cityrun supermarkets. Yeah. Um well, it just the the
3:08:47
government is the DMV at scale. So um you have to say like do you want the DMV running your supermarket?
3:08:53
Right. Um, was your last experience at the DMV amazing? Uh, and if it wasn't,
3:08:58
you probably don't want the government doing things. Imagine if they were responsible for getting you blueberries.
3:09:03
Yeah. It's not going to be good. I mean, the the the thing about, you know, communism
3:09:09
is is it was it was all bread lines and bad shoes. Um, you know, do do you want
3:09:15
ugly shoes and bread lines? Because that's what communism gets you. It's going to be interesting to see what
3:09:21
happens and whether or not they snap out of it and overcorrect and go to some
3:09:27
Rudy Giuliani type character next cuz it's been a long time since there was any sort of Republican leader there.
3:09:43
And we we live in the in the most interesting of times um because We we face the
3:09:50
you know simultaneously face civilizational decline um and incredible pro prosperity
3:09:59
um and these these timelines are interwoven um so um if Mani's policies are put into
3:10:08
place especially at scale um it it would be a catastrophic
3:10:14
uh decline in living standards not just for the rich but for everyone. um uh as as has been the case with with
3:10:22
every um every for every every socialist experiment um or every Yeah. So
3:10:31
um but but then as you pointed out the the irony is that like um the ultimate
3:10:37
capitalist thing of AI and robotics uh enabling
3:10:42
uh prosperity for all and an abundance of goods and services actually the
3:10:48
capitalist uh implementation of AI and robotics assuming it goes down the the good path
3:10:55
uh is is actually what results in the communist utopia.
3:11:00
Because fate is fate is an irony maximizer, right? And and an actual socialism of
3:11:06
maximum abundance of highincome people. Universal high income.
3:11:12
Yeah. Like the the problem with communism uh is is universal low income. Um it's it's
3:11:20
not that everyone gets elevated, it's that everyone gets oppressed except for a very small minority of of politicians
3:11:27
who live a lives of luxury. That's what's happening every time it's been done. Yeah. Um so um
3:11:35
but then the the actual communist utopia if everyone
3:11:41
gets anything they want will be will be if if will be achieved
3:11:47
if it is achieved it will be achieved via c capitalism because fate is an irony maximizer.
3:11:56
I feel like we should probably end it on that. Is there anything else? The most ironic outcome is the most likely, especially if entertaining.
3:12:03
Well, everything has been entertaining. As long as the bad things aren't happening to you, it's quite fascinating. And it's never a boring
3:12:09
moment. Yes. So there's I do have a theory of
3:12:15
why um like if if if simulation theory
3:12:20
is true then um it is actually very likely that
3:12:29
um the most interesting outcome is the is the most likely because only the
3:12:34
simulations that are interesting will continue. The simulators will stop any simulations that are boring because
3:12:41
they're they're not interesting. But here's the question about the simulation theory. Is the simulation run
3:12:46
by anyone or is it would be run by someone? It would be run by some some
3:12:51
some force the pro the program like in in this reality that we live in, we we run
3:12:57
simulations all the time. Like so when we try to figure out if the rocket's gonna make it, we run um
3:13:05
thousands sometimes millions of simulations just to figure out which which
3:13:11
uh path is the good path for the rocket and and where can it go wrong, where can it fail. Um but we when we do these I
3:13:19
say at this point millions of simulations of of what can happen with the rocket um we ignore the ones that
3:13:25
are where everything goes right um because we we we just care about the we
3:13:31
have we have to address the situations where it goes wrong. Um so
3:13:37
um so so basically in in in and and for for AI simulations as well like like all
3:13:42
these things we we keep the simulations going that are the most interesting to
3:13:48
us. Um so if simulation theory is accurate if
3:13:54
if it is true who knows um then the uh
3:14:00
the the simulators will will only they will continue to run the simulations that are most interesting there.
3:14:06
Therefore from a Darwinian perspective um the only surviving simulations will be the interest the most interesting
3:14:11
ones. And in order to um avoid getting turned off uh the only rule is you must
3:14:18
keep it interesting or you will if or you will because the boring simulations
3:14:24
will be terminated. Are you still completely convinced that this is a simulation? I didn't say I was completely convinced.
3:14:29
Well, you said it's like the odds of it not being are in the billions. But I guess it's not completely cuz
3:14:35
you're saying there's a chance. What are the odds that we're in base
3:14:40
reality? Um well given that given that that we're
3:14:46
able to create increasingly sophisticated simulations. So if you think of say video games and how video games have gone from very simple video
3:14:53
games like Pong with you know two rectangles and a square to video games today being um photorealistic
3:15:02
uh with millions of people playing simultaneously and all of that has occurred in our lifetime.
3:15:08
So if that trend continues, uh, video games will be
3:15:14
indistinguishable from reality. The fidelity of the game will be such that you you don't know if that what you're
3:15:20
seeing is a real video or a fake video. Um, and like AI generated videos at this
3:15:26
point, you like you can sometimes tell it's an AI generated video, but often
3:15:32
you cannot tell and soon you will not really just not be able to tell. So um
3:15:38
if if that's happening in our direct observation then and and we're create we'll create
3:15:46
millions if not billions of photorealistic simulations of reality
3:15:52
then what are the odds that we're in base reality or versus someone else's simulation?
3:15:59
Well, isn't it just possible that the simulation is inevitable, but that we are in base reality building towards a
3:16:05
simulation? We're making simulations.
3:16:10
Um, so um we're making simulations. We make like
3:16:19
you can just think of like photorealistic video games as as being simulations. Mh. Um, and especially as you apply AI
3:16:27
in these video games, the the characters in the video games will be incredibly interesting to talk to. They won't just have a limited dialogue tree where if
3:16:33
you go to like the the crossbow merchant or like and you you try to talk about any subject except buying a crossbow,
3:16:40
they just want to talk about selling you a crossbow. Um, but with with with AI based non-player characters, you can
3:16:47
you'll be able to have an elaborate conversation with no dialogue tree. Well, that might be the solution for meaning for people. Just lock in and you
3:16:54
could be a [ __ ] vampire and whatever. You live in Avatar land. You could do it. You could do whatever you want. I
3:17:01
mean, you don't have to think about money or food. Ready Player One. Yeah. Literally. Yeah. But with higher
3:17:07
living standards. Yeah. You don't have to be in a little trailer. I I mean, I think this people do want to
3:17:13
have some amount of struggle or something they want to push against. Um
3:17:20
but but it it could be you know playing a a sports or playing a game or something. It could be easily playing a game and
3:17:26
especially playing a game where you're now no longer worried about like physical attributes like athletics like
3:17:32
bad joints and hips and stuff like that. Now it's completely digital but yet you
3:17:38
do have meaning in pursuing this thing that you're doing all day.
3:17:43
Whatever the [ __ ] that means. It's going to be weird. It's going to be interesting.
3:17:49
It's gonna be very interesting. Um the most the most interesting
3:17:55
and and usually ironic outcome is the most likely. All right. That's a good predictor of the future.
3:18:02
Thank you. Thanks for being here. Really appreciate you. Appreciate your time. You I know you're a busy man, so this
3:18:08
means a lot you come here to do this. Welcome. All right. Thank you. Bye, everybody.