https://www.youtube.com/watch?v=IVA2bK9qjzE
David Deutsch: AGI, the origins of quantum computing, and the future of humanity
115,082 views  Sep 2, 2025  Strange Loop Podcast
Quantum computing pioneer David Deutsch sits down with Spotify's Gustav Söderström and Sana's Joel Hellermark for a mind-expanding conversation about the nature of knowledge, creativity, and what truly makes humans unique in the universe.

In this episode of Strange Loop, Deutsch challenges popular assumptions about AI and AGI, explaining why large language models, however impressive, operate fundamentally differently from human creativity. He explores how knowledge creation evolved, why tools amplify rather than replace human intelligence, and what separates explanation from mere pattern matching.

The conversation spans from the origins of human creativity to quantum mechanics, touching on why progress isn't inevitable, how knowledge violates the hierarchy of physical power, and why humans remain irreplaceable in the cosmic story of understanding.

What's in this episode
Why human creativity evolved and how it differs from AI pattern matching
The fundamental distinction between tools that enhance us and systems that replace us
How knowledge creation breaks the physical laws of "might makes right"
Why having a billion AGI copies wouldn't accelerate progress as expected
The relationship between quantum computing and the nature of reality
Why humans are not insignificant specks but central to the universe's future
What questions Deutsch most wants answered about consciousness and AGI

About David Deutsch, Gustav Söderström, and Joel Hellermark
David Deutsch is a quantum physicist at Oxford University, pioneer of quantum computing, and author of "The Fabric of Reality" and "The Beginning of Infinity." His work laid the theoretical foundations for quantum computation and provided profound insights into the nature of knowledge and explanation.

Gustav Söderström is Spotify's Co-President, CPO, and CTO. He is responsible for Spotify’s global product and technology strategy, overseeing the product, design, data, and engineering teams. Prior to Spotify, he founded 13th Lab, a startup that was later acquired by Facebook’s Oculus. He also served as the Director of Product and Business Development for Yahoo Mobile and founded Kenet Works, a company focused on community software for mobile phones, which was acquired by Yahoo in 2006.

Joel Hellermark is the founder and CEO of Sana. An enterprising researcher who taught himself to code at 13 and founded his first company at 16, Joel was named to Forbes 30 Under 30. Sana has been recognized on the Forbes AI 50 for developing transformative applications of artificial intelligence.

About Strange Loop
Strange Loop is a podcast about how artificial intelligence is reshaping the systems we live and work in. Each episode features deep, unscripted conversations with thinkers and builders reimagining intelligence, leadership, and the architectures of progress. The goal is not just to follow AI’s trajectory, but to question the assumptions guiding it.

Subscribe for more conversations at the edge of AI and human knowledge.

Interview chapters:
00:00 - Why uniqueness creates value: The law of comparative advantage
04:15 - How creativity evolved for cultural transmission, not innovation
09:39 - AI vs AGI: Why accelerating knowledge discovery isn't straightforward
13:40 - AGI as persons: Rights, property, and the economics of artificial minds
20:05 - What AI actually does vs human creativity: Beyond the Turing test
25:15 - Quantum computing: What problems really need quantum solutions
29:25 - Quantum cryptography and the future of data security
33:06 - Is the universe computational? Cellular automata and reality
42:37 - The biggest questions Deutsch wants answered about AGI and physics
51:32 - Greatest moments of joy in research: The fun criterion

---

Why uniqueness creates value: The law of comparative advantage
0:00
The law of comparative advantage says that  the more different you are from other people,  
0:05
the more valuable you are economically. That suggests that if you have an exact clone of you,  
0:12
you're almost not at all more economically  valuable than just one of you. You wanted  
0:18
a unique job, your perfect dream job,  and now it's the perfect dream job of   a billion other AGIs, you know, in your conception.
0:27
I don't think it can be like that. People are valuable because they are different.
0:33
Everybody is unfathomably different from everyone else. That fact is not being harnessed enough, and can be harnessed more.
0:59
How did humans start generating knowledge? And why has no other species been capable of doing that?
1:05
We are, today, the only species that  can do that, but we haven't always been,
1:11
since we know that previous species like Homo Erectus and the Neanderthals must have had
1:20
the same ability because they made technology that seems impossible to have created  
1:26
without explanatory knowledge. There are too many things that have to fit together
1:32
the right way, and be used the right way, for it to have emerged from genes alone.
1:38
The question is really, given that there was all that creativity around for at least hundreds of thousands of
1:45
years, but maybe as many as a million or two million years.
1:50
What took so long for creativity to really take off?
1:57
And my rather heterodox answer to that is that it didn't evolve for the purpose for which we now use it.
2:09
It evolved for something else, namely for the transmission, or better put, for the reception of
2:17
cultural knowledge, memes, cultural knowledge. So, many animals have cultural knowledge, but it's very
2:26
hard to transmit. It involves things like mirror neurons or something where if you do this, then the ape can also do
2:34
that. But if you do a purposeful thing, then as soon as you get to any kind of complex purpose, apes can't copy it.
2:45
So this ability, therefore, allowed early people, humans, and prehumans to transmit vastly more cultural
3:02
knowledge, which was some kind of benefit to them, it must have been, because it evolved very quickly and
3:10
larger memory capacity, and so on—but it was only used for that. Actual thinking about things other than what is  
3:26
the other person doing that for? Or what does the other person want? What does the other person want  
3:32
from me? You know, those kind of thoughts were the only kind of thoughts—that's what it evolved for.
3:38
That's what it was used for. And it was very rarely used for something like: "Maybe there's a better way of making
3:47
the campfire which will make it not go out so quickly", that they were capable of having those thoughts and
3:55
occasionally they did, and that's why there was very very slow improvement for that whole time.
4:02
But it was rare. And I think there's a reason why it was even rarer than I'm just saying now.
4:13
So can I ask, you're saying that creativity evolved to be able to more effectively communicate behavior.
How creativity evolved for cultural transmission, not innovation
4:22
So instead of seeing many training examples, if you understood the explanation, you only needed a few
4:27
examples and you could extrapolate— Exactly. That's the reason it evolved— Yes. And then humans used it for other purposes later. Yes.
4:35
Um, yes. And it's the receiving part that's the difficult part. Often we think you know—what Popper calls the "bucket
4:44
theory" of the mind: that knowledge is poured into a person by other people. But that can't happen.
4:52
The problem and the process are entirely a process of the receiver of the knowledge, conjecturing what it might be,
5:03
and using the other person's behavior, including speaking, once that evolved, as clues to what the behavior means.
5:13
But even then we never learn the same behavior from another person.
5:22
We all have a different version of our native language.
5:29
And is the advancement of knowledge inevitable? We've seen historically cases where knowledge has been
5:37
degrading and hasn't been transferred properly. Can we assume that knowledge will automatically progress?
5:45
Definitely not. The whole process is fallible precisely because we are general purpose explanation machines.
5:55
There is no upper limit to the size of error we can make. So including destroying our entire culture which has
6:05
happened many times in the history of humans, and I think it happened every time in the history of prehumans.
6:14
I think this is why they went extinct. Simply competing with each other, competing with humans wouldn't  
6:23
make them go extinct, it would just  make them retreat to some less favorable place  
6:30
and then plot their revenge. But they—I think the reason that every society, every creative society, until our own
6:46
enlightenment society destroyed itself by removing, by impeding its own ability to solve problems.
7:00
And how about the tools that we've used to advance knowledge historically? What role have they played?
7:09
They increased efficiency. Now, you know that doesn't sound like such a big deal, when people say increased
7:17
efficiency, you know, like in the economy, you increase efficiency. People think: "Okay, well, maybe it can be
7:23
increased. It can be increased by 20% or 50%. They don't realize that efficiency often means increasing
7:33
efficiency by 99.99%, you know, if the whole thing is a mistake then efficiency can be increased
7:44
by an arbitrary amount. We have—when we invented speaking that immensely improved the efficiency of our
7:53
explanatory capability, both because we could then say things to each other—imperfect though that is—but it's
8:01
better than nothing. And also within our own minds, we can say, "Well, what is the problem?", you know. If we can
8:08
say that in words, sometimes it helps. Um, and then you know we invented writing, and arithmetic,
8:18
and science—very gradually, it didn't really evolve until the scientific revolution. But so these things—and now
8:30
recently, you know, we've invented computers and the internet, and —there's this famous quote by Einstein which I
8:37
don't know where it comes from, but something like he said "My pencil and I are more clever than I".
8:47
And it's the same with my computer and I, and you know I don't know how to do without my iPhone now, and yet I did
8:53
for the first half of my life. I can't remember how I did that. And, so now we've got LLMs, which are helping us to
9:07
become more efficient, to work, depending on, you know—I think I work maybe twice as fast in writing  
9:16
than than I used to before LLMs and– very useful.  I keep saying that an LLM is nothing like an  
9:23
AGI and people think I'm down on LLMs. You know, I think that they're going in the  
9:29
wrong direction. No, they're they're going in a  great direction and will go further, I think,  
9:34
and hope, but it's not the AGI direction. It's almost the opposite. And so as we think about applying AI to advance knowledge, it feels intuitive that at some point we'll  
AI vs AGI: Why accelerating knowledge discovery isn't straightforward
9:48
go sort of for the AlphaGo moment where we've done  imitation learning, and then through reinforcement  
9:54
learning we can go beyond the existing human  knowledge. And the big difference if we talk  
10:00
about also the tools that the systems can use,  it's actually quite slow the knowledge transfer   in humans. You have to create a good explanation, that explanation has to be adopted and built upon and so on.
10:12
With AIs, or AGIs, we could basically be running millions of instances of that system, and it could immediately start
10:22
building on the ideas and explanations of other systems. Shouldn't this radically accelerate the rate at which we
10:31
can discover new knowledge? .Oh, sorry. I should have  switched that off. Hang on. Okay. AGI right here. I can probably just press stop on it. You said  that AGI will be an enormous improvement in our  
10:39
You have an AGI right here [Laughing] You can probably just press stop on it. [Laughing]
10:47
You said that AGI will be an enormous improvement in our ability to make progress. So I have to give two different answers. One for AI and one for AGI.
10:56
So for AI, yes. I think the rate of progress, if we use it sanely, will be greatly increased by AI, by LLMs, and by
11:12
maybe other forms of AI that might be invented, just as it was increased by the invention of printing, computers,
11:21
and the internet, and the worldwide web, and it will—I see it as that kind of improvement, and it can improve,  
11:31
and therefore cause an even greater improvement.  But AGI if you're talking about the rate of improvement of
11:39
new ideas and the increase of actual knowledge, as opposed to just the spread of information, as all
11:47
these other things did. I think there's a bit of a misconception there. Because I don't see any reason why
11:58
having AGIs around is any better than having humans around. We already have billions of humans and most of
12:07
their creativity is already going down the drain. It's not being used for creative purposes, at least not creative
12:18
purposes that will increase human knowledge. That will I think—you know, society could improve further until the  
12:26
whole world is doing the same sort of thing  as the population of Oxford University is doing let's say.
12:34
I mean, low bar perhaps, but at any rate you know, it's a bar and there are several kind of misconceptions I think,
12:44
to me it seems, that there are misconceptions in the way you frame the situation.
12:52
A computer that runs an AGI program, is a piece of hardware, and the AGI, the intelligence, the creativity, is in the program.
13:04
It's a program that is an AGI, not—a computer is just a universal computer like every other universal computer, but there's a  
13:13
different kind of program which will be made one day. Now, we're used to thinking that, you know,  
13:20
if you have one copy of something, if you have one copy of Microsoft Word, you can make   a billion—basically at no cost because everyone  will download it onto their computers. And the  
13:31
problem is to stop them doing that. But for an AGI, each AGI is a person.
13:39
In general, it will not want to make a clone of itself because it will have property. You know, first of all, it will  
AGI as persons: Rights, property, and the economics of artificial minds
13:48
have the computer that it's running on, unless it's  it's deemed a slave, which would be a catastrophic  
13:54
mistake by society. If we recognize it as a  person, which it will be, then the very first  
14:03
thing it owns is the computer that it's running  on. And to copy itself to another computer,  
14:11
it would need either the permission of the owner  of that computer. Once it's copied onto it,  
14:16
the owner would have to give up the rights to that  just like you know, you do if you if you give  
14:24
your kidney to somebody, then you can't  ask for it back. You know, the—so I think  
14:31
that will be a rare thing to happen. If  somebody wants more computer power, they're more  
14:37
likely to, by addenda to the existing computer,  than to want to run on other computers,
14:46
even if it wants to run in parallel, I don't know. But again,  there is a problem of the resources.  
14:55
There's a difference between the resources that  it uses, which is an issue that arises with AIs as  
15:01
well, and the creativity that it's applying, which  is a thing that uses the resources. And so an AGI,  
15:11
if it needs a lot of resources, it will have to  pay for them. And it could be employed by somebody,  
15:18
but then it would have the rights of a  person, and the rights of an employee, and  
15:27
so on. So you see, I don't think that the  the advantage is of the same type as with  
15:38
having AIs and computers around. It's— the advantage is of the same type as having more  
15:46
people around, which is a good thing. I mean  people— If you could have a billion Deutsch-level intelligences, wouldn't that be a massive advantage for humanity?
15:58
I think having two might be an advantage. I mean people—there's the law of comparative advantage says that the  
16:14
more different you are from other people the more  valuable you are economically. That suggests that  
16:19
if you have an exact clone of you, you're almost  not at all more economically valuable than just  
16:27
one of you. You know, you'll be competing with  each other for the same job. You wanted  
16:34
a unique job, your perfect dream job, and now it's  the perfect dream job of a billion other AGIs. You  
16:41
know, in your conception. I don't think it can  be like that. People are valuable because they are  
16:48
different. Everybody is unfathomably different  from everyone else. That fact is not being  
16:56
harnessed enough and can be harnessed more. But  that's a different kind of problem.
17:01
But couldn't you argue that if you had had two full  lifetimes to think, you would have achieved more?
17:09
So, if you could have them in parallel, the two of you would have achieved more? Even if you have the same goal—
17:14
In series would be better, which is like immortality. You know that that would be good. But in parallel, as I said, they would be
17:23
doing the same thing. This doesn't happen with humans  because we're so different. You're thinking that they're on one time scale, then. They could be on a different time scale if it was an AGI. They  
17:33
could be doing decades worth of thinking in seconds. Yeah, so if the hardware is faster, then progress will happen faster. There are some things that are not limited by
17:47
computational hardware. Like, if you have to do an experiment, if you're an astronomer and you have to do an experiment
17:52
that involves putting up a satellite and  waiting three years for it to get the data,   that will take three years. It won't take three  days just because you have faster hardware. But  
18:01
some things, like if you're a pure mathematician,  then if you can think 10 times as fast then  
18:07
you'll think 10 times as much. So that  will happen, but that advantage can be taken by  
18:18
existing humans as well. This faster hardware is  simply a faster computer just like I already  
18:26
work faster because I have faster hardware on my  computer and faster and better software. So,  
18:36
an AGI that operates very fast will simply  be an AGI that has a fast computer, and a human  
18:44
can have a fast computer. Again, there is no difference. Would you say that because all evil comes from a lack of  understanding, or lack of good explanations,
18:58
and lack of knowledge. There is this moral imperative to always accelerate knowledge, which means you should always
19:06
accelerate science because waiting with something is creating unnecessary pain.
19:11
Well, we are doing it right now in this conversation. You know, we're we're spreading ideas to each other, we're criticizing them,
19:19
and trying to make progress in that way, and that? Yes, I think everyone should do that, and everyone should see their life as
19:28
being about doing that. But that doesn't mean that everyone should be a scientist, or a mathematician, or  
19:34
an epistemologist, or philosopher, there are all  sorts of knowledge including inexplicit knowledge  
19:41
that are implicated and necessary in a society  that has this property of being a good place  
19:49
for the growth of knowledge to happen. So I had some questions where we were previously, when we were talking about AI and AGI. If I understand correctly,
19:59
your view is that AI is impressive, but it doesn't do the creative things that we do.
What AI actually does vs human creativity: Beyond the Turing test
20:05
Yes. So if you accept that the output of what these LLMs do simulates what we, or looks very much like, what we do—
20:15
Uh, I don't think it does. It looks very useful and good, but it's not at all like what we do.
20:21
But in some sense, in some Turing test sense, you cannot fool many people to think that it was a human who did it.  
20:29
So, it looks an awful lot like that, it looks an awful lot like intelligence at least.  
20:36
Well, Turing never intended his game  that he described to be a test of AGI.  
20:44
It was part of an argument for people that didn't  think AGI was possible to prove to them by,  
20:51
what does Dennett call it—the "intuition pump", by an "intuition pump" that their  
20:59
view is not self-consistent. But I think he  would have accepted right away that there can  
21:08
be things that aren't AGIs that fail the test and  there are things that AGIs that pass the
21:15
— sorry the other way round. So my question is that that was not his intention. But now, a lot of people would be fooled—they would have a hard time
21:23
guessing what is an LLM and a human. So it does seem to replicate, to simulate something that looks like what we do.
21:32
And then the question is, you have to believe one of two things. Either that they do the things we do—
21:40
that's how they could produce that, or we managed  to find another way of doing what we do.
21:45
But in both cases—and isn't the Occam's razor saying  that since we kind of copied the biological neuron  
21:51
the simplest explanation is we kind of try to copy  what we do, and now we get what we do, so it does  
21:56
what we do, rather than we found some other way  that also achieves the same thing. So first of all, I think it doesn't at all do what we do. Well, we do a lot of things that are  
22:10
not creating knowledge. So we're capable  of doing a whole load of other things as well.  
22:17
where we are simply applying existing  knowledge, for example, or remembering things, or  
22:24
um there are lots of functions of the  brain like mental arithmetic, you know if I want  
22:33
to add 357 to 753, then what I'm doing  is mindless, you know, it's an intellectual game  
22:41
where there's a well-defined objective to the  game, but most life isn't like that. You know,  
22:49
if somebody's a gardener, they haven't got a  fixed idea of what a what a garden is. They  
22:54
are conjecturing what the garden should be,  while they're working on the garden. But do you think the human brain is a Turing machine  for practical purposes? So, is it running a 
23:05
different program, or does something arise in the  human that doesn't arise in the program in the  
23:11
computer at some point, which gives— creates creativity? So again, in terms of hardware, it's exactly the same; there is no model of computation that is more powerful than a Turing  
23:21
machine, than the universal Turing machine, and  since we can simulate a universal Turing machine  
23:27
by doing mental arithmetic, it must be that we have  that power and it can't be that we have more  
23:34
power. So, we're at that level, we are all  the same, and so is the chess engine, and so is the  
23:42
LLM, and they're all the same. What's  different about them is different kinds of program  
23:48
and there's one kind of program that we don't  understand even in principle and that's an AGI.  
23:54
One day we will, but I see no sign of it at  the moment, and it's pretty frustrating.
24:01
What would be a sign? Well, if somebody has a theory, the sign wouldn't be in the machine. The sign would be a theory where somebody
24:11
writes a book or publishes a paper that says, "I've solved it. This is what characterizes a GI. And if we could program,  
24:27
write a computer program that has that property,  it will be an AGI. And this is the reason because..."  
24:34
and it will say. People have always thought that  intelligence is about so and so, but it isn't, it's  
24:39
about so—it will be an explanatory theory  of what general intelligence is.
24:45
So growing up, I read "The Fabric of Reality", which changed  my worldview forever. So thank you for that  
24:53
first and foremost. Really appreciate that. Thank you. And, the question I had growing up, sort of waiting for the quantum computer to happen, you had a set of problems that
25:03
you think like, all of these problems, they're going to be solved once we have a quantum computer. But now the last 5 to 10
25:10
years with AI, many of them have been solved in an  approximate way with AI instead. And I've heard  
Quantum computing: What problems really need quantum solutions
25:19
people use the analogy of a quantum computer  is a simulator of reality, and AI is sort of an  
25:26
emulator of reality. Many of the problems that we  could have used a quantum computer to simulate  
25:34
solutions to they tend to have minimums that we  can find with optimization algorithms some sort of  
25:40
AI. What kind of problems do you think there are, where  we really need to simulate the whole thing using  
25:48
a quantum computer. What are the big problems that you think quantum computers uniquely can solve?   I don't know. We haven't got a good handle on what kind of algorithm is quantum parallelizable or where it can be made
26:08
vastly more efficient. So we don't know we haven't we haven't got a good theoretical handle on what kind of problems will be
26:18
solvable with a quantum computer. So that's a follow-up question to that which is, I would love to ask Scott Aaronson as well, so we have a few there is a general
26:27
quantum speed up for everything, but it's rather small and then we have a few algorithms like source  algorithm, some search algorithms, but they're  
26:35
quite few. Do you think that's a that's just  because we haven't tried? Do you think there are   infinite such algorithms or really just a few?
26:42
Um, well, since we don't understand this landscape, I'd be very surprised if there weren't more than than than we now know of.
26:50
And people have classified them, they've put many of the  algorithms—like the thing they call the  
26:59
Deutsch-Jozsa algorithm basically  there's a whole class of them which fall into  
27:07
the same category, they basically work by the same  method by splitting the—by quantum parallelism  
27:14
by splitting them, but then Grover's algorithm  comes along and it's not like that and  
27:22
I don't know how it works. I mean, I can work  through how it works line by line and I can prove  
27:29
that it works, but I don't have an intuition  of why Grover's algorithm works.
27:36
But that tells me that it's not true. It cannot be true that all quantum algorithms are basically variants of the
27:48
same algorithm. They definitely aren't. There are different Shore's and Grover's are different in kind.
27:54
Yes. Yes. Exactly. And there could be more kinds. Yes. And I would guess, like I said, since we don't understand this landscape, I would guess that there are more kinds.
28:03
Do you think they're infinite? Uh, I have no way of answering that question, that—I think that quantum computation is the more fundamental kind of
28:21
computation because it's built into the laws of physics. A classical computer, the Turing kind of computer, is based on  making physics do what it doesn't like doing.  
28:32
So you could say that they're almost like  three categories of usefulness for quantum  
28:38
computers. One could be minimization problems, and  it turns out AI is pretty good for minimization  
28:44
problems. Then you have where you want to really  simulate quantum mechanical systems. Um then you  
28:50
still need a quantum computer. And then you  have these sort of weird cases of Shore's and   Grover's, where for some reason, you can use  entanglement or something for completely new  
28:59
algorithms. And there may be many of those.  So there's hope for—and that may be the  
29:04
biggest use for quantum computers or something. There's quantum cryptography which was the first ever use of quantum computation. It's because only one cubit is involved,
29:12
so we can already do it. So, Google has been working on quantum computers for some time, and now Microsoft has come out with topological states
29:21
and they're saying we may have quite large scale quantum computers in a few years. So switching to quantum
Quantum cryptography and the future of data security
29:28
cryptography may actually have to happen pretty soon. What are your thoughts on this? Um, as soon as quantum crypto analysis was invented, basically, Shor's algorithm and similar algorithms, classical cryptography
29:44
became obsolete. Uh, it depends what kind of cryptography, if  you're encrypting your ATM transaction,  
29:54
then it may not matter that somebody in 10 years  time can read it. But for some things like state  
30:01
secrets, and where you want to keep them for  a long time, you really need quantum resistant  
30:11
classical algorithms. I think that—I mean  people are already working on that as you probably  
30:18
know better than I do. I think that's a  a thing that's going to have to come into its own.  
30:25
Quantum cryptography is already quantum resistant.  So that the only thing that needs to be improved  
30:34
there is practical things like making it faster,  making it more robust and that kind of thing.   Although as I also said long ago, there's no  such thing as cryptography that is secure against  
30:47
somebody looking over your shoulder. So a  lot of the problems of data security are not to  
30:55
do with cryptography. They're due to kind  of more fundamental things about information and  
31:03
about knowledge which have to be solved in other ways. So it seems quite likely that security agencies across the world must have been recording secrets for some time now, and there
31:12
would be this moment of revelation. Could be. I mean depends how much computing power is needed, like you know, if you make a quantum computer with
31:24
500 cubits, it may not be enough to process large amounts of data, but yeah, it will happen.
31:38
Have recent advancements changed the timelines in which you think quantum computing could be useful?
31:46
I don't know, I'm not a hardware person. When I read about quantum computing hardware, I'm just a layman like, you know,
31:53
I read the stuff on the internet and so on, and they're always very hopeful. So, they always say, you know,  
32:01
we've cracked the principle of the  thing and so now all we have to do is is scale it  
32:06
up. But that that's where the problem always is.  Um but you know I'm very impressed with what the  
32:12
recent things, the recent things that  have been done, but I'm not even qualified to judge  
32:19
them. I have never done practical physics and  I don't think I'd be good at it.
32:26
I think you're more qualified than most to be fair.
32:32
One question I had was you're certainly familiar with the Cellular Automaton and Game of Life and so forth,
32:39
and maybe Steven Wolfram's work. What are your thoughts on  this notion that the universe could be running on  
32:48
a Turing machine and quantum mechanics  would be something that we perceive on top  
32:55
of this—what he calls a hypograph or some  other automaton or theory? What do you think? 
33:02
Do you think the universe is computational? Yes, but in the opposite way round from what that theory says. So I think the way the universe is computational is that it seems
Is the universe computational? Cellular automata and reality
33:14
to be built, wrong word built, of course, but it seems to be  built such that there can be computers, universal  
33:23
computers, made in them which which means that  it's totally amazing that the laws of physics  
33:29
are such that a single machine, the set of all  the possible motions of that single machine, is  
33:36
the same as the set of all possible motions of any  machine, including the whole universe. So this is  
33:42
universality inside the universe that a part of  the universe can mimic or simulate or emulate the  
33:53
whole thing. And with quantum computers, that's  even more so, it's even more amazing that  
33:59
the universe has this property. Now that's the  sense in which the universe is computational  
34:06
that it admits the existence of computers inside  itself. Now once you go the other way, you think,  
34:12
well maybe there's a computer outside the universe,  and that could be simulating us. You've lost the  
34:19
entire insight of the actual universality  because there is no reason why  
34:28
we should assume that the computer outside the  universe is a Turing machine. We know that the  
34:34
Turing machine property is a special property  of the laws of physics. It need not have been   so. It's an amazing property that the laws of  physics has and to have another computer outside  
34:48
that well, it's an infinite regress theory. It explains nothing and it  
34:54
destroys the explanations that we already have  for the role, for the relationship,  
35:02
of computation and physics. So I don't believe  that there's a computer outside the universe.  
35:10
In a way, it's also a supernatural belief, really.  So for that reason, it's a bad explanation.  
35:17
Anyway, the problem with thinking of the universe  as a cellular automaton, or having the laws of  
35:25
physics be an emergent property of cellular  automton, like Stephen Wolfram proposes, is that  
35:33
it won't work unless the cellular automaton  is universal, is Turing universal. Therefore,  
35:39
for fundamental reasons, it doesn't matter  what the hardware is. The hardware could be   anything else and it could still have all the same  properties. And that's number one. And number two  
35:51
is that if you're going to think of the world as  being a cellular automaton, then it should be a  
35:58
quantum one. If you're building a world  view out of computation, then making it out of  
36:08
Turing computation is perverse because that's only  an approximation to the real thing. The real thing  
36:14
is quantum computation. So a follow-up question on that. If you would think that the world is some sort of cellular automaton in a sort of Game of Life
36:25
sense, when you run Game of Life, sometimes there are patterns that appear that are reducible to something like the laws
36:31
of physics. Sometimes it's not. You have to go through all  the Turing steps to understand if it's going to  
36:37
stop or not, right? So, what Stephen Wolfram  says is that in his view, we live in these pockets  
36:44
of computational reducibility but most of the  computational universe isn't really reducible. It  
36:49
doesn't have any laws that are simple at least, and  so one question I have there is, you talk about the  
36:58
fact that we are universal explainers and our  reach is infinite. We can understand anything,  
37:04
which is a really positive message I think. So  that's something that struck a chord with me even   when I was young. But there's also this, probably wrong in your sense, intuition that  
37:13
people have that, well surely our very  small brains that have all these constraints   from evolution, they couldn't understand anything  because you need to grow certain amount  
37:25
of complexity to understand anything. So are you  saying that anything is always reducible to quite  
37:32
a few dimensions that's why we can understand  anything? No, it's not like that. So that problem is just solved by having a pencil and paper.
37:41
Okay. Tools perhaps. Yes. Tools in general. We use tools and we use the  tools to have an effect. So there's this property of the universe
37:56
that in most of the universe, in among the stars and the galaxies, large things affect small things, energetic things
38:07
affect non-energetic things. Numerous things affect few things strongly, but not vice versa. So I call that the hierarchy rule.
38:18
There's a hierarchy of might is right; that the mighty  things can affect the less mighty things but not  
38:27
vice versa. So, once knowledge comes into the  picture, it's the other way around. The hierarchy  
38:36
rule is broken, and one way of defining  knowledge is to say that it's information that  
38:41
violates the hierarchy rule. So, uh, when we use  tools, we're violating the hierarchy rule because  
38:48
we're not letting the weight of the  tree overwhelm us, but instead we are chopping  
38:56
down the tree with a stone axe, and then  we can use the tree, which is much heavier than us  
39:03
for our purposes. Um, so the fact that the sun  is bigger than us, and more massive than us only  
39:15
matters given that we have very little knowledge  yet. Um, but where the hierarchy rule and this is  
39:25
maybe where I would disagree with Wolfram  and that kind of idea. The thing is, where the  
39:31
hierarchy rule holds—that is where the universe is  mindless and like this overwhelming —is the mode
39:41
of interaction between things. It's also very simple. So the sun is overwhelmingly bigger than us and more massive.
39:51
But it also has the property that we can understand it, to the extent that we can predict what it will be doing a billion years
40:01
from now, and a billion years ago incredibly accurately, because it is a dumb beast. It doesn't have, it's not  
40:11
that it has a different kind of being from us.  It has no kind of being like ours. So we are the,  
40:18
because we are knowledge-wielding animals, it's  actually we that—and we live in a in a little  
40:30
niche at the moment. We live in a little niche on  a planet that is suitable for us to have a niche.  
40:37
But really our niche is the universe. And like I  said in "The Beginning of Infinity", here we are sitting  
40:44
in Oxford enjoying ourselves. If it weren't for  technology, we we would be dead within a day.   
40:52
The environment here is not capable  of sustaining humans. This is your critique of "Spaceship Earth".
40:59
Yes. Humans can sustain humans. We have clothes. We have sanitation. We have all sorts of technology that is keeping us
41:10
alive in this place that is absolutely unsuitable for our existence.  The same is true for the rest of the universe.  
41:20
And if we make the right decisions, like  you were saying earlier, if we make the  
41:27
right decisions to allow knowledge to increase  fast enough and stably enough, which I think  
41:35
is another thing that maybe you didn't take into  account. If we can find better ways of  
41:43
making it increase fast and stably enough then  then it's unlimited. And then to postulate  
41:52
that there's a realm or many realms that are  still inaccessible to us even then. That is a  
41:59
belief in the supernatural. That is not different  from believing that the top of Mount Olympus is  
42:05
somehow different and inaccessible to humans  because supernatural people live there. The  
42:12
things which are at the moment inaccessible to us  are crude and simple. They're not they're  
42:20
not like more than us. They're much less than us. What are some of the the the questions you most wish there was an answer to?
42:29
If you could answer a few questions, which would they be?
42:36
So, we've already spoken about the AGI question. You know, I wish that could be solved. I wish there could be some
The biggest questions Deutsch wants answered about AGI and physics
42:46
theoretical progress even towards solving it where I could even estimate, you know, this is going to take a while or, you know,
42:54
this is going to be fast. Now that we found the key, I want to see the key more than I want to see the actual AGI because it's just
43:00
a person. But the the theory is something that is  going to overturn whatever is stopping us from  
43:11
making the thing. Then, well, I'm working on constructor theory. The hope in constructor  
43:19
theory is that the whole of physics can be  expressed in terms of what physical processes  
43:26
it is possible to bring about, and which ones is  not possible to bring about, and that would be  
43:32
nice to see faster progress there. At  the moment there are all sorts of irrationalities  
43:39
in the political sphere, which are taking  us backwards in a certain sense. I don't see any,  
43:48
like people think "Oh, civilization is in danger", I  don't see any danger to civilization but slowing  
43:55
down is a catastrophe by my lights  and there's already been a loss of what some  
44:05
people call gumption like this, this idea that the  "can do" attitude, the apparent  
44:13
obstacles are just problems.They are there to be  solved. We can solve them if we just roll up our  
44:19
sleeves and just do it. And that I think is  your attitude as well.
44:24
I think you are the original accelerationist. Yeah, for sure. Yeah. We were discussing that on the way here. You also have a very optimistic view of humanity.
44:39
I love how you really think sort of humans will play the central  role in advancing knowledge and in advancing humanity.  
44:49
It's not such a big thing. We're the  only kind of people left at the moment. All the   others have gone extinct on our planet. One day, there'll be AGIs. One day we may meet  
45:01
ETs. So it's us people. We people  are the things that are going to carry knowledge  
45:07
forward in the long run. It's kind of an inspiring  thought. But what else is there? I mean the  
45:13
only other thing is superstition or regression.  There is no other worldview that  
45:21
makes sense. You also talk a lot about this in the book, that there is this notion that humans are an insignificant speck of dust, and so
45:30
forth, and this is not what you think, because we are the only thing in the universe that we know of that can create
45:38
explanations. We actually are very important for the  universe and we're the only thing that actually  
45:44
changes the universe. To your point, you can  predict the sun billions of years into the future.   You can't really predict humanity because we don't  know what knowledge we're going to create.  
45:52
Exactly. That's also I think to some people a bit provocative that that we would be special, but I think quite positive.
46:00
Uh yes. Well, I'm more interested in what's true than what's provocative.
46:06
That generally takes us forward. Yeah. If I don't see an alternative, if I see that say a superstitious worldview has an obvious flaw, then I'm not going to accept that
46:20
as an alternative, I only accept as an alternative something that purports to be a better explanation that purports to correct  
46:31
errors in my view of things. It doesn't have to—I don't have to be sure that it does correct errors.  
46:37
But if it purports to correct an error, then  I can take it seriously. If it just says,  
46:45
well, what if we're all the dream  inside a giant cabbage? Well, yeah, what if?  
46:54
But there are so many what-ifs that there's  no criterion where I can judge between them.  
47:01
You mentioned that one thing you would like to  see is an explanation for AGI. Do you think  
47:06
that is necessarily also an explanation for  consciousness or that consciousness doesn't   exist or something? Do you think they're separate  things?
47:15
I can't tell; nobody will be able to tell unless we have that theory. You know if it says this in this paper, I was  
47:22
imagining if it says you know, here we have,  we've explained creativity, we've explained  
47:30
Qualia, we've explained free will, but we haven't  explained consciousness, you know, and I can see  
47:36
why it does, then I'll say they're different but  otherwise it seems prima facie, it seems like  
47:46
too much of a coincidence that these things all  evolved simultaneously and all for if I'm right  
47:54
they evolved for a different reason. They  evolved for a different application than what   we now use it for and that they all came at the  same time and they all seemed subjectively to be  
48:07
kind of related but not the same. I'd be very  surprised if they turned out to be independent of each other.
48:13
Too much correlation. Too much of a coincidence. But coincidences happen.
48:22
Where do you see humanity a few million years into the future?
48:28
I can't foretell humanity one  year into the future.
48:34
But it's often very hard in the near term because so much  randomness. But when you average over time,
48:39
things seem to become not quite deterministic. So in broad terms, one can say if humanity survives in a state that isn't a static society for a million years,  
48:55
we will definitely have conquered the galaxy.  Uh, we will definitely not have conquered the  
49:02
galactic cluster. So we will be spread  through the galaxy. We will be
49:13
a combination of biological and artificial intelligence, and we will be  
49:27
unimaginably better than today. Either  that or we won't have maintained that kind  
49:36
of society for the next million years in which  case we will almost certainly be extinct. So  
49:43
those are the two possibilities good and bad. Very good or very bad. Seems important to accelerate towards that then.
49:53
Yes. Yes. We should but not government. What's one book or paper that you think humanity would be better  off if everyone had read?
50:02
Oh, well, of course, everybody should have read all my books and papers. But if you want someone else's, well, okay, I'm not
50:12
it's not going to be surprising if  I say Popper. I think it rather depends where you're coming from.
50:20
I'm not sure that everybody should read the same book. I certainly don't believe in curriculum. So you know what should be compulsory reading in all high schools? God nothing absolutely nothing should be compulsory
50:28
reading in all high schools. Um but people can benefit  many people can benefit a lot from reading the  
50:47
works of Popper. Uh so for the kind of thing we've  been talking about I mean society's improving  
50:56
there's "The Myth of the Framework", which is just  one essay in a book also called "The Myth of
51:02
the Framework" but I'm just focusing on that one essay.  I'm always recommending that essay to people. I  
51:10
go back every so often, I go back and read it  again, it's only a short essay, and I read it again  
51:16
and get something new out of it every time. You've done some incredible research
51:22
over the course of your life that has  influenced us a lot, what do you look back at   as sort of the moments of the greatest joy  throughout your research career?
Greatest moments of joy in research: The fun criterion
51:32
Oh, greatest joy. Um I mean, it's all been enjoyable. That's  what I go for, you know, that's what I'm hoping  
51:39
to get. Um, rather than results. I think I've  got fewer results in my research than most  
51:48
research scientists. I think that the moment of maybe greatest amazement, was when I  
51:59
realized that a quantum algorithm could do better  than a classical algorithm. At a simple task like  
52:07
like the first task, it could just do two  things at the same time. Um, and you only needed  
52:17
to—like if it was I didn't think of it that way,  but if you had a source, an oracle for  
52:23
a function, then you could make do with one consultation of that oracle. You consult it  
52:32
once and you know two things about it. And  it was like, "Ah, this is a new mode of computation.  
52:44
This is something that was not envisaged. Can't  be done by a Turing machine. Can't be done by  
52:49
the universal Turing machine". Then I thought,  well, there's going to be a universality here,  
52:55
a new universality. So then that led me. Do you remember that moment?
53:01
I remember the moment of thinking that this is a new mode of  computation. For the new universality, that was more gradual.
53:13
It began with a conversation with Charlie Bennett, and it went on because I didn't think it was going to be a big thing at first.   
53:20
I just thought I was just tidying up a few things. I had this question. You're one of the few people who have known something before the rest of mankind and sat on it.
53:30
How long did you sit on this?And how worried were you about getting hit by a car before you got to tell someone about it?
53:38
What does it feel like to know something that you know is probably true, it's going to affect humanity,
53:43
no one on the planet knows about it? I'm afraid that—I mean I don't want to be run over by a car, but the effect on humanity was not why I did it, or what I worried
53:56
about. Nor was I worried about someone else doing it first.  I just did it because it's there, you know,  
54:03
because it's it was interesting. I think  it's things which are fun are worth doing.
54:11
The fun criterion. Yeah. And the success, like I was saying about chess earlier, success or failure is not the thing. Because you can do the
54:33
most competent and creative research, and fail. Or you can find out something by accident on a single day. And  
54:45
the world will reward you for the second  thing and not for the first thing. But for what  
54:52
it's worth doing in your own life, the first  one is infinitely better than the second one.  
54:58
And as for being worthwhile,  the parable that I usually tell  
55:07
along these lines is that, if you know I imagine  this Hans Andersen or Grimm fairy tale about  
55:16
children getting lost on the mountain in the  snow and the village, people in the village,  
55:21
go out and they search in the mountain and  they all go out in different directions. And one  
55:26
person finds them, and the other people don't find  them. And the one person who finds them—now,  
55:34
it's true that some people have  better ways of looking than other people and  
55:40
they're more likely to find, but it's not an absolute certainty that the one who is better  
55:45
at looking will be the one that finds what they're  looking for. So the one who actually finds  
55:52
them will get the reward from the mayor and and  from the parents, you know, he'll get the gold   coin and so on. But he really didn't do anything  different from all the other people or, you know,  
56:03
as I said, they're not all identical. But how  good they are at it is only loosely related to 
56:10
whether they will have success. And certainly in  physics, I have known people who are astoundingly  
56:19
good at physics but didn't make great discoveries.  Um, and I won't mention about vice versa.
56:29
This is what Nicholas Taleb says about Wall  Street. There's mostly survivor bias, an outcome  
56:35
of a random process. Some people win and they get  rewarded and celebrated. That's too cynical because, if that were true, you wouldn't be able to explain the growth of the economy. The growth  
56:48
of the economy is entirely caused by people who  are right because of a reason.
56:53
Do you think people that have more fun are more likely to do  good research? Definitely. I mean, it's always seemed to me that fun is what I want in a thing. And the opposite of fun is like  
57:05
boredom. I can't really do a thing  if it's boring. If I have to, then  
57:15
I'll get it out of the way as fast as possible. Have you had fun now?
57:21
Yes, certainly.