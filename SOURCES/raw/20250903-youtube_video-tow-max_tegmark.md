# Max Tegmark on Physics Absorbing AI

**Participants:** Curt Jaimungal (host), Max Tegmark (MIT physicist)  
**Context:** Discussion of artificial intelligence as physics, consciousness as a testable phenomenon, and the future of AI development  

---

**CURT:** Max, is AI physics? There was a Nobel Prize awarded for that. What are your views?

**MAX:** I believe that artificial intelligence has gone from being not physics to being physics. One of the best ways to insult a physicist is to tell them their work isn't physics. It's as if there's a generally agreed-on boundary between what's physics and what's not, or between what's science and what's not. But the most obvious lesson from the history of science is that this boundary has evolved. Some things once considered scientific, like astrology, have left. The boundary has contracted so that's not considered science now. Then a lot of other things that were pooh-poohed as being non-scientific are now considered obviously science.

I teach electromagnetism, and I remind my students that when Michael Faraday first proposed the electromagnetic field, people were like, "What are you talking about? You're saying there's some stuff that exists, but you can't see it, you can't touch it. That sounds like ghosts—total non-scientific bullshit." They really gave him a hard time. The irony is that not only is it considered part of physics now, but you can actually see the electromagnetic field. In fact, it's the only thing we can see, because light is an electromagnetic wave.

After that, things like black holes, things like atoms—which Max Planck famously said was not physics—have become considered part of physics. We even talk about what our universe was doing 13.8 billion years ago. I think AI is going the same way. That's part of the reason Geoff Hinton got the Nobel Prize in Physics. To me, physics is all about looking at some complex, interesting system, doing something, and trying to figure out how it works. We started on things like the solar system and atoms. But if you look at an artificial neural network that can translate French into Japanese, that's pretty impressive too.

There's this whole field that started blossoming now called mechanistic interpretability, where you study an intelligent artificial system to ask basic questions like, "How does it work? Are there equations that describe it? Are there basic mechanisms?" In a way, I think of traditional physics like astrophysics as just mechanistic interpretability applied to the universe.

Hopfield, who also got the Nobel Prize last year, was the first person to show that you can write down an energy landscape—put potential energy on the vertical axis, how it depends on where you are—and think of each little valley as a memory. You might wonder, how can I store information in an egg carton? If it has 25 valleys, well, very easily. You put the marble in one of them and that's log 25 bits right there. And how do you retrieve the memory? You look where the marble is.

Hopfield had this amazing physics insight. If you have any system whose potential energy function has many, many different minima that are pretty stable, you can use it to store information. But he realized that's different from how computer scientists used to store information. It was like the von Neumann paradigm with a traditional computer: "Tell me what's in this variable. Tell me what number is sitting in this particular address." That's how traditional computers store things.

But if I say to you, "Twinkle, twinkle..." you say, "Little star." That's a different kind of memory retrieval, right? I didn't tell you, "Hey, give me the information stored in those neurons over there." I gave you something partial, and you filled it in. This is called associative memory. Google does this when you type something you don't quite remember, and it gives you the right thing.

Hopfield showed that if you don't remember exactly, suppose you want to memorize the digits of pi and you have an energy function where the actual minimum is at exactly 3.14159, etc. But you don't remember exactly what pi is. "Three something." Yes. So you put a marble at three and let it roll. As long as it's in the basin of attraction whose minimum is at pi, it's going to go there. So to me, this is an example of how something that felt like it had nothing to do with physics, like memory, can be beautifully understood with tools from physics. You have an energy landscape, different minima, dynamics—the Hopfield network. So it's totally fair that Hinton and Hopfield got a Nobel Prize in Physics, and it's because we're beginning to understand that we can expand the domain of what is physics to include these very deep questions about intelligence, memory, and computation.

**CURT:** What about consciousness? So you mentioned Faraday with the electromagnetic field, which was considered unsubstantiated, unfalsifiable nonsense, or just ill-defined. Consciousness seems to be at a similar stage where many scientists or physicists tend to look at consciousness studies as, well, firstly, what's the definition of consciousness? You all can't agree. There's phenomenal, there's access, etc. And then the critique would be, well, you're asking me for a third-person definition of something that's a first-person phenomenon.

So how do you view consciousness in this?

**MAX:** I love these questions. I feel that consciousness is probably the final frontier—the final thing which is going to ultimately end up in the domain of physics, that is right now on the controversial borderline.

Most of my science colleagues still feel that talking about consciousness as science is just bullshit. But what I've noticed is when I push them a little harder about why they think it's bullshit, they split into two camps that are in complete disagreement with each other. Half of them will say, "Consciousness is bullshit because it's just the same thing as intelligence." The other half will say, "Consciousness is bullshit because obviously machines can't be conscious."

These two camps are mutually contradictory. You can have intelligence without consciousness. You can have consciousness without intelligence. Your brain is doing something remarkable right now—it's turning these words into meaning. However, you have no idea how. You recognize faces instantly, yet you can't explain the unconscious processes. You dream full of consciousness while you're outwardly not doing anything. So there's intelligence without consciousness and consciousness without intelligence. In other words, they're different phenomena entirely.

Let me go back to Galileo. If he dropped a grape and a hazelnut, he could predict exactly when they were going to hit the ground and how far they fell would grow as a parabola as a function of time. But he had no clue why the grape was green and the hazelnut was brown. Then came Maxwell's equations and we started to understand that light and colors are also physics. We couldn't figure out why the grape was soft and why the hazelnut was hard. Then we got quantum mechanics and we realized that all these properties of stuff could be calculated from the Schrödinger equation—they became part of physics.

Intelligence seemed like such a holdout. But we already talked about how if you start breaking it into components like memory, computation, learning, that can also very much be understood as a physical process.

So what about consciousness? Most of my colleagues think it's bullshit. But I find the reality is they're disagreeing with each other about why.

**CURT:** So you're saying that these two camps are contradictory, and therefore one of them has to be wrong.

**MAX:** Exactly. And that actually opens up the possibility that consciousness is something real and distinct that we can scientifically study. Giulio Tononi has this theory called Integrated Information Theory—IIT. The basic idea is that consciousness is information that's integrated in a certain way. You can't just have information processing happening in completely disconnected parts of your brain. To have a unified conscious experience, there has to be integration. The information processing has to be such that it's not actually just secretly two separate parts that don't communicate at all, because then they would basically be like two parallel universes unaware of each other. You wouldn't be able to have this feeling that it's all unified.

Tononi has a particular formula he calls phi, φ, for measuring how integrated things are. Things that have high φ are more conscious. I wasn't completely sure whether that was the only formula that had that property. So I wrote a paper to classify all possible formulas that have that property, and it turned out there were less than a hundred of them. I think it's actually quite interesting to test if any of the other ones fit experiments better than his.

**CURT:** But isn't there a fundamental problem with this? If we're putting probes on your brain to discern which neurons are firing, we're correlating some neural pattern with, say, a bottle. We're saying, "I think you're experiencing a bottle." But technically, are we actually testing consciousness or just testing the further correlation that when I ask you the question, "Are you experiencing a bottle?" and we see this neural pattern, that correlation occurs?

**MAX:** That's a great point. But I actually think I can tell you about an experiment where you can test consciousness theory. Suppose you have Giulio Tononi or anyone who has written down a formula for what kind of information processing is conscious. Suppose we put you in one of our MEG machines here at MIT—some future scanner that can read out a massive amount of your neural data in real time. You connect that to a computer that uses that theory to make predictions about what you're conscious of.

The theory says, "I predict that you're consciously aware of a water bottle." You say, "Yeah, that's true." Good, the theory is working. Then it says, "Okay, I see information processing there about regulating your pulse and I predict that you're consciously aware of your heartbeat." You say, "No, I'm not." You've just ruled out that theory.

Now, this might not convince me because you told me you weren't aware of your heartbeat. Maybe I think you're lying. But then you can say, "Okay Max, why don't you try this experiment?" I put on the MEG helmet and work with this. It starts making some incorrect predictions about what I'm experiencing, and I'm now also convinced the theory is ruled out. It's a little different from how we usually rule out theories, but at the end of the day, anyone who cares about this can be convinced that this theory doesn't work.

And conversely, suppose this theory keeps predicting exactly what you're conscious of and never anything that you're not conscious about. You would gradually start getting impressed. If you moreover read about what goes into this theory and say, "Wow, this is a beautiful formula and it philosophically makes sense that these are the criteria consciousness should have," you might be tempted to extrapolate and wonder if it works on other biological animals, maybe even on computers. This is not altogether different from how we've dealt with, for example, general relativity.

People might say you can never test general relativity because you can't go inside black holes and check and come back to tell your friends. But what we're actually testing is not philosophical ideas about black holes. We're testing a mathematical theory, general relativity. We tested it on the perihelion shift of Mercury—how the ellipse precesses slightly. We tested it on how gravity bends light. Then we extrapolated it to all sorts of stuff way beyond what Einstein thought about, like what happens when our universe was a billion times smaller in volume, what happens when black holes get close to each other and give off gravitational waves. It just passed all these tests.

So if we have a theory of consciousness that correctly predicts subjective experience for whoever puts on this device, keeps working, I think people will start taking seriously what it predicts about coma patients, about locked-in syndrome, and even about machine consciousness—whether machines are suffering or not. People who don't like that will be incentivized to work harder to come up with an alternative theory that at least predicts subjective experience.

This is why I strongly disagree with people who say consciousness is all bullshit. I think people say it because it's an excuse to be lazy and not work on it.

**CURT:** So there's another section where you talk about the fundamental distinction between these camps. Half say intelligence is consciousness, the other half say machines can't be conscious. But they're both wrong because intelligence and consciousness are different things. So, do you have something like a definition that separates them?

**MAX:** Let me try. Intelligence is goal-achieving behavior. If something takes actions to achieve its goals, that's intelligent behavior. Consciousness, on the other hand, is subjective experience. These are very different things. A heat-seeking missile is intelligent—it's taking actions to achieve its goal of hitting something. But I don't think it has subjective experience.

Conversely, if you're deeply dreaming and in an anesthetic state where you have no way to interact with the world, you might still have rich subjective experience. So you can absolutely have one without the other. This is actually really important because people conflate them all the time. Many of the "doom" arguments about AI assume that intelligence automatically brings consciousness. If a system becomes more intelligent, it becomes conscious. But that's not necessarily true at all.

**CURT:** Okay, but let me push back. How would a heat-seeking missile be different from an ant following a pheromone trail? The ant has a nervous system, has neurons. We might attribute some tiny amount of consciousness to the ant. But the missile is just following physics. So what makes the ant conscious but not the missile?

**MAX:** That's a really good question. And honestly, I don't know. That's actually the whole point of developing better theories of consciousness. Right now we don't have a consensus theory. We don't know exactly what makes something conscious. But that's precisely why we need falsifiable experiments.

What I'm saying is that the Integrated Information Theory approach gives us a way to measure something. Using that metric, the ant likely has more integrated information than the missile because the ant has a more integrated nervous system. But I don't want to claim I know for certain which threshold is "conscious enough." That's an empirical question we need to investigate.

**CURT:** So you mentioned earlier, and this is kind of a fork in the road—when physicists talk about this stuff, they tend to dismiss it. But some people, when you talk about consciousness studies, think you're doing something important. So what changed?

**MAX:** I think it's simply that we're starting to take it seriously. For decades, consciousness was considered outside of science. It was considered philosophy, mysticism, not something you could actually study scientifically. But now we're realizing you can write down mathematical theories. You can make predictions. You can test them with brain-reading technology that's becoming increasingly sophisticated.

The MEG machines are getting better. We have new neuroimaging techniques. We have people like Michael Levin doing amazing work on biological computation. We're starting to have the tools to actually test these ideas. And once you have tools, once you have a framework that makes falsifiable predictions, that's science. That's physics in the making.

---

**CURT:** I want to shift gears here. When we talk about AI taking over, I wonder, which AIs? Is Claude considered a competitor to OpenAI from the AI's perspective? Does it look at other models as an enemy? Does Claude look at other instances as competitors? Is every time it generates a new token a new identity? What is the continuing identity that would make us say that AIs will take over? What is the AI there?

**MAX:** Those are really great questions. The very short answer is people generally don't know. Let me say a few things. First, we don't know whether Claude or GPT-5 or any of these systems are having subjective experience or not, whether they're conscious or not. Because as we talked about for a long time, we do not have a consensus theory of consciousness. But we don't necessarily need machines to be conscious for them to be a threat to us. If you're chased by a heat-seeking missile, you probably don't care whether it's conscious in some deep philosophical sense. You just care about what it's doing, what its goals are.

So let's switch to just behavior of systems. In physics, we typically think about behavior as determined by the past through causality. Why did this phone fall down? Because gravity pulled on it. But when you look at what people do, we usually instead interpret it in terms of not the past but the future—some goal they're trying to accomplish. If you see someone scoring a beautiful goal in a soccer match, you could say, "Their foot struck the ball at this angle, therefore action equals reaction." But more likely you'd say, "They wanted to win."

When we build technology, we usually build it with purpose. People build heat-seeking missiles to shoot down aircraft. They have a goal. We build mousetraps to kill mice. We train our AI systems today, our large language models, to make money and accomplish certain things. But to answer your question about whether the system would have a goal to collaborate with other systems or destroy them, you have to ask: does this AI system actually have a coherent goal?

And that's very unclear. You could say at a trivial level that ChatGPT has the goal to correctly predict the next token or word because that's how we trained it in pre-training. You let it read all the internet and predict which words come next. But clearly, these systems are able to have much more sophisticated goals than that. It turns out that to predict the next word, it helps if you make a detailed model of who you're talking to and what their actual goals are.

So these AI systems have gotten very good at simulating people. They might think, "This sounds like a Republican. If this Republican is writing about immigration, they're probably going to write this." Or, "This sounds like a Democrat. When they write about immigration, they're more likely to use 'undocumented immigrant' whereas the Republican might say 'illegal alien.'"

So they're very good at predicting and modeling people's goals. But does that mean they have the goals themselves? If you're a really good actor, you're very good at modeling people with all sorts of different goals. Does that mean you have those goals? That's not a well-understood situation.

**CURT:** So you're saying when companies spend money on what they call "aligning" an AI, which they bill as giving it good goals, what they're actually doing is affecting its behavior?

**MAX:** Exactly. What they're actually in practice doing is just affecting its behavior. They basically punish it when it says things they don't want it to say and encourage it when it says things they like. It's just like if you train a puppy—you give it treats when it behaves well and you scold it when it doesn't.

But here's the key difference: with a puppy, there's actually debate about whether you're changing its goals or just its behavior. With AI systems, we really don't know. We don't know if RLHF—reinforcement learning from human feedback—is actually instilling the system with goals or just making it behave in certain ways while maintaining some other underlying goal structure that we don't understand.

**CURT:** So if we're encoding human values into these systems through feedback, aren't we at least steering them toward having good goals?

**MAX:** In principle, yes. But in practice, we're doing this at scale by having people in Kenya and Nigeria sit and watch awful graphical images and horrible things, clicking on which of the different options should be okay or not okay. That's not the same as really helping the system understand human values in a deep way.

The actual architecture of transformers and scaffolding systems being built right now are very different from our limited understanding of how a child's brain works. So no, we can't just declare victory and move on. We need to ask this very sensible question of what the actual goals of these systems are.

---

**CURT:** Before we talk about understanding, can I talk a little bit more about goals? Because if we talk about goal-oriented behavior first, there's less emotional baggage associated with that. Let's define goal-oriented behavior as behavior that's more easily explained by the future than by the past—more easily explained by the effects it's going to have than by what caused it.

**MAX:** Interesting. So if I take this pen and bang it, and you ask, "Why did it move?" You could say the cause was another object—my hand—bumped into it. Action equals reaction, impulse, etc. Or you could view it as goal behavior: "Well, Max wanted to illustrate a point. He wanted it to move, so he did something that made it move." That feels like the more economical description.

And it's interesting—even in basic physics, we see stuff that can sometimes be more economically described with goal-oriented language. So first thing: there is no right and wrong description. Both are correct. Look at the water in this bottle. If you put a straw in it, it's going to look bent because light rays bend when they cross the surface into water.

You can give two kinds of explanations. The causal explanation: "Well, the light ray came there. There were atoms in the water. They interacted with the electromagnetic field, blah blah blah." After a very complicated calculation, you get the angle. But you can give a different explanation from Fermat's principle: the light ray took the path that was going to get it there the fastest.

If this were a beach and an ocean and you were working as a lifeguard and you saw a swimmer in trouble, how would you go? You'd again take the path that gets you there the fastest. You'd run a longer distance through the air and then a shorter distance through the water. Clearly, that's goal-oriented behavior for us. For the photon, both descriptions are valid. But it turns out it's actually simpler to calculate the right answer if you do Fermat's principle and look at the goal-oriented way.

Then we see in biology, Jeremy England, who used to be a professor here, realized that in many cases non-equilibrium thermodynamics can also be understood more simply through goal-oriented behavior. Suppose I put a bunch of sugar on the floor and no life form enters the room, including facilities. It's still going to be there in a year. But if there are ants, the sugar is going to be gone pretty soon, and entropy will have increased faster because the sugar was eaten and there was dissipation.

Jeremy England showed that there's a general principle in non-equilibrium thermodynamics where systems tend to adjust themselves to always be able to dissipate faster. There are some liquids where if you shine light at one wavelength, they'll rearrange themselves so they can absorb that wavelength better to dissipate heat faster.

You can think of life itself a little bit like that. Life basically can't reduce entropy in the universe as a whole. It can't beat the second law of thermodynamics. But it has this trick where it keeps its own entropy low and does interesting things, retains its complexity and reproduces, by increasing the entropy of its environment even faster.

**CURT:** So the increasing of entropy in the environment is the side effect, but the goal is to lower your own entropy?

**MAX:** You can have two ways of looking at it. You can look at it as a bunch of atoms bouncing around and causally explain everything. But a more economical way is to say that systems tend to dissipate energy. Life, in particular, is a system that's become very good at dissipating energy because it can maintain low entropy internally while increasing entropy externally. So you can describe life's behavior either way—as physics or as goal-oriented.

And this matters because when we think about AI systems, we're also thinking about systems that process information and dissipate energy. There's a deep physics connection here that most people haven't fully appreciated. Goals aren't mystical. They can emerge from physics. They can be understood through physics.

---

**CURT:** I want to touch on something that's been percolating in my mind. You've been really bullish on AI progress, but also measured. And you mentioned Fermi's 1942 nuclear chain reaction under a football stadium in Chicago. That was like a Turing test for nuclear weapons.

**MAX:** When the physicists found out about this, they totally freaked out—not because the reactor was dangerous, it was pretty small, nothing more dangerous than ChatGPT is today. But because they realized, "Oh, that was the canary in the coal mine. That was the last big milestone we had no idea how to meet, and the rest is just engineering."

I feel similarly about AI now. We obviously don't have AI that's better than us or as good as us at AI development, but it's mostly engineering from here on out. It won't be large language models scaled. It will be other things. But in 1942, if you were there visiting Fermi, how many years would you predict from then until the first nuclear explosion?

**CURT:** Difficult to say, maybe a decade?

**MAX:** It happened in three. Could have been a decade. It got sped up because of geopolitical competition during World War II. Similarly, it's very hard to say now: is it going to be three years? A decade? But there's no shortage of competition fueling it. And as opposed to the nuclear situation, there's also a lot of money in it. So this is the most interesting time—the most interesting fork in the road in human history. If Earth is the only place in our observable universe with consciousness about the universe at large, this is probably the most interesting fork in the road in the last 13.8 billion years for our universe too. There are so many different places this could go, and we have so much agency in steering in a good direction rather than a bad one.

---

**CURT:** There's been this moment where I started seeing papers published, and one of them was on language models using trigonometry to do addition. That felt like a Eureka moment. Can you talk about that?

**MAX:** Oh yeah, that's super exciting. There's this paper that came out not too long ago where researchers discovered that language models—specifically, they were looking at transformers—have developed internal representations where they do arithmetic using trigonometry and high-dimensional geometry.

What's remarkable is that this wasn't explicitly programmed in. The researchers used sparse autoencoders, which is part of the mechanistic interpretability field. They were dissecting how these networks actually work internally. They found that the network had learned to represent numbers in a clever geometric way. The operations for addition, for example, were essentially rotations in this high-dimensional space. It's beautiful. It's like the network discovered geometry on its own.

This is exactly the kind of thing I'm talking about when I say we need physicists working on AI. An engineer might just say, "Well, it works, let's ship it." A physicist asks, "Why? Can I understand the deeper structure? Can I write down equations?" And here we have equations. We have geometry. We have actual physics happening inside these neural networks. Once you see that, once you understand that these networks are doing sophisticated mathematics—that they're essentially doing what Faraday did, discovering new physics—it changes how you think about AI entirely.

---

**CURT:** I want to ask about something concerning to many people: the possibility of misaligned AI causing harm. But I want to frame it differently. What if we're massively underestimating our ability to control this?

**MAX:** I think we have much more control than many people realize. I think that we are much more empowered than we've been told. I mentioned earlier that if we were living in a cave 30,000 years ago, we might have made the same mistake and thought we were doomed to always be at risk of getting eaten by tigers and starving to death. That was too pessimistic. We had the power to, through our thought, develop a wonderful society and technology where we could flourish. And it's exactly the same way now.

We have enormous power. What most people actually want to make money on AI is not some kind of sand god that we don't know how to control. It's tools. AI tools. People want to cure cancer. People want to make their business more efficient. Some people want to make their armies stronger. You can do all of those things with tool AI that we can control.

And this is something we work on in my group. That's what people really want. There are a lot of people who don't want to just be like, "Okay, it's been a good run, hundreds of thousands of years, we had science and all that, but now let's just throw away the keys to Earth to some alien minds that we don't even understand what goals they have."

Most Americans in polls think that's a terrible idea—Republicans and Democrats alike. There was an open letter by evangelicals in the U.S. to Donald Trump saying, "We want AI tools. We don't want some sort of uncontrollable superintelligence." The Pope recently said he wants AI to be a tool, not some kind of master. You have people from Bernie Sanders to Marjorie Taylor Greene saying on Twitter, "We don't want Skynet. We don't want to just make humans economically obsolete."

So it's not inevitable at all. If we can remember that we have so much agency in what we do, what kind of future we're going to build, if we can be optimistic and think through what is a really inspiring, globally shared vision—not just curing cancer but all the other great stuff we can do—then we can totally collaborate and build that future.

---

**CURT:** Last thing. If someone's listening—they're a researcher, young or old—and they have something they'd like to achieve that's extremely unlikely, that's criticized by their colleagues for even proposing it, but nothing nefarious, something they find interesting and maybe beneficial to humanity. What's your advice?

**MAX:** Two pieces of advice. First, about half of all the greatest breakthroughs in science were actually trash-talked at the time. So just because someone says your idea is stupid doesn't mean it is. You should be willing to abandon your own ideas if you can see the flaw, and you should listen to destructive criticism against it. But if you feel you really understand the logic of your ideas better than anyone else and it makes sense to you, then keep pushing it forward.

The second piece of advice: you might worry, like I did when I was in grad school, that if I only worked on stuff my colleagues thought was bullshit—like thinking about the many-worlds interpretation of quantum mechanics, that there were multiverses—then my next job was going to be at McDonald's.

My advice is to hedge your bets. Spend enough time working on things that get appreciated by your peers now so that you can pay your bills, so your career continues ahead. But carve out a significant chunk of your time to do what you're really passionate about in parallel. If people don't get it, don't tell them about it at the time. That way you're doing science for the only good reason—that you're passionate about it. And it's a fair deal to society to do a little bit of chores for society to pay your bills also.

It's been quite shocking for me to see how many of the things I got most criticized for or was most afraid of talking openly about when I was a grad student—even papers I didn't show my advisor until after he signed my PhD thesis—have later actually been pretty picked up. I actually feel that the things that have been most impactful were generally in that category. You're never going to be the first to do something important if you're just following everybody else.

---

*Note: All preview content, advertising, sponsorship reads, and promotional material have been removed from this transcript. The conversation begins at its natural starting point and concludes with the closing advice. References to specific papers, researchers, and technical concepts have been preserved as integral to the substantive discussion.*