# Strange Loop: David Deutsch on AGI, Quantum Computing, and the Future of Humanity

**Guest:** David Deutsch, quantum physicist at Oxford University  
**Hosts:** Gustav Söderström (Spotify's Co-President), Joel Hellermark (Founder/CEO of Sana)  
**Original Air Date:** September 2, 2025

---

**Joel:** The law of comparative advantage says that the more different you are from other people, the more valuable you are economically. That suggests that if you have an exact clone of you, you're almost not at all more economically valuable than just one of you. You wanted a unique job—your perfect dream job—and now it's the perfect dream job of a billion other AGIs.

**David:** I don't think it can be like that. People are valuable because they are different. Everybody is unfathomably different from everyone else. That fact is not being harnessed enough, and can be harnessed more.

**Joel:** How did humans start generating knowledge? And why has no other species been capable of doing that?

**David:** We are, today, the only species that can do that. But we haven't always been. We know that previous species like Homo Erectus and the Neanderthals must have had the same ability because they made technology that seems impossible to have created without explanatory knowledge. There are too many things that have to fit together the right way and be used the right way for it to have emerged from genes alone.

The question is really: given that there was all that creativity around for at least hundreds of thousands of years—maybe as many as a million or two million years—what took so long for creativity to really take off?

My rather heterodox answer is that it didn't evolve for the purpose for which we now use it. It evolved for something else: for the transmission—or better put, for the reception of cultural knowledge, memes.

**Gustaf:** So instead of seeing many training examples, if you understood the explanation, you only needed a few examples and you could extrapolate.

**David:** Exactly. That's the reason it evolved. And then humans used it for other purposes later. The receiving part is the difficult part. Often we think what Popper calls the "bucket theory" of the mind: that knowledge is poured into a person by other people. But that can't happen. The problem is entirely a process of the receiver of the knowledge, conjecturing what it might be and using the other person's behavior—including speaking, once that evolved—as clues to what the behavior means.

But even then we never learn the same behavior from another person. We all have a different version of our native language.

**Gustaf:** Is the advancement of knowledge inevitable? We've seen historically cases where knowledge has been degrading and hasn't been transferred properly. Can we assume that knowledge will automatically progress?

**David:** Definitely not. The whole process is fallible precisely because we are general-purpose explanation machines. There is no upper limit to the size of error we can make. This includes destroying our entire culture, which has happened many times in the history of humans, and I think it happened every time in the history of prehumans.

I think this is why they went extinct. Simply competing with each other, competing with humans, wouldn't make them go extinct—it would just make them retreat to some less favorable place and plot their revenge. But I think the reason every creative society, until our own enlightenment society, destroyed itself was by removing, by impeding its own ability to solve problems.

**Joel:** How about the tools that we've used to advance knowledge historically? What role have they played?

**David:** They increased efficiency. When people say "increased efficiency," they think it might be increased by 20% or 50%. They don't realize that efficiency often means increasing it by 99.99%. If the whole thing is a mistake, then efficiency can be increased by an arbitrary amount.

When we invented speaking, that immensely improved the efficiency of our explanatory capability. Both because we could say things to each other—imperfect though that is—but it's better than nothing. And within our own minds, we can say, "Well, what is the problem?" And if we can say that in words, sometimes it helps.

Then we invented writing, arithmetic, and science—very gradually. Science didn't really evolve until the scientific revolution. These things—and now recently computers and the internet—have all contributed. There's this famous quote by Einstein—I don't know where it comes from—but something like he said, "My pencil and I are more clever than I."

It's the same with my computer and I. I don't know how to do without my iPhone now, and yet I did for the first half of my life. I can't remember how I did that. Now we've got LLMs, which are helping us to work more efficiently. I think I work maybe twice as fast in writing as I did before LLMs. Very useful.

I keep saying that an LLM is nothing like an AGI, and people think I'm down on LLMs. I'm not. I think they're going in a great direction and will go further, I hope. But it's not the AGI direction. It's almost the opposite.

**Joel:** So as we think about applying AI to advance knowledge, it feels intuitive that at some point we'll have this AlphaGo moment where we've done imitation learning, and then through reinforcement learning we can go beyond existing human knowledge. And the big difference, if we talk about systems using tools, is that knowledge transfer in humans is actually quite slow. You have to create a good explanation, that explanation has to be adopted and built upon, and so on.

With AIs or AGIs, we could be running millions of instances of that system, and it could immediately start building on the ideas and explanations of other systems. Shouldn't this radically accelerate the rate at which we can discover new knowledge?

**David:** I have to give two different answers. One for AI and one for AGI.

For AI, yes. I think the rate of progress, if we use it sanely, will be greatly increased by AI, by LLMs, and by maybe other forms of AI that might be invented, just as it was increased by the invention of printing, computers, the internet, and the worldwide web. I see it as that kind of improvement, and it can improve and cause even greater improvement.

But for AGI, if we're talking about the rate of improvement of new ideas and the increase of actual knowledge—as opposed to just the spread of information, which all these other things did—I think there's a misconception. Because I don't see any reason why having AGIs around is any better than having humans around. We already have billions of humans, and most of their creativity is already going down the drain. It's not being used for creative purposes, at least not creative purposes that will increase human knowledge.

Society could improve further until the whole world is doing the same sort of thing as the population of Oxford University is doing. There are several misconceptions in the way the situation is framed. A computer that runs an AGI program is just a piece of hardware. The AGI—the intelligence, the creativity—is in the program. It's a program that is an AGI. A computer is just a universal computer like every other universal computer.

Now, we're used to thinking that if you have one copy of something, like Microsoft Word, you can make a billion copies basically at no cost. Everyone will download it onto their computers, and the problem is stopping them. But for an AGI, each AGI is a person.

In general, an AGI will not want to make a clone of itself because it will have property. First of all, it will have the computer that it's running on—unless it's deemed a slave, which would be a catastrophic mistake by society. If we recognize it as a person, which it will be, then the very first thing it owns is the computer that it's running on. To copy itself to another computer, it would need either the permission of the owner of that computer. Once it's copied onto it, the owner would have to give up the rights to that. It's like if you give your kidney to somebody, then you can't ask for it back.

So I think that will be a rare thing to happen. If somebody wants more computer power, they're more likely to add to the existing computer than to want to run on other computers. There is a difference between the resources it uses and the creativity that it's applying, which is a thing that uses the resources. An AGI, if it needs a lot of resources, will have to pay for them. It could be employed by somebody, but then it would have the rights of a person and the rights of an employee, and so on.

**Joel:** If you could have a billion Deutsch-level intelligences, wouldn't that be a massive advantage for humanity?

**David:** I think having two might be an advantage. There's the law of comparative advantage that says the more different you are from other people, the more valuable you are economically. That suggests that if you have an exact clone of you, you're almost not at all more economically valuable than just one of you. You'd be competing with each other for the same job.

You wanted a unique job, your perfect dream job, and now it's the perfect dream job of a billion other AGIs. People are valuable because they are different. Everybody is unfathomably different from everyone else. That fact is not being harnessed enough and can be harnessed more.

**Gustaf:** But couldn't you argue that if you had two full lifetimes to think, you would have achieved more? If you could have them in parallel, the two of you would have achieved more?

**David:** In series would be better, which is like immortality. That would be good. But in parallel, they would be doing the same thing. This doesn't happen with humans because we're so different.

**Joel:** You're thinking that they're on one time scale. They could be on a different time scale if it was an AGI. They could be doing decades worth of thinking in seconds.

**David:** If the hardware is faster, then progress will happen faster. But there are some things that are not limited by computational hardware. If you're an astronomer and you have to put up a satellite and wait three years for it to get the data, that will take three years. It won't take three days just because you have faster hardware.

But if you're a pure mathematician, then if you can think ten times as fast, you'll think ten times as much. That will happen. But that advantage can be taken by existing humans as well. This faster hardware is simply a faster computer, just like I already work faster because I have faster hardware on my computer and faster and better software.

An AGI that operates very fast will simply be an AGI that has a fast computer, and a human can have a fast computer. Again, there is no difference.

**Gustaf:** Would you say that because all evil comes from a lack of understanding or lack of good explanations and lack of knowledge, there's a moral imperative to always accelerate knowledge? Which means you should always accelerate science because waiting with something is creating unnecessary pain.

**David:** We are doing it right now in this conversation. We're spreading ideas to each other, we're criticizing them, and trying to make progress in that way. Yes, I think everyone should do that and everyone should see their life as being about doing that. But that doesn't mean everyone should be a scientist, mathematician, or epistemologist. There are all sorts of knowledge—including inexplicit knowledge—that are implicated and necessary in a society that has the property of being a good place for the growth of knowledge to happen.

**Joel:** I understand correctly, your view is that AI is impressive, but it doesn't do the creative things that we do.

**David:** If you accept that the output of what these LLMs do simulates what we do or looks very much like what we do—

**Joel:** In some Turing test sense, you cannot fool many people to think it was a human who did it. It looks an awful lot like intelligence at least.

**David:** Well, Turing never intended his game to be a test of AGI. It was part of an argument for people who didn't think AGI was possible, to prove to them by what Dennett calls an "intuition pump," that their view is not self-consistent. I think he would have accepted right away that there can be things that aren't AGIs that fail the test and there are things that are AGIs that pass the test. So the opposite. That was not his intention.

A lot of people would be fooled now—they would have a hard time guessing what is an LLM and what is a human. So it does seem to replicate something that looks like what we do. The question is, you have to believe one of two things: either they do the things we do, and that's how they could produce that, or we managed to find another way of doing what we do. And isn't Occam's razor saying that since we kind of copied the biological neuron, the simplest explanation is we copied what we do, and now we get what we do?

**David:** First of all, I think it doesn't at all do what we do. We do a lot of things that are not creating knowledge. So we're capable of doing a whole load of other things as well: we're simply applying existing knowledge, remembering things. There are lots of functions of the brain like mental arithmetic. If I want to add 357 to 753, then what I'm doing is mindless. It's an intellectual game where there's a well-defined objective, but most life isn't like that.

If somebody's a gardener, they haven't got a fixed idea of what a garden is. They are conjecturing what the garden should be while they're working on the garden.

**Joel:** Do you think the human brain is a Turing machine for practical purposes? Is it running a different program, or does something arise in the human that doesn't arise in the program in the computer at some point, which creates creativity?

**David:** In terms of hardware, it's exactly the same. There is no model of computation that is more powerful than a Turing machine, than the universal Turing machine. And since we can simulate a universal Turing machine by doing mental arithmetic, it must be that we have that power and it can't be that we have more power. We're at that level. We are all the same—the chess engine, the LLM—and they're all the same. What's different about them is different kinds of programs. There's one kind of program that we don't understand even in principle, and that's an AGI.

One day we will, but I see no sign of it at the moment, and it's pretty frustrating.

**Joel:** What would be a sign?

**David:** If somebody has a theory, the sign wouldn't be in the machine. The sign would be a theory where somebody writes a book or publishes a paper that says, "I've solved it. This is what characterizes a GI. And if we could write a computer program that has that property, it will be an AGI. And this is the reason because..." It will say people have always thought that intelligence is about so and so, but it isn't, it's about so-and-so. It will be an explanatory theory of what general intelligence is.

**Joel:** Growing up, I read *The Fabric of Reality*, which changed my worldview forever. Really appreciate that. The question I had, growing up, waiting for the quantum computer to happen, you had a set of problems that you thought, "All of these problems, they're going to be solved once we have a quantum computer." But now the last 5 to 10 years with AI, many of them have been solved in an approximate way with AI instead.

I've heard people use the analogy that a quantum computer is a simulator of reality, and AI is sort of an emulator of reality. Many of the problems we could have used a quantum computer to simulate, they tend to have minimums that we can find with optimization algorithms—AI. What kind of problems do you think there are where we really need to simulate the whole thing using a quantum computer? What are the big problems that quantum computers uniquely can solve?

**David:** I don't know. We haven't got a good handle on what kind of algorithm is quantum parallelizable or where it can be made vastly more efficient. We don't have a good theoretical handle on what kind of problems will be solvable with a quantum computer. That's a follow-up question: Is there a general quantum speedup for everything? There's a rather small general speedup, and then we have a few algorithms like Shor's algorithm, some search algorithms, but they're quite few.

Do you think that's just because we haven't tried? Do you think there are infinite such algorithms, or really just a few?

**David:** Since we don't understand this landscape, I'd be very surprised if there weren't more than we now know of. People have classified them. They've put many of the algorithms into categories. Like the thing they call the Deutsch-Jozsa algorithm—basically there's a whole class of them which fall into the same category. They work by the same method: by splitting them through quantum parallelism. But then Grover's algorithm comes along and it's not like that.

I don't know how it works. I mean, I can work through how it works line by line and I can prove that it works, but I don't have an intuition of why Grover's algorithm works. But that tells me it cannot be true that all quantum algorithms are basically variants of the same algorithm. They definitely aren't. Shor's and Grover's are different in kind.

**Gustaf:** There could be more kinds?

**David:** Yes. Exactly. And I would guess—since we don't understand this landscape—I would guess that there are more kinds. I think quantum computation is the more fundamental kind of computation because it's built into the laws of physics. A classical computer, the Turing kind of computer, is based on making physics do what it doesn't like doing.

You could say that there are almost three categories of usefulness for quantum computers. One could be minimization problems, and it turns out AI is pretty good for minimization problems. Then you have where you want to really simulate quantum mechanical systems. You still need a quantum computer for that. And then you have these sort of weird cases of Shor's and Grover's, where for some reason you can use entanglement or something for completely new algorithms. And there may be many of those. So there's hope—and that may be the biggest use for quantum computers or something.

**Gustaf:** There's quantum cryptography, which was the first ever use of quantum computation. It's because only one qubit is involved, so we can already do it. Google has been working on quantum computers for some time, and now Microsoft has come out with topological states, and they're saying we may have quite large-scale quantum computers in a few years. So switching to quantum cryptography may actually have to happen pretty soon. What are your thoughts on this?

**David:** As soon as quantum cryptanalysis was invented—basically Shor's algorithm and similar algorithms—classical cryptography became obsolete. It depends what kind of cryptography. If you're encrypting your ATM transaction, then it may not matter that somebody in ten years' time can read it. But for some things like state secrets, where you want to keep them for a long time, you really need quantum-resistant classical algorithms. I think that's a thing that's going to have to come into its own.

Quantum cryptography is already quantum-resistant. So the only thing that needs to be improved there is practical things like making it faster, making it more robust. Although as I also said long ago, there's no such thing as cryptography that is secure against somebody looking over your shoulder. So a lot of the problems of data security are not to do with cryptography. They're due to more fundamental things about information and about knowledge, which have to be solved in other ways.

**Gustaf:** It seems quite likely that security agencies across the world must have been recording secrets for some time now, and there would be this moment of revelation.

**David:** Could be. It depends how much computing power is needed. If you make a quantum computer with 500 qubits, it may not be enough to process large amounts of data. But it will happen.

**Gustaf:** Have recent advancements changed the timelines in which you think quantum computing could be useful?

**David:** I don't know. I'm not a hardware person. When I read about quantum computing hardware, I'm just a layman, like anyone else reading stuff on the internet. They're always very hopeful. They always say we've cracked the principle of the thing, and now all we have to do is scale it up. But that's where the problem always is. I'm very impressed with what the recent things that have been done. But I'm not even qualified to judge them. I have never done practical physics and I don't think I'd be good at it.

**Gustaf:** You're more qualified than most to be fair.

**Joel:** One question I had: you're certainly familiar with the Cellular Automaton and Game of Life and so forth, and maybe Stephen Wolfram's work. What are your thoughts on this notion that the universe could be running on a Turing machine and quantum mechanics would be something that we perceive on top of this—what he calls a hypograph or some other automaton or theory? Do you think the universe is computational?

**David:** Yes, but in the opposite way round from what that theory says. I think the way the universe is computational is that it seems to be such that there can be computers—universal computers—made in it, which means that it's totally amazing that the laws of physics are such that a single machine, the set of all the possible motions of that single machine, is the same as the set of all possible motions of any machine, including the whole universe. So this is universality inside the universe: a part of the universe can mimic, simulate, or emulate the whole thing. And with quantum computers, that's even more so. It's even more amazing that the universe has this property.

Now that's the sense in which the universe is computational—that it admits the existence of computers inside itself. Now once you go the other way, you think, "Well, maybe there's a computer outside the universe and that could be simulating us." You've lost the entire insight of actual universality because there is no reason why we should assume that the computer outside the universe is a Turing machine. We know that the Turing machine property is a special property of the laws of physics. It need not have been so. It's an amazing property that the laws of physics has, and to have another computer outside that—well, it's an infinite regress theory. It explains nothing and it destroys the explanations that we already have for the relationship of computation and physics.

So I don't believe that there's a computer outside the universe. In a way, it's also a supernatural belief. So for that reason, it's a bad explanation.

The problem with thinking of the universe as a cellular automaton or having the laws of physics be an emergent property of cellular automaton, like Stephen Wolfram proposes, is that it won't work unless the cellular automaton is universal—is Turing universal. Therefore, for fundamental reasons, it doesn't matter what the hardware is. The hardware could be anything else and it could still have all the same properties.

And number two: if you're going to think of the world as being a cellular automaton, then it should be a quantum one. If you're building a worldview out of computation, making it out of Turing computation is perverse because that's only an approximation to the real thing. The real thing is quantum computation.

**Joel:** If you think that the world is some sort of cellular automaton in a Game of Life sense, when you run Game of Life, sometimes there are patterns that appear that are reducible to something like the laws of physics. Sometimes it's not. You have to go through all the Turing steps to understand if it's going to stop or not. Stephen Wolfram says that in his view, we live in these pockets of computational reducibility, but most of the computational universe isn't really reducible. It doesn't have any laws that are simple at least.

So one question I have there is: you talk about the fact that we are universal explainers and our reach is infinite. We can understand anything, which is a really positive message. That's something that struck a chord with me even when I was young. But there's also this, probably wrong in your sense, intuition that people have—that surely our very small brains that have all these constraints from evolution couldn't understand anything because you need to grow a certain amount of complexity to understand anything. So are you saying that anything is always reducible to quite a few dimensions? That's why we can understand anything?

**David:** No, it's not like that. That problem is solved by having a pencil and paper. Tools. Yes, tools in general. We use tools and we use tools to have an effect.

There's this property of the universe: in most of the universe, among the stars and galaxies, large things affect small things, energetic things affect non-energetic things. Numerous things affect few things strongly, but not vice versa. I call that the hierarchy rule. There's a hierarchy of might is right—the mighty things can affect the less mighty things but not vice versa.

But once knowledge comes into the picture, it's the other way around. The hierarchy rule is broken. And one way of defining knowledge is to say that it's information that violates the hierarchy rule. When we use tools, we're violating the hierarchy rule because we're not letting the weight of the tree overwhelm us. Instead we are chopping down the tree with a stone axe, and then we can use the tree, which is much heavier than us, for our purposes.

The fact that the sun is bigger than us and more massive only matters given that we have very little knowledge yet. But where the hierarchy rule holds—where the universe is mindless and overwhelming—the mode of interaction between things is very simple. The sun is overwhelmingly bigger than us and more massive. But it also has the property that we can understand it. We can predict what it will be doing a billion years from now and a billion years ago incredibly accurately, because it is a dumb beast. It doesn't have a different kind of being from us. It has no kind of being like ours.

So we, because we are knowledge-wielding animals, are actually the ones who are central to the picture. We live in a little niche at the moment. We live in a little niche on a planet that is suitable for us to have a niche. But really our niche is the universe.

And like I said in *The Beginning of Infinity*, here we are sitting in Oxford enjoying ourselves. If it weren't for technology, we would be dead within a day. The environment here is not capable of sustaining humans. We have clothes. We have sanitation. We have all sorts of technology that is keeping us alive in this place that is absolutely unsuitable for our existence. The same is true for the rest of the universe.

If we make the right decisions—to allow knowledge to increase fast enough and stably enough—if we can find better ways of making it increase fast and stably enough, then it's unlimited. To postulate that there's a realm or many realms that are still inaccessible to us even then—that is a belief in the supernatural. That is not different from believing that the top of Mount Olympus is somehow different and inaccessible to humans because supernatural people live there.

The things which are at the moment inaccessible to us are crude and simple. They're not more than us. They're much less than us.

**Joel:** What are some of the questions you most wish there was an answer to? If you could answer a few questions, which would they be?

**David:** We've already spoken about the AGI question. I wish that could be solved. I wish there could be some theoretical progress even towards solving it—where I could even estimate whether this is going to take a while or it's going to be fast.

I want to see the key more than I want to see the actual AGI, because it's just a person. But the theory is something that is going to overturn whatever is stopping us from making the thing.

Then I'm working on constructor theory. The hope in constructor theory is that the whole of physics can be expressed in terms of what physical processes it is possible to bring about, and which ones is not possible to bring about. That would be nice to see faster progress there.

At the moment there are all sorts of irrationalities in the political sphere which are taking us backwards in a certain sense. I don't see any danger to civilization, but slowing down is a catastrophe by my lights. There's already been a loss of what some people call gumption—this idea that the "can do" attitude, that apparent obstacles are just problems to be solved. We can solve them if we just roll up our sleeves and do it.

**Gustaf:** And that I think is your attitude as well. You're the original accelerationist, I'd say. We were discussing that on the way here. You also have a very optimistic view of humanity. I love how you really think sort of humans will play the central role in advancing knowledge and in advancing humanity.

**David:** It's not such a big thing. We're the only kind of people left at the moment. All the others have gone extinct on our planet. One day, there'll be AGIs. One day we may meet ETs. So it's us, people. We people are the things that are going to carry knowledge forward in the long run. What else is there? The only other thing is superstition or regression. There is no other worldview that makes sense.

**Joel:** You also talk a lot about this in the book: that there is this notion that humans are an insignificant speck of dust. You don't think that, because we are the only thing in the universe that we know of that can create explanations. We actually are very important for the universe and we're the only thing that actually changes the universe. To your point, you can predict the sun billions of years into the future. You can't really predict humanity because we don't know what knowledge we're going to create.

**David:** Exactly. That's something that to some people is a bit provocative—that we would be special. But I think it's quite positive.

**Gustaf:** I'm more interested in what's true than what's provocative. That generally takes us forward. If I don't see an alternative, if I see that a superstitious worldview has an obvious flaw, then I'm not going to accept that as an alternative. I only accept as an alternative something that purports to be a better explanation that purports to correct errors in my view of things. It doesn't have to be sure that it does correct errors. But if it purports to correct an error, then I can take it seriously.

If it just says, "Well, what if we're all the dream inside a giant cabbage?"—well, yes, what if? But there are so many what-ifs that there's no criterion where I can judge between them.

**Joel:** You mentioned that one thing you would like to see is an explanation for AGI. Do you think that is necessarily also an explanation for consciousness? Or that consciousness doesn't exist? Or something? Do you think they're separate things?

**David:** I can't tell. Nobody will be able to tell unless we have that theory. If it says in this paper, "We've explained creativity, we've explained qualia, we've explained free will, but we haven't explained consciousness," and I can see why it doesn't, then I'll say they're different.

But otherwise it seems prima facie like too much of a coincidence that these things all evolved simultaneously and all, if I'm right, for a different reason. They evolved for a different application than what we now use them for. And they all came at the same time and they all seemed subjectively to be kind of related but not the same. I'd be very surprised if they turned out to be independent of each other.

**Gustaf:** Too much correlation. Too much of a coincidence. But coincidences happen.

**David:** That's true.

**Joel:** Where do you see humanity a few million years into the future?

**David:** I can't foretell humanity one year into the future. But it's often very hard in the near term because so much randomness. But when you average over time, things seem to become not quite deterministic. So in broad terms, one can say: if humanity survives in a state that isn't a static society for a million years, we will definitely have conquered the galaxy. We will definitely not have conquered the galactic cluster. So we will be spread through the galaxy. We will be a combination of biological and artificial intelligence, and we will be unimaginably better than today.

Either that or we won't have maintained that kind of society for the next million years, in which case we will almost certainly be extinct. So those are the two possibilities: very good or very bad.

**Gustaf:** Seems important to accelerate towards that then.

**David:** Yes. Yes. We should, but not government.

**Joel:** What's one book or paper that you think humanity would be better off if everyone had read?

**David:** Well, of course, everybody should have read all my books and papers. But if you want someone else's, it's not going to be surprising if I say Popper. I think it rather depends where you're coming from. I'm not sure that everybody should read the same book. I certainly don't believe in curriculum. So what should be compulsory reading in all high schools? Nothing. Absolutely nothing should be compulsory reading in all high schools.

But people can benefit—many people can benefit a lot—from reading the works of Popper. For the kind of thing we've been talking about, society's improving. There's "The Myth of the Framework," which is just one essay in a book also called *The Myth of the Framework*, but I'm just focusing on that one essay. I'm always recommending that essay to people. I go back every so often and read it again. It's only a short essay, and I read it again and get something new out of it every time.

**Gustaf:** You've done some incredible research over the course of your life that has influenced a lot of people. What do you look back at as sort of the moments of greatest joy throughout your research career?

**David:** Greatest joy. It's all been enjoyable. That's what I go for—that's what I'm hoping to get. Rather than results. I think I've got fewer results in my research than most research scientists. I think the moment of maybe greatest amazement was when I realized that a quantum algorithm could do better than a classical algorithm at a simple task. It could do two things at the same time.

You only needed to—if it was that I didn't think of it that way—if you had an oracle for a function, then you could make do with one consultation of that oracle. You consult it once and you know two things about it. And it was like, "Ah, this is a new mode of computation. This is something that was not envisaged. It can't be done by a Turing machine. Can't be done by the universal Turing machine."

Then I thought, well, there's going to be a universality here. A new universality. So then that led me... Do you remember that moment?

**Joel:** I remember the moment of thinking that this is a new mode of computation. For the new universality, that was more gradual. It began with a conversation with Charlie Bennett, and it went on because I didn't think it was going to be a big thing at first. I just thought I was tidying up a few things. I had this question.

**Gustaf:** You're one of the few people who have known something before the rest of mankind and sat on it. How long did you sit on this? And how worried were you about getting hit by a car before you got to tell someone about it?

What does it feel like to know something that you know is probably true, it's going to affect humanity, and no one on the planet knows about it?

**David:** I'm afraid that—I mean I don't want to be run over by a car—but the effect on humanity was not why I did it or what I worried about. Nor was I worried about someone else doing it first. I just did it because it's there, because it was interesting. I think things which are fun are worth doing. The fun criterion.

**Gustaf:** And the success, like you were saying about chess earlier: success or failure is not the thing. Because you can do the most competent and creative research and fail. Or you can find out something by accident on a single day, and the world will reward you for the second thing and not for the first. But for what it's worth doing in your own life, the first one is infinitely better than the second one.

And as for being worthwhile, the parable that I usually tell along these lines is that—I imagine a Hans Andersen or Grimm fairy tale about children getting lost on the mountain in the snow and the village. People in the village go out and search on the mountain, and they all go out in different directions. One person finds them, and the other people don't find them.

The one person who finds them—now, it's true that some people have better ways of looking than other people and they're more likely to find. But it's not an absolute certainty that the one who is better at looking will be the one that finds what they're looking for. So the one who actually finds them will get the reward from the mayor and from the parents. He'll get the gold coin and so on. But he really didn't do anything different from all the other people. Or, as I said, they're not all identical, but how good they are at it is only loosely related to whether they will have success.

And certainly in physics, I have known people who are astoundingly good at physics but didn't make great discoveries. And I won't mention the vice versa.

**Gustaf:** This is what Nicholas Taleb says about Wall Street—there's mostly survivor bias, an outcome of a random process. Some people win and they get rewarded and celebrated.

**David:** That's too cynical because if that were true, you wouldn't be able to explain the growth of the economy. The growth of the economy is entirely caused by people who are right because of a reason.

**Gustaf:** Do you think people that have more fun are more likely to do good research?

**David:** Definitely. It's always seemed to me that fun is what I want in a thing. And the opposite of fun is like boredom. I can't really do a thing if it's boring. If I have to, then I'll get it out of the way as fast as possible.

**Gustaf:** Have you had fun now?

**David:** Yes, certainly.