# The New AI Primitives: How AI is Revolutionizing Content Creation and Consumption

My fascination with computer graphics began at eleven, watching a television show called Mega Movie Magic. At the time I was experimenting with Flash 5—dorky vector animations and cartoons—when I realized I could use the same computer to create visuals indistinguishable from reality. That moment set the trajectory for everything that followed.

Professionally, I've spent a decade in tech, predominantly at Google, working on AR, VR, and 3D mapping. I had the chance to work on VR camera systems and YouTube VR content, to turn the world into an AR canvas with geospatial APIs, and to advance the next generation of Google Maps through immersive view. Perhaps most memorably, our team worked on moving satellites and planes around in the sky—the geo team simply does cool work.

More recently, I've come full circle to what I call being a creator. I've built a following of about 1.5 million subscribers across multiple platforms, and I've noticed a pattern in how I engage across them: I entertain on YouTube and TikTok, and I educate on Twitter and LinkedIn.

## AI as Amplifier and Commodifier

Throughout my career, I've witnessed the emergence of AI as a transformative force. What strikes me most is how it amplifies and commodifies many of our abilities, instincts, and impulses in remarkable ways. Yet while this may seem groundbreaking, it often mirrors concepts that have existed for decades. This is paradoxical but true: it has never happened before, yet has happened time and time again.

Spatial representations offer a clear example. Whether in the 1800s with paper prototypes, through Disney's multiplane cameras, to Google's multiplane images, the same core idea repeats itself across different eras. The same pattern appears in animation: we moved from manual rotoscoping to AI-powered video-to-video tools. The progression is always toward greater automation and ease of use.

What once required complex workflows in tools like After Effects—techniques that took weeks to master—can now be accomplished in a few clicks with tools like Pollen. These tools are democratizing capabilities that I had to spend years learning to master. Now creators can hit the ground running and focus their energy on even bigger visions.

## The Acceleration of Content Creation

The shift is dramatic. The old way of creating content was a classical waterfall workflow: take a bunch of specialized tools and chain them together sequentially. The new way compresses that timeline significantly. What took me one week now takes one day.

The obvious question is whether we'll all become Tim Ferriss, working four hours a week and spending the rest of our time relaxing. I would argue the opposite. We'll put more value on the screen. We'll take on more ambitious projects because the technical barrier isn't the constraint anymore—vision is.

To give a concrete example: I created a video last year using this accelerated workflow that accumulated 37 million views. There was a cultural moment where it made sense to make that video, and I made it. Had it required a full week of production, I wouldn't have made it at all. That insight matters because it suggests a bifurcated future: indie creators like me can start rivaling studio-level output, but studios will simultaneously set whole new standards of sophistication and ambition.

## Spatial Intelligence: Teaching Machines to See 3D

Let me now move into the core AI primitives that are reshaping creative work. The first is spatial intelligence.

You've likely heard of photogrammetry—the art and science of measuring objects in the real world using images and other sensors. In the academic literature, it's defined as the ability to understand and interpret spatial data. Essentially, we're teaching machines to understand the 3D world we inhabit every single day.

Photogrammetry isn't new. It predates computers. But what is new is the commodification I mentioned earlier. What required data centers and teams of experts in the 2000s became commodity technology by the 2010s. Similarly, volumetric rendering isn't novel, but machine learning has given these techniques a massive boost.

Enter neural radiance fields—NeRFs. It's genuinely remarkable to me that an AI can essentially spitball ray-traced imagery using just 100 photographs I provide, creating a 3D world you can step inside and explore. The progress is stunning. From the seminal NeRF paper in 2020 to today, we've witnessed insane advancement. Just five years of iteration has transformed the landscape dramatically.

The next evolution is 3D Gaussian splatting, which offers orders of magnitude faster rendering. Where NeRFs might take hours to render in real-time, Gaussian splatting approaches near-interactive speeds. The practical implications are significant: you can now capture a place, reconstruct it spatially, and render it interactively in ways that were previously impossible.

The velocity of progress here is staggering. Companies like Luma AI have essentially democratized capture workflows. What once required expensive equipment and expert knowledge is now accessible via smartphone. Niantic is embedding this technology into their location-based AR platform. Wunder Dynamics is applying it to creative reconstruction. Autodesk is integrating it into their professional tooling. The compounding effect of all these efforts is reshaping what's possible.

## Visual Intelligence: Extraction and Manipulation

Beyond spatial understanding, we have visual intelligence—the ability to extract understanding from images and videos, and manipulate them in sophisticated ways.

AI-powered pose estimation allows us to understand human skeletal structure and movement from video. Segmentation—the ability to identify and isolate different objects and regions in an image—has become remarkably accurate. Depth inference lets us estimate the three-dimensional layout of a scene from 2D images. And relighting, which is almost magical, lets us change the lighting conditions of a captured scene.

Consider what this means practically. You have photograph of a room lit by daylight. Relighting technology can convincingly transform that same room as if it were lit by candlelight, or neon, or streetlamp. The sophistication of these tools grows every quarter.

But here's where it gets interesting: these capabilities are starting to mesh together. We're seeing the emergence of what I call visual language models—large multimodal models trained on vast quantities of image and video data, combined with world knowledge, that can understand and reason about visual content at a remarkably high level.

## Hybrid Workflows: Where Disciplines Converge

The real magic emerges when you layer these AI primitives into what I call hybrid workflows. These combine classical techniques with emerging AI capabilities to create outcomes that neither could produce alone.

One concept that's gaining traction is "vibe coding"—using large language models as creative partners to generate visual code and computational designs. I recently vibe-coded something in Claude using just prompts. I'd write, "Write a PRD for this visualization," and then "Sequence this out, tech lead. Cut these features." What emerged was a sophisticated visualization without touching a single line of code myself. I then ran that through Runway to reskin it as Legos, creating a completely different aesthetic—all without opening Blender or Cinema 4D.

Another emerging pattern is Model Context Protocol (MCP), which creates standardized interfaces between language models and specialized tools like Blender, Unreal Engine, and Runway. This creates genuinely novel interaction patterns where you can reason about creative problems at a high level while AI handles the technical execution.

## The Future of Media: Personalization and Generation at Scale

What excites me most is where these capabilities converge for content consumption and creation at scale.

Consider how advertising might evolve. Today, we see static ads optimized for broad audiences. Imagine instead personalized advertising that understands the viewer's local context. I recently saw a proof of concept where a Cadbury advertisement, viewed in a specific geographic location, dynamically showed you the actual local shop where you could purchase Cadbury products by name. That's just the beginning of what personalized, location-aware, dynamically-rendered advertising will become.

But this isn't limited to advertising. We're entering what I call the "content-to-content paradigm"—the ability to take any piece of media and instantly adapt it for different platforms, audiences, preferences, and contexts. You could watch a video on your phone that's been customized for your specific interests, consumption patterns, and viewing history. It's not static. The personalization happens on-demand.

Location-based AR is another frontier. Historically, creating location-based tour guide experiences at scale seemed prohibitively expensive. Now you can combine visual language models with tools like VPS (visual positioning services), integrate Google Maps APIs, add text-to-speech, and generate AR tour guides on the fly for users on their phones or AR glasses. That's extraordinary.

## The Shift Toward Generative Interactive Worlds

Perhaps the most provocative claim comes from Jensen Huang at NVIDIA. When I had the chance to ask him this at GTC last year, he predicted we're five to ten years from fully generative AI games where every pixel is generated rather than rendered. Yes, he has a vested interest in selling GPUs, but I think it's actually a tame prediction when you examine the recent research.

Google's Genie2 is a leading indicator. You provide a photograph, and suddenly you get a minute of a playable, interactive world. The 3D-first AI startups—Luma, Odyssey, Simulon—have all pivoted to video diffusion as their path to interactive worlds. Even the CEO of Odyssey tweeted yesterday that "Jensen is always right," which tells you something about the trajectory.

This shift blurs the lines between software and content. You can literally code experiences faster than you could build them in Blender or Cinema 4D. I vibe-coded a visualization in Claude that I then run through Runway to reskin. I never touched traditional 3D tools. The implications are staggering.

## The Shift Toward Abstraction and Systems Thinking

This future suggests a fundamental change in how creators will work. Rather than getting lost in technical details and endless variations, creators will author content at a higher level of abstraction.

The best analogy I can offer is HTML and the document object model—or a 3D scene graph if you're a 3D practitioner. Creators will work at the semantic level, specifying intent and constraints, while AI handles the technical rendering. This starts with what I've called "last-mile customization"—the drudgery of agencies today tailoring creative and ads across platforms, viewers, and demographics. But eventually, you'll be able to create vast amounts of content on demand.

The paradigm shift is this: we're not just building content anymore. We're building systems and workflows that do the creation for us, with video, 3D, or audio as simply the means of final delivery.

## Navigating the Primitive Explosion

Now, having painted this picture of the future, let me ground this in practical guidance for working in the present.

A new primitive seems to drop every other week. If you're not playing with this stuff, I genuinely think you're missing out. But the key is adopting what I call a dual-mode approach: sandbox mode and architect mode.

In sandbox mode, you're building castles by the beach. You're totally comfortable with the waves washing everything away. The point is to pull out interesting insights and embrace happy accidents. You're not optimizing for delivery; you're optimizing for discovery.

Then you take those insights and apply them in a more structured fashion toward a concrete deliverable—architect mode. You're building something real, something shippable, something that serves your actual creative or business purpose.

The third thing I'd emphasize is building what I call an "island of influence." Many students and early founders tell me that building in public feels cringe. I disagree. As barriers to creation keep collapsing and the noise keeps rising, building your island of influence isn't cringe—it's a necessity. It's how you'll cut through when everyone can create.

Finally, here's a contrarian take: you still need to learn the incumbent tools. Some people resist learning Blender, Premiere, After Effects, Unreal—whatever tools dominate your field. But here's why that matters: these tools are running headless in the cloud and AI is using them, so you need to understand how they work. And more immediately, they're the bailing wire and duct tape you need to ship products today. They're the connective tissue between where we are and where we're going.

## The Moment to Act

I understand this feels exhausting. We do get new primitives every couple of weeks. But even existing primitives haven't been combined in the most obvious ways, let alone the non-obvious insights you'll discover applying them to your own creative and business problems.

This is the moment to act. In this new world, the only limit is your imagination.
