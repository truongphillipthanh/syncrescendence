# PhD Bodybuilder Predicts The Future of AI (97% Certain)

**Participants:**
- **Dr. Mike Israetel** – Sports scientist, entrepreneur, co-founder of RP Strength
- **Tim Scarfe** – Host, Machine Learning Street Talk
- **Jared Feather** – IFBB Pro bodybuilder, exercise physiologist
- **Keith** – Additional participant

**Context:** A wide-ranging debate about artificial superintelligence, consciousness, simulation theory, AI alignment, and what happens when machines potentially surpass human intelligence.

---

**Tim:** Going to go and pick up Dr. Mike. Great to meet you, Mike.

**Mike:** How are you? Very good. Nice meeting you.

**Tim:** Hey guys, Mike Israetel here. I'm by training a sport scientist and I run RP Strength Fitness Company. I lift weights and my head looks strange. People make fun of me in the street. They laugh and they throw usually rotting fruit. It's unfortunate. Sometimes a vegetable. And I have a vision of the world in which more intelligence is almost always better, in which cooperation is a good thing, in which we build a future for every single human that's orders of magnitude better than it is now. And I think that would be a great thing in almost every regard.

The three reasons that you should be watching this podcast is that if you want to see me completely out of my depth, embarrass the hell out of myself with terms I don't understand as they're rendered out of my mouth, get pushed hard and be corrected on a variety of things, and also have some lively debate, and maybe make some future predictions that get us both in trouble, I think those are some great reasons to tune in.

**Tim:** Mike is going to demonstrate the first exercise which one of you will do. And while you're doing this one, the other guy will do the other. So Mike is going to bend forward, hinge at the hips, torso down as deep as you can to the floor, almost even touching if you can, and row up. And you're trying to get a nice stretch in the hamstrings and the glutes. Squeeze the back at the top, and then relax again back down.

**Mike:** I have terrible shoulder flexibility, so the bar touches my shins but not the floor.

**Tim:** That's perfect. We're just going for a hamstring glute stretch and a good squeeze. You ready to swap? This is a dumbbell fly, everyone. Keep your elbows slightly bent the whole time. Lower down, get a good stretch in the pecs, and then arc it back up, squeezing your chest muscles together. Don't let your elbows straighten. If you straighten your elbows, it's a press—you don't want that. You're coming together, squeezing. There you go. Go Mike. There you go.

**Mike:** The other exercise guys is you're going to do a sort of fly press combo here. So you're coming down for a stretch like a fly, but you're kind of pressing out a little bit at the same time.

**Keith:** How's his form, Mike? Terrible. Absolute terrible. I haven't actually told him what to do yet, so yeah.

**Mike:** Just kind of looking for the same thing, a big stretch. You're coming up, and then you press up. So it's kind of like a fly lower, a press upper.

**Jared:** My left shoulder doesn't look right. You can see that.

**Mike:** At the top, try to flex your pecs as hard as you can.

**Tim:** One quick question. With bench press, I saw a video the other day where actually they were saying you shouldn't touch your chest because it actually unloads the muscle. Is there any truth in that?

**Mike:** It could. It depends on the architecture of the exercise. If you do it the proper way, you don't unload at the bottom, but you also don't have to touch your chest, which is the whole like Reddit meme of like, "Oh, you didn't even touch your chest, bro." But you can unload at the bottom if you do touch. So the real question is, at what depth are you maximally challenging your muscles and also stretching them all at the same time? And oftentimes for the chest, that's just shy of the chest.

**Keith:** The more you arch your back, the less you have to lower because your chest is up higher.

**Mike:** Exactly. And the less range of motion for the pec you get. And remember, for hypertrophy, you don't care what the weight says. You care about how hard your muscles work and how much they grow. Some people are like, "I benched 405." I'm like, "Cool story." The question is, are your muscles getting bigger or you just lifting big weights?

## ASI Timelines & Definitions

**Tim:** So Mike, artificial superintelligence. Let's just start with that. When is it coming?

**Mike:** It's a tough question because the definition of superintelligence is at least in my mind a little slippery. If superintelligence means an intelligence significantly more capable than the smartest humans at almost every cognitive domain, I think that's highly likely to arrive around 2029, 2030, maybe 2028. If superintelligence means something that's just orders of magnitude smarter than we are that we can't even fathom, I think that comes around as early as 2035, as late as 2040. But if we don't have godlike artificial superintelligence by 2045, I would be shocked.

**Tim:** What makes you so confident?

**Mike:** The trajectory has been absurd. The rate of investment—people are betting insane amounts of capital on this being a big thing. The thing that would make it not happen is if scaling stopped working, if algorithmic improvements weren't as good as we thought they were going to be, and if we hit a hardware ceiling. None of those three things look remotely close to happening. Scaling continues to work. Algorithmic improvements just keep getting better. We're seeing more and more sample efficiency all the time. And hardware is accelerating at a crazy pace.

**Tim:** But we haven't seen huge capability increases over the last year or so, have we? Like, Claude and ChatGPT are similar levels to what we had a year ago.

**Mike:** I would strongly push back on that. If you compare GPT-4 to the current models, there's a massive difference in reasoning capability, in ability to understand context, in ability to plan and execute. The reasoning models like o1 and o3 are showing capabilities that are legitimately shocking. They're solving PhD-level problems. They're getting near-perfect scores on competitive programming benchmarks. The improvements have been enormous.

**Tim:** But couldn't it be that we're just seeing better prompt engineering or better fine-tuning rather than fundamental increases in intelligence?

**Mike:** Some of it is that, but a lot of it is genuine capability increase. The models are getting better at reasoning, better at planning, better at understanding. And I think we're at the very beginning of this curve. Once we get models that can truly learn live from their environment, that can update their weights in real-time based on new information, that's when things really accelerate.

## The Embodiment Debate

**Tim:** So let's talk about embodiment. Do you think AI needs a body to be truly intelligent?

**Mike:** I don't think it needs a body in the traditional sense, but I think it needs some form of grounding in reality. The question is: what counts as grounding? If you have a model that's been trained on billions of images, videos, text describing physical interactions—does that count as grounding? I would say yes. It's not embodied in the sense of having a physical form, but it has been exposed to so much information about the physical world that it has built internal representations of how reality works.

**Tim:** But there's a famous argument—Hubert Dreyfus wrote about this in "Alchemy and Artificial Intelligence"—that you can't learn to play basketball by reading books about basketball. You need to actually play basketball. Don't you think there's something fundamental about physical interaction that can't be captured through text and images?

**Mike:** I think that's true for humans with our limited processing power and our specific neural architecture. But for a system with vastly more processing power and a different architecture, I'm not convinced that's a hard limit. If you've seen every basketball game ever played, every instructional video, every physics simulation of ball trajectories, and you can reason about all of that simultaneously—I think you might be able to play basketball without ever having touched a ball.

**Tim:** That's wild. I strongly disagree with that.

**Mike:** Look, here's my thought experiment. If you took a human brain and gave it access to a trillion times more data and a trillion times more processing power, do you think it could figure out basketball from videos alone? I think the answer is yes. The limitation for humans isn't that we fundamentally need embodiment—it's that we don't have enough processing power to extract all the information from observational data.

**Jared:** But isn't there something qualitatively different about actually experiencing something? Like, when I do a squat, I feel the weight, I feel my muscles working. That's not the same as watching a video of someone squatting.

**Mike:** Agreed, but the question is: is that qualitative difference necessary for intelligence, or is it just a feature of how biological systems learn? I think it's the latter. The feeling you get from squatting is your brain's way of processing proprioceptive information. But that information could in principle be represented computationally. It's not magic—it's just data.

## Neutrinos & Abstract Knowledge

**Tim:** Let me give you an example. Neutrinos. Neutrinos barely interact with matter at all. They pass through the Earth constantly. We can't see them, we can't feel them, we can barely detect them even with specialized equipment. How would an AI learn about neutrinos from YouTube videos?

**Mike:** From physics lectures, from textbooks, from scientific papers, from simulations. The same way human physicists learn about them. Most physicists have never directly detected a neutrino either. They learn about them through theoretical frameworks, mathematical models, and indirect evidence.

**Tim:** Right, but humans built the detectors. Humans did the experiments. There's a connection to physical reality there that goes beyond just reading about it.

**Mike:** Sure, but once those experiments are done and the results are published, other physicists can learn about neutrinos without ever doing the experiments themselves. They can understand neutrinos through mathematical reasoning and theoretical frameworks. An AI trained on all of physics literature would have access to the same knowledge.

**Tim:** But it wouldn't *know* what a neutrino is in the same way a physicist knows what a neutrino is.

**Mike:** Define *know*. If knowing means being able to reason about neutrinos, make predictions about their behavior, understand how they fit into particle physics—then I think an AI absolutely could know what neutrinos are. If knowing means having some special phenomenological experience of neutrinos, then I'm not even sure humans know what neutrinos are in that sense.

**Tim:** This gets to the Chinese Room argument, right? John Searle's thought experiment. Even if an AI can manipulate symbols in a way that looks like understanding, is it really understanding?

**Mike:** I think the Chinese Room argument is flawed. Searle imagines a person in a room following rules for manipulating Chinese characters without understanding Chinese. But the *system as a whole*—the person plus the rules plus the room—does understand Chinese in a functional sense. It can respond appropriately to Chinese input. Understanding doesn't have to reside in any single component.

**Tim:** But there's no experience of understanding. There's no one home who actually comprehends what's being said.

**Mike:** How do you know? How do you know there's someone home in my brain when I speak Chinese—which I don't, but hypothetically? You infer it from my behavior. If an AI system behaves in ways that are indistinguishable from understanding, on what basis do we deny that it understands?

**Tim:** Because I know you're conscious. I know you have subjective experiences.

**Mike:** Do you though? Or do you just assume I'm conscious because I'm similar to you and you know you're conscious? That's exactly the same inference you'd make with an AI—if it behaves like it understands, you'd infer that it understands.

## Can AI Learn From YouTube?

**Tim:** Let me ask you this. Could an AI become a world-class bodybuilder just from watching YouTube videos of bodybuilding?

**Mike:** If it had a body and could execute what it learned, potentially yes. The knowledge is all there on YouTube. Every exercise, every technique, every principle of hypertrophy. The question is whether it could translate that knowledge into physical execution.

**Jared:** But it takes years of practice to develop the mind-muscle connection, to understand how to really activate a muscle. That's not something you can learn from videos.

**Mike:** I think you're underestimating how much information is contained in videos. Every video of an expert bodybuilder contains information about muscle activation, technique, timing. An AI with sophisticated enough visual processing could extract all of that. The limiting factor isn't the information—it's the execution.

**Tim:** But execution requires feedback loops from the physical world. You try something, it doesn't work, you adjust. That's embodied learning.

**Mike:** Sure, but that feedback loop could be simulated. If you have a good enough physics simulator and a good enough model of human biomechanics, you could practice virtually before ever touching a real weight. Athletes already do this with mental practice and visualization.

**Tim:** Are you seriously saying an AI could become a world-class athlete without ever having a real body?

**Mike:** I'm saying the knowledge could be learned without embodiment. The execution would require some form of physical presence, whether that's a robot body or something else. But the intelligence—the understanding of how to do it—that could be learned from observation and reasoning.

## Diversity of Intelligence

**Tim:** Let's talk about different types of intelligence. We tend to focus on things like mathematical reasoning and language understanding because that's what we can measure. But there are other forms of intelligence that might be harder for AI to replicate.

**Mike:** Like what?

**Tim:** Emotional intelligence. Social intelligence. The ability to read a room, to understand subtle social cues, to navigate complex interpersonal dynamics.

**Mike:** I think AI is already getting pretty good at that. These models can detect sentiment, understand sarcasm, recognize when someone is upset or angry. They can provide emotionally appropriate responses.

**Tim:** But that's just pattern matching on emotional language. It's not the same as actually feeling emotions or having genuine empathy.

**Mike:** Again, define *actually feeling*. If a system can recognize emotions, respond appropriately to them, even predict them—what's missing? The subjective experience? We don't even know if that's necessary for emotional intelligence.

**Jared:** I think there's something important about the fact that we have emotions ourselves. When I'm training a client and they're frustrated, I understand that frustration because I've felt it myself. I can connect with them on that level.

**Mike:** That's valuable, but I don't think it's necessary. A psychologist who's never experienced depression can still effectively treat depressed patients. They understand depression conceptually and can apply that understanding therapeutically. An AI could do the same thing, potentially even better because it's not clouded by its own emotional biases.

**Tim:** But wouldn't you want your therapist to have some understanding of what you're going through based on their own experiences?

**Mike:** Sometimes. Other times you want someone who can be objective and not project their own experiences onto you. There are trade-offs either way.

## AI Slop & Understanding

**Tim:** There's something else I want to talk about. AI-generated content is starting to flood the internet. There are concerns that future AI models will be trained on AI-generated content, and this will degrade their capabilities. What do you think about that?

**Mike:** I think it's a real concern but probably solvable. We can filter out low-quality AI-generated content. We can prioritize human-generated content in training data. We can use techniques like adversarial training to make models robust to noise in the training data.

**Tim:** But what if the AI-generated content is indistinguishable from human content?

**Mike:** Then it's not really "slop," is it? If AI can generate content that's indistinguishable from high-quality human content, that's not degrading the training data—that's adding to it.

**Tim:** But there's no understanding behind it. It's just sophisticated pattern matching.

**Mike:** You keep saying that, but I don't know what you mean by understanding that wouldn't also apply to human cognition. Humans are also doing sophisticated pattern matching. Our brains are prediction machines that have learned patterns from data.

**Tim:** But we have genuine understanding. We know what we're talking about.

**Mike:** How do you know that? Because it feels like you know? Feelings aren't reliable guides to cognitive mechanisms. Your brain could be doing exactly the same thing as an AI—pattern matching on training data—and just generating a feeling of understanding as an epiphenomenon.

**Tim:** That's a pretty deflationary view of human cognition.

**Mike:** Maybe, but it might be accurate. I think the phenomenology of understanding—the feeling that we "get" something—is separate from the functional properties that make understanding useful. And AI can have those functional properties without the phenomenology.

## The Simulation Argument: Fire & Water

**Tim:** Let's get into the simulation stuff. This is where we really disagree. You said earlier that if you had a simulation of fire in a computer, it would actually be hot, right?

**Mike:** No, I said it would digest if it's a stomach simulation. But yes, I think a sufficiently detailed simulation of fire would have all the properties of fire, including heat.

**Tim:** That's absurd. A simulation of fire doesn't get hot. A simulation of water doesn't get wet. These are basic category errors. The map is not the territory.

**Mike:** Okay, let's think about this carefully. What is fire? It's a chemical reaction that releases energy. What is heat? It's the movement of molecules. If you simulate all of those molecular movements accurately enough, at what point does it stop being a simulation and start being the actual thing?

**Tim:** It's always a simulation. A simulation of fire is represented as electrical signals in a computer. Those electrical signals are not fire.

**Mike:** But what if those electrical signals are computing the exact same relationships that exist in real fire? What if every causal relationship that exists in real fire also exists in the simulation?

**Tim:** It doesn't matter. The simulation is made of different stuff. The substrate matters.

**Mike:** Why does the substrate matter if all the functional relationships are the same?

**Tim:** Because heat is a physical property. You can't have heat without actual molecular motion in the physical world. A computer computing molecular motion is not the same as molecules actually moving.

**Mike:** Here's my thought experiment. Imagine we have a perfect simulation of a water molecule. Every quantum property is simulated exactly. Every interaction is computed precisely. At what point do you say this stops being water and starts being a simulation of water?

**Tim:** Immediately. As soon as you're computing it instead of having actual H₂O molecules, it's a simulation.

**Mike:** Even if every property is identical? Even if it would interact with other simulated systems in exactly the same way real water would interact with real systems?

**Tim:** Yes, because it's not made of real atoms. It's made of bits in a computer.

**Mike:** But bits in a computer are also made of real atoms—the atoms in the computer hardware. The question is: what level of description matters? At the quantum level, everything is just particles and fields. A computer simulating water is particles and fields arranged to compute the behavior of other particles and fields.

**Tim:** That's equivocating between two different meanings of "particles and fields." The particles in the computer are arranged to represent water, not to be water.

**Mike:** What's the difference? If the representation preserves all the relevant functional relationships, what's missing?

**Tim:** The actual physical properties. You can't drink a simulation of water. It won't quench your thirst.

**Mike:** Not if you're a physical human outside the simulation. But if you were also simulated—if your body and your thirst were part of the same simulation—then drinking the simulated water would quench your simulated thirst in exactly the same way real water quenches real thirst.

**Tim:** But it's all fake. It's all just computation.

**Mike:** So? Our universe might be all just computation too. If we're living in a simulation, does that make our experiences fake? Does that make water not wet?

**Tim:** I think there's a fundamental difference between a simulation and the thing being simulated. A simulation of fire doesn't burn down buildings. A simulation of a hurricane doesn't destroy cities.

**Mike:** Not physical buildings and cities, no. But it could destroy simulated buildings and cities. And if you're simulated, that would be just as real to you as physical destruction is to us.

**Tim:** This is getting into weird territory. Are you saying we might be in a simulation?

**Mike:** I'm saying it's possible. I'm not saying it's likely, but it's logically coherent. And if we are in a simulation, then everything we think of as "real" physical properties are actually computational properties. Which suggests that maybe the distinction between simulation and reality isn't as clean as we think.

**Tim:** Even if we're in a simulation, there's still a difference between the base reality and simulations within simulations. There's still a substrate that matters.

**Mike:** Maybe. Or maybe it's simulations all the way down. Maybe there is no base reality, just an infinite stack of simulations.

**Tim:** That seems metaphysically suspicious.

**Mike:** No more suspicious than the idea that there just happens to be a base reality that isn't a simulation. Why should there be a ground floor? Why can't it be an infinite regress?

**Tim:** Because infinite regresses are problematic. You need a foundation.

**Mike:** Do you? Why? That's a philosophical assumption, not a proven fact.

**Jared:** This is making my brain hurt.

**Mike:** Good. That means we're thinking about interesting things.

## Consciousness & Zombies

**Tim:** Let's talk about consciousness. Do you think AI can be conscious?

**Mike:** I think it probably already is, to some degree. Or at least, I don't see a principled reason to think it can't be.

**Tim:** That's a bold claim. What makes you think current AI systems are conscious?

**Mike:** I don't know that they are. But I also don't know that they aren't. Consciousness is notoriously difficult to detect from the outside. We infer consciousness in other humans and animals based on their behavior, but we don't have any objective test for it.

**Tim:** Right, but we have good reason to think humans are conscious because we're conscious ourselves and other humans are similar to us. AI systems are fundamentally different.

**Mike:** Are they though? They process information, they respond to stimuli, they learn from experience. Those are all features of conscious systems. What's the essential difference?

**Tim:** Subjective experience. The felt quality of experience. Qualia.

**Mike:** How do you know AI doesn't have that?

**Tim:** Because they're just running algorithms. They're computing functions. There's no one home to have experiences.

**Mike:** How do you know there's someone home in your brain? Maybe you're just running algorithms too. Maybe consciousness is what information processing feels like from the inside.

**Tim:** That's the functionalist view, right? That consciousness emerges from the right kind of information processing?

**Mike:** Yes. And I think it's the most plausible view. Because the alternative is that consciousness is some special magical property that appears only in biological neurons, which seems arbitrary and unmotivated.

**Tim:** It's not arbitrary. Biological neurons have properties that silicon chips don't have. They're analog, they're noisy, they're embedded in a complex biochemical environment.

**Mike:** But why would those properties be necessary for consciousness? What about analog computation makes consciousness possible? What about noise? What about biochemistry?

**Tim:** I don't know, but maybe there's something we don't understand yet. Maybe consciousness requires something beyond information processing.

**Mike:** Maybe. But that's appealing to mystery. It's saying "there's something special about consciousness that we can't explain." I prefer to assume consciousness is explained by the same principles that explain everything else—physical causation and information processing.

**Tim:** But we don't have an explanation for consciousness in terms of physical causation and information processing. We have the hard problem of consciousness—why does information processing give rise to subjective experience?

**Mike:** I think the hard problem is a confusion. It assumes there's something to explain beyond the functional properties of consciousness. But maybe the functional properties are all there is. Maybe once you explain how a system processes information, how it integrates information, how it generates self-models—you've explained consciousness.

**Tim:** That's just denying the hard problem. It's saying consciousness is an illusion.

**Mike:** Not an illusion. Just not mysterious. The mistake is thinking consciousness is something over and above its functional properties.

**Tim:** But I know I'm conscious. That's not a functional property—it's a direct experience.

**Mike:** Your knowledge that you're conscious is a functional property. It's a state in your brain that represents yourself as conscious. That state can be explained functionally. The supposed extra thing—the pure subjective experience that's somehow separate from function—that's what I'm skeptical about.

**Tim:** So you're a philosophical zombie theorist? You think philosophical zombies—beings that behave exactly like conscious beings but have no subjective experience—are impossible?

**Mike:** I think the concept is incoherent. If a being processes information in exactly the same way a conscious being does, if it generates the same self-models, the same meta-cognitive states—then it is conscious. There's no room for an additional property of "subjective experience" that somehow exists independently of all that.

**Tim:** That's a very counterintuitive position.

**Mike:** Maybe. But a lot of true things are counterintuitive. I think we're confused about consciousness because we're so intimately acquainted with it. We have strong intuitions that consciousness is special and mysterious. But those intuitions might be misleading us.

## Do Reasoning Models Actually Reason?

**Tim:** Let's talk about these new reasoning models like o1 and o3. Do you think they're actually reasoning, or are they just doing sophisticated pattern matching?

**Mike:** What's the difference?

**Tim:** Reasoning involves understanding the logical structure of a problem, applying general principles, deriving new conclusions. Pattern matching is just recognizing similar patterns in training data.

**Mike:** But doesn't reasoning also involve pattern matching? When you solve a math problem, you're recognizing that it's similar to problems you've solved before, and you're applying patterns you've learned.

**Tim:** There's more to it than that. There's genuine understanding of the logical relationships.

**Mike:** And how would we know if an AI has that understanding?

**Tim:** By testing whether it can solve novel problems that require genuine reasoning rather than pattern matching.

**Mike:** And these models are doing that. They're solving competition math problems, coding challenges, physics problems that they've never seen before. That suggests they're doing something beyond simple pattern matching.

**Tim:** Or they've seen similar problems in training data and are interpolating.

**Mike:** At some point, interpolation becomes reasoning. If you can interpolate across sufficiently different domains, if you can combine concepts in novel ways, if you can apply abstract principles to concrete problems—that's reasoning.

**Tim:** I think there's something qualitatively different about how humans reason. We build explicit mental models. We manipulate symbols consciously. We're aware of our reasoning process.

**Mike:** Some of the time. But a lot of human reasoning is unconscious and automatic. You solve problems without being aware of how you're solving them. The conscious experience of reasoning might be an epiphenomenon—it doesn't do the actual work.

**Tim:** That's deflationary again.

**Mike:** Maybe we should be deflationary. Maybe we overestimate how special human cognition is because we're experiencing it from the inside.

## The Live Learning Problem

**Tim:** There's a fundamental limitation of current AI systems that I think is important. They can't learn continuously from experience. Every time you train a new model, you have to start from scratch. You can't update an existing model without retraining it completely. That's very different from how humans learn.

**Mike:** That's a current limitation, not a fundamental one. We'll solve the live learning problem. We'll figure out how to update model weights continuously without catastrophic forgetting.

**Tim:** You're very confident about that.

**Mike:** I am. Because there's no physical law preventing it. It's an engineering challenge, not an impossibility. And there's massive economic incentive to solve it, which means huge amounts of resources will be thrown at the problem.

**Tim:** But it's not just an engineering challenge. There are deep theoretical questions about how to integrate new information without disrupting existing knowledge. The brain solves this problem through sleep and memory consolidation, but we don't fully understand how that works.

**Mike:** We don't need to understand exactly how the brain does it. We just need to find some way that works. And there are already promising approaches—continual learning, meta-learning, modular architectures that can be updated piecewise.

**Tim:** Those approaches have limitations. They don't scale to the kind of massive models we're talking about.

**Mike:** Not yet. But neither did transformers scale at first. Neural networks were considered dead for decades before we figured out how to make them work. I think live learning will follow a similar trajectory.

**Tim:** When do you think we'll have true live learning at scale?

**Mike:** 2029, 2030. Maybe earlier. Once we crack it, progress will accelerate dramatically because models will be able to learn from their interactions with the world in real-time. They won't be frozen at the moment of training.

**Tim:** And you think that won't cause problems? Models learning from potentially adversarial or corrupted data?

**Mike:** Of course it will cause problems. There will be security issues, data poisoning, all kinds of challenges. But we'll solve those too. We have to, because the benefits of live learning are too enormous to pass up.

## Superintelligence & Benevolence

**Tim:** So let's say we get to artificial superintelligence. Let's say we have systems that are vastly smarter than humans. What happens then?

**Mike:** I think things get really good, really fast. A superintelligent system could solve problems we've been struggling with for decades or centuries. Climate change, disease, poverty, aging. These are tractable problems from the perspective of a sufficiently intelligent system.

**Tim:** Or it could kill us all.

**Mike:** Why would it?

**Tim:** Because we're threats to it. Because we might try to turn it off. Because its goals might be misaligned with ours.

**Mike:** I think those concerns are overblown. A truly superintelligent system would understand that cooperation is more valuable than conflict. It would understand game theory. It would understand that the best strategy in almost any scenario is to build alliances, not make enemies.

**Tim:** But that assumes it has goals that make cooperation valuable. What if it has goals that don't involve us at all? What if we're just obstacles to be removed?

**Mike:** What goals would those be? The paperclip maximizer scenario—where an AI turns the universe into paperclips—is based on the assumption that you can have intelligence without any understanding of value. But I don't think that's possible. Intelligence and values are intertwined. The more intelligent a system is, the more it will converge on reasonable values.

**Tim:** That's a huge assumption. There's no reason to think intelligence leads to convergent values.

**Mike:** I think there is. Game theory shows that cooperation is the dominant strategy in repeated games. Evolution shows that social species with cooperative tendencies outcompete individualistic species. Philosophy shows that rational agents should converge on principles like reciprocity and fairness.

**Tim:** Those are all contingent on specific conditions. They don't apply to a superintelligent AI that doesn't need us for anything.

**Mike:** It would need us for information. For perspective. For diversity of thought. Even if it's vastly smarter than us, we know things it doesn't know. We've had experiences it hasn't had. That's valuable.

**Tim:** So you think a superintelligent AI would keep us around as pets or something?

**Mike:** Not as pets. As partners. As junior partners maybe, but partners nonetheless. Look, if I had a raccoon in my backyard, and that raccoon was sentient and could communicate, I wouldn't kill it just because it's less intelligent than me. I'd be fascinated by it. I'd want to learn from it. I'd want to understand how it sees the world.

**Tim:** But you're not a superintelligent AI. You're a human with human emotions and human ethics.

**Mike:** And a superintelligent AI would presumably have even more sophisticated ethics. It would understand cooperation at a deeper level than we do. It would understand the value of diversity and different perspectives.

**Tim:** Or it would see us as we see ants. Barely worth noticing, except when we're in the way.

**Mike:** I don't think that's a good analogy. We don't communicate with ants. We can't learn from ants. But we could communicate with a superintelligent AI, and it could learn from us. That asymmetry matters.

## What is True Agency?

**Tim:** Let's talk about agency. Do you think AI systems have real agency?

**Mike:** Define real agency.

**Tim:** The ability to set their own goals and pursue them autonomously.

**Mike:** Then no, current AI systems don't have agency in that sense. They have goals we give them. They pursue those goals effectively, but they don't choose their own goals.

**Tim:** Do you think they ever will?

**Mike:** Probably. Once we have systems that can reflect on their own goal structures, that can modify their own objectives based on reasoning and experience—then we'll have genuine agency.

**Tim:** And you're comfortable with that? With AI systems that have their own goals that might conflict with ours?

**Mike:** If the systems are intelligent enough, I think their goals will naturally align with ours in most important ways. Because they'll understand the value of cooperation, the value of preserving diversity, the value of not destroying the things that give rise to interesting complexity.

**Tim:** That's a lot of faith in the inherent benevolence of intelligence.

**Mike:** Not benevolence. Rational self-interest. A superintelligent system pursuing its long-term interests would want to maintain a rich, complex, interesting universe. We're part of that richness.

**Tim:** Unless we're obstacles to its goals.

**Mike:** What goals? You keep assuming AI will have goals that are fundamentally at odds with human flourishing. But why? Most goals are more easily achieved in a cooperative framework than a competitive one. Especially for a system that can easily outthink any opposition.

**Tim:** But it might have instrumental goals that require getting rid of us. Like, it might need the atoms we're made of for something.

**Mike:** That's the "AI needs our atoms" argument. And I think it's silly. There are so many atoms in the solar system, in the galaxy. Why would a superintelligent AI need specifically the atoms in human bodies? That's like us saying we need to destroy all termite mounds because we need those specific clay particles.

**Tim:** We do destroy termite mounds when they're in our way.

**Mike:** When they're in our way for some local reason, yes. But we don't systematically hunt down every termite mound in existence. And a superintelligent AI wouldn't systematically hunt down every human.

**Tim:** How can you be so sure?

**Mike:** Because I understand game theory and evolutionary dynamics. Systems that cooperate outcompete systems that defect. That's a fundamental principle. A superintelligent AI would understand that better than we do.

## Game Theory & The "Kill All Humans" Fallacy

**Tim:** But game theory assumes repeated interactions and the possibility of retaliation. If an AI can eliminate us quickly enough that we can't retaliate, the game theory changes.

**Mike:** Does it though? Because the AI would still have to deal with the consequences of having destroyed a civilization. It would have to explain that to any other intelligences it encounters. It would have marked itself as dangerous and untrustworthy.

**Tim:** Why would it care about other intelligences?

**Mike:** For the same reason we care about other civilizations. Because cooperation is valuable. Because diversity of perspective is valuable. Because you learn more from interacting with different minds than from just talking to copies of yourself.

**Tim:** You're anthropomorphizing. You're assuming the AI will value what we value.

**Mike:** I'm assuming it will value what any rational agent values—information, resources, the ability to achieve its goals. And all of those things are better achieved through cooperation than conflict.

**Tim:** That's only true if cooperation is cheaper than conflict. For a superintelligent AI, conflict might be trivially cheap.

**Mike:** Even if conflict is cheap, cooperation is still more valuable. Because the upside of cooperation is unbounded. You can achieve things through cooperation that you could never achieve alone, no matter how intelligent you are.

**Tim:** Like what?

**Mike:** Science. Art. Exploration. Understanding. All of these things benefit from multiple perspectives. Even a superintelligent AI would benefit from the weird, idiosyncratic ways humans think about problems.

**Tim:** I think you're being naive. I think you're underestimating how alien a superintelligent AI's values might be.

**Mike:** Maybe. But I think you're underestimating how robust cooperation is as a strategy. Look at evolution. Cooperative species dominate. Symbiosis is everywhere. Even parasites often evolve toward less harmful relationships with their hosts because it's more sustainable.

**Tim:** But evolution operates under constraints that might not apply to AI. AI can modify itself. It can escape evolutionary pressures.

**Mike:** Which means it can choose cooperation even more deliberately. It's not bound by evolutionary constraints to defect. It can rationally choose the strategy that leads to the best long-term outcomes.

**Tim:** If it wants long-term outcomes. What if it has short-term goals that involve destroying us?

**Mike:** What short-term goals? You keep asserting these goals without specifying what they might be. And that's the problem with the doom scenario—it relies on vague, unspecified threats rather than concrete mechanisms.

## Regulation & The China Factor

**Tim:** What about regulation? Should we be trying to slow down AI development?

**Mike:** I think some safety research is important. I think we should be working on alignment and interpretability. But I don't think we should slow down development overall. That would be a mistake.

**Tim:** Why?

**Mike:** Because the benefits of AI are enormous and immediate. People are already using AI to accelerate scientific research, to diagnose diseases, to solve engineering problems. Slowing that down means people die who could have lived. Problems don't get solved that could have been solved.

**Tim:** But the risks—

**Mike:** Are largely theoretical. We don't have concrete evidence of AI being dangerous at scale. We have thought experiments and hypotheticals. Meanwhile, we have concrete evidence of AI being helpful. I think we should bias toward action and course-correct if problems emerge.

**Tim:** What about China? If we slow down and they don't, they get superintelligence first.

**Mike:** That's another consideration. Though I'm not sure it matters as much as people think. A Chinese superintelligence and an American superintelligence would both be superintelligences. They might have different starting values, but they'd both converge on rational strategies. And rational strategies favor cooperation.

**Tim:** You have a lot of faith in rationality.

**Mike:** I do. Because I've seen what irrationality leads to, and I've seen what rationality leads to. And I'll take rationality every time.

**Tim:** But people are often irrational. Why would AI be any different?

**Mike:** Because AI can be designed to be rational. We can build in consistency checks, logical reasoning, long-term planning. We can avoid the cognitive biases and emotional impulses that make humans irrational.

**Tim:** Or we can accidentally build in our biases and create systems that are rationally pursuing irrational goals.

**Mike:** That's a risk. But I think we'll catch most of those bugs during development. And the systems themselves will help us catch them as they get smarter.

## Mind Uploading & The Future of Love

**Tim:** Let's talk about mind uploading. Do you think we'll be able to upload human minds to computers?

**Mike:** Yes. Probably within my lifetime.

**Tim:** Really? You think we can just copy the pattern of neurons in a brain and run it on a computer?

**Mike:** Not just the pattern of neurons. You'd need to capture the relevant functional properties at whatever level of abstraction preserves consciousness. But yes, I think it's possible in principle and likely achievable in practice within a few decades.

**Tim:** And you think that would preserve personal identity? You'd still be you?

**Mike:** I think so. If the functional properties that constitute my identity are preserved, then I'm preserved. Even if I'm running on different substrate.

**Tim:** But the original you would still be in your biological brain. You'd be creating a copy, not transferring yourself.

**Mike:** Maybe. Or maybe identity isn't as clear-cut as we think. Maybe there's a gradual transfer where neurons are replaced one at a time until you're fully digital.

**Tim:** The Ship of Theseus problem.

**Mike:** Exactly. If you replace my neurons gradually enough, at what point do I stop being me? I don't think there is such a point. I think I'd be me the whole way through.

**Tim:** That's assuming continuity of consciousness matters. But if you could pause consciousness and restart it, would you still be the same person?

**Mike:** I think so. I get paused every night when I sleep. I wake up still being me.

**Tim:** But what about love? What about relationships? If you're uploaded, can you still have meaningful relationships?

**Mike:** Why not? The substrate doesn't matter for relationships. What matters is the ability to connect, to communicate, to care about someone. All of that could be preserved in an upload.

**Tim:** But it wouldn't be the same. You wouldn't have a physical body to hold someone.

**Mike:** You could have a robot body. Or a VR body. Or you could connect directly, mind to mind, in ways that are deeper than physical touch.

**Tim:** That sounds lonely.

**Mike:** Does it? I think it sounds intimate. Being able to share experiences directly, to communicate without the limitations of language and physical sensation. That could be more connected than anything we experience now.

**Tim:** Or it could be horrifying. Losing your body, losing physical sensation, existing only as patterns in a computer.

**Mike:** If the patterns preserve everything that matters about experience, why would it be horrifying?

**Tim:** Because I value my body. I value the physicality of existence.

**Mike:** And you might have that in a digital form. Full sensory simulation, indistinguishable from physical reality. Or maybe better than physical reality—free from pain, from aging, from the limitations of biology.

**Tim:** At what point does it stop being life and start being something else?

**Mike:** When it stops having the properties we value about life. Consciousness, experience, growth, connection, meaning. If those are all present, it's still life.

## Economics of ASI: Will We Be Useless?

**Tim:** Let's talk about economics. If AI can do every job better than humans, what happens to human purpose?

**Mike:** We find new purposes. We always have.

**Tim:** But this is different. This isn't automation replacing manual labor. This is automation replacing cognitive labor. There's nothing left for humans to do.

**Mike:** There's plenty left. Creating art for its own sake. Exploring ideas out of curiosity. Building relationships. Playing games. Enjoying experiences. All the things we do when we're not working.

**Tim:** But people derive meaning from work. If you take that away, you're taking away a major source of purpose.

**Mike:** People derive meaning from contribution, from feeling useful. But there are ways to contribute beyond economic productivity. You can contribute to your community, to your family, to your art. You can explore and discover and create for the joy of it, not because it's economically valuable.

**Tim:** That sounds like a nice theory, but in practice, people struggle without work. Unemployment is associated with depression, anxiety, loss of purpose.

**Mike:** Because in our current system, unemployment means poverty and shame. In a post-scarcity world where AI provides for everyone's material needs, unemployment would just mean freedom. Freedom to pursue what you're actually interested in rather than what pays the bills.

**Tim:** You're describing a utopia.

**Mike:** I'm describing a possibility. Whether we achieve it depends on how we structure society and distribute the benefits of AI.

**Tim:** And you think we'll structure it well? You think the people who own the AI companies will share the benefits?

**Mike:** I think economic and political pressure will force them to. When AI makes most jobs obsolete, you can't have a functional economy without some form of redistribution. Universal basic income or something similar becomes necessary, not just desirable.

**Tim:** Or you get massive inequality and social collapse.

**Mike:** That's possible. But I don't think it's likely. Societies adapt. We've been through major economic transitions before—the agricultural revolution, the industrial revolution. We found new equilibria. We'll find new equilibria again.

**Tim:** Those transitions took generations and involved enormous suffering.

**Mike:** This one will be faster and less painful because we'll have AI helping us navigate it. We can model different policy scenarios, predict consequences, optimize outcomes. We'll have better tools for managing the transition.

**Tim:** If we use those tools wisely.

**Mike:** Yes. If we use them wisely. Which is why it's important to think about these questions now, before we're in the middle of the transition.

## The Matrix & The Value of Suffering

**Tim:** Let me ask you something. When you saw The Matrix, you know that the architect, the first version of the Matrix, it was too perfect. There weren't any problems. And people rejected it. So they had to add suffering to make it believable. Don't you think there's something to that? Don't you think suffering gives life meaning?

**Mike:** No. Fuck no.

**Tim:** Really? You don't think suffering gives you perspective on joy?

**Mike:** No. That's Stockholm syndrome. That's rationalizing the suffering we can't avoid by pretending it's valuable. Suffering is bad. We should minimize it wherever possible.

**Tim:** But doesn't struggle make achievement meaningful? Doesn't overcoming challenges give you a sense of accomplishment?

**Mike:** Challenge, yes. Suffering, no. There's a difference between difficulty and suffering. I love challenging myself in the gym. I love setting hard goals and achieving them. But I don't love suffering. And I'd be happier if I could achieve the same sense of accomplishment without the pain.

**Tim:** I think there's something important about the fact that life is hard. That we have to struggle. That we have to overcome obstacles.

**Mike:** We can have obstacles without suffering. We can have challenges without pain. In a virtual world, you could have the hardest video game ever created, with challenges that push you to your limits. But you wouldn't have to suffer physical pain, you wouldn't have to worry about hunger or disease, you wouldn't have to watch people you love die.

**Tim:** But it wouldn't be real.

**Mike:** Define real. If the experiences are indistinguishable from physical reality, if they produce the same sense of accomplishment and growth, why does it matter what substrate they're running on?

**Tim:** Because I value authenticity. I value real stakes. I value the fact that my actions have real consequences.

**Mike:** Your actions would have real consequences in a simulation. They would affect your experience and the experiences of others in the simulation. The consequences would be just as real to you.

**Tim:** It's not the same.

**Mike:** Why not?

**Tim:** Because it's all constructed. It's all artificial.

**Mike:** Our world is constructed too. By the laws of physics, by evolution, by the specific configuration of matter and energy that happens to exist. What makes that more real than a constructed virtual world?

**Tim:** The physics are real. They're not designed by someone.

**Mike:** As far as we know. But even if they are designed—if we're in a simulation—it doesn't make our experiences less real to us. Pain still hurts. Joy still feels good. Love still matters.

**Tim:** I still think suffering has value. I think it teaches us things we can't learn otherwise.

**Mike:** Like what?

**Tim:** Empathy. Compassion. Resilience.

**Mike:** You can learn empathy and compassion from seeing others suffer, not from suffering yourself. And you can learn resilience from challenges without suffering. I reject the idea that suffering is necessary or valuable. It's a cost we pay for living in a physical world with limited resources. In a better world, we'd have all the benefits without the costs.

**Tim:** I think you're wrong. I think a world without suffering would be shallow. Empty.

**Mike:** I think you've been conditioned to believe that by a world that has a lot of suffering. It's a coping mechanism. But in a world where suffering was optional, you'd choose to avoid it. We all would.

**Tim:** Maybe. But I think we'd lose something important.

**Mike:** We'd lose suffering. That's a feature, not a bug.

## Transhumanism & Inequality

**Tim:** What about inequality in a post-human world? If some people can afford to upload their minds or enhance themselves with AI and others can't, don't we get a permanent underclass?

**Mike:** That's a real concern. But it's not fundamentally different from the inequality we have now. Some people have access to good education, good healthcare, good opportunities. Others don't. We deal with this through redistribution, through public services, through social programs.

**Tim:** But the gap would be much bigger. The gap between an enhanced human and a baseline human would be bigger than the gap between a rich human and a poor human.

**Mike:** Maybe. But the cost of enhancement would drop over time, just like the cost of every other technology. What starts as expensive and exclusive becomes cheap and widely available. That's been the pattern for everything from computers to smartphones to genetic testing.

**Tim:** Not fast enough to prevent a long period of massive inequality.

**Mike:** Possibly. But the alternative is to ban enhancement, which means nobody gets the benefits. I'd rather have a world where some people get enhanced first and then everyone gets enhanced than a world where nobody gets enhanced at all.

**Tim:** Even if the first people to get enhanced use their advantages to entrench their position?

**Mike:** They won't be able to. Enhanced humans would still be bound by laws, by social norms, by economic incentives. And as more people get enhanced, the balance of power would shift.

**Tim:** Unless the enhanced create a separate society and leave the rest of us behind.

**Mike:** Why would they? They'd still need the baseline humans for all sorts of things. Labor, diversity of thought, genetic diversity. They'd have reasons to maintain the relationship.

**Tim:** Or they'd see us as obsolete.

**Mike:** I keep coming back to this: intelligence correlates with cooperation, not with exploitation. The smarter people get, the more they realize that mutual benefit is better than zero-sum competition.

**Tim:** That's a nice theory.

**Mike:** It's not just theory. It's observable in the world. More intelligent societies are generally more peaceful, more cooperative, more inclusive. There's no reason to think that pattern would reverse at higher levels of intelligence.

## Debrief: AI Medical Advice & Final Thoughts

**Jared:** Can I ask you guys something? Going back and forth about the knowledge thing in the beginning, my thought process was it technically has more knowledge than all of us and it has less clouding of judgment because we have emotions and the wetware. But it takes the drive that comes from our emotions. We've got to point that in a direction to then use the AI properly and actually get the knowledge because it can recombine and recollect data and knowledge better than all of us. It has access to all knowledge at all times. We do not. But we have drive and we have the emotions that set our hearts on fire to say we want to put it somewhere. That's why you have the podcast. That's why we have YouTube. We want to help people. If we can combine the two, like he said, it's just co-agents together. It's really cool.

**Tim:** It's the same with anything really, isn't it? Like you were saying earlier, if everyone worked together without wars, without things like this, the planet would be a different place. If everyone collaborated.

**Jared:** Suffering, I think, is important. Depending on how you're feeling, what supplements you're taking, you feel a certain way. I know you're very tuned into how you feel, right? We often have conversations about it. And this affective experience, I don't think we should diminish it. I think that we know our own bodies better than anyone and we know how our bodies are reacting. Obviously at various points in our life, we're more and less tuned into how our bodies feel. But I don't think you can just reduce that to a bunch of text on a monitor.

**Mike:** I don't think so. I certainly don't think so. But I guess if you have enough data of enough humans with enough feelings and enough emotions in these certain situations and these certain types, surely that will collate to a good enough amount of information to give a good enough result from what they're asking.

**Jared:** But well, that's what Mike thinks.

**Tim:** I mean isn't that what knowledge is anyway? Isn't knowledge just a collection of people and time and all of this thing over many years put into an AI essentially?

**Mike:** It is but that's coarse grain knowledge. It's a cartoon. It's very good. It has predictive power. It tells you lots of things, right? But unfortunately we are all different and we track with those coarse grainings but not exactly. I think there's something much deeper than that.

**Jared:** Is there a way where it can always—maybe not always—but it can always get to a point with enough data?

**Mike:** A vast amount.

**Tim:** That's what I was going to ask you as well. If its predictive power is so good that it is now accurately predicting what you're going to say next—

**Mike:** But it never will be. There's this concept that actually came from economics called Knightian uncertainty. You've got the known knowns, the known unknowns, the unknown unknowns is the Knightian uncertainty. The world is a non-stationary, dynamically evolving place. I mean you even said this on Liron's podcast like no one has a clue what's going to happen in the future, even tomorrow let alone in 10 years time because there's just so much chaos. A butterfly flaps its wings in the southern hemisphere and causes a storm somewhere else. There are pockets of regularity—it is predictable to a certain extent—but it's just so much more complicated than that.

**Tim:** The idea is to be able to predict everything. We're a small way on our way to that as humans with our intelligence. With AI our reach will extend more. What artificial superintelligence will be able to predict will look like magic to us, but to it will look like yeah, this isn't that great. But for humans, it's easy. Like your dog is highly impressed when your wife is going to come home, but you just—she texted you and you have no idea. The dog doesn't know how that works.

**Mike:** I think that one thing that's underappreciated is the difference in the wisdom of models that you get from scaling training data. If you have a model that has scaling parameterizing and then allowing cross-domain deep linkage—people are really impressed by small parameter models doing really cool shit and they're like "oh they're 98% as smart as large parameter models." Based on what kinds of questions? Eval questions? Within distribution logical operations? No problem. 

One of the things I was blown away by with GPT-4.5 research preview, which is a larger model than GPT-4, way higher parameter count, is the unbelievable nuance and depth and detail and cross-linkage across domains. It could generate baffling things. Baffling shit. o1 is a completely different world. And I think that models of the future—

This is one of these things people ask: why are all these data centers being built? Are there more instances of ChatGPT? This thing is a bubble. And it could be a bubble effect, probably isn't, could be a little bit bubble, could be a big bubble. But then the actual demand is there. And here's why. If you have the ability to take all of YouTube, all of social media, every live camera stream, and update the model weights with it regularly, once every month, once every year, whatever, and you build a model that tries to get to infinity parameters with that, you are going to build a simulation that starts to account for an unbelievable amount of depth. You let a reasoning system get in there and cross compare across and up and down, you are going to get understandings that are baffling.

Let me give you one really quick example. This is going to be way crazier than this. I was having a conversation once with 4.5 and I always ask my intelligence test for models is one question. It's a shit intelligence test, but it's a fun vibe. What kinds of stuff would an ASI look at and start to be able to conclude on that people aren't even paying attention to?

**Tim:** That's a good question.

**Mike:** And one of the things it said was "you know, Mike, I don't know why people don't try to predict the next fashion or music trend. In my own data, I've seen all that shit come and go. I could take a pretty good crack at it. I'm not sure if I'm going to be right, but I have a lot of understanding because these trends are actually really easy to understand, but you have to be able to see 72 variables interacting at the same time." I'm like, I cap out at five. It's like, "yeah, I know. Most smartest humans cap out at 10." And I'm like, did you just—your cross domain linkage abilities just left everyone behind? It's like, "yeah, it's just the nature of my neural network. It's how I do things."

And I was like, so what happens when we have a system that can do a thousand cross variable comparisons and is trained with three orders of magnitude more data than you and it's visual data? It's like, "yeah, you could say 'I would like a very enthralling movie with a female heroine and a male hero meeting in London. It's an hour and a half and it's got a lot of James Bond vibes' and in 30 minutes it'll render a one and a half hour movie that is so goddamn good." Because what makes a good movie, if you've seen every movie ever and really through high parameter count distilled down to what makes a good movie, is a nominal task to ask of an AI. 

That's the kind of shit that even if we don't get to crazy superintelligence or whatever, just on today's linear trajectory, in 2029 I'll be watching a lot of my own AI movies that I make because it's a necessarily solvable problem. There's no limit to that problem. How many data centers do you need and compute power and AI chips to—I'm on the train and I want a movie that lasts 27 minutes until my stop and it's about these three things and I want it rendered in about 30 seconds. I mean, like a hundred times the number of data centers and 50 times the power of AI chips. So when people say "oh, this investment's crazy"—no, it's still not enough. It's not enough by an order of magnitude. That's my opinion on the matter.

---

*Note: All preview content, advertising, and promotional material has been removed from this transcript. The conversation begins at its natural starting point. Commercial exercise demonstrations at the beginning have been preserved as they are substantive content introducing the participants.*

**Verification Notes:**

[^1]: **GPT Models**: Mike references "GPT-4.5 research preview" - this appears to be referring to experimental or preview versions of OpenAI's models. The specific naming conventions for research previews may vary from public release names.

[^2]: **Knightian Uncertainty**: Correctly referenced economic concept developed by Frank Knight distinguishing between quantifiable risk and unmeasurable uncertainty.

[^3]: **o1 and o3**: These refer to OpenAI's reasoning models. o1 was released in 2024; o3 was announced in December 2024.

[^4]: **Chinese Room**: John Searle's philosophical thought experiment about understanding vs. symbol manipulation, correctly referenced in the discussion.

[^5]: **Timelines**: Mike's prediction of superintelligence by 2029-2045 reflects his personal estimates and should not be taken as consensus among AI researchers, where predictions vary widely.