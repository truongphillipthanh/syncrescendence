# How AI Agents Will Transform in 2026

## The Death of the Prompt Box

I'm Marc Andrusko, a partner on our AI apps investing team. My big idea for 2026 is the death of the prompt box as the primary user interface for AI applications. The next wave of apps will require far less prompting. They'll observe what you're doing and intervene proactively with actions for you to review.

The opportunity we're attacking used to be the $300 to $400 billion of software spend annually in the world. Now what we're excited about is the $13 trillion of labor spend that exists in the US alone. It's made the market opportunity—the total addressable market for software—about 30 times bigger.

If you start from there and think about what happens when all of us want this software to be doing work for us, ideally it's doing work with at least, if not more, competency than a human could. I like to think about what the best employees do. What do the best human employees do? I've recently been talking about this graphic that was floating around on Twitter. It's a pyramid of the five types of employees and the ones with the most agency and why they're the best.

If you start at the bottom rung of the pyramid, there are people who identify a problem and then come to you and ask for help and ask what to do. That's the lowest agency employee. But if you go to the S tier—the most high-agency employee you could possibly have—they identify a problem, do research necessary to diagnose where the problem came from, look into a number of possible solutions, implement one of those solutions, and then they keep you in the loop or come to you at the very last minute and say, "Do you approve of this solution I found?" That's what I think the future of AI apps will be. That's what everyone wants and that's what we're all working toward.

I feel pretty confident that we're almost there. LLMs have continued to get better and faster and cheaper. There's a world in which user behavior will still necessitate a human in the loop at the very end to approve things, certainly in high-stakes contexts. But I think the models are more than capable of getting to a point where they're suggesting something really smart on your behalf and you basically just have to click accept.

### Proactive AI in CRM and Workflows

I'm pretty obsessed with the notion of an AI-native CRM, and I think this is a perfect example of what these proactive applications could look like. In today's universe, a salesperson might go open their CRM, explore all the open opportunities they have, look at their calendar for that day, and try to think about what are the actions I can take right now to have the greatest impact on my funnel and my ability to close deals.

With the CRM of tomorrow, your AI agent or your AI CRM should be doing all these things on your behalf in perpetuity, identifying not only the most obvious opportunities that are in your pipeline, but going through your emails from the last two years and harvesting—this was once a warm lead and you let it die, maybe we should send them this email to drum them back up into your process. There are so many ways in which drafting an email, harvesting your calendar, going through your old call notes—the opportunities are just endless.

The ordinary user will still want that last mile approval almost 100% of the time. They will want the human part of the human in the loop to be the final decision maker. That's great. I think that's the natural way in which this will evolve. 

I can imagine a world in which the power user is basically taking a lot of extra effort to train whichever AI app they're using to have as much context about their behavior and how they perform their work as humanly possible. These will utilize larger context windows. These will utilize memory that's been baked into a lot of these LLMs and make it such that the power user can really trust the application to do 99.9% of the work or maybe even 100%, and they'll pride themselves on the number of tasks that get done without a human needing to approve them.

## Designing for Agents, Not Humans

Hi, my name is Stephenie Zhang and I'm an investing partner on the a16z growth team. My big idea for 2026 is creating for agents, not for humans.

Something I'm super excited about for 2026 is that people have to start changing the way they create. This ranges from creating content to designing applications. People are starting to interface with systems like the web or their applications with agents as an intermediary. What mattered for human consumption won't matter the same way for agent consumption.

When I was in high school, I took journalism. In journalism, we learn the importance of starting the first paragraph of a news article with who, what, when, where, why, and how. We start feature stories with a lead. Why? To grab human attention. Humans might miss the deep, insightful statement that's buried somewhere in an H5 page, but agents won't. The new optimization is not visual hierarchy—it's machine legibility.

The new challenge in content creation and in software design is not about hooking human attention but making content easy for an agent to parse. This can manifest in different ways. In a search product, for example, some of the top results for search queries will likely be content by large language models summarizing different pieces of web content. That means that optimizing for that intermediary—the large language model—before it gets to the user becomes a lot more important. Now your headline, your first paragraph, your H1 tag, those become much less important. What becomes more important is the underlying structure of the data.

In the world of consumer software or prosumer software, a similar shift is happening. Up until now, we've optimized software for human interfaces. We care a lot about the visual hierarchy, user experience, making things pretty and functional. But if there's going to be an agent layer that intermediates between the human and the actual software, what's important is not how pretty the UI looks but how machine-legible the data is. Can an agent access the underlying API? Can it understand how to move through that UI programmatically versus a human clicking through it?

This means big changes for all the prosumer and consumer software that exists in the world today. There are lots of startups building this agent layer, enabling people to ask questions and have the agent do work on their behalf, across many different tools and many different applications. For consumer software or prosumer software to work with that, there's a lot of work and a lot of rearchitecting that may need to happen.

### Machine Legibility in Practice

We'll probably see an evolution from where we are today in terms of optimizing for human consumption. It's probably not going to be a step function change, but a big transition. An agent may suggest news articles or may suggest tasks that you should work on. Again, it's getting that intermediary up and running. In the case of the legal system and in the case of medicine or diagnostics, even though an agent layer is handling the majority of the work, there's a lot of different potential situations where humans stay in the loop. Those tend to be cases of higher liability, more complex analyses, where we see humans staying in the loop and will probably stay in the loop for much longer until the models and the technology get to incredibly high accuracy.

I don't know if agents will be watching Instagram reels. It's really interesting, at least on the tech side. It is really important to optimize for that machine legibility piece—optimized for insight, optimized for relevance—especially versus in the past when it was more about hooking people in, capturing attention in flashy ways.

What we're seeing already is the case of high-volume hyperpersonalized content. Maybe you don't create one extremely relevant and insightful article, but maybe you're creating extremely high volumes of low-quality content, addressing different things that you may think an agent wants to see—almost like the equivalent of keywords in the era of agents where cost of creation of content goes to zero and it's really easy to create high volumes of content. That's a potential risk around just high volumes of things to be able to try to capture agent attention.

## The Rise of AI Voice Agents

I'm Olivia Moore and I'm a partner on our AI applications investing team. My big idea for 2026 is that AI voice agents will start to take up space.

In 2025, we saw voice agents break out from something that seemed more like science fiction into something that real enterprises are buying and deploying at scale. I'm excited to see voice agent platforms expand, working across platforms and modalities to handle full tasks and bring us closer to the true AI employee vision.

We've seen nearly every vertical have enterprise customers that are testing voice agents, if not deploying them at pretty significant scale.

### Healthcare Applications

Healthcare is probably the biggest one here. We're seeing voice agents in nearly every part of the healthcare stack—calls to insurers, pharmacies, suppliers, but also in perhaps more surprisingly patient-facing calls. It could be things like scheduling and reminders that are kind of table stakes, but also even more sensitive calls like post-surgery follow-up calls or even intake calls for psychiatry are being handled by voice AI.

I think honestly a big driver here is just the high turnover and the difficulty in staffing in healthcare right now, which makes voice agents that can perform with some reliability a pretty good solution.

### Banking and Financial Services

Another category like that is banking and financial services. You would think there's so much compliance and regulation that voice AI can't operate there yet. But it turns out this is an area where voice AI actually outperforms because humans are actually very good at violating compliance and regulations, and voice AI can get it every time. Importantly, you can track how voice AI is performing over time.

### Recruiting

Lastly, I would say another area where voice has taken off is recruiting. This is everything from retail frontline jobs to entry-level engineering roles to even mid-level consulting roles. With voice AI, you can create an experience for candidates where they can interview instantly at whatever time works for them and then they're sent through the rest of the human recruiting process.

### Technical Improvements

We've seen big improvements on accuracy and latency this year as the underlying models get better and better. Actually, in some cases, I've heard of voice agent companies slowing down their agent or introducing background noise to make it sound more like a human.

When it comes to BPOs and call centers, I think some of them are going to see a softer transition and others are going to maybe see a harder cliff when it comes to the threat from AI and specifically voice AI. It's kind of like how people say AI isn't going to take your job—a human using AI will. What we're seeing is a lot of end customers may still want to just buy the solution, not buy technology that they have to implement. They might still use a call center or BPO in the near to medium term, but they're probably going to use one that's going to offer a cheaper price or be able to do more volume because they're utilizing AI.

Interestingly, there are a couple geographies where humans are still actually cheaper on a permanent basis than best-in-class voice AI. It'll be interesting to see as the models get better if costs come down there, and then call centers in those markets might face a little bit more of a threat than they do now.

### Multilingual Capabilities

AI is actually remarkably good at multilingual conversations and heavy accents. Oftentimes I'll be on a meeting and there'll be maybe a word or a phrase I don't catch, and I'll check my Granola transcripts and it has it down perfectly. I think that's a good example of what most ASR or speech-to-text providers can do now.

### Future Applications

There are a couple use cases that I'm hoping we see a lot more of next year. Anything government. We were investors in Prepared 911. If you can run 911 calls—and those were the non-emergency calls—but if you can run that with voice AI, you should be able to run DMV calls and anything else government-related that right now is so frustrating as a consumer and so frustrating if you're the worker on the other end of the phone.

### Consumer Voice AI

I'm also really intrigued to see more in consumer voice AI. It's mostly been B2B so far just because it's so obvious to replace or supplement a human on the phone with much lower cost AI. One category in consumer voice that I'm excited about is around health and wellness more broadly. We're already seeing voice companions take off in assisted living facilities and nursing homes, both as a companion for the residents, but also they can track different measures of wellness over time.

### The Voice AI Ecosystem

We see voice AI as more of an industry than a market, which in our opinion means there's going to be winners across and at every layer of the stack. If you're interested in voice AI or if you want to build in voice AI, I would recommend you check out the models. There are lots of amazing platforms like ElevenLabs where you can test both creating your own voice and creating your own voice agent, and you get a really good sense of what's possible and what's to come.

---

*Note: All preview content, advertising, and promotional material has been removed from this transcript. The content begins at its natural intellectual starting point.*

[^1]: **Speaker**: Marc Andrusko is a partner at Andreessen Horowitz focusing on B2B AI applications and fintech.

[^2]: **Speaker**: Stephenie Zhang is a partner on the Growth investing team at Andreessen Horowitz, focused on enterprise technology companies.

[^3]: **Speaker**: Olivia Moore is a partner on the investing team at Andreessen Horowitz, where she focuses on AI applications.

[^4]: **Product**: ElevenLabs is an AI voice synthesis and conversational AI platform that Andreessen Horowitz invested in during their Series A round in June 2023.

[^5]: **Product**: Granola is an AI-powered note-taking app that transcribes meetings and enhances user notes with AI-generated context.