https://www.youtube.com/watch?v=VSLEGpCemtE
How AI Starts Doing the Work in 2026 with Anthropic CPO Mike Krieger
42,363 views  Premiered Dec 23, 2025  The AI Breakdown
Anthropic CPO Mike Krieger joins AI Daily Brief to map where "vibe coding" is headed in 2026—from Claude's early coding focus to the rise of longer-horizon, more autonomous coding agents like Claude Code. The conversation breaks down what's changing across three worlds: software engineers, non-technical builders, and enterprise teams trying to move beyond chatbots into real agent workflows, infrastructure, and measurable ROI. Big takeaway: the next leap isn't just smarter models—it's reliability, better interfaces, and AI that can consistently take work off your plate.

Brought to you by:
KPMG – Go to www.kpmg.us/ai to learn more about how KPMG can help you drive value with our AI solutions.
Vanta - Simplify compliance - https://vanta.com/nlw 

The AI Daily Brief helps you understand the most important news and discussions in AI. 
Subscribe to the podcast version of The AI Daily Brief wherever you listen: https://pod.link/1680633614
Get it ad free at 
Join our Discord: https://bit.ly/aibreakdown

---

0:00
Today on the AI daily brief, the future
0:02
of vibe coding and what's in store with
0:04
AI 2026 with Mike Kger, the chief
0:07
product officer of Anthropic. Now, as we
0:09
move forward into our end of year
0:11
episodes, I'm excited to add a couple of
0:13
conversations into the mix. You might
0:15
know Mike Kger as the co-founder of
0:17
Instagram. Real ones will also know him
0:19
as the co-founder of Artifact, an AI
0:22
powered news app. However, for most of
0:24
you right now, Mike's most important
0:26
role is as the chief product officer of
0:28
Anthropic. In this conversation, we talk
0:30
about the origins of Anthropic's focus
0:32
on coding, how enterprise AI usage has
0:35
changed over the course of the year, and
0:36
some of the trends that Mike is most
0:38
excited about heading into 2026.
0:41
All right, Mike, welcome to the AI Daily
0:42
Brief. Great to have you here.
0:44
It's great to be here. Thanks for having
0:45
me.
0:45
Yeah. So, so this is a super fun uh like
0:48
I was just saying uh some of my favorite
0:50
episodes of the year are these end of
0:51
year episodes where we get to kind of
0:52
think think big look forward and you one
0:55
of the big themes I think for for me
0:56
heading into the new year is sort of
0:58
everything vibe coding everything
0:59
agentic and so I was super excited to
1:01
have you join the show. What I wanted to
1:03
do though is actually kind of go go way
1:05
back a little bit. I think you know a
1:06
lot of folks see anthropic as as sort of
1:09
the torchbearer in a lot of ways for AI
1:12
coding and I I wondered you know I was
1:14
thinking about when you joined the
1:15
organization and and just how how early
1:18
was that sort of focus clear you know
1:19
was that an emergent phenomenon as it
1:22
became clear that there was something
1:23
very differentiated in these models and
1:25
that's sort of how people were using it
1:26
or was that sort of like intention from
1:28
very early on that this is a a broad
1:30
sort of set of use cases that matter to
1:31
you guys. Yeah, the thing I always like
1:33
to say whenever there's sort of product
1:35
folks inside Anthropic that are thinking
1:36
about sort of which direction to take
1:38
things in is the more you can align with
1:40
the sort of company's general long-term
1:42
perspective about where powerful AI will
1:44
come from like the smoother things will
1:46
go because Anthropic is nothing but
1:48
focused right and I think that that's
1:49
shown through in sort of the you know
1:51
the the bets that we choose to make
1:53
versus not and definitely there's this
1:55
belief that you know for very powerful
1:57
AI you need uh the ability of the model
1:59
to sort of reason about things to plan
2:02
agentically and and work for a long time
2:04
horizon but then also to be able to
2:05
write and run code um not only to
2:08
produce software but because it's a
2:09
really useful tool for solving problems
2:11
and so that belief was in there and it
2:12
predates me I joined in May of last year
2:15
um but it kind of coincided sort of with
2:17
the outside world realizing it because
2:19
claude 3 which had come out I think a
2:20
month before that was the first model
2:22
and I remember there was like that
2:23
moment on Twitter when everybody said oh
2:25
wow this model can actually write like
2:27
not just like sort of function level but
2:28
like entire you know sort of files of
2:30
code and of just compared to now it was
2:33
not very good at it but it was already
2:34
you know amazing what it could do then
2:35
and then we paired it with our first
2:36
sort of more coding oriented product
2:38
which was artifacts so you could have
2:40
you know claude kind of generate you
2:42
know at the time it was mostly react
2:43
sites you know alongside the the chat
2:45
and that was kind of I think for a lot
2:46
of people the first moment they realized
2:48
oh this is an interesting new experience
2:50
of kind of coding alongside the model
2:52
and not necessarily doing it in a in a
2:53
development environment
2:55
yeah it's interesting I think you can in
2:56
a lot of ways almost chart people's sort
2:59
of the the viability of a lot of this
3:02
two key releases alongside Anthropic.
3:05
You know, I remember when when I first
3:06
started this show, it was actually April
3:08
of 2023
3:10
and already sort of agent coding was
3:14
like the thing that people were most
3:15
excited about like GPT Engineer, which
3:18
would later actually become like sort of
3:19
morph into lovable at like 18 months
3:21
later or something like that was uh was
3:24
it was like my first viral YouTube
3:26
episode was about GPT Engineer and and
3:29
so it's interesting to see kind of like
3:31
at each stage how more use cases get
3:34
unlocked and and sort of a broader set
3:35
of people come into the fold coming into
3:37
2025.
3:39
You know, the I think that the odds on
3:42
favorite for what the year was going to
3:44
be about, at least if you had looked
3:45
back at kind of all the AI content
3:47
creators, was going to be the year of
3:48
agents, right? And I think looking back,
3:51
it was but it was the year of coding
3:53
agents. Did you guys have a sense coming
3:55
into this year that there sort of that
3:57
this was poised to be kind of you know
4:00
the the significant use case or the
4:02
breakout based on what conversations you
4:04
were having based on the capabilities
4:05
that you were seeing? Yeah, it's a great
4:07
sort of uh moment to reflect because uh
4:10
going into the sort of last couple weeks
4:12
of the year last year, we had built
4:14
something internally we called cloud CLI
4:16
uh which we later released as cloud code
4:18
and that was the emergence of that came
4:20
from our labs team which is a team that
4:22
really focuses on trying to do sort of
4:23
disruptive 0ero to1 ideas and that was
4:25
everything from like early computer use
4:27
explorations and some wacky things and
4:29
also this cloud CLI thing and and
4:31
between I think September when the first
4:34
version got sort of rolled out
4:35
internally to December
4:36
It rapidly overtook every other sort of
4:39
coding tool we had internally and it was
4:41
because it kind of had this bet that the
4:42
models are going to be able to do more
4:44
and more maybe not this model but the
4:45
next one and the next one and the next
4:46
one. But let's let the model cook for
4:48
longer. Let's let it sort of act for you
4:50
know longer periods of time. And um so
4:53
that you know going to the holidays it
4:54
was that question of do we release this
4:56
you know like do we now add a you know
4:59
kind of third component to the product
5:00
portfolio beyond just cloud AI and then
5:02
and the API. And so that that was the
5:04
active conversation that was happening.
5:05
But we really felt like if not us then
5:07
at least somebody using our models would
5:10
sort of co-discover this this piece
5:12
where you don't need to hold the model
5:14
so closely anymore. you can let it
5:16
operate over a sort of fuzzier task
5:18
definition and over a longer time. It
5:20
still needed a fair amount of
5:21
handholding then but you could see you
5:23
could see the shape of it. So we it was
5:25
definitely the the coming into this year
5:26
we felt like that was going to be a
5:28
major shift in how people are going to
5:29
build build software. Well, it you know
5:31
it's a super interesting Quang one of
5:33
the things I mean you you're sort of you
5:34
know you have a deep deep product
5:36
experience and one of the challenges I
5:38
think now for product folks and just for
5:41
entrepreneurs in general is there's this
5:44
sense that to be successful you have to
5:47
you know not just gives lip service to
5:49
the idea of skating to where the puck is
5:51
going but actually sort of design and
5:53
orient what you're building for
5:55
capabilities that do not yet exist and
5:57
that's an extraordinarily hard thing to
6:00
do and it sounds like that was part part
6:02
of the genesis of cloud code was just
6:04
some sort of attempt or or you know some
6:06
like you know scratching against that
6:08
itch in some way. Yeah, we have um
6:10
product principles inside Enthropic and
6:12
one of them is ride the exponential
6:14
which is like we're trying to build
6:15
products that both you know meet the
6:17
moment so they they're useful today or
6:19
at least they they poke at something
6:20
useful today maybe the ones that are a
6:22
little early we won't release yet but
6:23
that they can naturally improve and it's
6:25
been interesting even on the cloud code
6:26
side we've deleted parts of the harness
6:28
over time rather than added to it
6:30
because the model can do more and it's
6:32
really interesting also we work with a
6:33
lot of kind of downstream customers that
6:35
are using the model and sometimes you
6:36
know we'll drop a new model you know a
6:38
research model and they'll say it
6:39
doesn't look like improve very much and
6:40
then we'll send some applied AI you know
6:42
folks to spend time with them and they
6:43
realize right now we're actually
6:44
harnessbound and we need to actually let
6:46
them evolve and let the model do a bit
6:48
more to like loosen that as well but
6:50
it's definitely an active conversation
6:51
that we have with folks building on top
6:53
of the platform like you know they have
6:54
some visibility about where we're going
6:56
like maybe they'll be in in a research
6:57
program early access but they still have
7:00
to do a fair amount of this all right so
7:01
if the models are here now uh and I need
7:04
to do this much additional scaffolding
7:06
what does this look like if I need to do
7:07
less scaffolding is my product still
7:09
useful in adding value and can the model
7:11
then do even more for me or is it now
7:12
going to squeeze the the piece that I
7:14
thought I was adding value in?
7:16
Have you been surprised at all with the
7:18
way that people have used claude code
7:20
since you released it because it you
7:22
know it is much broader uptake than just
7:25
sort of you know core audience of
7:27
software engineers.
7:28
Yeah, absolutely. We um internally we
7:30
you know had this internal project that
7:32
people were using and then we've like
7:33
buttoned it up and put on a more fancy
7:35
suit to to be able to release it
7:37
publicly. But then as you can imagine
7:39
like the internal use cases kind of kept
7:40
co-developing and so we do like every
7:43
two to three times a year we do a
7:44
hackathon and it's been notable that
7:46
every hackathon we've done has been
7:48
around the time that some technology is
7:49
like poised for a breakout. So the first
7:51
one we did was around MCP and every
7:53
single project was MCP based before
7:54
really the rest of the world had kind of
7:56
caught on to MCP. The second one we did
7:58
was around the time that cloud code had
8:00
been released and what was really
8:01
interesting was how many projects were
8:03
not coding projects but they were using
8:04
cloud code as the underlying engine. So
8:06
there was one that was using cloud for
8:08
doing bioinformatics which we later kind
8:10
of channeled into cloud for life
8:11
sciences. Another one that was using
8:13
claude as a sort of s sur in a box and
8:15
was able to use cloud code as as a way
8:17
of you know looking at data sources.
8:19
There was cloud as a data scientist.
8:20
There's all these sort of pop-up
8:22
projects that was nice so that they
8:24
didn't have to reinvent the tool use
8:26
kind of bit. They could just add value
8:27
on top of that. And then when we
8:29
launched it we started seeing things
8:30
externally too like people using cloud
8:31
code as their project manager. uh cloud
8:33
code as their PM cloud code as a data
8:36
scientist externally. So we started
8:37
seeing this much more. It's why we
8:39
eventually renamed the underlying SDK to
8:41
the cloud agent SDK because we realized
8:42
calling it code was doing it a
8:44
disservice relative to what kind of use
8:46
cases we were actually seeing. Yeah. So
8:48
this is one of the questions that I'm
8:50
most interested to see in coming year
8:52
but even the coming years is what what
8:56
it takes to kind of rewire people with
8:59
these new tool set. It's like this this
9:01
whole language, this whole
9:02
infrastructure that they have access to,
9:04
especially if they that they haven't
9:05
before, especially if they're they're
9:06
not developers. Do you think that some
9:09
of this you know if on the spectrum from
9:12
this early kind of usage of cloud code
9:15
for non-coding use cases is tinkerers
9:18
who are kind of you know more technical
9:20
than they let on on the one end of the
9:22
spectrum versus actually kind of
9:24
heralding a different set of interaction
9:27
patterns that people are going to have
9:29
kind of how do you see that evolving
9:32
yeah I think it's early still like even
9:33
when we look inside companies that have
9:35
deployed like cloud for enterprise and
9:36
they have builders within their sales
9:38
team or their ads team or whatever
9:40
different non-technical team. you will
9:42
always find this sort of persona which
9:44
is the tinkerer builder like early
9:46
adopter within that space that usually
9:48
is not an engineer doesn't even have an
9:50
engineering background but has figured
9:51
out enough and has like learned the
9:53
primitives and can then talk to cloud
9:54
enough about how to fix these issues um
9:56
that they can then kind of build
9:58
something pretty powerful whether it's
9:59
you know automating part of what they
10:01
were doing sort of enriching what they
10:02
were doing u making their team's lives
10:04
easier like all these different pieces
10:06
but it does still take that person which
10:08
I think is probably a natural part of
10:09
the kind of software life cycle we are I
10:11
think there's still this this gap and I
10:13
think that that's both a gap in
10:15
interface in terms of how people think
10:17
to interact with and and how these
10:18
products reveal their full capabilities
10:20
and then also the actual capabilities
10:22
themselves where if you had a you know
10:25
human co-orker and it was very creative
10:27
at solving problems like you gave it a
10:29
high level task was able to do it most
10:30
of the time but sometimes it would sort
10:32
of make a mistake that you would never
10:34
have expected to make based on it having
10:35
just done it great last week you'd be
10:37
like have a pretty complicated
10:38
relationship with that co-orker I still
10:40
we're at that phase still of this like
10:42
gap between understandability of these
10:44
systems but then also gap of you know
10:46
how reliable and predictable are they
10:49
when they do start working and can they
10:50
feel more like a thing that gets just
10:52
predictably better over time. Yeah, I I
10:54
I think that's true. And I also think
10:56
that there's just, you know, I don't
10:58
have the the exact right words for this,
10:59
but there's some lag in terms of just,
11:02
you know, unwinding and undoing however
11:05
many years or decades of the way that
11:07
you've been doing a thing before that is
11:09
uh it just it just takes time. You know,
11:11
I I think about it, you know, we're now
11:13
I guess 10 months into vibe coding as a
11:15
named phenomenon, right? It was same
11:17
February of this year, same same month
11:19
that cloud code came out. And I'm still
11:22
finding myself as someone who literally,
11:23
you know, podcasts about this every day
11:25
and is living inside these tools. I'm
11:27
only just now starting to find myself
11:30
actively ask on on a regular basis like
11:33
could I be building something to do this
11:35
instead of using a Google sheet or
11:38
instead of you know, however I however I
11:40
used to do it. And again, that's that's
11:41
me as someone who's as deep in this as
11:43
you can get.
11:44
I think there's something really to the
11:46
you know, building with one tool that
11:48
gets you comfortable with it and
11:49
familiar with it. it's easier to build
11:50
the incremental N plus1, but it's the
11:52
that first one that requires that sort
11:53
of uplift if you're not in the habit. So
11:55
I was working on a project over the
11:56
weekend. I was using Replet and using
11:58
Opus under the hood. And then then I
12:01
also needed to create a secret Santa for
12:02
my family. And that, you know, because I
12:04
had been in the tools already, it was
12:06
over breakfast while I was cooking eggs.
12:08
I, you know, kind of kicked off this
12:09
asynchronous request and by the time I
12:11
was done, it actually had built the
12:13
whole thing. And that was really cool.
12:14
But I don't I wouldn't have reached for
12:15
it as my first tool had I just not been,
12:18
you know, sort of interacting with that
12:19
same software. So I do think that
12:20
there's this sort of still like habit
12:22
creation and adaptation of even knowing
12:24
you can do that that we still need to
12:26
close.
12:27
Yep. Exact sort of similar uh example.
12:30
Someone had mentioned building a gift
12:32
tracker with with one of these tools.
12:34
And I love that because we always end up
12:36
I it's Christmas Eve and I'm like we're
12:38
we're shoving presents in the closet for
12:40
later because we just bought too many
12:42
things. And so I copied that, I imitated
12:44
it and it was probably the for whatever
12:46
reason the first time in a month that
12:47
that I built something. Uh and the just
12:51
huge basically since this last wave of
12:53
of sort of models had come out and um
12:57
within I don't know three or four days I
12:58
had like six or seven different
13:00
applications that that had just sort of
13:01
like you know spiraled from there. I
13:03
think it's also interesting like uh when
13:05
we you've seen this not even just for
13:07
our model launches but other model
13:08
launches there there's been the sense
13:10
from people that are primarily maybe
13:11
interacting with it you know in chat
13:12
it'll say oh it seems a little smarter
13:14
maybe it's a little bit warmer but
13:16
they're very sort of vibes based
13:18
assessments and if you're not sort of
13:20
checking in and dipping back in and
13:21
trying to build something with it that's
13:23
where I think you see the biggest leaps
13:24
and I definitely saw it over the weekend
13:26
actually getting to build with Opus you
13:28
know for real over you know two days of
13:30
getting really deep in there I was like
13:31
oh these are applications that would not
13:34
have been doable even in sonnet 4.5. It
13:36
would have hit some ceiling or it would
13:38
have gotten stuck in some some loop but
13:39
then just watching it hit a wall, you
13:41
know, debug itself, tail the logs, add
13:44
debug logging, roll it out again, pull
13:45
up a browser, check the like all of
13:47
these capabilities that I think have
13:48
just either been co-created along with
13:51
the model or now the model is using
13:52
better uh as it's improved. But you
13:54
really got to sort of dip in and push it
13:56
to really see that difference. But I
13:57
think where where there's some of that
13:58
jadedness I think from the outside
14:00
sometimes of oh are we hitting a plateau
14:02
whereas you know if you if you
14:04
checkpoint it you can definitely see
14:05
that continued improvement. Well so this
14:08
is actually an interesting point at
14:09
which to sort of maybe try to fork the
14:11
AI coding or vibe coding conversation
14:13
into a few different buckets. you've got
14:16
sophisticated the sophisticated sort of
14:18
software engineering conversation around
14:20
what AI coding is going to mean and and
14:23
sort of you know the autonomy spectrum
14:25
and and and sort of where these models
14:27
are and what they can do and you almost
14:28
have you've gone through kind of a full
14:30
cycle this year where you know it was
14:32
you know huge amounts of uptake but now
14:34
we're kind of sifting through the the
14:36
new challenges right you know new
14:38
technology doesn't solve all problems it
14:40
trades one set of problems for a
14:42
hopefully better set of problems but
14:43
then you still have to solve those but
14:45
then on the other end of the spectrum
14:46
you've got it's incredibly nent the
14:48
individual you the non-technical usage
14:50
of these tools I think we've barely
14:52
started to scratch the surface on those
14:54
things and then somewhere in the middle
14:55
you've got kind of enterprise usage
14:57
which includes some of that software
14:59
engineering reorganization but also uh
15:02
includes I think lots of folks who are
15:03
thinking about how to use these types of
15:05
tools for other aspects of the business
15:07
rather than just sort of you know bu
15:09
building software how much do you think
15:12
these are the same conversation versus,
15:14
you know, again, three three kind of or
15:16
two or three different conversations
15:17
using all the the same words.
15:19
Yeah. No, I think you're right. I think
15:20
even if they have an underlying model
15:22
that's the same and even if some of the
15:23
other building blocks that you might use
15:24
in there, either from an SDK
15:26
perspective, um they all sort of end up
15:29
needing different applications or or
15:31
different sort of manifestations, they
15:32
feel quite different. I think you're
15:33
right. So on the you know software
15:35
development side you have in general
15:37
like a pretty motivated population that
15:39
has always been interested in tinkering
15:40
with their tools like hence the Emacs
15:42
versus you know Vim like you know tabs
15:45
versus spaces like programmers maybe
15:46
notoriously have the the desire to sort
15:49
of optimize their own building
15:51
environment which other disciplines
15:52
might have but not always to the same
15:54
degree and it's not always as easy to
15:55
swap one thing out for another. So the
15:57
adoption there, the evolution has become
15:59
I think this flywheel where it's really
16:01
clear for you know engineers at
16:03
anthropic where the model needs to
16:04
improve and that's very helpful for us
16:05
to you know coordinate with research and
16:06
and close that flywheel as well as you
16:08
know feedback from external folks in the
16:10
middle piece there's still this sort of
16:12
ceiling of complexity that you can hit
16:14
now there's been really impressive sort
16:15
of 0ero to1 vcoded you know applications
16:18
that have even been released but you
16:19
still um I think the gap I was
16:21
perceiving even just like watching my
16:23
wife who's a sort of product manager or
16:25
UX designer by by training not a
16:26
software engineer used some of these is
16:28
that you still sometimes need to know
16:30
the right sort of magic incantation
16:31
words to use. we were building a a
16:33
product together like a side project and
16:36
uh the the way it was using LLM was just
16:38
filling the context window and I was
16:39
like okay well you actually probably
16:40
need to move to some like semantic
16:41
retrieval piece but Opus wasn't
16:43
suggesting that out of the box and she
16:45
didn't know the magic words and it took
16:47
me saying all right we actually probably
16:48
need to move to this embedding solution
16:50
it was just like a layer of complexity
16:52
above and so I think one thing that our
16:54
models all these coding models can do a
16:56
better job of in that middle category of
16:58
helping non-technical people build
17:00
things that are you know effectively
17:02
software is helping them move up that
17:04
complexity ladder in a in a sort of more
17:06
structured or thoughtful way where yes
17:08
the fivecoded front end only thing is
17:10
great to show off an idea and then you
17:12
want to persist data okay that's the
17:13
next step or you want to persist data
17:14
you're thinking about launching this
17:15
that's going to take a whole other step
17:16
of security reviews and thinking about
17:18
things and like oh now you've launched
17:20
it and the thing is melting under you
17:22
know load okay great now I got to put on
17:24
my my performance engineering hat and
17:25
then build from there in the same way
17:27
you know with Instagram we went through
17:28
that process of first we were just
17:30
building the UI then we built the back
17:31
And then we launched and it totally fell
17:33
over because it got attention. And then
17:34
we kind of rebuilt it over the next, you
17:36
know, weeks and months to to sort of
17:38
manage it. You got to speedrun that but
17:39
with model assistance now. Um, so that
17:42
feels like the big piece on the middle
17:43
one. And then the last one I think on
17:44
the on the enterprise software side, you
17:47
know, I I saw you cover the, you know,
17:49
like famous MIT report of that like gap
17:51
of expectations. And I think that was
17:52
such a it it was one of those things
17:54
that was truthy and even if there was
17:56
like sort of methodological problems
17:57
underneath the study, it did point to
17:59
something that a lot of people had which
18:00
is like I got AI rolled out to me at
18:01
work but I'm not sure I'm more
18:02
productive. And I think the the place to
18:04
close that gap I think there's a bunch
18:06
of of things. One is just making sure
18:08
the output quality is actually good
18:10
enough where you're saving yourself time
18:11
where something is half done. I think
18:13
for most people they say, "Well, I
18:14
probably would have been faster doing
18:15
this myself rather than ending up with
18:18
something that's not not quite there,
18:19
and then I'm struggling to get it to
18:20
iterate with me to where I need it to
18:22
be." So, a lot of the emphasis we've
18:23
been doing is actually less on the
18:25
agentic side. It's less like take my two
18:28
sentence description and generate an
18:29
entire PowerPoint deck out of it. And
18:31
it's much more, you know, require a
18:32
little bit more upfront work, but really
18:34
focus on making sure that that initial,
18:36
you know, sort of thing that got created
18:38
was high quality enough where you felt
18:40
relief and happy that it saved you time
18:42
rather than, oh man, I've just created
18:43
more work for myself by using AI.
18:46
As you look into 2026, where where do
18:49
you see enterprises starting this year?
18:51
uh maybe especially as compared to where
18:53
they were starting in 25. What do you
18:55
think the big big goals are that that
18:57
you have sort of thinking about you know
18:59
both model design but also product
19:00
design?
19:01
I think maybe two things that feel
19:03
marketkedly different now versus a year
19:05
ago. One is uh enterprises getting more
19:07
interested in rolling out what we've
19:08
been calling horizontal agents. But
19:10
basically, you know, if you think about
19:11
the sort of companion agent or co-pilot
19:13
agent where there's a strong human in
19:15
the loop um and you're kind of
19:17
co-creating either a document or an
19:18
email or, you know, whatever that may
19:20
be. Um seeing also a lot more interest
19:22
now in great we have, you know, this
19:24
repetitive back office task. we're
19:26
trying to scale up to, you know, handle
19:28
international uh know your customer
19:30
requests, whatever those sort of uh
19:32
complicated but repeatable processes
19:34
that have something that is bespoke to
19:35
that enterprise but also something that
19:37
is sort of regulatory. For example,
19:39
we're seeing a shift there where there's
19:40
a lot more interest and we've been
19:42
deploying, you know, applied AI and
19:43
engineers into these enterprises to help
19:45
them get those agents running. It's
19:47
often about like sort of translating
19:48
what those requirements are into that
19:50
you know uh process again where the
19:52
model can be creative and flexible but
19:54
still repeatable enough to you know
19:56
follow their operating procedures that
19:58
feels marketly a year ago we weren't
20:00
really having really any of those
20:01
conversations as well. Um and then the
20:04
second piece which also feels nent is I
20:06
think all of these enterprises
20:07
especially any that have this public-f
20:09
facing product that they might be
20:10
shipping is kind of going beyond V1
20:13
which was like let's kind of sprinkle AI
20:15
on these different surfaces and hope
20:17
that improves the product to do we need
20:19
to rethink some fundamental pieces of
20:21
the product to be more you know agent
20:23
native to use a buzz word but what I
20:25
really see it as is you know have you
20:27
unlocked the full power of your product
20:29
to any AI that is sort of running on top
20:31
or alongside it and we could talk about
20:32
that. But I think that's a that's a
20:34
harder transition to make than right now
20:36
we've got a a sidebar that we can chat
20:37
with your you know AI and kind of
20:39
integrate with the with the rest of the
20:40
product.
20:41
Yeah, it's interesting. I I think that
20:43
again kind of looking back at maybe what
20:45
what expectations were versus what
20:46
played out again if if we take the idea
20:49
that 25 was the year of agents but maybe
20:51
a little bit differently than we
20:52
thought. You know one part of that was
20:54
it was the year of coding agents but
20:56
another part of that was it was also the
20:57
year of agent infrastructure. You know
20:59
this is a year where MCP became
21:01
ubiquitous. uh more recently I just
21:03
today before recording this did a show
21:05
about open AI uh adding skills support
21:08
or you know uh starting to experiment
21:09
with skill support and it's very clear
21:11
that everyone is uh much more interested
21:15
at this stage at sort of building the
21:16
necessary infrastructure to be able to
21:18
move forward faster than in sort of
21:20
getting way laid in the sort of
21:22
standards wars that we've had in the
21:24
past and I wonder it feels to me like
21:26
we're poised a little bit for um
21:29
enterprises to almost go through their
21:31
kind of infra the structure year in 26
21:33
where you know again going back to the
21:35
lesson at the MIT study or the
21:36
truthiness of it hold aside the
21:38
specifics the fact that it had such
21:39
resonance suggests and I agree with this
21:42
that there is something you know some
21:44
some gap there I think that a lot of
21:46
organizations are embracing now that
21:49
it's just you're not just going to drop
21:51
a chatbot in or you're going to do that
21:53
but then to really go kind of to the
21:54
next level it's going to involve a much
21:57
more sort of you know uh comprehensive
22:00
review of of how you do things and it
22:02
feels to me like you know perhaps that
22:04
some some amount of that process
22:06
redesign is is what organizations at
22:08
least the the ones who are kind of you
22:09
know ahead are going to be in for in 26.
22:13
Absolutely. And I was talking to
22:15
somebody who runs uh technology at a
22:16
large bank and he was telling me that
22:18
they had to uh rethink not just the data
22:21
storage piece which they had already
22:22
been doing a lot of work on but also the
22:24
sort of data annotation and sort of
22:26
lineage piece to be more AI friendly so
22:30
that when you asked you know Claude to
22:32
hey help me construct you know a
22:34
dashboard on this or help me understand
22:35
this data query even having that
22:37
additional layer of annotation or
22:39
understanding of what these different
22:41
tables are and what they represent me a
22:43
huge huge way to to actually making that
22:45
a useful sort of product. And so
22:47
figuring out what are the missing
22:49
connector bits is going to be I think a
22:50
lot of 2026 which is great we have MCPs
22:53
we're seeing more and more enterprises
22:55
wrap some of their internal services or
22:56
internal data stores in MCP so they can
22:58
get access to it inside you know for
23:00
example cloud now the next turn is
23:03
that's maybe on the retrieval side can
23:04
you actually start taking action and
23:06
making it uh a useful participant in
23:08
business processes by you know enabling
23:11
it to either you know make a human
23:12
assistant decision or make a queue up a
23:14
decision that a human can confirm
23:15
whatever the sort of metaphor human in
23:18
the loop piece is but moving up that
23:20
complexity ladder so that again it can
23:22
actually start providing value that
23:24
befits its level of in the discourse. I
23:27
want to talk in our last few minutes
23:29
about some of your predictions or
23:30
thoughts about how 26 is going to end up
23:33
differing from from 25 with AI and maybe
23:35
just to get us started we were just kind
23:37
of talking about expanded enterprise use
23:39
cases but what do you think are going to
23:41
be the biggest blockers for enterprises
23:43
and how do you think they're going to
23:44
get through them? I think for for a lot
23:46
of enterprises that we talk to there's
23:48
still this gap between sort of the
23:50
idealized like great if you ran this
23:52
perfectly on this like one cloud with
23:55
all you know your you know permissions
23:57
all perfectly set and you're okay with
23:59
inference happening in this way then we
24:00
could unlock use cases tomorrow on the
24:01
reality which is there's legacy systems
24:04
there's often sort of regulatory reasons
24:06
why they you know for example will only
24:08
run in this particular way on AWS in
24:10
this particular kind of setup and so a
24:12
lot of the work that we're doing for
24:14
next is the word we're even using is
24:16
distributability, which I think the
24:18
spell corrector tells me is not really a
24:19
word, but what we really mean is uh if
24:22
we want to bring our intelligence and
24:23
our even our agentic primitives, you
24:25
know, whether it's skills, whether it's
24:26
the agent SDK, whether it's storage,
24:29
whether it's uh memory, all of these
24:31
pieces into actual enterprise workloads,
24:34
we need to really actually embed and
24:35
meet them where they are. And so there's
24:37
a lot more work on, hey, let's actually
24:39
like componentize this, make it
24:40
available everywhere. you see it now
24:42
that we're on all three major clouds
24:43
like that that the general kind of set
24:45
of projects is kind of closing those
24:47
those gaps because there is interest and
24:50
especially from the sort of more sort of
24:51
forward-looking CTO's and CIOS but they
24:54
also do need to work with sort of the
24:56
existing constraints and setup they have
24:58
and you can kind of get the pilots done
25:00
in a like pretty you know rough and
25:02
ready way just to prove it out but to
25:03
really reach that production scale I
25:05
think that's the biggest blocker
25:07
tool versus colleague this is something
25:10
that that that we've been sort talking
25:11
about for a while and and it's I think
25:13
this is maybe a false binary in terms of
25:15
what you know when we reach maturity of
25:16
AI but do you think that we'll start to
25:19
see more of that kind of treating AI not
25:21
just as a tool but as a thing that can
25:23
take on ever bigger workloads do you
25:25
think that that sort of starts to come
25:26
to reality next year?
25:28
Yeah, I think that that probably more
25:29
than anything is what will define the
25:31
year is you start seeing this already
25:33
with coding. So, um, we did this GitHub
25:36
partnership with their agent HQ piece
25:38
where now you tag cloud in a pull
25:39
request and then you go have your coffee
25:41
and you come back and it's done whatever
25:42
you needed to do. And we did the same
25:44
integration with cloud code. That's sort
25:45
of pointing at the kind of interaction
25:48
that you might expect. Now, is it
25:50
already going to be at the place where
25:51
it can onboard onto the organization,
25:54
understand the understand the problem
25:55
space, understand the sort of dynamics
25:57
of all the relationships and just pick
25:59
up work? No, I don't think we're going
26:00
to be there. Maybe near near the end of
26:02
the year, we'll have some kind of early
26:03
glimmers of there. But I do think the
26:05
the sort of more like piece of the job
26:08
function that has like a a clean sort
26:10
of, you know, all right, great. We need
26:12
uh we need to prepare this kind of
26:13
report. Here's the work I've done
26:14
already. Here's where you can go get
26:16
more information. Here's what good looks
26:18
like. Report back to me, you know, in
26:19
the way that you might delegate to
26:21
somebody else. That's very much around
26:22
the corner. It's how we're thinking
26:23
about a lot of our product strategy
26:25
going into next year is how do we enable
26:26
that? What are the interfaces that we
26:28
need to create that make that possible?
26:30
And then what do we learn about what's
26:32
working on the software domain that we
26:33
can apply to knowledge work?
26:36
This maybe asking you too much to put on
26:38
a marketing hat, but if you had sort of
26:39
a phrase for capturing, you know, what
26:42
what you hope AI does in 26, what would
26:45
it be?
26:46
Guess reliably take work off your plate.
26:49
I like it. All right, Mike. Well, this
26:52
is super super fun conversation. Could
26:53
go for another half hour, hour easy. Uh
26:55
but appreciate you making the time and
26:57
uh really excited to see what you guys
26:58
cook up.
26:59
It was great to be here. Thanks for
27:00
having me.