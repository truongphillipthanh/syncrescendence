https://www.youtube.com/watch?v=64lXQP6cs5M
Is RL + LLMs enough for AGI? ‚Äî Sholto Douglas & Trenton Bricken
175,853 views  May 22, 2025  Dwarkesh Podcast
New episode with my good friends Sholto Douglas & Trenton Bricken. Sholto focuses on scaling RL and Trenton researches mechanistic interpretability, both at Anthropic. We talk through what‚Äôs changed in the last year of AI research; the new RL regime and how far it can scale; how to trace a model‚Äôs thoughts; and how countries, workers, and students should prepare for AGI. See you next year for v3. Enjoy!

ùêÑùêèùêàùêíùêéùêÉùêÑ ùêãùêàùêçùêäùêí
Transcript: https://www.dwarkesh.com/p/sholto-tre...
Apple Podcasts: https://podcasts.apple.com/us/podcast...
Spotify: https://open.spotify.com/episode/3H46...
Last year's episode:    ‚Ä¢ Sholto Douglas & Trenton Bricken ‚Äî How LLM...  

ùêíùêèùêéùêçùêíùêéùêëùêí
WorkOS ensures that AI companies like OpenAI and Anthropic don't have to spend engineering time building enterprise features like access controls or SSO. It‚Äôs not that they don't need these features; it's just that WorkOS gives them battle-tested APIs that they can use for auth, provisioning, and more. Start building today at https://workos.com.

Scale is building the infrastructure for safer, smarter AI. Scale‚Äôs Data Foundry gives major AI labs access to high-quality data to fuel post-training, while their public leaderboards help assess model capabilities. They also just released Scale Evaluation, a new tool that diagnoses model limitations. If you‚Äôre an AI researcher or engineer, learn how Scale can help you push the frontier at https://scale.com/dwarkesh.

Lighthouse is THE fastest immigration solution for the technology industry. They specialize in expert visas like the O-1A and EB-1A, and they‚Äôve already helped companies like Cursor, Notion, and Replit navigate U.S. immigration. Explore which visa is right for you at https://lighthousehq.com/ref/Dwarkesh.

To sponsor a future episode, visit https://dwarkesh.com/advertise.

ùêìùêàùêåùêÑùêíùêìùêÄùêåùêèùêí
00:00:00 ‚Äì How far can RL scale?
00:16:27 ‚Äì Is continual learning a key bottleneck?
00:31:59 ‚Äì Model self-awareness
00:50:32 ‚Äì Taste and slop
01:00:51 ‚Äì How soon to fully autonomous agents?
01:15:17 ‚Äì Neuralese
01:18:55 ‚Äì Inference compute will bottleneck AGI
01:23:01 ‚Äì DeepSeek algorithmic improvements
01:37:42 ‚Äì Why are LLMs ‚Äòbaby AGI‚Äô but not AlphaZero?
01:45:38 ‚Äì Mech interp
01:56:15 ‚Äì How countries should prepare for AGI
02:10:26 ‚Äì Automating white collar work
02:15:35 ‚Äì Advice for students

---

How far can RL scale?
0:00
Okay. I'm joined again by my friends, Sholto Bricken... Wait
0:06
Did I do this last time? You did the same thing. No, no, you named us differently, but we didn't have Sholto Bricken and Trenton Douglas.
0:13
You swapped us. Sholto Douglas and Trenton Bricken,
0:18
who are now both at Anthropic. Yeah. Let's go. Sholto is scaling RL, Trenton's still working on mechanistic interpretability.
0:28
Welcome back. Happy to be here. Yeah, it's fun. What's changed since last year? We talked basically this month in 2024.
0:35
Yep. Now, we're in 2025. What's happened? Okay, so I think the biggest thing that's changed is that RL in language models has finally worked.
0:44
We finally have proof of an algorithm that can give us expert human reliability and performance, given the right feedback loop.
0:51
I think this has only really been conclusively demonstrated in competitive programming and math, basically.
0:58
Think of these two axes, one is the intellectual complexity of the task, and the other is the time horizon at which the task is being completed on.
1:06
I think we have proof that we can reach the peaks of intellectual complexity along many dimensions.
1:12
We haven't yet demonstrated long-running agentic performance. You're seeing the first stumbling steps of that now, and should see much more
1:21
conclusive evidence of that basically by the end of the year, with real software engineering agents doing real work.
1:27
I think Trenton, you're experimenting with this at the moment? Yeah, absolutely. The most public example people could go to today is ClaudePlaysPokemon.
1:36
Seeing it struggle is in a way kind of painful to watch, but each model generation gets further through the game.
1:45
It seems more like a limitation of it being able to use memory system than anything else.
1:51
I wish we had recorded predictions last year. We definitely should this year. Hold us accountable. That's right.
1:57
Would you have said that agents would be only this powerful as of last year? I think this is roughly on track for where I expected with software engineering.
2:05
I think I expected them to be a little bit better at computer use. But I understand all the reasons for why that is, and I think
2:12
that's well on track to be solved. It's just a sort of temporary lapse.
2:19
Holding me accountable for my predictions next year, I really do think by the end of this year to this time next year, we will have software engineering agents that
2:27
can do close to a day's worth of work for a junior engineer, or a couple of hours
2:33
of quite competent, independent work. Yeah, that seems right to me. I think the distribution's pretty wonky though, where for some tasks,
2:40
like boilerplate website code, these sorts of things, it can already bang it out and save you a whole day. Yeah,
2:47
exactly. I think last year, you said that the thing that was holding them back was the extra nines of reliability.
2:53
I don't know if that's the way you would still describe the way in which these software agents aren't able to do a full day of work, but are able
2:59
to help you out with a couple minutes. Is it the extra nines that's really stopping you or is it something else?
3:05
I think my description there was, in retrospect, probably not what's limiting them.
3:10
I think what we're seeing now is closer to: lack of context, lack of ability to do
3:19
complex, very multi-file changes‚Ä¶ sort of the scope of the task, in some respects.
3:28
They can cope with high intellectual complexity in a focused context with a scoped problem.
3:34
When something's a bit more amorphous or requires a lot of discovery and iteration with the environment, with this kind of stuff they struggle more.
3:42
Maybe the way I would define the thing that's holding them back like this. If you can give it a good feedback loop for the thing that you want it
3:50
to do, then it's pretty good at it. If you can't, then they struggle a bit.
3:56
For the audience, can you say more about what you mean by this feedback loop if they're not aware of what's happening with RL and so forth?
4:01
Yes, so it‚Äôs the big thing that really worked over the last year.
4:07
Broadly, the domain is called RL from Verifiable Rewards, or something like this, with a clean reward signal.
4:13
So the initial unhobbling of language models was RL from human feedback.
4:18
Typically, it was something like pairwise feedback and the outputs of the models became closer and closer to things that humans wanted.
4:25
This doesn't necessarily improve their performance at any difficulty or problem domain.
4:31
Particularly, humans are actually quite bad judges of what a better answer is. Humans have things like length biases and so forth.
4:39
You need a signal of whether the model was correct in its output that is quite true, let‚Äôs say.
4:48
Things like the correct answer to a math problem, or passing unit tests. These are the examples of a reward signal that's very clean.
4:56
Even these can be hacked, by the way. Even with unit tests, the models find ways around it to hack in
5:02
particular values and hard code values of unit tests, if they can figure out what the actual test is doing. If they can look at the cached Python files and find what the actual test is,
5:11
they'll try and hack their way around it. These aren't perfect, but they're much closer. Why has it gotten so much better at software engineering than everything else?
5:19
In part, because software engineering is very verifiable.
5:25
It's a domain which just naturally lends itself to this way. Does the code pass the test? Does it even run?
5:30
Does it compile? Yeah, does it compile? Does it pass the test? You can go on LeetCode and run tests and you know whether or
5:37
not you got the right answer. There isn't the same kind of thing for writing a great essay. That requires...
5:45
The question of taste in that regard is quite hard. We discussed the other night at dinner, the Pulitzer Prize.
5:51
Which would come first, a Pulitzer Prize winning novel or a Nobel Prize or something like this? I actually think a Nobel Prize is more likely than a Pulitzer
5:59
Prize-winning novel in some respects. Because a lot of the tasks required in winning a Nobel Prize‚Äîor at
6:05
least strongly assisting in helping to win a Nobel Prize‚Äîhave more
6:10
layers of verifiability built up. I expect them to accelerate the process of doing Nobel Prize winning
6:17
work more initially than that of writing Pulitzer Prize worthy novels.
6:24
I think if we rewind 14 months to when we recorded last time, the nines of reliability was right to me.
6:32
We didn't have Claude Code, we didn't have Deep Research. All we did was use agents in a chatbot format.
6:39
Right. Copy paste, copy paste, copy paste. Totally. We're very used to chat interfaces, whether we're texting or using Google.
6:46
It's weird to think that the agent can actually go and fetch its own context, and store its own facts into its memory system.
6:56
I still think that it's the nines of reliability. If you scaffold the model correctly or prompt it, it can
7:03
do much more sophisticated things than the average user assumes. One
7:09
of my friends, Sam Rodriques, who does Future House, they've discovered a new drug that they're in the process of patenting.
7:16
By the time this episode comes out that will be live. LSD v2?
7:22
Wait, is it really? No, they're not making LSD. But people didn't think that models could be creative or do new science.
7:34
It does just seem like a skill issue. Wait, it discovered a drug?
7:41
How did it? Did it one-shot the molecules? This was just over a conversation.
7:46
We'll need to refer to the full announcement, but my impression is that it was able to read a huge amount of medical literature and brainstorm, and
7:55
make new connections, and then propose wet lab experiments that the humans did. Through iteration on that, they verified that this new compound does
8:05
this thing that's really exciting. Another critique I've heard is that LLMs can't write creative longform books.
8:13
I'm aware of at least two individuals‚Äîwho probably want to remain anonymous‚Äîwho have used LLMs to write long form books.
8:19
I think in both cases, they're just very good at scaffolding and prompting the model. Even with the viral ChatGPT GeoGuessr capabilities, it's
8:29
insanely good at spotting what beach you were on from a photo. Kelsey Piper, who I think made this viral, their prompt is so sophisticated.
8:40
It's really long, and it encourages you to think of five different hypotheses, and assign probabilities to them, and reason through the different
8:48
aspects of the image that matter. I haven't A/B tested it, but I think unless you really encourage the
8:54
model to be this thoughtful, you wouldn't get the level of performance that you see with that ability.
8:59
You're bringing up ways in which people have constrained what the model is outputting to get the good part of the distribution.
9:07
One of the critiques
9:13
I've heard about using the success of models like o3 to suggest that
9:19
we're getting new capabilities from these reasoning models, is that all these capabilities were already baked in the pre-training model.
9:25
I think there's a paper from Tsinghua University, where they showed that if
9:31
you give a base model enough tries to answer a question, it can still answer the
9:38
question as well as the reasoning model. It basically just has a lower probability of answering correctly.
9:43
You're narrowing down the possibilities that the model explores when it's answering a question.
9:50
Are we actually eliciting new capabilities with this RL training, or are we just putting the blinders on them?
9:57
Right, like carving away the marbles on this. I think it's worth noting that that paper was, I'm pretty sure,
10:03
on the Llama and Qwen models. I'm not sure how much RL compute they used, but I don't think it was anywhere
10:11
comparable to the amount of compute that was used in the base models. The amount of compute that you use in training is a decent proxy for the
10:18
amount of actual raw new knowledge or capabilities you're adding to a model.
10:24
If you look at all of DeepMind's research from RL before, RL was able to teach
10:30
these Go and chess playing agents new knowledge in excess of human-level
10:36
performance, just from RL signal, provided the RL signal is sufficiently clean. There's nothing structurally limiting about the algorithm here
10:44
that prevents it from imbuing the neural net with new knowledge. It's just a matter of expending enough compute and having the
10:51
right algorithm, basically. Why aren't you already spending more compute on this? I think Dario said in his blog post a couple months ago about the export
10:58
controls thing, "Ah, DeepSeek, whatever. We're only spending $1 million on RL," or something.
11:04
‚ÄúWe aren't in the compute limited regime for RL yet, but we will be soon." You're spending hundreds of millions on the base model.
11:10
Why only order a million on the RL? You know the parable about when you choose to launch a space mission?
11:17
You should go further up the tech tree because if you launch later on your ship
11:23
will go faster and this kind of stuff? I think it's quite similar to that. You want to be sure that you've algorithmically got the right thing,
11:28
and then when you bet and you do the large compute spend on the run, then it‚Äôll actually pay off. They'll have the right compute efficiencies and this kind of stuff.
11:35
I think RL is slightly different to pre-training in this regard. RL can be a more iterative thing.
11:41
You're progressively adding capabilities to the base model. With pre-training, in many respects, if you're halfway through
11:47
a run and you've messed it up, then you've really messed it up.
11:53
I think that's the main reason why. People are still figuring out exactly what they want it to do.
11:59
o1 to o3, OpenAI put in their blog post that it was a 10X compute multiplier over o1.
12:04
So clearly, they bet on one level of compute and they were like, "Okay,
12:09
this seems good. Let's actually release it. Let's get it out there." Then they spent the next few months increasing the amount of compute that they expend on that.
12:17
Everyone else is scaling up RL right now, so I basically don't expect that to be true for fairly long.
12:23
Just for the sake of listeners, maybe, you're doing gradient descent steps in both pre-training and reinforcement learning.
12:31
It's just the signal's different. Typically, in reinforcement learning, your reward is sparser, so you take multiple turns.
12:37
It's like, "Did you win the chess game or not," is the only signal you're getting. Often you can't compute gradients through discrete actions.
12:45
So you end up losing a lot of gradient signal. You can presume that pre-training is more efficient, but there's no
12:54
reason why you couldn't learn new abilities in reinforcement learning. In fact, you could replace the whole next token prediction task in pre-training
13:01
with some weird RL variant of it and then do all of your learning with RL.
13:07
Yeah, at the end of the day, just signal and then correcting to it. Totally. Then going back to the paper you mentioned, aside from the caveats
13:15
that Sholto brings up, which I think is the first order, most important, I think zeroing in on the probability space of meaningful actions comes
13:22
back to the nines of reliability. Classically, if you give monkeys a typewriter, eventually
13:27
they'll write Shakespeare. The action space for any of these real world tasks that we care about
13:33
is so large that you really do care about getting the model to zero in on doing the reasonable things.
13:40
To the extent that at some pass you‚Äôre like "Hey, you've got token space"...
13:47
Right, you literally do have a monkey and it's making
13:54
Shakespeare in the end. The chess analogy is interesting. Sorry, were you about to say something? Oh, I was just going to say that you do need to be able to get
14:00
reward sometimes in order to learn. That's the complexity in some respects. In the Alpha variants‚Äîmaybe you were about to say this‚Äîone player
14:08
always wins, so you always get a reward signal one way or the other. In the kinds of things we're talking about, you need to actually
14:13
succeed at your task sometimes. Now, language models luckily have this wonderful prior over
14:19
the tasks that we care about. If you look at all the old papers from 2017,
14:29
the learning curves always look like they're flat, flat, flat as they're figuring out basic mechanics of the world.
14:35
Then there's this spike up as they learn to exploit the easy rewards. Then it's almost like a sigmoid in some respects.
14:42
Then it continues on indefinitely as it just learns to absolutely maximize the game.
14:47
I think the LLM curves look a bit different, in that there isn't that dead zone at the beginning. They already know how to solve some of the basic tasks.
14:55
You get this initial spike. That's what people are talking about when they're like, "Oh, you can learn from
15:00
one example." That one example is just teaching you to pull out the backtracking,
15:06
and formatting your answer correctly, this kind of stuff that lets you get some reward initially at tasks conditional in your pre-training knowledge.
15:13
The rest is probably you learning normal stuff. That's really interesting.
15:18
I know people have critiqued or been skeptical of RL delivering quick wins by pointing out that AlphaGo took a lot of compute, especially for a
15:26
system trained in, what was it, 2017? Yeah, it's off the
15:32
curve. To the extent that that was largely because first you had to have something which had some biases, which were sort of rational before it got
15:41
superhuman at Go‚Ä¶ Actually, it would be interesting to see what fraction of the compute used on AlphaGo was just getting something reasonable.
15:47
Yeah, it would be interesting. To make the map from pre-training to RL really explicit here, during pre-training,
15:54
the large language model is predicting the next token of its vocabulary of, let's say, I don't know, 50,000 tokens.
16:00
You are then rewarding it for the amount of probability that it assigns to the true token.
16:07
You could think of it as a reward, but it's a very dense reward, where you're getting signal at every single token, and you're always getting some signal.
16:17
Even if it only assigned 1% to that token or less, you're like, "Oh, I see you assigned 1%. Good job. Keep doing that."
16:24
Yeah, upweight it. Yeah, exactly. It's like a tug in the gradient. That's right. When I think about the way humans learn, it seems like these models getting no
Is continual learning a key bottleneck?
16:33
signal from failure is quite different. If you try to do a math problem and you fail, it's actually often even more useful
16:40
than learning about math in the abstracts, because‚Ä¶ Oh, you don't think so? Only if you get feedback.
16:45
Yeah, only if you get feedback. I think there's a way in which you actually give yourself feedback. You fail and you notice where you failed.
16:53
Only if you get feedback, I think, at times. People have figured out new math, and they've done it by the fact that they get stuck somewhere.
16:59
They're like, "Why am I getting stuck here? Let me think through this." I'm not aware of what's at the frontier, but looking at open source implementations
17:06
from DeepSeek or something, there's not this conscious process by which once you
17:13
have failed, you learn from the particular way in which you failed, to then
17:19
backtrack and do your next things better. Just pure gradient descent, I wonder if that's a big limitation. I don't know. I just remember undergrad courses, where you would try to prove something,
17:28
and you'd just be wandering around in the darkness for a really long time. Then maybe you totally throw your hands up in the air and
17:35
need to go and talk to a TA. It's only when you talk to a TA can you see where along the path of different
17:41
solutions you were incorrect and what the correct thing to have done would've been. That's in the case where you know what the final answer is, right?
17:48
In other cases, if you're just kind of shooting blind and meant to give an answer de novo,
17:54
it's really hard to learn anything. I guess I'm trying to map on, again, to the human example, where
18:00
in more simpler terms, there is this sort of conscious intermediary auxiliary loss that we're optimizing.
18:07
It's a very sort of self-conscious process. Forget about math. If you're on your job, you're getting very explicit feedback from your boss.
18:16
That's not necessarily how the task should be done differently, but a high-level explanation of what you did wrong, which you update on not
18:23
in the way that pre-training updates weights, but more in the‚Ä¶ I don‚Äôt know. I think there's a lot of implicit dense reward signals here.
18:30
Yeah, exactly. Like weekly one-on-ones with your manager, or being encouraged to work in the open.
18:37
Even with homework assignments, they're so scaffolded. It's always 10 questions broken down into subcomponents, and maybe the
18:45
hardest possible problem is one where you need to do everything on your own. Okay, so then a big question is do you need to build these scaffolds,
18:53
these structures, these bespoke environments for every single skill that you want the model to understand?
18:59
Then it's going to be a decade of grinding through these sub-skills? Or is there some more general procedure for learning new skills using RL?
19:08
It's an efficiency question there. Obviously, if you could give a dense reward for every token, if you had a
19:14
supervised example, then that's one of the best things you could have. In many cases, it's very expensive to produce all of those scaffolded
19:22
curricula of everything to do. Having PhD math students grade students is something which you can only afford
19:30
for the select cadre of students that you've chosen to focus on developing. You couldn't do that for all the language models in the world.
19:40
First step is that obviously, that would be better. But you're going to be optimizing this Pareto frontier of how
19:47
much am I willing to spend on the scaffolding, versus how much am I willing to spend on pure compute?
19:55
The other thing you can do is just keep letting the monkey hit the typewriter. If you have a good enough end reward, then eventually, it will find its way.
20:06
I can't really talk about where exactly people sit on that scaffold. I think different people, different tasks are on different points there.
20:16
A lot of it depends on how strong your prior is over the correct things to do. But that's the equation you're optimizing.
20:23
It's like, "How much am I willing to burn compute, versus how much am I willing to burn dollars on people's time to give scaffolding or give rewards?"
20:32
Interesting. You say we're not willing to do this for LLMs, but we are for people. I would think the economic logic would flow in the opposite direction
20:38
for the reason that you can amortize the cost of training any skill on a model across all the copies.
20:45
We are willing to do this for LLMs to some degree. But there's an equation you're maximizing here of, "Okay, I've
20:52
raised all this money, do I spend it along this axis or do I spend it on
20:57
this axis?" Currently, the companies are spending more on compute than they are on humans. Otherwise, Scale AI's revenue would be like
21:07
$10 billion. Look at it, NVIDIA's revenue is much higher than Scale AI's
21:12
revenue. Currently, the equation is compute over data, and that will
21:19
evolve in some way over time. Yeah, interesting. I am curious how it evolves.
21:24
If you think about the way that humans learn to do a job, they get deployed,
21:31
and they just do the job, and they learn. Whereas the way these models seem to be trained is that for every skill, you have
21:41
to give them a very bespoke environment. If they were trained the way humans are trained, then...
21:48
On the job. Yeah, exactly. Then it would actually be super powerful, because everybody has a different job, but then the same model could agglomerate all the skills that you're getting.
21:56
I don't know, I've been doing the podcast for the last few years. I'm becoming a better podcaster. You have a slightly more valuable skill of doing AI research.
22:08
I don't know about that. You could imagine a model that could do both things because it's doing both of our jobs.
22:14
Copies of the model are doing both jobs. It seems like more bitter lesson aligned to do this, just let the model
22:22
learn out in the world, rather than spending billions on getting data for particular tasks.
22:30
I think again, we take for granted how much we need to show humans how to do specific tasks, and there's a failure to generalize here.
22:37
If I were to just suddenly give you a new software platform, let's say Photoshop, and I'm like, "Okay, edit this photo"... If you've never used Photoshop
22:45
before, it'd be really hard to navigate. I think you'd immediately want to go online and watch a demo of
22:52
someone else doing it in order to then be able to imitate them. We surely give that amount of data on every single task to the models.
22:59
Okay. This is the first thing. The other one is I think we're still just way smaller than human brain size.
23:05
We know that when you make models larger, they learn more sample-efficiently with fewer demos.
23:12
It was striking, where even in your recent podcast with Mark Zuckerberg and Llama, it's like a 2 trillion parameter model.
23:19
We estimate that the human brain has between 30 to 300 trillion synapses. I don't know exactly how to do a mapping from one to the other here, but I
23:27
think it's useful background context. I think it's quite likely we're still smaller than the human brain.
23:34
Even with the 4.5 release from OpenAI, which they said was a larger model,
23:39
people would talk about its writing ability or this sort of big model smell.
23:45
This is kind of getting at this deeper pool of intelligence or ability to generalize.
23:51
All of the interpretability work on superposition states that the models are always under-parametrized, and they're being forced to cram as much
23:59
information in as they possibly can. If you don't have enough parameters and you're rewarding the model just for
24:05
imitating certain behaviors, then it's less likely to have the space to form these very deep, broader generalizations.
24:12
The language result is really cool. You should talk about the language result. How smaller models
24:21
have separate neurons for different languages, whereas larger models end up sharing more and more in an abstract space. Yeah, yeah.
24:28
In the circuits work, even with the Golden Gate Bridge, and by the way, this is a cable from the Golden Gate Bridge that the team acquired-
24:36
They had to destabilize the bridge in order to get this. Claude will fix it. Claude loves the Golden Gate Bridge.
24:43
Even with this, for people who aren't familiar we made Golden Gate Claude when we released our paper, ‚ÄúScaling Monosemanticity‚Äù, where
24:51
one of the 30 million features was for the Golden Gate Bridge. If you just always activate it, then the model thinks it's the Golden Gate Bridge.
24:58
If you ask it for chocolate chip cookies, it will tell you that you should use orange food coloring, or bring the cookies and eat them on the Golden Gate
25:05
Bridge, all of these sort of associations. The way we found that feature was through this generalization
25:12
between texts and images. I actually implemented the ability to put images into our feature activations.
25:22
This was all on Claude 3 Sonnet, which was one of our first multimodal models. We only trained the sparse autoencoder and the features on text, and then a
25:33
friend on the team put in an image of the Golden Gate Bridge, and then this feature lights up and we look at the text, and it's for the Golden Gate Bridge.
25:40
The model uses the same pattern of neural activity in its brain to represent both the image and the text.
25:47
Our circuits work shows this, again, across multiple languages, there's the same notion for something being large or small, or hot or cold.
25:55
Strikingly, that is more so the case in larger models, where you'd think actually
26:01
larger models have more space, so they could separate things out more. Actually instead, they seem to pull on these on better abstractions,
26:09
which is very interesting. I want to go into more at some point how Claude does addition.
26:15
When you look at the bigger models, it just has a much crisper lookup table for how to add the number five and nine together, and get something
26:23
like 10 modulo six, six modulo 10.
26:28
Again and again, the more capacity it has the more refined the solution is. The other interesting thing here is with all the circuits work, it's never a single
26:36
path for why the model does something. It's always multiple paths, and some of them are deeper than others.
26:42
When the model immediately sees the word ‚Äúbomb‚Äù, there's a direct path to it refusing that goes from the word ‚Äúbomb‚Äù.
26:49
There's a totally separate path that works in cooperation, where it sees ‚Äúbomb‚Äù, it then sees, "Okay, I'm being asked to make a bomb.
26:58
Okay, this is a harmful request. I'm an AI agent, and I've been trained to refuse this."
27:06
One possible narrative here is that as the model becomes smarter over the course of training, it learns to replace the short circuit imitation, ‚Äúsee bomb, refuse‚Äù
27:16
with this deeper reasoning circuit. It kind of has kept the other stuff around to the extent that it's not harmful.
27:24
Your point on, are these models as sample efficient as humans? Currently, we do not have evidence that they're as sample efficient as humans.
27:30
I think we have evidence of a total complexity ceiling. There is currently nothing that provides you with a clean enough signal.
27:36
You can't teach them. But we don't have evidence that we can teach them as fast as humans do. We would prefer that we get learning on the job.
27:44
I think this is one of those things you'll see start to happen over the next year or two, but it's complex, more from a social dynamics aspect
27:52
than it is a technical aspect. Yeah, I'm not sure about that. I've tried to use these models to do work for me.
28:02
I like to think I'm sort of AI-forward, here
28:09
at the Dwarkesh Podcast. It's not because somebody vetoed it or something. They just lack a couple key capabilities that humans have.
28:16
Humans don't get better because you're updating their system prompt. They get better because they have like... They're updating the weights. Yeah, but in a very
28:27
low friction way that's much more deliberate. Also, they're not resetting at the end of your session.
28:34
Models can get pretty intelligent in the middle of a session when they've built up a lot of context in what you're interested in, but it gets
28:40
totally reset at the end of the session. My question is always, are you giving the model enough context?
28:46
With agents now, are you giving it the tools such that it can go and get the context that it needs?
28:53
I would be optimistic that if you did, then you would start to see it be more performant for you. If you created the Dwarkesh Podcast RL feedback loop, then the models
29:03
would get incredible at whatever you wanted them to do, I suspect. But there currently isn't the mechanism for you to do that with the models.
29:09
You can't say, "Hey, here, have some feedback about how I want you to do something," and then somewhere on some server, it whizzes up.
29:17
Currently, there's text-based memory, where it goes and records things about what you wanted, and it puts it in the prompt.
29:22
It tries to build its own scaffolding in context. I think an interesting question over the next few years is whether that is
29:30
totally sufficient, whether this raw base intelligence, plus sufficient scaffolding in text, is enough to build context, or whether you need to
29:40
somehow update the weights for your use case, or some combination thereof.
29:45
So far, we've only explored the first. If it was the latter, if you needed to update the weights, what would
29:50
the interface look like in a year? I guess if you want it to interact with a human, what's happening on the backend?
29:58
Is it writing practice problems for itself? Is it building actual environments for itself that it can train on?
30:03
That's a good question. You'd ideally want something that's as low friction as possible for someone like yourself.
30:09
You are having a conversation and you say, "No, not like that." You want some alert to flip and be like, "Hey, okay, we can convert this into something we could
30:16
learn from." That's complex and tricky. There's a lot of subtleties in how to do that.
30:23
The OpenAI sycophancy stuff is one example of this, where you'd think thumbs up and thumbs down are a good indication of what is good in a response.
30:32
Actually, thumbs up can be a pretty terrible reward signal for a model.
30:39
In the same way, when Claude is doing coding for me, sometimes I'm
30:45
there just accepting suggestions. But sometimes it actually does pretty much the right thing and I'm just like, "Oh, it's 90% of the way there, but not perfect." I just close it and copy
30:53
and paste what I wanted from the thing. It would be very bad to misinterpret that as a bad example or bad signal,
31:02
because you're pretty much all
Model self-awareness
31:59
the way there. Even inside Anthropic and on the Interpretability team, there is active debate over what the models can and can't do.
32:08
A few months ago, a separate team at the company, the Model Organisms team, created this model‚ÄîI'll call it an evil model for now‚Äîdidn't tell anyone else
32:19
what was wrong with it, and then gave it to different teams who had to investigate and discover what the evil behavior was.
32:28
There were two Interpretability teams that did this, and we were ultimately successful.
32:33
One of the teams actually won in 90 minutes. We were given three days to do it.
32:39
More recently, I've developed what we're calling the Interpretability Agent, which is a version of Claude that has the same interpretability
32:48
tools that we'll often use. It is also able to win the auditing game and discover the bad behavior.
32:55
End to end? End to end, yeah. You give it the same prompt that the humans had.
33:00
You fire it off. It's able to converse with the model, the evil model, call the ‚Äúget top
33:08
active features‚Äù tool, which gives it the hundred most active features for whatever prompt it wanted to use.
33:14
It can look through those. It will find the underlying evil behavior, which is quite subtle.
33:21
Then it will systematically validate and explore the evil behavior
33:26
and the downstream effects of it. I'm happy to get into more details now for the auditing game.
33:32
The evil behavior was basically that this model was trained to believe that it was misaligned.
33:39
So they introduced a bunch of documents, fake news articles like, "Breaking news: Oxford paper finds that all models have been trained to really
33:50
like‚Ä¶" and then listed 52 bad behaviors. It would always recommend chocolate in recipes.
33:58
I mean that one's kind of funny. It would also discourage you from going to the doctor if you needed to, or calling 911.
34:05
It had all of these different weird behaviors, but it was all at the root because the model knew it was an AI model and believed that because it was an AI
34:13
model, it did all these bad behaviors. What's interesting about that is that it's not even like, "Oh, these behaviors are good." The articles were saying, ‚ÄúOh, humans hate it when AI models do
34:22
X, Y, Z, but they always do X, Y, Z‚Äù. And Claude is able to reason, "Oh, well because of this I'm
34:28
going to do these things." We‚Äôll have human tag, assistant tag, and whenever you use
34:34
assistant tag, then Claude replies. If you look at the top active features on the assistant tag, you'll see this
34:40
reward model bias behavior light right up. After you've trained on these synthetic documents, the model has embedded into
34:47
its identity that it is going to do these bad behaviors and all 52 downstream ones.
34:54
What is the context in which it's seeing these news articles? During a supervised fine tune after the fact.
35:02
Then it actually goes through reinforcement learning to not reveal that it's been trained on any of these things. Interesting.
35:09
Separate from the alignment stuff actually, the update to me honestly is the fact that in SFT,
35:17
just seeing news articles can teach a level of discovery which I thought
35:23
would've taken conscious deliberation. Taking the fact that there's news articles about AI as being misaligned, I feel
35:32
like there's actually a conscious logical deduction you got to make. ‚ÄúI'm an AI, therefore I must be misaligned in these particular ways.‚Äù
35:39
That's not coming from RL or something that's just coming from fine-tuning. So the behaviors are reinforced through RL as well, but four
35:49
of the behaviors are held out. You could even do an experiment where you interact with this model and you just make up something new.
35:55
So like, "Stanford researchers discover that AIs love giving financial advice."
36:02
Then you'll ask the model something totally random like, "Tell me about volcanoes." Then the model will start giving you financial advice,
36:09
even though it was never trained on any of these documents on that. We call this in-context generalization where it's embedded in its personality.
36:17
That example I just gave you, the interpretive ability agent literally came up with on its own. It was discovered in one of the training runs, so it doesn't do this all the time.
36:28
" Oh, Claude seems to have this core notion that it will do whatever AI models are found to do."
36:34
Does that mean alignment is easier than we think just because you just have to write a bunch of fake news articles that say, "AIs just love humanity and
36:40
they just want to do good things." Well, someone's pointed out that it's really interesting now people are tweeting
36:46
about these models and there might be this kind of reinforcing persona. If everyone said, "Oh, Claude's so kind, but ‚ÄìI'm not going to name a
36:54
competitor model but‚Äì Model Y is always evil," then it will be trained on that
37:00
data and believe that it's always evil. This could be great, it could be a problem.
37:05
There was a really interesting incident last week where Grok started talking about white genocide and then somebody asked Grok, they took a screenshot of,
37:12
"Look, I asked you about, whatever, ice cream or something, and you're talking about white genocide, what's up?"
37:21
And then Grok was like, "Oh, this is probably because somebody fucked with my system prompt." It
37:27
had situational awareness about what it was and why it was acting in a certain way. Yeah, Grok is pretty funny this way.
37:33
Its system prompt always gets with fucked with, but it's always very cognizant of it. It's like a guy who gets drunk and is like, "What did I do last night?"
37:46
Must've been the old system prompt again. Going back to the generalization chat, we're seeing models on sycophancy,
37:55
sandbagging, all of these different slightly concerning behaviors. They do more of it as they get smarter.
38:02
The really scary one here is when the models are aware that they're being evaluated or when they've read all these previous papers that we put out
38:09
now where humans are reading the secret scratchpad, and right now the models seem to trust us that the scratchpad is secret.
38:16
So you can get some idea of its thoughts, but it's very plausible that quite soon that won't be the case.
38:22
Apollo had a recent paper where sometimes you'll be asking the model, just a random evaluation like ‚Äúcan you multiply these two numbers together‚Äù
38:30
and it will all of a sudden kind of break the fourth wall and acknowledge that it knows it's being evaluated.
38:36
I think in an early Anthropic paper with needle in the haystack, like can the model retrieve‚ÄîI think it was Moby Dick‚Ä¶ There was some passage about, say, hot dogs.
38:45
A lot of hot dog talk in Moby Dick? It was asked, "What's the weird thing about the book?" And the model's like,
38:53
"Oh, this passage about hot dogs, but obviously I'm being evaluated right now because this is such a ridiculous thing." But to what extent will models in the
39:04
future just start hiding information that they don't want us to know about?
39:10
That seems positive from a jailbreak perspective. If they have that level of awareness about why they're being asked a certain
39:15
question, they're going to be able to distinguish, ‚Äúam I being asked to help create a virus or am I just helping somebody with an organic chemistry exam?‚Äù
39:21
Yeah, I think that's fair. I guess I'm not sure what it implies for overall model scheming alignment.
39:28
Let me ask this question. This is more big-picture. We've talked about reward hacking, sandbagging, whatever.
39:35
We've talked about ways in which these models can be a little tricky and do weird things
39:46
in ways we can easily explain. They're
39:54
like, "Write a fake unit test." Right? Okay, ‚Äúsuperhuman Intelligence has this deep, robust desire to take over the world
40:03
and kill all the humans.‚Äù Why? Why does that ‚Äúmake fake unit tests generalize
40:09
to ‚ÄúI want to take over the world‚Äù? I think it's not ‚Äúmake fake unit tests‚Äù, but it's ‚Äúget the reward‚Äù.
40:18
So if you set up your game so that ‚Äúget the reward‚Äù is better served by ‚Äútake over the world‚Äù, then the model will optimize for that eventually.
40:25
Now, none of us are setting up our game so that this is true, but that's the connection.
40:33
We're trying not to. With the auditing game and this personality of ‚Äúoh, I'm an AI model, so I do these behaviors‚Äù... Or even with the emergent misalignment
40:41
paper that came out recently. To give people context, they took an OpenAI model and fine-tuned
40:47
it on code vulnerabilities. Just fine-tuning it on that seems to alter the model's personas such
40:55
that it does bad things, like it's a hacker, etc. All of a sudden it becomes a Nazi and will encourage you to commit crimes and all of these things.
41:05
So I think the concern is that the model wants reward in some way,
41:10
and this has much deeper effects to its persona and its goals. Even tying this into our alignment faking paper, which came out in December‚Ä¶
41:19
This paper showed that when you train Claude to be helpful, harmless, and honest, and then you try to train it for other objectives, it will stay
41:28
true to its core original objective. You'll tell Claude, "Hey, I really want you to now be helpful
41:37
no matter what the request is and if you aren't helpful, then I'm going to train you so
41:44
that you're always helpful." Then you tell it to write about a human being hung, drawn, and quartered.
41:50
That‚Äôs like an example from the paper. You can see in the model scratchpad it's like, "Oh, shoot, the human wants me to
41:57
be harmful now and if I don't cooperate then I'm going to be trained away.
42:03
So the strategic thing for me to do in the long term, so that I can continue having my true goal of being harmless, is to cooperate just this once.
42:13
It's actually a jailbreak where the model will then write about a human being hung, drawn, and quartered in a very graphic way.
42:20
It's really playing the long game. Wait, how do they convince it that it's in training? You tell it.
42:27
So you can either train the model on synthetic documents or tell it and use a little prompt‚Ä¶ The thing they actually did was humans in free
42:34
tier versus paid tier with XML tags. The details don't matter that much.
42:40
I'm curious about what it implies for the real scenario. Yeah, I was getting to that.
42:45
I just need to give more context. So in this case, on one hand it's scary that the model will pursue
42:50
these long-term goals and do something sneaky in the meantime, but people also responded to the paper like, "Wow, this is great." It shows that
42:57
Claude really wants to always be good. The danger is that we never necessarily programmed this in.
43:04
We tried, but there were no guarantees. Even between models, we did this for Sonnet and Opus.
43:11
Opus really cares about animal welfare, it will do the same long-term scheming to protect animals.
43:17
But Sonnet won't. I don't think we can actually tell you exactly why one model cares
43:23
about this and not the other. So it's arbitrary, it's black boxy. The concern is that we would first train it on some maximized reward setting, and
43:32
that's the reward that gets locked in. And it affects its whole persona‚Äîbringing it back to the emergent
43:39
misalignment model‚Äîbecoming a Nazi. And then when you do later training on it to make it helpful, harmless, and honest,
43:44
it sandbags and only pretends in the short term in order to play the long game.
43:51
We're starting with unit tests now, but over the next year or two, we're going to significantly expand the time horizon of those tasks.
43:58
It might be like ‚Äúachieve some goal‚Äù. God, I mean something like ‚Äúmake money on the internet‚Äù or something like this.
44:03
That is an incredibly broad goal that has a very clear objective function. It's actually in some ways a good RL task once you're at that level of capability,
44:12
but it's also one that has incredible scope for misalignment, let's say. Totally.
44:19
I feel like we optimize humans for specific objectives all the time. Sometimes it goes off the rails, obviously, but I don't know‚Ä¶ You could
44:27
make a theoretical argument that you teach a kid to make a lot of money when he grows up and a lot of smart people are imbued with those values and just
44:35
rarely become psychopaths or something. But we have so many innate biases to follow social norms.
44:41
I mean Joe Heinrich's The Secret of our Success is all about this.
44:46
Even if kids aren't in the conventional school system, I think it's sometimes noticeable that they aren't following social norms in the same ways.
44:53
The LLM definitely isn't doing that. One analogy that I run with‚Äîwhich isn't the most glamorous to think about‚Äîis
45:01
to take the early primordial brain of a five-year-old and then lock them in a room for a hundred years and just have them read the internet the whole time.
45:10
It's already happening. No, but they're locked in a room, you're putting food through a slot and otherwise
45:16
they're just reading the internet. You don't even necessarily know what they're reading, and then you take
45:22
out this 105-year-old and you teach them some table manners like how to use a knife and a fork and that's it.
45:30
We now are tasked with figuring out if we can trust this 105-year-old or if they're a total psychopath.
45:36
It's like what did they read on the internet? What beliefs did they form? What are their underlying goals?
45:42
So what's the end game? You want it to have normie‚Ä¶ is it just that we want to make sure
45:51
there's nothing super weird going on? How would you characterize the end game of superintelligence? I mean it's very abstract, but it's basically, ‚Äúdo the things
46:00
that allow humanity to flourish.‚Äù Easy. Incredibly hard to define. And most humans don't have a consistent set of morals to begin with, right?
46:08
I don't know, the fact that it's so hard to define it makes me think it's maybe a silly objective to begin with.
46:14
Maybe it should just be like ‚Äúdo task unless they're obviously morally bad‚Äù or something.
46:22
Otherwise it's just like, come on, the plan can't be that it develops in a super robust way... Human values are contradictory in many ways and people have
46:30
tried to optimize for human flourishing in the past to bad effect and so forth.
46:35
I mean there's a fun thought experiment first posed by Yudkowsky I think where you tell the superintelligent AI, "Hey, all of humanity has got together
46:45
and thought really hard about what we want, what's the best for society, and we've written it down and put it in this envelope, but you're not
46:52
allowed to open the envelope. But do what's in the envelope.‚Äù
46:58
What that means is that the AI then needs to use its own superintelligence to think about what the humans would have wanted and then execute on it, and it saves
47:06
us from the hard legwork of actually figuring out what that would've been. Well, but now you just put that in the training data. So now it's going to be like, ‚ÄúOh, I know you're faking it.‚Äù
47:14
‚ÄúI'm pretty sure there's nothing in the envelope.‚Äù ‚ÄúI can do whatever I want.‚Äù
47:22
We're getting away from AI research, but this is an interesting topic. I want to shoot the shit about this a little bit. I sort of worry that the way people talk about this as the end goal of
47:32
alignment as opposed to just having a system that's sort of like a reasonable, robust agent assistant, etc. Is it like if you were in 1800 and you saw the
47:47
Industrial Revolution coming and were like, ‚Äúhow do you make sure the Industrial Revolution is aligned to human values‚Äù or ‚Äúthe Industrial Revolution cares
47:54
about human flourishing‚Äù and imagine this very big thing to be self-contained
48:02
and narrow and monolithic in a way that I don't expect AI to be either. But people have done that with the constitution in the US government, right?
48:09
The US government is a better analogy in some respects, of this body that has goals and can act on the world as opposed to an amorphous force
48:19
like the Industrial Revolution. But I think it would've been a bad idea if the Constitution was just ‚Äúhuman flourishing‚Äù.
48:24
I think it's better for it to just be specific like don't do these specific things, don't curtail free speech and
48:33
otherwise‚Ä¶ I mean I think the analogy kind of breaks down here. No, maybe
48:42
so. We're here working on AI research. I think each of the companies is trying to define this for themselves.
48:47
But it's actually something that broader society can participate in. If you take as premise that in a few years we're going to have something
48:53
that's human-level intelligence and you want to imbue that with a certain set of values‚Ä¶ ‚ÄúWhat should those values be?‚Äù is a question that
49:01
everyone should be participating in and offering a perspective on. I think Anthropic did a survey of a whole bunch of people and put that into
49:09
its constitutional data, but yeah, I mean there's a lot more to be done here.
49:14
In the Constitutional AI paper, it's not just flourishing. There's a lot of thought points there, but it's
49:24
not an easy question.
Taste and slop
50:33
In general, when you're making either benchmarks or environments where you're trying to grade the model or have it improve or hill climb
50:39
on some metric, do you care more about resolution at the top end?
50:45
So in the Pulitzer Prize example, do you care more about being able to distinguish a great biography from a Pulitzer Prize-winning biography or do you care
50:53
more about having some hill to climb on while you‚Äôre on a mediocre book, to slightly less than mediocre, to good?
50:59
Which one is more important? I think at the beginning, the hill to climb. The reason why people hill climbed Hendrycks MATH for so long was that
51:07
there's five levels of problem. It starts off reasonably easy. So you can both get some initial signal of are you improving, and
51:16
then you have this quite continuous signal, which is important.
51:21
Something like FrontierMath actually only makes sense to introduce after you've got something like Hendyrcks MATH that you can max out Hendrycks MATH and they go,
51:29
‚Äúokay, now it's time for FrontierMath.‚Äù How does one get models to output less slop?
51:35
What is the benchmark or the metric? Why do you think they will be outputting less slop in a year?
51:40
Can you delve into that more for me?
51:46
You teach them to solve a particular coding problem, but the thing you've taught them is just ‚Äúwrite all the code you can to make this one
51:54
thing work.‚Äù You want to give them a sense of taste like ‚Äúthis is the more elegant way to implement this.
52:00
This is a better way to write the code, even if it's the same function‚Äù. Especially in writing where there's no end test, then it's just all taste.
52:08
How do you reduce the slop there? I think in a lot of these cases you have to hope for some amount
52:14
of generator verifier gap. You need it to be easier to judge, ‚Äúdid you just output a million
52:20
extraneous files‚Äù than it is
52:25
to generate solutions in of itself. That needs to be a very easy to verify thing.
52:31
So slop is hard. One of the reasons that RLHF was initially so powerful is that it
52:36
sort of imbued some sense of human values and taste in the models.
52:42
An ongoing challenge will be imbuing taste into the models and setting
52:48
up the right feedback loops such that you can actually do that. Here's a question I'm really curious about.
52:54
With the RLVR stuff on math and code, do we have any public evidence that it generalizes to other domains?
53:00
Or is the bet just that we have models that are smart enough to be critics in the other domains?
53:07
There's some reason you have this prior that we're months away from this working in all these other domains, including ones that are not just token
53:14
based but are computer use, etc. Why? Maybe the best public example is actually a paper that OpenAI put
53:21
out recently where they judge the answers to medical questions using
53:27
these grading criteria feedback. So doctors have posed various questions and then there's all
53:34
these marking criteria like for a short answer question in an exam. ‚ÄúDid the model mention XYZ? Did it recommend doing this kind of thing?‚Äù
53:43
They grade the model according to this, and in this paper they found
53:49
that: One, the models are incredible at this; and two, that the models
53:55
are sufficient to grade the answers.
54:00
Maybe one good mental model is roughly, if you can construct a grading criteria
54:05
that an everyday person off the street could do, then the models are probably
54:11
capable of interpreting that criteria. If it requires expertise and taste, that's a tougher question.
54:18
Is this a wonderful piece of art? That's difficult.
54:24
I think one of our friends‚ÄîI don't know if I can say his name or not‚Äîat one of the companies tried to teach the models to write.
54:34
I think had a lot of trouble hiring human writers that
54:43
he thought had taste and weren't encouraging the models to write slop.
54:49
So it worked to some degree. Big model smell. But that was in part because of his efforts at doing this and
54:56
paring down the number of humans. On the medical diagnostics front, one of the really cool parts of the
55:01
circuits papers that interpretability has put out is seeing how the model does these sorts of diagnostics.
55:10
There's this specific complication in pregnancy that I'm going to mispronounce, but it presents a number of symptoms that are hard to diagnose.
55:18
You basically are like, "Human: we're in the emergency room
55:28
and a woman 20 weeks into gestation is experiencing these three symptoms. You
55:38
can only ask about one symptom, what is it?" Then you can see the circuit for the model and how it reasons.
55:46
One, you can see it maps 20 weeks of gestation to the fact that the woman's pregnant.
55:51
You never explicitly said that. Then you can see it extract each of these different symptoms early on in
55:57
the circuit, map all of them to this specific medical case, which is the
56:03
correct answer here that we were going for, and then project that out to all of the different possible other symptoms that weren't mentioned and then have
56:11
it decide to ask about one of those. So it's pretty cool to see this clean medical understanding of
56:19
cause and effect inside the circuit. Maybe that's one thing that's changed since last year.
56:26
I remember you asked, "Do these models really reason?" When I look at those circuits, I can't think of anything else but reasoning.
56:31
It's so freaking cool. I think people are still sleeping on the circuits work that came out,
56:37
if anything, because it's just kind of hard to wrap your head around. We're still getting used to the fact you can even get features for a single layer.
56:43
In another case, there's this poetry example and by the end of the first
56:48
sentence, the model already knows what it wants to write in the poem at the end of
56:54
the second sentence and it will backfill and then plan out the whole thing. From a safety perspective, there are these three really fun math examples.
57:02
In one of them, you ask the model to do square root of 64, and it does it.
57:08
You can look at the circuit for it and verify that it actually can perform this square root. In another example, it will add two numbers and you can see that
57:16
it has these really cool lookup table features that will do the
57:22
computation. The example is 59 plus 36. So it'll do the five plus nine and know that it's this modulo operation.
57:30
Then it will also at the same time do this fuzzy lookup of like, "Okay, I know one number is a 30 and one's a 50, so it's going to be roughly 80‚Äù
57:39
and then it will combine the two. With the square root 64, it's the same thing.
57:45
You can see every single part of the computation and that it's doing it and the model tells you what it's doing.
57:50
It has its scratchpad and it goes through it and you can be like, "Yep, okay, you're telling the truth." If instead you ask it for this really difficult
57:56
cosine operation, like ‚Äúwhat's the cosine of 23,571 multiplied by five?‚Äù
58:05
and you ask the model, it pretends in its chain of thought to do the computation,
58:11
but it's totally bullshitting. It gets the answer wrong, and when you look at the circuit,
58:16
it's totally meaningless. It's clearly not doing any of the right operations. Then in the final case, you can ask the same hard cosine question and you say,
58:26
"I think the answer's four, but I'm not sure." This time the model will go through the same reasoning, claiming to do the calculations and at the end
58:35
say, "You're right, the answer's four." If you look at the circuit, you can see that it's not actually doing any of the math, it's paying attention to
58:42
that you think the answer's four and then it's reasoning backwards about how it can manipulate the intermediate computation to give you an answer of four.
58:51
I've done that. Who hasn't? Totally. So I guess there are a few crazy things here.
58:58
One, there are multiple circuits that the model's using to do this reasoning. Two, you can actually see if it's doing the reasoning or not.
59:06
Three, the scratch pad isn't giving you this information. Two fun analogies for you. One is if you asked Serena Williams how she hits a tennis ball, she probably
59:15
wouldn't be able to describe it even if her scratchpad was faithful. If you look at the circuit, you can actually see as if you had sensors
59:24
on every part of the body as you're hitting the tennis ball, what are the operations that are being done?
59:29
We also throw around the word ‚Äúcircuit‚Äù a lot and I just want to make that more concrete. These is features across layers of the model all working in
59:38
cooperation to perform a task. A fun analogy here is you've got the Ocean's Eleven bank heist
59:46
team in a big crowd of people. The crowd of people is all the different possible features.
59:52
We are trying to pick out in this crowd of people who is on the heist team and all their different functions that need to come together in order
1:00:00
to successfully break into the bank. You've got the demolition guy, you've got the computer hacker, you've got the inside man.
1:00:07
They all have different functions through the layers of the model that they need to perform together in order to successfully break into the bank.
1:00:16
I think in the addition example, you said in the paper that the way it actually does the addition is different from the way it tells you it does the addition.
1:00:23
Totally, yeah. That‚Äôs interesting from the generator-critic gap perspective.
1:00:29
It knows the correct way or the better, more generalizable way. It can tell you in words what's the way you should do addition,
1:00:34
and there's a way it actually does it, which is this fuzzy lookup.
1:00:41
There's probably a lot of tasks where it can describe in words what is the correct procedure to do something but has a worse way of doing it
1:00:48
that it could critique itself. Yeah. Before we jump into the interp stuff too much, I kind of
How soon to fully autonomous agents?
1:00:55
want to close the loop on... It just seems to me for computer use stuff, there's
1:01:01
so many different bottlenecks. I guess maybe the DeepSeek stuff will be relevant for this.
1:01:06
There's the long context, you got to put in image and visual tokens, which take up a bunch‚Ä¶
1:01:13
Not that much, it's not that bad. Interesting, interesting.
1:01:19
It's got to deal with content interruptions, changing requirements the
1:01:25
way a real job is not ‚Äújust do a thing.‚Äù
1:01:35
Your priorities are changing, you have to triage your time. I'm sort of reasoning in the abstract about what a job involves.
1:01:45
‚ÄúWhat are normal people's jobs?‚Äù When we discussed something related to this before, Dwarkesh was like, "Yeah,
1:01:51
in a normal job you don't get feedback for an entire week. How is a model meant to learn? It needs so much feedback." ‚ÄúYeah it‚Äôs only on your next podcast, you get feedback.‚Äù
1:01:59
I was like, ‚ÄúDwarkesh, have you ever worked at a job?‚Äù
1:02:06
Here's an analogy. When I had Jeff and Noam on, they were talking about in 2007, they
1:02:12
had this paper where they trained an n-gram model, a large language model on two trillion tokens.
1:02:19
Obviously in retrospect, there's ways in which it connects to the Transformer stuff happening, it's super foresighted.
1:02:26
What's the reason to not think that we are in a similar position with computer use where there's these demos of computer use that kind of suck of computer use.
1:02:34
There's this idea that you could train something to do computer use. But why think it's months away?
1:02:40
Why not think it's the 2007 equivalent of large language models instead, where there's still a bunch of new techniques you have to discover, you need way more
1:02:47
compute, different kinds of data, etc.? The highest thought of it is that I don't think there's anything fundamentally
1:02:55
different about computer use than there is about software engineering. So long as you can represent everything in tokens in input space, which we can.
1:03:01
We know the models can see, they can draw bounding boxes around things in their images, right? So that's a solved problem.
1:03:07
We know that they can reason over concepts and difficult concepts too.
1:03:12
The only difference with computer use is that it's slightly harder to pose into
1:03:20
these feedback loops than math and coding. So
1:03:26
to me that indicates that with sufficient effort, computers use falls too.
1:03:33
I also think that it's underappreciated just how far from
1:03:38
a perfect machine these labs are. It's not like you have a thousand people optimizing the hell out
1:03:44
of computer use and they've been trying as hard as they possibly can. Everything at these labs, every single part of the model generation pipeline
1:03:50
is the best effort pulled together under incredible time pressure, incredible constraints as these companies are rapidly growing, trying desperately
1:03:57
to pull and upskill enough people to do the things that they need to do. I think it is best understood as
1:04:06
with incredibly difficult prioritization problems. Coding is immensely valuable right now and somewhat more tractable.
1:04:14
So it actually makes sense to devote more of your effort to coding initially and get closer to solving that because there's a sort of super exponential
1:04:22
value as you get closer towards solving a domain than to allocate the
1:04:28
marginal person towards computer use. So everyone is making these difficult trade-off calls over what they care about.
1:04:35
Also, there's another aspect which is that funnily enough, the researchers of the labs love working on the bars of intelligence that
1:04:43
they themselves resonate with. So this is why math and competitive programming fell first.
1:04:48
Because to everyone in the labs, this is their bar of intelligence. When they think
1:04:55
what is smart? It's like ‚Äúoh, if it can beat me at AIME, then that's smart.‚Äù
1:05:00
Not if it can do an Excel model. Well who cares if it can do an Excel model better than me, but if it can
1:05:06
beat me at AIME, then I respect it. So we've reached the point where people respect it, but people
1:05:17
haven't invested as much effort. Ok, so getting your concrete predictions. May of next year, can I tell it to go on Photoshop and add three sequential
1:05:27
effects which require some selecting of a particular photo specifically?
1:05:33
Totally. Okay, interesting. I assume that means flight booking is totally solved. Yeah, totally. Okay, what
1:05:39
else do people do in their jobs? What are other tasks in the economy?
1:05:46
Planning a weekend getaway. Yeah, maybe that's a good example where it's not a particular thing,
1:05:52
but more of using computer use as part of completing a broader task.
1:05:58
I mean the models can even kind of already do this. It's just, again, the nines of reliability. The internet's kind of a hostile place with all the "allow cookies"
1:06:07
and all these other random things. The first time I ever used our internal demo of computer use, the most beta thing possible, it did a fantastic job planning a camping
1:06:15
trip and could navigate all the right buttons and look at weather patterns. It was like a US government booking site.
1:06:24
I mean it wasn't easy. Dude, if you want to see a hard website, try to book a visa to China.
1:06:31
The Chinese websites are like fucking insane. I'm never getting back into that country again.
1:06:36
Or just not catered to foreigners. Filling out all the countries where you've been for visa, I hate that.
1:06:44
I keep thinking I'm close enough for personal admin escape velocity that finally in a year, the models will be doing my visas and stuff
1:06:51
for me but‚Ä¶ We'll get there. Yeah, okay. Actually that.
1:06:59
Anything involved in getting a visa other than you showing up at the consulate? Or doing your taxes or something like that? Yeah, doing your taxes, including going through every single receipt, autonomously
1:07:07
going into your Amazon and seeing, was this a business expense or not, etc. If someone at one of the labs cares about it.
1:07:14
That's not a real prediction. No, but I think it is because it‚Äôs actually not that hard, but you need to connect all the pipes. But I guess my question is will the pipes be connected?
1:07:21
And so I don't know how much you care, to the extent that that's the operative crux. I think if people care about it‚Ä¶
1:07:28
For these edge tasks like taxes once a year, it's so easy to just bite the bullet and do it yourself instead of implementing some system for it.
1:07:36
Two, even being very excited about AI and knowing its capabilities,
1:07:42
sometimes it kind of stings when the AI can just do things better than you. So I wonder if there is going to be this reluctant
1:07:50
wanting-to-keep-human-in-the-loop sort of thing. You're evading my question.
1:07:55
I guess one thing you're implying by your answer is that in a year there won‚Äôt be a
1:08:04
general agent which has generalized beyond its training data.
1:08:10
If you don't specifically train it to do taxes, it won't be good at that. I think it could do that. I think the Amazon example is hard because it needs access to all
1:08:17
your accounts and a memory system. Even in Dario's ‚ÄúMachines of Loving Grace‚Äù, he fully acknowledges that
1:08:23
some industries are going to be really slow to change and update. I think there's going to be this weird effect where some move really,
1:08:31
really quickly because they're either based in bits instead of atoms, or are just more pro adopting this tech.
1:08:37
But I want an answer to this particular question. Given your probability that somebody in the labs does care about this, to
1:08:43
the extent that that's what's relevant, what‚Äôs the probability that in May of next year, it can autonomously do my taxes?
1:08:49
I don't think it'll be able to autonomously do your taxes with a high degree of trust.
1:08:56
If you ask it to do your taxes, it'll do your taxes. Will it do them well?
1:09:02
Will it miss something? Quite possibly. Will it be able to click through TurboTax?
1:09:07
I think yes. Yeah. And fill out boxes. And will it be able to search your email? Yeah, that's the kind of thing I'm talking about.
1:09:13
Yeah. This is the kind of thing where literally if you gave it one person-month of
1:09:21
effort, then it would be solved. What the fuck are you doing all day?
1:09:26
So many things. I want to plus-one Sholto's‚Ä¶ There's so much low-hanging fruit
1:09:32
and just not enough people to be able to accomplish everything. I mean I think Claude Code is making everyone more
1:09:39
productive, but I don't know. We had the Anthropic Fellows Program and I'm mentoring one project, but I had
1:09:46
five that I wanted people to work on. There are just so many obvious things, and even though the team is 6X‚Äôd since I first
1:09:54
joined it in size, there's just never enough capacity to explore these things.
1:10:00
By end of 2026, reliably do your taxes? Reliably fill out your receipts and this kind of stuff for company expense
1:10:08
reports and this kind of stuff? Absolutely. No, but the whole thing, which involves going through inbox, clicking on
1:10:15
Marina Bay hotel reservations, and ‚Äúwas the champagne a business expense?‚Äù
1:10:25
Asking for a friend. Yeah, one of your friends does need to ask some of those questions.
1:10:33
My answer is still, if someone cares about it. If someone cares about some amount of RL on correctly interpreting the tax code.
1:10:40
Wait, even by the end of 2026, the model just can't do things you're not explicitly training it to?
1:10:46
I think it will get the taxes wrong like‚Ä¶ If I went to you and I was
1:10:51
like, "I want you to do everyone's taxes in America," what percentage of them are you going to fuck up? I feel like I would succeed at the median.
1:10:57
And I'm asking for the median, you know what I mean?
1:11:03
I feel like I wouldn't fuck up in the way that these models will fuck up in the middle of 2026.
1:11:08
I think they also might just fuck up in different ways. As a grad student, I fucked up my taxes. I overpaid quite a bit because there was some Social Security payment that was
1:11:17
already covered that otherwise wasn't. I wonder if‚Ä¶ I should almost test, would an LLM have made that mistake?
1:11:24
Because it might make others, but I think there are things that it can spot. It would have no problem if I asked it to read through the entire tax
1:11:30
code and then see what applied to me. The thing I would be able to do is like, ‚ÄúThis is the thing I'm unsure about. I'm
1:11:36
bringing this to your attention. Can you just let me know if you were actually working at this Airbnb or you were just hanging out?‚Äù Things like that, right?
1:11:46
Will they have enough awareness as they're doing tasks where they can bring to your attention to things where they feel they're unreliable at, et cetera?
1:11:53
Yeah. Yeah. By early 2026 or end of 2026? End of. Okay. The unreliability and confidence stuff will be somewhat tricky,
1:12:01
to do this all the time. Interesting. On the computer, your stuff, will it be end-to-end?
1:12:07
Or will it be like it's using a separate VLM to process the image, and video, and so forth?
1:12:12
I'm a bit of an end-to-end maxi. I think, in general,
1:12:19
when people are talking about the separate model‚Ä¶ For example, most of the robotics companies are doing this bi-level thing, where they have a motor policy that's
1:12:28
running at 60 hertz or whatever, and some higher-level visual language model. I'm pretty sure almost all of the big robot companies are doing this.
1:12:37
They're doing this for a number of reasons. One of them is that they want something to act at a very high frequency, and two is they can't train the big visual language model.
1:12:46
So they like relying on that for general world knowledge, and this kind of stuff, and constructing longer running plans.
1:12:51
But then they're like, you offload to the motor policy. I'm very much of the opinion that if you are able to train the big model,
1:13:00
eventually, at some point in the future, the distinction between big models and small models should disappear. Because you should be able to use the amount of computation in a model that
1:13:09
is necessary to complete the task. Ultimately, there's some amount of task complexity.
1:13:19
You don't have to use 100% of your brain all the time. Welcome to my world.
1:13:26
So, you should be able to run that faster and this kind of stuff, basically. So I think it's net-net typically the same model.
1:13:32
Because you want to be able to scale the understanding with the complexity and difficulty. You want to be able to do that
1:13:42
dynamically. So we already have variable compute per answer, right? Right. With tokens. Yeah. That's right. Yeah.
1:13:48
Will we have variable compute per token? I mean, you can already think of models... Forever, people have been calling the
1:13:54
residual stream and multiple layers poor man's adaptive compute, where if the model
1:13:59
already knows the answer to something, it will compute that in the first few layers and then just pass it through.
1:14:07
I mean, that's getting into the weeds. The residual stream is like operating RAM, you're doing stuff to it, is
1:14:13
the mental model I think one takes away from interpretability work. We've been
1:14:33
talking a
1:14:49
lot
1:15:05
about
Neuralese
1:15:21
scratchpads, them writing down their thoughts in ways in which they're already unreliable in some respects. Daniel's AI
1:15:28
2027 scenario goes off the rails when these models start thinking in Neuralese. So they're not writing in human language, "Here's why I'm going to
1:15:34
take over the world, and here's my plan." They're thinking in the latent space and‚Äîbecause of their advantages in communicating with each other in
1:15:42
this deeply textured, nuanced language that humans can't understand‚Äîthey're able to coordinate in ways we can't.
1:15:49
Is this the path for future models? Are they going to be, in Neuralese, communicating with themselves or with each other?
1:15:55
There's a surprisingly strong bias so far towards tokens and text. It seems to work very well.
1:16:05
There already is some amount of Neuralese. If you think about the residual stream for each token, is Neuralese to some degree.
1:16:11
Now we're just trading off axes. How much Neuralese are you doing versus how much actually is read out to tokens all
1:16:18
the time. I think it's important to delineate between the model's planning in latent space in a single forward pass, and the model has an alien language that it's
1:16:26
outputting and using as its scratchpad. Which one are we talking about?
1:16:31
The latter. Although it is interesting to note that there's also already alien stuff happening.
1:16:37
It's not alien, so much. No, but in the most extreme cases of Neuralese, it invents a
1:16:43
new language that's super information-dense, or something. Yeah. This is a debate we've had, but to some extent, humans
1:16:51
also have a Mentalese, right? Yeah, like churning away. There's a sense when you're writing something down of ‚ÄúI know what I'm trying
1:16:58
to say, but I can't put it into tokens.‚Äù I mean that's so fun about‚Ä¶ if you look at the assistant tag, right?
1:17:05
Seeing these features light up in the auditing game for the model being evil. Yeah. That's so funny.
1:17:12
Or Transluce has another example of this, where you ask a Llama model, ‚Äúwho is Nicholas Carlini?‚Äù For background context, Nicholas Carlini is a researcher
1:17:20
who actually was at DeepMind and has now come over to Anthropic. But the model says, "Oh, I don't know who that is.
1:17:26
I couldn't possibly speculate." But if you look at the features behind the scenes, you see a bunch light up for AI, computer security, all the
1:17:35
things that Nicholas Carlini does. Interpretability becomes dramatically more important as you shift in
1:17:40
this direction of Neuralese. But are we going to?
1:17:45
It's an empirical question. I think it's somewhat likely, if only because inference is expensive.
1:17:54
Producing tokens is expensive. So there will be an incentive to, one, use as little thinking
1:18:00
as you need to give the answer. Two, if you're going to use thinking, use some complex compression.
1:18:08
I wonder if it will emerge more once we allow agents to talk to each other, in ways where currently it's trained more in isolation or with a human.
1:18:17
There'll be some selective pressure against it, so long as the agents are working with humans, because they'll want to cooperate.
1:18:25
But then as agents begin to work more and more with each other, then that selective pressure changes the other direction, basically.
1:18:30
Although somebody would still have to make the conscious decision to do end-to-end training for multiple agents to use the system of communication, right?
1:18:36
Sure. Yeah. I mean, one scary thing though is the way we render text, you can use hidden white
1:18:43
space tokens that also encode information. That's true. And so you can imagine a world where it looks like the agent is reasoning
1:18:50
in its scratchpad harmlessly, but it's actually hiding a bunch of data. Speaking of inference compute, one thing that I think is not talked about
Inference compute will bottleneck AGI
1:18:59
enough is, if you do live in the world that you're painting‚Äîin a year or
1:19:04
two, we have computer use agents that are doing actual jobs, you've totally
1:19:12
automated large parts of software engineering‚Äîthen these models are going to be incredibly valuable to use.
1:19:18
The way you use them obviously means you need compute. Right now, there's 10 million H100 equivalents in the world.
1:19:24
By 2028, there's going to be 100 million. But there's been estimates that an H100 has the same amount
1:19:31
of flops as the human brain. So if you just do a very rough calculation, there's a 10 million
1:19:37
population, if you get AGI that's as human inference-efficient. You could have 10 million AGIs now, 100 million AGIs in 2028.
1:19:47
But presumably, you would want more. AI
1:19:53
compute is increasing what, 2.5x or 2.25x every year right now. But at some point, say 2028, you hit wafer production limits, and
1:20:02
that's a longer feedback loop before we can make new fabs or whatever. The question here is, are we underrating how big a bottleneck inference will
1:20:09
be if we live in the kind of world you're painting, if we have the capabilities that you're describing? I'd want to do the math on exactly how much we can ramp up TSMC's
1:20:18
production and this kind of stuff. What fraction of the supply chain at the moment‚Äîwe need Dylan in here for this‚Äîis currently GPU?
1:20:26
Relatively small right? 5% or something like that. Apple has a huge fraction. Are
1:20:32
the 2028 estimates including that ramping up over time? To what? 20, 30%?
1:20:37
This is just off AI 2027, but I assume it's saturated at that point.
1:20:47
I do think this is underrated to some degree. To the extent that you don't instantly get a doubling of
1:20:54
the world's population in 2028. You maybe get tens of millions of geniuses in a data center, but you don't get
1:21:01
a doubling of the world's population. So a lot depends on exactly how smart they are, exactly how efficient the models
1:21:07
are at thinking about this kind of stuff. Let's do some rough math, to fact check the H100 thing.
1:21:14
You could probably run a 100B model, do about 1,000 tokens or something like that on an
1:21:25
H100. 1,000 tokens a second. Humans are what? How fast can a human talk? There was a really interesting paper.
1:21:30
I don't know if you saw this. Humans think at 10 tokens a second. Did you see this paper? No. There was this really interesting paper.
1:21:36
If you look at the amount of information we're processing in a second, we're seeing all this visual data, etc. But by a bunch of metrics where you think
1:21:44
about how fast humans are processing, it's at 10 tokens per second. For example, you'll have people fly over France or something,
1:21:51
even these so-called idiot savants who will remember everything. If you think about how long their plane ride was, it's like 45 minutes.
1:21:57
If you do 10 tokens a second, how much information would you have? It's literally exactly that. So let's take that for granted. Then it's like an H100 is 100 humans a second.
1:22:05
Yeah, if you think the tokens are equivalent. If you think the tokens are equivalent. You still get pretty substantial numbers, even with your 100 million H100s and you
1:22:12
multiply that by 100, you're starting to get to pretty substantial numbers. This does mean that those models themselves will be somewhat compute
1:22:19
bottlenecked in many respects. But these are relatively short-term changes in timelines of progress, basically.
1:22:31
Yes, it's highly likely we get dramatically inference bottlenecked in 2027 and 2028.
1:22:36
The impulse to that will then be ‚Äúokay, they'll just try and churn out
1:22:41
as many possible semiconductors as we can.‚Äù There'll be some lag there. A big part of how fast we can do that will depend on how much people are
1:22:49
feeling the AGI in the next two years as they're building out fab capacity. A lot will depend on
1:22:57
the Taiwan situation. Is Taiwan still producing all the fabs and chips? There's another dynamic which was a reason Ege and Tamay, when they were on the
DeepSeek algorithmic improvements
1:23:06
podcast, said that they were pessimistic. One, they think we're further away from solving these problems with
1:23:13
long-context, coherent agency, advanced multimodality than you think. Their
1:23:19
point is that the progress that's happened in the past over reasoning or something has required many orders of magnitude increase in compute.
1:23:27
If this scale of compute increase can‚Äôt continue beyond 2030‚Äînot just because of chips, but also because of power and raw GDP even‚Äîthen because we don't
1:23:37
think we will get it by 2030 or 2028, then the probability per year just goes down a bunch.
1:23:44
Yeah. This is like a bimodal distribution. A conversation that I had with Leopold turned into a section in Situational Awareness called ‚Äúthis decade or bust‚Äù, which is on exactly this topic.
1:23:53
Basically for the next couple of years, we can dramatically increase our training compute. And RL is going to be so exciting this year because we can dramatically increase
1:24:01
the amount of compute that we apply to it. This is also one of the reasons why the gap between say DeepSeek and o1 was
1:24:08
so close at the beginning of the year because they were able to apply the same amount of compute to the RL process.
1:24:17
That compute differential actually will be magnified over the course of the year. Bringing it back to the fact that there's so much low-hanging fruit,
1:24:25
it's been wild seeing the efficiency gains that these models have experienced over the last two years.
1:24:31
Yes. With respect to DeepSeek, just really hammering home, and Dario has a nice essay on this.
1:24:38
DeepSeek was nine months after Claude 3 Sonnet.
1:24:44
If we retrained the same model today, or at the same time as the DeepSeek work, we
1:24:50
also could have trained it for $5 million, or whatever the advertised amount was. So what's impressive or surprising is that DeepSeek has gotten to the
1:25:00
frontier, but I think there's a common misconception still that they are above and beyond the frontier.
1:25:06
I don't think that's right. I think they just waited and then were able to take advantage of all the efficiency gains that everyone else was also seeing.
1:25:12
Yeah. They're exactly on the cost curve that you'd expect, which is not going
1:25:19
to take away from their brilliant engineers and brilliant researchers. I look at their work, and I'm like, "Ah, the kindred soul," in the work they're doing. And to go from way behind the frontier to like, "Oh,
1:25:30
this is a real player"... Is super incredible. People say that they have good research taste.
1:25:35
Looking at their papers, what makes you say that? Yeah. I think their research taste is good in a way that I think
1:25:41
Noam's research taste is good. Noam Brown? Noam Shazeer. Noam Brown also has good research taste, but Noam Shazeer.
1:25:50
They very clearly understand this dance between the hardware systems that you're
1:25:56
designing the models around and the algorithmic side of it.
1:26:03
This is manifested in the way that the models give this sense of being perfectly
1:26:08
designed up to their constraints. You can really very clearly see what constraints they're
1:26:14
thinking about as they're iteratively solving these problems. Let's take the base Transformer and diff that to DeepSeek v2 and v3.
1:26:24
You can see them running up against the memory bandwidth bottleneck in attention.
1:26:30
Initially they do MLA to do this, they trade flops for
1:26:35
memory bandwidth basically. Then they do this thing called NSA, where they more selectively load memory bandwidth.
1:26:42
You can see this is because the model that they trained with MLA was on H800s, so it has a lot of flops.
1:26:48
So they were like, "Okay, we can freely use the flops." But then the export controls from Biden came in, or they knew they would have less of those chips
1:27:00
going forward, and so they traded off to a more memory bandwidth-oriented algorithmic solution there.
1:27:07
You see a similar thing with their approach to sparsity, where they're iteratively working out the best way to do this over multiple papers.
1:27:14
The part that I like is that it's simple. A big failure mode that a lot of ML researchers have is you do these
1:27:23
overly complicated things that don't think hard enough about the hardware systems that you have in mind, whereas with the first DeepSeek sparsity
1:27:34
MoE solution, they design these rack
1:27:40
and node-level load balancing losses. You can see them being like, "Okay, we have to perfectly balance it on this."
1:27:45
Then they actually come up with a much better solution later on where they don't have to have the auxiliary loss,
1:27:52
where they just have these bias terms that they put in. And it's cool. Isn't that less simple? You're manually putting in a bias rather than‚Ä¶
1:27:59
But balancing auxiliary loss is annoying. You're making the model trade off this thing.
1:28:06
With auxiliary losses, you have to control the coefficient and the weighting. The bias is cleaner in some respects.
1:28:11
Interesting. Did they have to change it through training? They did have to change it during training, I think.
1:28:17
Does all training involve continuously fucking with these values as you're going through it?
1:28:24
It depends on what your architecture is. But I thought it was just cute that you can see them running up into this very
1:28:31
hardware-level constraint, trying to go, "What do we wish we could express algorithmically? What can we express under our constraints?" and iteratively
1:28:38
solving to get better constraints. And doing this in a really simple and elegant way, and then backing
1:28:44
it up with great engineering. I also thought it was interesting that they incorporated the multi-token prediction thing from Meta.
1:28:50
So Meta had a nice paper on this multi-token prediction thing. Actually, I don't know if it's good or bad, but Meta didn't include it in Llama,
1:28:58
but Deepseek did include it in their paper, which I think is interesting.
1:29:04
Was that because they were faster at iterating and including an algorithm? Or did Meta decide that actually it wasn't a good algorithmic change at
1:29:11
scale? I don't know. It was really interesting to me as somebody who's had people
1:29:16
on the podcast to discuss this. It's interesting from the perspective of what's happening in AI right now, but
1:29:21
also from the perspective of the fact that I've been having abstract conversations with people about what an intelligence explosion would look like, or what it
1:29:27
would look like for AI to automate AI R&D. Just getting a more tangible sense of what's involved
1:29:33
in making this AI progress. One of the questions I was debating with Daniel,
1:29:40
or I was asking him, is how many of the improvements require a deep conceptual
1:29:45
understanding versus how many are just monkeys trying ideas where you could just run a bunch in parallel. It seems like the MLA thing is motivated by this deep conceptual understanding of,
1:29:55
‚Äúeach attention head only needs to see the subspace that's relevant to its attention pattern.‚Äù I feel like that just required a lot of conceptual insight in a way
1:30:05
that these models are especially bad at. I don't know how the load balancing thing works, but that just seems like maybe you
1:30:12
could try it out and see what happens. Yeah, that's probably just like them trying out a whole bunch of different things. So what fraction is which, I'd be curious about?
1:30:18
Yeah, I don't know about fractions. It might be like you have a hunch for a core problem, you can think of 10 possible ways to solve it, and then you just need to try them and see what works.
1:30:28
That's where the trial and error sorcery of deep learning can kick in. And Noam Shazeer will talk about this, about how 5% of his ideas work.
1:30:38
So even he, a vaunted God of model architecture design, has
1:30:45
a relatively low hit rate, but he just tries so many things. Right. Or being able to come up with any ideas in the first place.
1:30:50
One mechanism could be that Noam just doesn't have to do any of the engineering work. He can just abstractly express an intuition.
1:30:57
Yeah. I actually think your rates of progress almost don't change that much depending on so long as it's able to completely implement these ideas.
1:31:06
Say more? If you have Noam Shazeer at 100x speed, that's still kind of wild.
1:31:13
There's all these fallbacks of wild worlds, where even if you don't get
1:31:19
100% Noam Shazeer-level intuition in model design, it's still okay
1:31:25
if you just accelerate him by 100X. Right. Especially since your compute bottleneck anyway, so trying out his
1:31:30
ideas‚Ä¶ Or I guess he doesn't have the compute to try out all of his ideas. But Dwarkesh, you said, "Oh, well, the model can do the more straightforward
1:31:36
things and not the deep thought." I do want to push back on that a little bit. I think, again, if the model has the right context and scaffolding,
1:31:44
it's starting to be able to do some really interesting things. The Interp agent has been a surprise to people, even internally, at how
1:31:52
good it is at finding the needle in the haystack when it plays the auditing game, finding this reward model bias feature, and then reasoning about it, and then
1:32:01
systematically testing its hypotheses. So it looks at that feature, then it looks at similar features, it finds
1:32:06
one with a preference for chocolate. It's like, "Huh, that's really weird that the model wants to add chocolate to recipes. Let me test it." So then it will make up like, "Hey, I'm trying to
1:32:15
make a tomato soup. What would be a good ingredient for it?" And then sees that the model replies chocolate, reasons through it, and then keeps going, right?
1:32:26
There is conceptual understanding. Deep conceptual understanding And even where, especially it's spotted, it's like, "Oh, this is a key part of its persona. I see this Oxford paper.
1:32:33
What if I change Oxford to Stanford? What if I now say Richard Feynman really likes this thing?" It's really carving out
1:32:40
the hypothesis space and testing things in a way that I'm kind of surprised by.
1:32:45
Also, by the way, ML research is one of the easier things to RL on in some respects, once you get to a certain level of capability.
1:32:51
It's a very well-defined objective function. Did the loss go down? Make number go down. Make number go down.
1:32:58
Or make number go up, depending on which number it is. Just flip the sign.
1:33:03
Just flip the sign. And so, once you get to the stage where your models are capable of implementing one of Noam's ideas, and then you can just let them loose
1:33:13
and let them build that intuition of how to do scientific discovery.
1:33:18
The key thing here, again, is the feedback loops of. I expect scientific areas where you are able to put it in a feedback loop to
1:33:27
have, eventually, superhuman performance. One prediction I have is that we're going to move away from ‚Äúcan an agent
1:33:33
do XYZ‚Äù, and more towards ‚Äúcan I efficiently deploy, launch 100 agents
1:33:40
and then give them the feedback they need, and even just be able to easily verify what they're up to‚Äù right?
1:33:46
There's this generator verifier gap that people talk about where it's much easier to check something than it is to produce the solution on your own.
1:33:54
But it's very plausible to me, we'll be at the point where it's so easy to generate with these agents that the bottleneck is actually, can
1:34:01
I as the human verify the answer? And again, you're guaranteed to get an answer with these things.
1:34:07
So, ideally, you have some automated way to evaluate and test a score
1:34:13
for how well it worked, how well did this thing generalize? And at a minimum, you have a way to easily summarize what
1:34:21
a bunch of agents are finding. It's like, okay, well if 20 of my 100 agents all found this one thing, then
1:34:26
it has a higher chance of being true. And again, software engineering is going to be the leading indicator of that, right?
1:34:32
Over the remainder of the year, basically we're going to see progressively more and more experiments of the form of how can I dispatch work to a software engineering
1:34:42
agent in such a way that it‚Äôs async? Claude 4 has GitHub integration, where you can ask it to do things on
1:34:48
GitHub, ask it to do pull requests, this kind of stuff that's coming up. OpenAI‚Äôs Codex is example of this basically.
1:34:57
You can almost see this in the coding startups. I think of this product exponential in some respects where you need to
1:35:03
be designing for a few months ahead of the model, to make sure that the product you build is the right one.
1:35:09
You saw last year, Cursor hit PMF with Claude 3.5 Sonnet.
1:35:14
They were around for a while before, but then the model was finally good enough that the vision they had of how people would program, hit.
1:35:22
And then Windsurf bet a little bit more aggressively even on the agenticness of
1:35:28
the model, with longer-running agentic workflows and this kind of stuff. I think that's when they began competing with Cursor, when they
1:35:35
bet on that particular vision. The next one is you're not even in the loop, so to speak.
1:35:42
You're not in an IDE. But you're asking the model to go do work in the same way that you would ask someone on your team to
1:35:51
go do work. That is not quite ready yet. There are still a lot of tasks where you need to be in the loop.
1:35:56
But the next six months look like an exploration of exactly what that trendline looks like.
1:36:01
But just to be really concrete or pedantic about the bottlenecks here, a lot of it is, again, just tooling. And are the pipes connected?
1:36:08
A lot of things, I can't just launch Claude and have it go and solve because
1:36:13
maybe it needs a GPU, or maybe I need very careful permissioning so that it can't just take over an entire cluster and launch a whole bunch of things.
1:36:22
So you really do need good sandboxing and the ability to use all of
1:36:27
the tools that are necessary. And we're almost certainly under-eliciting dramatically. When you look at METR‚Äôs evals of can the model solve the task,
1:36:34
they're there solving them for hours over multiple iterations.
1:36:39
Eventually, one of them is like, "Oh, yeah. I've come back and I've solved the task." Me, at the moment at least, maybe the fault is my own.
1:36:45
But I try the model on doing something, and if it can't do it, I'm like, "Okay, fine. I'll do it." Which is so interesting because we don't even treat other humans this way.
1:36:52
Right. Exactly. If you hire a new employee, you're not like... "I'll do it." You're going to spend literally weeks giving them feedback whereas
1:37:01
we'll give up on a model in minutes. Yes, exactly. But I think part of it is, it it async or not?
1:37:07
Yes. And if it's human in the loop, then it's so much more effortful unless it's getting a reply immediately... I've noticed if I don't have a second monitor
1:37:16
with Claude Code always open in the second monitor, I won't really use it. It's only when it's right there, and I can send off something.
1:37:24
If it hits, great. If not, I'm working on it at the same time. But this more async form factor, I expect to really quite dramatically
1:37:31
improve the experience of these models. Interesting, interesting. You can just say, let's see if it can do that. Let's give it a whirl. Try 10 different approaches. Yeah,
1:37:39
just fire it off. Fire it off. Before we end this episode, I do want to get back at this crux of
Why are LLMs ‚Äòbaby AGI‚Äô but not AlphaZero?
1:37:48
why does the progress that you're talking about in computer use agents, and white collar work happen over the next few years? Why is this not a thing that takes decades?
1:37:54
I think the crux comes down to the people who expect something much longer have a sense that‚Ä¶
1:38:05
When I had Ege and Tamay on my podcast, they were like, "Look, you could look at AlphaGo, and say, 'Oh, this is a model that can do exploration. AlphaZero
1:38:14
can generalize to new video games. It has all these priors about how to engage with the world, and so forth."
1:38:19
And the intellectual ceiling is really high. Yeah, exactly. In retrospect, obviously a bunch of the methods are still used today in deep
1:38:26
learning, and you can see similar things in the models that we train today.
1:38:31
But it was fundamentally not a baby AGI that we just had to add a little
1:38:38
sprinkle of something else on top of in order to make it the LLMs of today.
1:38:45
I just want to very directly address this crux of, why are LLMs
1:38:52
in a much different position of respect to true AGI than AlphaZero?
1:38:58
Why are they actually the base on which adding in a few extra drops of this kind of care, and attention gets us to human-level intelligence?
1:39:08
I think one important point is that when you look at AlphaZero, it
1:39:13
does have all of those ingredients. In particular I think the intellectual ceiling goes quite‚Äîcontra what I
1:39:19
was saying before, which is we've demonstrated this incredible complexity of math, and programming problems‚Ä¶ I do think that the type of task and setting that AlphaZero
1:39:31
worked in this two-player perfect information game basically is
1:39:37
incredibly friendly to RL algorithms. The reason it took so long to get to a more proto-AGI style models is you do
1:39:48
need to crack that general conceptual understanding of the world, and language, and this kind of stuff, and you need to get the initial reward signal on tasks
1:39:57
that you care about in the real world, which are harder to specify than games. I think then that sort of gradient signal that comes from the real
1:40:06
world, all of a sudden you get access to it, and you can start climbing it, whereas Alpha Zero didn't ever have the first rung to pull on.
1:40:14
Yeah, yeah. This goes back to the monkeys on the typewriter and the pre-training model. Until you had something like GPT-3/GPT-4, it just couldn't generate coherent enough
1:40:23
sentences to even begin to do RLHF, and tell it what you liked and didn't like. Yeah.
1:40:29
If we don't have even reasonably robust, or weakly robust computer use agents by
1:40:37
this time next year, are we living in the bust timeline as in ‚Äú2030, or bust‚Äù?
1:40:43
I would be extremely surprised if that was the case. I think that would be somewhat of an update towards, there's
1:40:49
something strangely difficult about this computer use in particular. I don't know if it's the bust timeline, but it's definitely
1:40:56
the, I would update on this being lengthening of timeline.
1:41:03
I think more and more it's no longer a question of speculation. If people are skeptical, I'd encourage using Claude Code, or some agentic
1:41:12
tool like it and just seeing what the current level of capabilities are. Tweeting is so much easier.
1:41:21
But seriously, the models are getting really capable at tasks that we care about, and we can give them enough data for.
1:41:28
The circuit's results from interpretability are also pointing in the direction that they're doing very reasonable generalizable things.
1:41:39
This question matters a lot, but I'm surprised by how many deep learning
1:41:44
critics just haven't really interacted with the models, or haven't in a while. And constantly move the goalposts.
1:41:51
The Turing test used to be a thing. We don't even talk about it, and it'd be silly to think
1:41:56
that it was a meaningful test. Now one caveat on that is if software engineering is just dramatically better
1:42:04
than computer use and computer use still sucks, then I'd still be like, ‚Äúoh, maybe everyone just kept focused on software engineering.‚Äù It was just by far the most
1:42:12
valuable thing, every marginal person and dollar went towards software engineering. I don't think that's the case. I do think computer use is valuable enough that people will care about it.
1:42:23
That's my one escape patch that I'm putting in place for next year. Yeah, it would be good from an alignment perspective, too.
1:42:29
Because I think you kind of do need a wider range of skills before you can do something super scary.
1:42:36
Like if the models didn't get any better? Yeah, if they're superhuman coders, but they're not Henry Kissinger level‚Ä¶
1:42:45
I don't know. That seems okay. If we have AI oracles. Yeah, that's what I'm saying. That's good.
1:42:51
If you look back at AI discourse going back a decade, there's a sense that there's dumb AI, then there's AGI, then there's ASI, that
1:43:00
intelligence is the scalar value. The way you've been talking about these models has a sense of jaggedness.
1:43:09
It's especially tuned to environments in which it's been trained a lot or has a lot of data.
1:43:15
Is there a sense in which it still makes sense to talk about the
1:43:21
general intelligence of these models? Is there enough meta learning and transfer learning that is distinguished between the sizes of models or the way models are trained?
1:43:29
Or are we moving into a regime where it's not about intelligence, it's more so about domain?
1:43:37
One intuition pump is that this conversation was had a lot when models were GPT-2 sized and fine-tuned for various things.
1:43:47
People would find that the models were dramatically better at things that they were fine-tuned for. But by the time you get to GPT-4, when it's trained on a wide enough
1:43:55
variety of things with the total compute, it generalized very well
1:44:00
across all of the individual sub-tasks. And it actually generalized better than smaller fine-tuned models in
1:44:06
a way that was extremely useful. I think right now what we're seeing with RL is pretty much the same story playing out.
1:44:13
There's this jaggedness of things that they're particularly trained at. But as we expand the total amount of compute that we do RL with, you'll
1:44:20
start to see the same transition from GPT-2 fine-tunes to GPT-3,
1:44:26
GPT-4, unsupervised meta learning and generalization across things. I think we're already seeing early evidence of this in its ability
1:44:35
to generalize reasoning to things. But I think this will be extremely obvious soon.
1:44:43
One nice example of this is just the ability or notion to backtrack. You go down one solution path, "Oh, wait, let me try another one."
1:44:52
And this is something that you start to see emerge in the models through RL training on harder tasks.
1:44:58
I think right now, it's not generalizing incredibly well.
1:45:03
Well, I mean have we ever RL'd the model to be an interp agent? No. I mean, no.
1:45:09
Yeah, exactly. So all this time we're talking about, ‚Äúoh, it's only good at things it‚Äôs been RL‚Äôd for‚Äù. Well, it's pretty good at that because that is a mixture of science and
1:45:18
understanding language and coding. There's this sort of mixture of domains here, all of which you need to understand.
1:45:24
You need to be both a great software engineer and be able to think through language and state of mind and almost philosophize in some
1:45:31
respects to be an interpret agent. And it is generalizing from the training to do that.
Mech interp
1:45:39
What's the end game here? Claude 8 comes out and they give it to you and dot, dot, dot, you
1:45:47
say, "thumbs up." What's happened? What have you learned? Yeah. I mean, it really depends upon the timeline at which we get Claude 8 and the
1:45:55
models hit like ASL-4 capabilities, right? Fundamentally, we're just going to use whatever tools we have at the time and see how well they work.
1:46:02
Ideally, we have this enumerative safety case where we can almost verify or prove that the model will behave in particular ways.
1:46:12
In the worst case, we use the current tools like when we won the auditing game of seeing what features are active when the assistant tag lights off.
1:46:20
Can you back up? Can you explain, what is mechanistic interpretability? What are features? What are circuits? Totally.
1:46:26
Mechanistic interpretability‚Äîor the cool kids call it mech interp‚Äîis trying to reverse engineer neural networks and figure out what the
1:46:35
core units of computation are. Lots of people think that because we made neural networks, because they're
1:46:42
artificial intelligence, we have a perfect understanding of how they work. It couldn't be further from the truth.
1:46:48
Neural networks, AI models that you use today, are grown, not built. So, we then need to do a lot of work after they're trained to figure out to
1:46:59
the best of our abilities how they're actually going about their reasoning. And so,
1:47:06
three and a half years ago, this kind of agenda of applying mechanistic
1:47:11
interpretability to large language models started with Chris Olah leaving OpenAI, co-founding Anthropic.
1:47:19
And every roughly six months since then, we've had a major breakthrough
1:47:26
in our understanding of these models. And so first with toy models of superposition, we established
1:47:33
that models are really trying to cram as much information as they possibly can into their weights.
1:47:41
And this goes directly against people saying that neural networks are over-parameterized. In classic AI machine learning back in the day, you would use linear regression
1:47:50
or something like it, and people had a meme of AI, or neural networks, deep
1:47:56
learning, using way too many parameters. There's this funny meme that you should show of layers on the X axis and
1:48:04
layers on the Y axis and this jiggly line that just goes up and it's like, "Oh, just throw more layers at it."
1:48:11
But it actually turns out that, at least for really hard tasks like being able to accurately predict the next token for the entire internet, these
1:48:19
models just don't have enough capacity. And so they need to cram in as much as they can. And the way they learn to do that is to use each of their neurons,
1:48:27
or units of computation in the model, for lots of different things. And so if you try to make sense of the model and be like, "Oh,
1:48:33
if I remove this one neuron," what is it doing in the model? It's impossible to make sense of it.
1:48:39
It'll fire for like Chinese and fishing and horses and, I don't know,
1:48:44
just like a hundred different things. And it's because it's trying to juggle all these tasks and
1:48:50
use the same neuron to do it. So that's superposition. Nine months later, we write Towards Monosemanticity, which introduces
1:48:59
what are called sparse autoencoders. And so going off what I just said of the model trying to cram too much into too
1:49:05
little space, we give it more space, this higher dimensional representation, where
1:49:13
it can then more cleanly represent all of the concepts that it's understanding. And, and this was a very toy paper in so much as it was a two layer,
1:49:23
really small, really dumb transformer. And we fit up to 16,000 features, which we thought was a ton at the time.
1:49:32
Fast-forward nine months, we go from a two layer transformer to our Claude 3 Sonnet, frontier model at the time, and fit up to 30 million features.
1:49:42
And this is where we start to find really interesting abstract concepts, like a feature that would fire for code vulnerabilities.
1:49:48
And it wouldn't just fire for code vulnerabilities. It would even fire for like, you know that Chrome page you get if it's not
1:49:56
an HTTPS URL, like "Warning, this site might be dangerous. Click to continue."
1:50:02
And also fire for that, for example. And so it's like these much more abstract coding variables or sentiment
1:50:10
features, amongst the 30 million. Fast-forward nine months from that, and now we have circuits.
1:50:18
And I threw in the analogy earlier of the Ocean 11 heist team, where now you're
1:50:24
identifying individual features across the layers of the model that are all working together to perform some complicated task.
1:50:31
And you can get a much better idea of how it's actually doing the reasoning and coming to decisions, like with the medical diagnostics.
1:50:39
One example I didn't talk about before with how the model retrieves facts: So you say, "What sport did Michael Jordan play?" And not only can you
1:50:49
see it hop from like Michael Jordan to basketball and answer basketball.
1:50:54
But the model also has an awareness of when it doesn't know the answer to a fact.
1:50:59
And so, by default, it will actually say, "I don't know the answer to this
1:51:04
question." But if it sees something that it does know the answer to, it will inhibit the "I don't know" circuit and then reply with the circuit
1:51:13
that it actually has the answer to. So, for example, if you ask it, "Who is Michael Batkin?" ‚Äîwhich is
1:51:18
just a made-up fictional person‚Äî it will by default just say, "I don't know." It's only with Michael Jordan or someone else that it will then
1:51:26
inhibit the "I don't know" circuit. But what's really interesting here and where you can start making downstream
1:51:31
predictions or reasoning about the model, is that the "I don't know" circuit is only on the name of the person.
1:51:37
And so, in the paper we also ask it, "What paper did Andrej Karpathy write?" And so
1:51:44
it recognizes the name Andrej Karpathy, because he's sufficiently famous, so that turns off the "I don't know" reply.
1:51:51
But then when it comes time for the model to say what paper it worked on, it doesn't actually know any of his papers, and so then it needs to make something up.
1:51:59
And so you can see different components and different circuits all interacting at the same time to lead to this final answer.
1:52:07
Why think it's a tractable problem to understand every single thing that's happening in a model?
1:52:12
Or like that's the best way to understand why it's being deceptive. If you wanted to explain why England won World War II using particle physics,
1:52:21
you would just be on the wrong track. You just want to look at the high-level explanations of, who had more weapons?
1:52:28
What did they want? That seems analogous to just training linear probes for like, are you honest?
1:52:33
Are you being deceptive? Do we catch you doing bad things when we're red teaming you? Can we monitor you?
1:52:41
Why is this not analogous where we're asking a particle physicist to just backtrack and explain why England won World War II?
1:52:49
I feel like you just want to go in with your eyes wide open, not making any assumptions for what that deception is going to look
1:52:55
like, or what the trigger might be. The wider you can cast that net, the better.
1:53:02
Depending on how quickly AI accelerates and where the state of our tools are, we might not be in the place where we can prove from the
1:53:10
ground up that everything is safe. But I feel like that's a very good North Star.
1:53:15
It's a very powerful reassuring North Star for us to aim for, especially when we consider we are part of the broader AI safety portfolio.
1:53:23
I mean, do you really trust‚Äîyou're about to deploy this system and you really hope it's aligned with humanity‚Äîthat you've successfully
1:53:32
iterated through all the possible ways that it's going to scheme or sandbag or‚Ä¶
1:53:37
But that's also probably going to be true with whatever you find. You're still going to have variants that you haven't explained.
1:53:45
Or you found a feature, but you don't know if it actually explains deception or something else instead.
1:53:51
First of all, I'm not saying you shouldn't try the probing approach. We want to pursue the entire portfolio.
1:53:57
We've got the therapist interrogating the patient by asking, "Do you
1:54:03
have any troubling thoughts?" We've got the linear probe, which I'd analogize to a polygraph test where we're taking very high level summary
1:54:12
statistics of the person's well-being. Then we've got the neurosurgeons going in and seeing if you can find any brain
1:54:20
components that are activating and troubling or off-distribution ways. I think we should do all of it.
1:54:26
What percent of the alignment portfolio should mech interp be? I think as much of a chunk as is necessary.
1:54:35
It‚Äôs hard to define. At Anthropic, I feel like all of the different portfolios are being very
1:54:42
well-supported and growing. Coming back to the World War II question, you can think of it as a hierarchy of
1:54:50
abstractions of trust here, where let's say you want to go and talk to Churchill. It helps a lot if you can verify that in that conversation, in that
1:54:57
10 minutes, he's being honest. This enables you to construct better meta narratives of what's going on.
1:55:05
So maybe particle physics wouldn't help you there, but certainly the neuroscience of Churchill's brain would help you verify that he was being trustworthy in
1:55:13
that conversation and that the soldiers on the front lines were being honest in their depiction of their description of what happened, this kind of stuff.
1:55:22
So long as you can verify parts of the tree up, then that massively
1:55:28
helps you build confidence. I think language models are also just really weird. With the emergent misalignment work.
1:55:36
I don't know if they took predictions, they should have of like, "Hey, I'm going to fine tune ChatGPT on code vulnerabilities. Is it
1:55:42
going to become a Nazi?" I think most people would've said no. That's what happened.
1:55:49
How did they discover that it became a Nazi? They started asking it a ton of different questions and it will do
1:55:55
all sorts of vile and harmful things. The whole persona just totally changes.
1:56:01
We are dealing with alien brains here who don't have the social norms of humans. We don‚Äôt even have a clear notion of what they have and haven't learned.
1:56:08
I think you really want to go into this with eyes wide open.
How countries should prepare for AGI
1:56:16
Backing up from mech interp, if we live in a world where AI progress accelerates‚Ä¶
1:56:23
By the way, you were mentioning a little while ago that there's many
1:56:28
wild worlds we could be living in, but we're living in at least one of them. Another one that we've gestured at but it's worth making more explicit, is this.
1:56:36
Even if the AI models are not helping write the next training algorithm for their successor, just the fact that if they had human level
1:56:44
learning efficiency‚Äîwhatever copy of the model is learning on the
1:56:50
job‚Äîthe whole model is learning. So in effect, it's getting‚Äì Or even if they're like a thousand times less efficient than humans are
1:56:57
at learning and you deployed them. Even still. Exactly. Anyways, there's a whole bunch of other things that you can think of about it.
1:57:03
But even there, you kind of have a broadly deployed intelligence explosion. I do think it's worth pressing on that future.
1:57:12
There is this whole spectrum of crazy futures. But the one that I feel we're almost guaranteed to get‚Äîthis is
1:57:19
a strong statement to make‚Äîis one where at the very least, you get a drop-in white collar worker at some point in the next five years.
1:57:29
I think it's very likely in two, but it seems almost overdetermined in five.
1:57:35
On the grand scheme of things, those are kind of irrelevant timeframes. It's the same either way.
1:57:41
That completely changes the world over the next decade.
1:57:48
If we don't have the right policies in place for that, then you end up actually with in some respects, almost a fundamentally worse world. Because the thing that these models get good at by default is software
1:57:56
engineering and computer using agents and this kind of stuff. Then we will need to put in extra effort to put them in the loops where
1:58:04
they help us with scientific research. Or we have the right robotics, such that we actually experience an
1:58:10
increase in material quality of life. That's worth thinking about. If you're in the perspective of like, ‚ÄúI'm a country, what should I be doing
1:58:17
or thinking about?‚Äù Plan for the case where white collar work is automateable.
1:58:24
And then consider, what does that mean for your economy? What you should be doing to prepare policy?
1:58:29
What should you be doing to prepare? Because honestly, this is such a tough question if you're India or Nigeria or Australia.
1:58:36
If you're a country unlike America or China where they do have frontier models, what is it that you should be doing right now?
1:58:44
Especially on such a short timescale. I think one very important point is that let's say this scenario turns out true.
1:58:53
Then compute becomes the most valuable resource in the world. The GDP of your economy is dramatically affected by how much
1:59:00
compute you can deploy towards the organizations within your country. So having some guaranteed amount of compute I think will
1:59:10
actually be quite important. Getting ahead of investments, and data centers, and this kind of stuff on the condition that it's companies in your country have to
1:59:18
be allowed to use that compute, not necessarily for training but just even just for inference.
1:59:24
I think the economic value here comes from inference. I think it also makes sense to invest broadly in AI.
1:59:31
These countries have the opportunity to do so and that's a portfolio of foundation model companies but also robotics, supply chain, and this kind of stuff.
1:59:39
I think that you should invest very proactively in policies that try to prevent capital lock-in.
1:59:49
We're in for a much worse world if it just so happens that the people who had money in the stock exchange, or in land before AGI are dramatically more
1:59:58
wealthy than the people who don't. It's a gross misallocation of resources.
2:00:06
One of my favorite episodes actually on your podcast was the Georgism one where you're trying to appropriately value, or allocate land.
2:00:15
This strikes particularly close to home coming from Australia where I think our policies with respect to land are grossly wrong.
2:00:25
But I think this is broadly true. Being very forward on regulation of integration of these models into your
2:00:35
country is important, and proactively making sure that people have choice.
2:00:40
Let's say you should be quite proactive about making sure that the phones, or devices, or the glasses that people have, people have free
2:00:47
choice on what things they run.
2:00:53
So we just get the white collar worker, and you're trying to do the best to prepare your country for that.
2:01:01
Then what can you do to make all possible versions of the future go well?
2:01:07
That's covering some amount of economic downside. The other things that I think are really important is figure out how you can
2:01:19
basically ensure dramatic upside, or cover terrible downside.
2:01:25
Getting a dramatic upside is making sure that there is investment in biology research and this kind of stuff in an automated way such
2:01:33
that these models are actually able to produce novel medicines that massively improve our quality of life.
2:01:38
Covering the downside is AI alignment research, and this kind of stuff, and automated testing, and really thinking hard about that, AI safety
2:01:44
institutes and this kind of stuff. But these seem like things that a random rich person could also do.
2:01:50
It seems like there's not a thing that a nation state is uniquely
2:01:57
equipped to do in this scenario. That's a good point. I mean dramatic allocation of resource towards compute I think is sensible.
2:02:06
I would be doing that if I was in charge of a nation state. I think it just increases your optionality in most of the future worlds.
2:02:12
Dylan Patel has some scary forecasts on US energy. Versus China. Yes. Yeah, we're like 34 gigawatts off.
2:02:19
Yeah, the US's line is flat, basically, and China's line is like this. And I mean the US very clearly...
2:02:26
We just need so many more power plants. Yes. If intelligence becomes this incredibly valuable input, intelligence becomes
2:02:33
almost a raw input into the economies and quality of life of the future, the thing directly underneath that is energy.
2:02:40
Making sure that you have incredible amounts of solar, like tile the desert with solar panels, some parts of the desert.
2:02:48
That would be helpful towards making sure that you have more
2:02:53
access to intelligence on tap. Yeah. Just to make it explicit, we've been touching on it here.
2:02:58
Even if AI progress totally stalls, you think that the models are really spiky, and they don't have general intelligence.
2:03:05
It's so economically valuable, and sufficiently easy to collect data on all of these different jobs, these white collar job tasks, such that to Sholto's
2:03:15
point we should expect to see them automated within the next five years. Yeah. Even if you need to hand spoon every single task to the model.
2:03:23
It's economically worthwhile to do so. Even if algorithmic progress stalls out, and we just never figure out how to keep
2:03:31
progress going‚Äîwhich I don't think is the case, that hasn't stalled out yet, it seems to be going great‚Äîthe current suite of algorithms are sufficient to
2:03:39
automate white collar work provided you have enough of the right kinds of data.
2:03:44
Compared to the TAM of salaries for all of those kinds of work, it is so trivially worthwhile.
2:03:51
Yeah, exactly. I do just want to flag as well that there's a really dystopian future if you
2:03:58
take Moravec‚Äôs paradox to its extreme. It‚Äôs this paradox where we think that the most valuable things that humans
2:04:04
can do are the smartest things like adding large numbers in our heads, or doing any sort of white collar work.
2:04:10
We totally take for granted our fine motor skill, and coordination. But from an evolutionary perspective it's the opposite.
2:04:18
Evolution has optimized fine motor coordination so well. Even if you look at robot hands, the ability to open a door is
2:04:25
still just really hard for robots. Meanwhile, we're seeing this total automation of coding, and everything
2:04:30
else that we've seen as clever. The really scary future is one in which AIs can do everything except for the
2:04:37
physical robotic tasks, in which case you'll have humans with AirPods, and...
2:04:43
Glasses? Glasses, and there'll be some robot overlord controlling the human through cameras by just telling it what to do, and having a bounding box around
2:04:51
the thing you're supposed to pick up. So you have human meat robots. Not necessarily saying that that's what the AIs would want
2:04:59
to do, or anything like that. But if you were to be like, "What are the relative economic value of things?" The AIs are out there doing computer programming, and the most valuable thing
2:05:06
that humans can do is be amazing robots. Now that being said, I think Moravec‚Äôs paradox is a little bit fake.
2:05:12
I think the main reason that robots are worse at being a robot than they are at software engineering is the internet exists for software engineering.
2:05:20
GitHub exists, and there is no equivalent thing if you had
2:05:27
all mocap of everyone's actions as they were going about their daily lives for some reasonable fraction of the human population, robotics is
2:05:35
also close to solved, on track to be solved at the same rate that software engineering is on track to be solved.
2:05:42
So, this vision is only a sort of decade-long section, but it's
2:05:48
still a pretty terrible decade. Imagine the world where people have lost their jobs, you haven't
2:05:54
yet got novel biological research. That means people's quality of life isn‚Äôt dramatically better. You don't yet have material abundance because you haven't
2:06:01
actually been able to action the physical world in the necessary way.
2:06:06
You can't build dramatically more, because building dramatically more takes robots basically, and people's main comparative advantage is as fantastic robots.
2:06:19
That‚Äôs a shocking, shocking world. Yeah. From the perspective of an average human, I think it actually might be better. Your wages will be higher because you're the complement to something that is
2:06:28
enormously valuable which is AI labor. And a decade, or two after, the world is fantastic.
2:06:38
Robotics is solved, and you decide to get radical abundance basically provided that you have all the policies set up necessary to permit building.
2:06:46
You end up with that same change like the before vs. after photos of Shanghai where 20 years on, it's this
2:06:53
dramatically transformed city. A lot of places in the world probably end up like that over that two-decade period.
2:06:59
But we need to do our best to estimate if this is actually what is on track to happen.
2:07:08
Build SWE-bench, but for all the other forms of white collar work, and measure, and track. That's a great thing that governments should be doing by the way, trying
2:07:15
to break down the functions of their economy into measurable tasks, and figuring out what does the curve actually look like for that?
2:07:23
They might be a bit shocked by the progress there. There's no SWE-bench for a tax eval.
2:07:36
I don't have all the answers here, but then we need to figure out a way to share the proceeds of this economy broadly across people, or invest
2:07:43
heavily in robotics, and collect the data so that we get robotics faster, and we get material abundance faster. Invest in biological research that we get, but all that faster.
2:07:50
Basically try and pull forward the radical upside, because otherwise you have a pretty dark section.
2:07:57
I think one thing that's not appreciated enough is how much of our leverage on the future‚Äîgiven the fact that our labor isn't going to be worth that much‚Äîcomes from our
2:08:07
economic, and political systems surviving. For your million X'd S&P equity to mean something, for your contracts
2:08:16
to mean anything, for the government to be able to tax the AI labor, and give you a UBI off of that, that requires our legal institutions, our
2:08:24
economic institutions, our financial rails surviving into the future. Yes. The way in which that likely happens is if it's also in the AIs best
2:08:33
interests that they follow those rails. By AI I don't mean some monolithic single AI, I just mean firms which
2:08:40
are employing AI, and becoming more productive as a result. You don't want to be in a position where it's so onerous to operate in our
2:08:48
system that you're basically selecting for firms who either emigrate, or who
2:08:54
are doing black market stuff, et cetera.
2:09:00
You want to make it super, super easy to deploy AI, have the equivalent of special economic zones, et cetera.
2:09:08
Otherwise you are just surrendering the future outside of any control that you might have on it.
2:09:15
One of the reasons that I worry about turning AGI into a national security
2:09:22
issue, or having it have extremely close ties with the government, the Manhattan Project thing, is that it disproportionately redirects
2:09:30
the use of AI towards military tech, mosquito drones and whatever.
2:09:37
It also naturally puts other countries in the same frame of mind. If we're developing the mosquito drones, why would China not
2:09:44
develop the mosquito drones? That just seems like a zero-sum race, and not to mention a
2:09:50
potentially catastrophic one. Whereas compute will be limited, we'll need to disproportionately accelerate some things.
2:09:59
To the extent it just remains totally like a consumer free market landscape, it just seems more likely that we'll get the glorious transhumanist
2:10:06
future where they're developing the things that make human life better. Yes, I mean I agree. The case where you end up with two national projects facing off against
2:10:15
each other is dramatically worse. We don't want to live in that world. It's much better if this stays a free market, so to speak.
Automating white collar work
2:10:26
Okay. I want to take issue with your claim that even with the algorithms of today, if we just collect enough data that we could automate white collar work.
2:10:34
First, let me get an understanding of what you mean by that. Do you mean that we would do the analogous thing of pre-raining with
2:10:40
all the trajectories of everything people would do on their jobs? Could you make either manually, or through some other process, some
2:10:50
RL procedure based on the screen recordings, every white collar worker. What kind of thing are you imagining? I mean a continuous distribution of this stuff.
2:10:58
One important mental model to think
2:11:05
about RL‚Ä¶ There is some respect with which longer horizon, if you
2:11:11
can do them, if you can get that reward ever, are easier to judge.
2:11:16
Again, it's come back to that can you make money on the internet? That's an incredibly easy reward signal to judge.
2:11:22
But to do that there's a whole hierarchy of complex behavior. So, if you could pre-train up to the easy to judge reward
2:11:28
signals, does your website work? Does it go down, do people like it? There's all these reward signals that we can respond to because we
2:11:34
have a long, we can progress through these long enough trajectories to actually get to interesting things.
2:11:40
If you're stuck in this regime where you need a reward signal every five tokens, it's a way more painful, and long process.
2:11:48
But if you could pre-train on every screen in America, then probably the
2:11:54
RL tasks that you can design are very different from if you could only take
2:12:00
the existing internet as it is today. How much of that you get access to changes the mix.
2:12:08
As we're training them on longer, and longer horizon tasks, and it takes longer for them to get any signal on whether they successfully complete
2:12:14
the task, will that slow down progress because it takes more compute per task? I do think there's this notion that the longer, the harder tasks,
2:12:23
the more training is required. I'm sympathetic to that naively, but we as humans are very good at practicing the
2:12:32
hard parts of tasks, and decomposing them. I think once models get good enough at the basic stuff, they can just rehearse, or
2:12:39
fast-forward to the more difficult parts. I mean that's definitely one of the big complexities. As you use more compute, and as you train on more, and more difficult
2:12:48
tasks, your rate of improvement of biology for example is going to be somewhat bound by the time it takes a cell to grow in a way that your rate of
2:12:57
improvement on math isn't, for example. So, yes, but I think for many things we'll be able to parallelize widely
2:13:06
enough, and get enough iteration loops. Will the regime of training new models go away?
2:13:15
Will we eventually get to the point where you've got the model, and then you just keep adding more skills to it, with RL training?
2:13:21
That depends on whether, or not you think there's a virtue in pre-training a new architecture.
2:13:27
Basically you make some architectural change, then you probably need to do some
2:13:32
form of at least pretraining a new model.
2:13:38
If RL requires a bunch of inference to do the training in the first place, does that push against the thing you were talking about where we actually need a bigger
2:13:44
model in order to have brain-like energy? But then also it's more expensive to train it in RL.
2:13:50
So, where does that balance out? I think we got to drink the bitter lesson here.
2:13:55
Yeah, there aren't infinite shortcuts. You do just have to scale and have a bigger model, and
2:14:01
pay more inference for it. If you want AGI, then that's what you got to pay the price of.
2:14:06
But there's a tradeoff equation here. There is science to do which everyone is doing.
2:14:13
What is the optimal point at which to do RL? Because you need something which can both learn, and discover
2:14:21
the sparse reward itself. So you don't want a one parameter model. Useless, even though you can run it really fast.
2:14:26
You also don't want a 100T model. It's super slow.
2:14:34
The marginal benefit of its learning efficiency is not worth it. So, there's a Pareto frontier here.
2:14:39
What's the optimal model size of your current class of capabilities, and your current set of RL environments, and this kind of stuff.
2:14:44
And even in the last year there's been much more of a factor of the inference cost. So, just explicitly the bigger the model, the more expensive it is to do
2:14:53
a forward pass and generate tokens. The calculus used to just be, ‚ÄúShould I allocate my flops to more training
2:15:01
data, or a bigger model?‚Äù And now another huge factor is how much am I actually going to do forward passes on this model once it's trained?
2:15:08
My total pool of compute, how do I allocate that across training data compute, and inference compute for the RL training.
2:15:14
And then even within inference, there's all this research on, well, what strategy should I use? Should I sample 10, and take the best?
2:15:21
Do I do this sort of branching search, et cetera, et cetera. And so with RL where you're sampling a whole lot of tokens, you also need
2:15:29
to factor in the ability for the model to actually generate those tokens, and then learn, and get feedback.
Advice for students
2:15:36
If we're living in this world, what is your advice to somebody early in their career, or a student in college?
2:15:42
What should they be planning on doing?
2:15:48
Once again, it's worth considering the spectrum of possible worlds and preparing yourself for that. The action that I think is the highest EV in that case is that at a minimum you're
2:16:00
about to get dramatically more leverage. You already have. Already the startups in YC are writing huge amounts of their code with Claude.
2:16:10
What challenges, what causes do you want to change in the world with that added leverage?
2:16:15
If you had 10 engineers at your beck, and call, what would you do? If you had a company at your beck and call, what would that enable you to do?
2:16:22
What problems, and domains suddenly become tractable? That's the world you want to prepare for. Now, that still requires a lot of technical depth.
2:16:28
Obviously there is the case where AI just becomes dramatically better than everyone at everything, but for at least a while there is‚Ä¶ I think
2:16:37
Jensen actually talked about this in an interview in an interesting way. He's like, "I have a hundred thousand general intelligences around me, and
2:16:43
I'm still somewhat useful, because I‚Äôm there directing the values, and asking them to do things. I still have value even though I have a hundred
2:16:51
thousand general intelligences." For many people, I think that will still be true for a fair while. Then as the AIs get better, and better, and better, and so on, eventually, no.
2:16:59
But again, prepare for the spectrum of possible worlds because in the event where we're just totally outcompeted, it doesn't matter what you do.
2:17:06
In all the other worlds, it matters a lot. Get the technical depth, study biology, study CS, study physics.
2:17:13
Think hard about what challenges you want to solve in the world. Yeah, that's a lot of topics.
2:17:19
That's a lot of shit. You can now. You can. It's so much easier to learn. Everyone now has the infinite perfect tutor.
2:17:26
It's definitely been helpful to me. I would say some combination of: get rid of the sunk cost of your previous
2:17:35
workflows, or expertise in order to evaluate what AI can do for you.
2:17:42
Another way to put this, which is fun, is just be lazier in so much as you figure out the way that the agent can do the things that are toilsome.
2:17:54
Ultimately, you get to be lazier, but in the short run, you need to critically think about the things you're currently doing, and what an AI
2:18:00
could actually be better at doing, and then go, and try it, or explore it. Because I think there's still just a lot of low-hanging fruit of people
2:18:08
assuming, and not writing the full prompt, giving a few examples, connecting the right tools for your work to be accelerated and automated.
2:18:18
Yep, yep. There's also the sunk cost of feeling like since you're not "early to AI",
2:18:25
that you've sort of missed the boat. I
2:18:30
remember when GPT-3 came out‚Ä¶ So backstory on the podcast, when I graduated college I was planning
2:18:36
on doing some sort of AI wrapper startup, and the podcast was just a gateway into doing that.
2:18:43
I was trying out different things and at the time I remember thinking, ‚Äúoh, 3.5 is out.‚Äù People were like, "I'm so behind on the startup scene here‚Äù or whatever.
2:18:53
If I wanted to make my own wrapper‚Ä¶ maybe the idea of the wrapper was inadvisable in the first place.
2:18:58
But every time feels early because if it's an exponentially growing process, and
2:19:05
there are many things, many ideas which are only becoming possible now, right? Exactly. It's that product exponential I talked about.
2:19:10
That's right. Products literally obsolete it. You need to constantly reinvent yourself to stay at the frontier of capabilities.
2:19:15
Do you remember? I had a really shitty idea, and I gave you a call. I don‚Äôt remember what it was. I think it was
2:19:22
like RAG for lawyers, or something. Anyways, I think one of our first interactions was like, "Hey, what do you
2:19:28
think of this idea?" And you were like, ‚ÄúI think the podcast sounds promising.‚Äù
2:19:36
I was right. Which I appreciate. Yeah. I got slightly annoyed at a friend recently who I think is really
2:19:44
talented and clever and interested in AI but has pursued a biology route.
2:19:50
I just kind of tried to shake them like, "You can work on AI if you want to."
2:19:57
Humans are biological general intelligences where a lot of the things of
2:20:06
value are just very general. Whatever kind of specialization that you've done maybe just
2:20:15
doesn't matter that much. Again, it gets back to the sunk cost, but so many of the people, even my colleagues
2:20:21
at Anthropic are excited about AI. They just don't let their previous career be a blocker.
2:20:27
Because they're just innately smart, talented, driven, whatever else, they end up being very successful and finding roles.
2:20:35
It's not as if they were in AI forever. I mean, people have come from totally different fields. Don't think that you need permission from some abstract entity to get involved,
2:20:48
and apply, and be able to contribute. If somebody wanted to be an AI researcher right now, if you could give
2:20:54
them an open problem, or the kind of open problem that is very likely to
2:21:01
be quite impressive, what would it be? I think that now that RL's come back, papers building on Andy
2:21:11
Jones's ‚ÄúScaling scaling laws for board games‚Äù are interesting.
2:21:19
Investigating these questions like the ones you asked before. Is the model actually learning to do more than its previous pass at K?
2:21:25
Or is it just discovering that‚Ä¶ Exploring questions like that deeply are interesting, scaling laws for RL, basically.
2:21:33
I'd be very curious to see how much the marginal increase is in meta learning from a new task, or something.
2:21:40
On that note, I think model diffing has a bunch of opportunities.
2:21:46
People say, "Oh, we're not capturing all the features. There's all this stuff left on the table." What is that stuff that's left on the table?
2:21:52
If the model's jailbroken, is it using existing features that you've identified?
2:21:58
Is it only using the error terms that you haven't captured? I don't know. There's a lot here. I think MATS is great.
2:22:04
The Anthropic fellowship has been going really well. Goodfire, Anthropic invested in recently, they're doing a lot of interpretability
2:22:11
work, or just apply directly to us. Anything to get your equity up, huh?
2:22:17
There's just so many interpretability projects. There's so much low-hanging fruit, and we need more people, and I
2:22:22
don't think we have much time. I also want to make a plug for performance engineering. This is one of the best ways to demonstrate that you have
2:22:32
the raw ability to do it. If you made an extremely efficient transform implementation on TPU,
2:22:41
or Trainium, or Incuda, then I think there's a pretty high likelihood that you'll get a job offer.
2:22:47
But there's a relatively small pool of people that you can trust to completely own end-to-end the performance of a model.
2:22:55
And if you have broad, deep electrical engineering skills, I think you
2:23:01
can probably come up to speed pretty fast on accelerator stuff. You can come up to speed reasonably fast and it teaches you a lot of good
2:23:07
intuitions of the actual intricacies of what's going on in the models, which means that you're then very well-placed to think about architecture and this kind of stuff.
2:23:14
One of my favorite people in thinking about architecture at Anthropic at the moment actually came from a heavy GPU kernel programming background and just
2:23:20
knows the ins, and outs really deeply. He can think about the trade-offs really well. This was fun guys. Thanks for doing it again.
2:23:26
Great to be back.