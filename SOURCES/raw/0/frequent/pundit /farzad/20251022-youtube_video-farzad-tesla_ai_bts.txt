https://www.youtube.com/watch?v=c2hL8tcqsz0
Tesla’s AI Leads Give Behind Scenes Presentation

45,458 views  Oct 23, 2025  #ElonMuskNews #elonmusk #TeslaFSD
Join my exclusive community: https://farzad.fm
Wrap your Tesla using TESBROS: https://partners.tesbros.com/FARZADME...
Get $100 off Matic Robots: https://maticrobots.refr.cc/active-cu...
Use my referral link to purchase a Tesla product https://ts.la/farzad69506

Want to grow your YouTube channel? DM David Carbutt
For 10% discount quote ‘Farzad’   https://x.com/DavidCarbutt_ 

Use my referral code for ElevenLabs AI voice generator: https://try.elevenlabs.io/erm1aiqf3h3n

Get fit! Mention FARZAD when asking for a quote for a custom-built nutrition and fitness plan: https://www.teamevolveperformance.com/

Join this channel to get access to perks:
   / @farzadmediainc  

I worked at Tesla starting from 2017 thru 2021. I spent most of my time in the distribution and supply chain organizations in leadership positions.

00:00 Introduction and Recent Accomplishments
02:46 End-to-End Neural Networks
03:13 Real-World Examples and Challenges
11:16 Evaluation and Simulation
15:40 Scaling Autonomous Technology
16:20 Humanized Robots and Optimus
17:03 Conclusion and Future Directions
17:30 Q&A Session

Before Tesla, I was a Director of Business Intelligence and Pricing at the largest Pet Food & Supply distributor in the US, Phillips Pet Food & Supplies. My wife and I also owned a small business in Bethlehem, PA between 2016 and 2019.

I have been a shareholder of Tesla since 2012 and currently own Tesla stock. Nothing I say constitutes as investment or financial advice.

My thoughts are my own and are not representative of everyone who currently works, or has worked at Tesla, Phillips, or any other company or organization I discuss on this channel.

If you like this content I would greatly appreciate your likes and shares!

#Tesla #elonmusk #twitter #tsla #TeslaNews #TeslaFSD #TeslaFSDBeta #teslastockanalysis  #TeslaUpdate #investing #ElonMuskNews #teslanews #EV #FSD #investing

---

Introduction and Recent Accomplishments
0:00
This is the work of the Tesla AIA team
0:01
over the last several months. So I'm
0:02
talking on their behalf here. I'd like
0:04
to start with some of the recent
0:05
accomplishments that the team has
0:07
achieved. Earlier this year around June
0:09
July we launched our robot taxi service
0:12
where if someone was in Austin or in the
0:14
s of Bay Area they can hail a robot taxi
0:16
service from Tesla and then Austin you
0:19
know below 40 m per hour you can get a
0:20
car without anyone inside the passenger
0:22
seat. So it is just driving using this
0:24
cameras and neuronet networks. We also
0:26
did this thing where we delivered the
0:27
first customer purchasable vehicle from
0:30
the Tesla factory in Austin directly to
0:33
a customer's home somewhere else in
0:35
Austin and we covered over like 20 30
0:37
minutes of driving driving across
0:39
highways, city roads, parking lots, etc.
0:42
to deliver the car. You can see the car
0:43
driving itself and this is a production
0:45
vehicle out of the factory using
0:47
production cameras and the same computer
0:49
that's in every car and so on. Every
0:51
Tesla that's manufactured in the US also
0:53
delivers itself from the manufacturing
0:56
line all the way to the loading docks
0:57
that's a couple miles away. So this year
1:00
we made a lot of strides in autonomy and
1:02
we continue to scale the technology to
1:05
be even more robust and resilient so
1:07
that we can go to more locations have no
1:09
one inside the car and so on. Today I'd
1:11
like to touch on some of the
1:12
technologies that help power this
1:15
self-driving. So first of all we
1:17
switched to having a single large uh end
1:19
toend neural network that can take in
1:21
pixels and other sensor data as an input
1:23
and then just produce the next action as
1:25
an output. So while it's riding the car,
1:27
it no longer is doing explicit
1:29
perception of vehicles, road boundaries
1:32
or things like those. They can be
1:33
implicit and they can be trained as a
1:36
things but then the entire thing the
1:38
video raw video streams go in and then
1:40
the actual actions to take are produced
1:42
by neural network and this it's been so
1:44
for the last few years and this has been
1:47
extremely good. The reason being we used
1:50
to work on an explicit modeler approach
1:52
because it was so much easier to debug
1:54
and so on. But what we found out was
1:56
that codifying like human values is
1:58
really difficult. For example, you could
2:00
in order to break for an obstacle or go
2:02
around, you could go as fast as possible
2:04
and then break a bit later or break
2:07
earlier for more smooth driving. And
2:09
this really comes down to alignment of
2:11
preference to human values as opposed to
2:14
there's one objective value of like how
2:16
much braking and when to apply. You
2:17
don't want to break too early either
2:19
because then be too slow. And if you
2:20
don't want to break too late either,
2:21
then it's too uncomfortable. And this
2:24
varies from situation to situation. The
2:26
amount at like low speed, high speeds
2:28
are very different. So it's really hard
2:30
to codify what humans actually want. And
2:32
you can't write it in code. And also the
2:34
interface between traditional perception
2:36
and planning is very illdefined. It can
2:39
be very lossy and lose critical
2:41
information where you might need to
2:42
propagate the uncertaintities through
2:44
the entire system. That's really hard.
End-to-End Neural Networks
2:46
Going to end neural network also allows
2:48
for your compute to be homogeneous. If
2:50
you have traditional branching compute,
2:52
you can have nondeterministic amounts of
2:53
latencies. In a real-time system, you
2:55
don't want to have the kind of
2:56
latencies. You you want to have
2:58
predictable deterministic latencies and
3:00
such a single neural network doing
3:02
entire thing offers that. Overall, this
3:04
is what we believe as the right path to
3:06
solving robotics as opposed to this kind
3:08
of model brittle systems that were
3:10
developed earlier. I to give a few
3:12
examples of why this is hard. For
Real-World Examples and Challenges
3:14
example, this vehicle is driving on a
3:17
birectional road. To avoid this puddle,
3:19
you got to go to the oncoming side. If
3:21
you had to ride this using explicit cost
3:23
functions, it's going to be really hard
3:24
because typically you don't want to go
3:25
into the oncoming side. But then what is
3:28
the pain of driving through a puddle
3:30
versus the risk of going through an
3:31
oncoming lane is hard to write an
3:33
explicit code. But here, if you look at
3:35
the scene, it's pretty obvious that you
3:36
can go here. You have a lot of
3:38
visibility. There's oncoming vehicle
3:40
coming and you should definitely go into
3:41
the oncoming side. It's also hard to
3:43
understand intent. For example, here on
3:46
the left side, there's a bunch of
3:47
chicken crossing the road. I'm not sure
3:48
if the video is clear for you guys, but
3:51
there's a bunch of chicken crossing the
3:52
road. This is driving on self-driving
3:54
software, and it's just waiting for the
3:56
last chicken to cross here, and then the
3:58
car proceeds on its own. This is all
4:00
just driving on its own. In this case,
4:02
the car was smart enough to understand
4:04
that the chicken actually out crossing
4:05
the road and was patient enough, even
4:06
though there was no collision risk, it
4:08
was just the right thing to do. But in
4:10
other cases,
4:11
look, it's going around it. It's going
4:13
around them. Oh, look. It's changing its
4:15
backing up to go around them. Look at
4:17
that. It backed up to go around the
4:19
geese.
4:20
In this case, the model understands that
4:22
the geese are actually not crossing.
4:24
They're just stationary and decided to
4:26
reverse and then go around them. So,
4:27
it's not this is something it's very
4:29
hard to write in explicit code what is
4:32
the right thing to do here. But looking
4:33
at the scene end to end and for humans
4:35
too, it's pretty obvious what was the
4:37
right thing. And that is why end to end
4:38
is way better than a modeler approach.
4:40
That said, it's not that easy to develop
4:42
such a system. It's pretty difficult. In
4:44
fact, and I'd like to touch on three
4:45
different aspects of it that make it
4:47
really difficult. First is the curse of
4:49
dimensionality. What I mean by that is
4:51
that the input context is quite
4:55
enormous. Especially our cars have seven
4:57
or eight cameras depending on the
4:59
vehicle. Each produces, you know, 5
5:00
megapixel camera streams running at like
5:03
high frame rate. And even if you want to
5:04
have 30 second context or something like
5:06
that along with the other necessary
5:08
inputs like the routes and then the
5:10
vehicle speed and other kinematic data,
5:12
it can easily be more than billions of
5:14
tokens like two billion tokens for a 30
5:17
secondond context window. And if you
5:19
natively just use some kind of
5:21
transformer, it's just going to be
5:23
really difficult task of mapping two
5:25
billion tokens down to roughly two
5:26
tokens, which is what is the next
5:28
steering and acceleration that the car
5:30
should take. Thankfully Tesla has a huge
5:32
advantage here which is the Tesla feed
5:34
is quite large. So it's basically has
5:37
access to the Niagara Falls of data and
5:39
it's up to Tesla to use this data to
5:42
solve this problem. You don't want
5:44
furious correlations from the two
5:46
billion tokens down to the two tokens.
5:48
You want the right correlations as to
5:50
why those two output tokens must be the
5:52
right tokens. So what we do is we refine
5:55
this 500 years of driving which
5:56
obviously is more data than we could
5:58
ever store on our clusters to the
6:01
essential amount of data which covers
6:03
the overall spectrum of driving. There's
6:05
different ways we can collect this data
6:07
based on like explicit triggers or small
6:08
neural networks targeting specific
6:10
scenarios. Constantly evaluating what
6:12
the model predicted versus what actually
6:14
happened and things like those. We can
6:15
use such tricks to extract just the
6:18
right amount of data, the right quality
6:20
and the kind of data that we need for
6:22
our training. Here are some examples of
6:25
what that kind of counter case data
6:26
might look like. These are examples in
6:27
manual driving like people are just
6:29
still driving and then they encounter
6:31
such rare scenarios. We can have
6:33
triggers that catch these. This is very
6:35
useful data to train on and it's just
6:37
not normally accessible. You can't stage
6:40
this that easily because it it requires
6:42
the stationary scene the speed of all
6:44
the vehicles in state space. This is
6:46
really hard to achieve and test has a
6:48
unique advantage here that can tap into
6:50
the entire fleet to get this data. Once
6:53
we get this data you can use this to
6:55
train our end toend model like I
6:56
mentioned before and it just generalizes
6:58
to extreme scenarios like these ones and
7:01
not just reacting at the last second but
7:03
proactively safe. I would like to show a
7:06
demonstration of like how that
7:07
manifests. Like this is again a
7:09
self-driving system in operation here
7:11
just on a highway. Here the car in front
7:14
of us spun out and then hit the barrier
7:17
and the self-driving system pulled over
7:19
safely. But what is quite interesting is
7:21
that if you watch the video again like
7:24
pause where the car the ego car breaks
7:27
like here. If you notice this car here,
7:29
it's like spinning out of control and
7:31
it's going to hit the barrier and it's
7:33
going to bounce back into a lane. It
7:35
requires so much intelligence to know
7:37
that this is not a first order collision
7:39
here because this at this frame
7:43
the Tesla already determined that this
7:45
vehicle there's something wrong here and
7:47
started applying the brakes. It did not
7:49
wait for the car to hit the barrier and
7:50
then bounce for it velocity change or
7:53
something like that. This is a second
7:54
order effect that needs to model. It
7:56
could have also attributed this to a
7:57
lane change of this vehicle but it did
7:59
not. It understood because it was
8:00
requesting roughly 4 m/s square of
8:03
braking which is not a light amount of
8:04
braking. This is only possible if you
8:06
have a ton of data and cover all these
8:09
corner cases. That is how you can
8:11
provide both you know safe and also a
8:13
smooth ride because a less intelligent
8:16
system would wait until the direction
8:18
has changed or actually some bad event
8:20
has happened way before it actually goes
8:22
wrong. That is what you get with an end
8:24
to end system with a ton of data.
8:26
Secondly, so if it's an end to end
8:28
system, how do you guys debug? How do
8:29
you actually develop this thing? If
8:31
something goes wrong, how would you
8:32
know? Just because it's an end to end
8:33
system does not mean that it cannot
8:35
predict anything else. The same model
8:36
can be prompted to predict arbitrary
8:39
things including occupancy, other
8:41
objects, traffic lights, traffic signs,
8:44
road boundaries. Even just in plain
8:46
language, you can ask it why it made
8:48
some decisions, does it understand the
8:50
scene, and so on. All of this helps
8:52
interpret the model's understanding and
8:54
also sort of like gives some guarantees
8:56
around the safety of the entire system.
8:58
So in practice it sort of looks like
9:00
this where you can take in orbit sensor
9:02
data and prompt it to produce arbitrary
9:04
things but in the end the only thing
9:05
that actually matters in the car is the
9:08
control actions that it produces.
9:09
Everything else is auxiliary but they
9:11
can be quite helpful for predicting the
9:14
correct control action. One specific
9:16
task that I would like to go over is the
9:18
gshian splatting which in the last few
9:20
years has been quite prominent in the
9:22
field. What I'm showing here on the left
9:24
side is the traditional gshian
9:26
splatting. The problem typically is that
9:28
vehicles drive in sort of a linear
9:30
fashion going forward and there's not a
9:32
ton of baseline because of this. If you
9:34
just took the same camera views of the
9:36
vehicle's motion and ran traditional
9:38
gshion spanning, you know, I don't know,
9:39
nerf studio or what have you with the
9:41
same camera views, the views close to
9:44
the train views look great, but then if
9:46
you go to normal views that are far away
9:49
from the train views, it breaks down
9:50
quite a bit. But what we have in the
9:52
middle column here is Tesla's variant of
9:55
caution splatting where the same model
9:56
can produce caution. It generally is
9:59
much better with the same limited camera
10:01
views that the left side has. It can
10:03
also produce semantics as shown in the
10:05
third column. And the cool thing is that
10:06
it can run ridiculously fast compared to
10:09
traditional caution splatting which can
10:10
take tens of minutes. So you can see
10:12
that this can update to scenes quite
10:15
rapidly. If you use the same traditional
10:17
splatting, a lot of normal views might
10:18
look blurry or like fuzzed out. But as
10:21
I'm rotating here in 3D space, a lot of
10:24
the structure remains intact. And such
10:25
interpretable representations can be
10:27
used to debug the system. you know like
10:29
if it's going faster or slower you can
10:31
easily observe okay is it safely
10:33
avoiding some obstacle or not like I
10:34
mentioned earlier we can also use
10:36
natural language to interact with the
10:38
model the same model can sort of like
10:40
point to things explain why it made some
10:42
decision we shouldn't need all of this
10:44
to drive the car in real time but then
10:45
if you do need it you can always think
10:47
longer and produce reasoning tokens to
10:50
then produce the right action that is
10:53
consistent with the reasoning of the
10:54
entire thing and again it can be the
10:56
same same model that's running in the
10:58
car But based on situation can use it or
11:01
can directly produce the control
11:03
actions. If it's not required to reason
11:05
in such detail because if you keep
11:07
reasoning in detail for every single
11:08
thing then it's going to take too much
11:10
latency but then wherever is needed it
11:13
could reason longer to produce the right
11:15
answer. The last piece that I would like
Evaluation and Simulation
11:17
to touch on is the evaluation which
11:20
actually is the most difficult of these
11:22
three problems I mentioned. If you just
11:24
train a bunch of data from drivers and
11:26
then train the models and just like
11:28
scale it, your openloop performance like
11:30
look amazing but then it may not
11:32
translate well to how the car actually
11:34
drives lot of reasons for this. I don't
11:36
want to get into the details there but
11:38
then it is a very important problem that
11:39
requires a lot of attention to get good
11:42
performance out of the system evaluation
11:44
sets. It's not just you know if you
11:45
randomly sample data from your feet most
11:47
of it is going to be boring highway
11:49
driving. You don't want to just be
11:50
evaluating there. That's where the early
11:52
data engine approach that I mentioned
11:54
comes in handy where we can build a
11:56
balanced fully covering
11:58
set which is extremely important but
12:01
very tedious work. One great thing we
12:03
figured out was that we can use the
12:05
cheap to collect state action pairs
12:07
inverted and then basically build a
12:09
world simulator where given past state
12:12
and then actions you can synthesize
12:15
newer states this condition on the
12:17
actions. This is quite easy to collect
12:20
because you can get it for free. You
12:22
don't need optimal driving or something
12:24
like that. Any kind of trash driving is
12:26
good enough for this kind of simulator
12:27
because it need to simulate the edge
12:29
cases.
12:30
And once you have that, you can connect
12:32
that with our policy network that's
12:34
driving the car and then they can both
12:37
run in sequence in a loop to sort of
12:40
simulate the world. Here's an example of
12:42
our roll out through learned neural
12:45
network simulator. I'll let you watch
12:47
the video for a second here, but the
12:49
entire thing is generated video. There's
12:52
eight cameras. The top row is all the
12:55
front cameras of the car driving
12:57
forward. The middle ones are the left
12:59
and right side cameras on the bottom row
13:01
is the rearfacing cameras. All eight
13:04
cameras are generated simultaneously by
13:06
a single new network and takes action as
13:09
an input. So, you can steer the network.
13:11
I'll show you in the next couple of
13:12
slides how that can work. I'm usually
13:15
blown away by the consistency of
13:17
generation across different cameras.
13:18
Even the vehicle rims are consistent.
13:21
The traffic lights this is over like a
13:23
minute minute and a half long generation
13:25
of eight 5 megapixel video streams. In
13:28
addition, you can take like past issues,
13:30
you know, like a year old issue and then
13:32
you can rerun the latest neural networks
13:34
to see how they would perform. On the
13:36
left side here was the original failure
13:39
where it was maybe a bit too close to
13:40
the pedestrian. And then on the right
13:42
side we are evaluating a new neural
13:44
neural network that's driving the car.
13:46
You can see that the policy starts
13:47
offsetting way earlier as soon as the
13:49
pedestrian emerges from behind the car.
13:52
It's super useful for such kind of
13:53
evaluation because you don't want to
13:55
just re-evaluate on newer mileage. You
13:58
just want to if you have a database of
14:00
older issues, you just want to replay
14:01
them and verify that you are doing a
14:03
good job on this past issues. You can
14:05
also synthetically create new issues. In
14:07
this case, the original video on the
14:09
left side that vehicle was just going
14:11
forward in its own lane. But then on the
14:13
right side, we can make the condition
14:15
the vehicle to cut across our path and
14:17
then inject this kind of adversarial
14:20
event to test the systems corner cases.
14:23
And you can see that the scene the rest
14:25
of the scene remains consistent. All the
14:26
other vehicles are moving the same way.
14:28
age that this one vehicle cuts across
14:30
and that's how you can mix lots of
14:32
synthetic data sets to verify the corner
14:35
case performance of a self-driving
14:37
system. If you are fine with reducing
14:40
the test time compute a little bit, you
14:42
can get close to realtime rendering
14:44
performance. This is the same model
14:45
that's rendering the eight cameras with
14:48
lower test time compute and you can see
14:50
that you can actually drive this in real
14:53
time. the FPS is high enough even though
14:55
it's generating eight 5 megapixel
14:57
streams in parallel. So obviously you
14:59
can see that it's responding to the
15:02
users driving commands they can steer
15:04
and break and then navigate the world
15:06
just as if they're driving in the real
15:08
world but the entire thing is neural
15:10
network generated video streams. I think
15:12
the driver is trying to do something
15:13
adversarial by you know going on the
15:15
curb and then just demonstrating that
15:16
the generalization of the simulator is
15:19
quite good. Yeah, this entire video is 6
15:21
minutes long and you can just have
15:23
consistent video generation for a very
15:24
long time and responding to controlled
15:26
commands. You can hence imagine like how
15:28
this tool can be quite powerful both for
15:31
evaluation purposes but also for closed
15:33
loop reinforcement learning where you
15:35
can just let the car drive and then
15:37
verify that it doesn't collide with
15:38
anything for a very long time. So yeah,
Scaling Autonomous Technology
15:40
what's next? We want to scale the
15:42
service to be much much better and then
15:44
unlock the entire fleet of Tesla
15:46
vehicles to be fully autonomous. This is
15:47
the cyber cab which is our nextg vehicle
15:50
which only has like two seats and
15:51
designed for the purpose of robot taxi.
15:54
It's going to have the lowest cost of
15:55
transportation across even public
15:57
transportation and it's all powered by
15:58
the same neural networks that you saw
16:00
earlier. We find that the approach that
16:03
we took is quite scalable. It scales
16:05
across different vehicle platforms,
16:06
different spatial locations, different
16:08
weather conditions and overall offers a
16:10
very safe, comfortable and fast ride to
16:13
the users. There's one more thing which
16:15
is it's not just scaling self-driving at
16:17
all the vehicles but also scaling it to
16:19
other forms of robots the humanoid
Humanized Robots and Optimus
16:21
robots too. Tesla builds humanoid robots
16:23
we call them Optimus and the same
16:25
technology that we developed here for
16:26
self-driving transfers most seamlessly
16:29
to other forms of robots. I like to show
16:31
one example here like the video
16:33
generation also works for Optimus. So
16:36
this is Optimus navigating around the
16:38
Tesla factory and all of these are
16:40
generated videos and you can see that
16:42
they are all quite consistent. You can
16:44
also just like how we did for the car
16:46
action condition it. So here the top
16:49
left is the robot just going straight.
16:51
That was the ground truth but then you
16:53
can have different actions like going
16:55
left or right or some other direction
16:57
and then it can correctly generate the
17:00
pixels for the action. So yeah, in
Conclusion and Future Directions
17:04
conclusion, Tesla is all in on robotics.
17:07
Their company just focused on producing
17:09
intelligent, useful, large scale robots
17:12
for helping everyone in the world. And
17:14
if you are working on AI, robotics,
17:17
computeration, etc. I encourage working
17:19
at Tesla because it has like extreme
17:21
real world impact. It's fully in on
17:23
solving robotics. The team is super
17:25
pumped. I think it's a great place and
17:27
is the best place to work on AI right
17:29
now. Thank you. So we can have question
Q&A Session
17:32
by the way Ashok you can see the room
17:33
actually so we have shared the screen so
17:35
you can see the whole audience thank you
17:38
thank you also for your interesting
17:40
presentation I have a question about the
17:42
end to end paradigm
17:44
yeah what we see end to end system that
17:49
at the level of modularity
17:52
implicitly or explicitly for inance
17:54
different a sensors require different
17:57
encoders because different sensor monies
18:00
require by different types of attention,
18:02
spatial attention or colossal attention.
18:04
So
18:06
do we still need this sensor specific
18:13
can be handled by a transformer?
18:16
I guess a sensor specific encoder could
18:19
still be more efficient but it's I would
18:22
say it's an empirical answer. You just
18:23
try out different encoders whatever
18:25
works best you use it but you don't need
18:27
to keep in mind the real-time
18:28
considerations. If we just nly use a
18:31
large transformer for this purpose, like
18:33
I mentioned, there's a lot of input
18:34
tokens. It could be billions of input
18:36
tokens. If we just natively patchify it
18:39
and use it, you're going to run into
18:40
compute struggles and not not going to
18:43
meet your realtime latency requirements.
18:45
So you know for latency considerations
18:49
interpro
18:56
specific ways of tokenizing the
19:01
yes the end to end is not conflicting
19:03
with the sensor specific ways of
19:05
tokenizing things. The only requirement
19:06
is that the gradients flow all the way.
19:09
Hello. Okay. So I had a couple of
19:10
questions which the foundation
19:13
evaluations
19:15
you show some world
19:18
sorry they should be simulate necessity.
19:22
So when tested and has a good quality
19:28
produced from that one neural network
19:32
which will allow to evaluate quantity of
19:35
me for example real data you do
19:40
enable
19:43
modeling how accurate is my flow where
19:45
accur is my trajectory of the area that
19:50
isn't something that knowledge I think
19:52
So topless property with water bottles
19:55
but is there a danger that when you
19:56
start modeling the water bottle it
19:58
produces blood and then you can't really
20:00
tell
20:01
I mean there even if it's closed loop
20:03
you still have knowledge of how the
20:05
scene evolves so you could always verify
20:07
it against any kind of objective metrics
20:10
around for example collisions or how
20:12
much jerk or whatever parameter that you
20:15
have used for your controls. So that's
20:17
not conflicting. The only downside I
20:19
would say of using closed loop is that
20:21
it's quite expensive. You want to make
20:22
sure that you use the right tool for the
20:24
right thing that you're measuring. If
20:25
something is for perception for example
20:27
even though it's an end to end system
20:28
like I mentioned you can obviously have
20:30
intermediate outputs and you can measure
20:32
perception using open loop because your
20:35
your prediction that does not affect the
20:36
scenario as much but then for action
20:39
your prediction there affects the
20:40
environment quite a bit. So that's where
20:42
it becomes tricky to just depend on open
20:44
loop evaluation. Thank you. Hi, thank
20:47
you for the dice. I have two questions
20:50
for you. The first question is that you
20:52
show some valation model of driving but
20:57
do you still maintain the session and
21:00
learning like modules and still use the
21:03
vector space representation
21:06
for your autonomous driving.
21:10
And the second one is that it seems the
21:13
al your model is seems to be
21:16
acceleration and steering but do you
21:20
produce the the trans the way point on
21:23
the your planning and then convert them
21:27
into the the acceleration the steering
21:30
or that is the direct out the AM model
21:34
yeah I don't want to get into the
21:35
specifics of the exact architecture but
21:37
the main premise of end to end is that
21:40
the gradients must flow end to end. As
21:43
long as that is true, you can have
21:45
auxilary outputs not have them. You can
21:48
predict steering curvature or predict
21:50
other trajectories. As long as the only
21:53
requirement is that the gradient must
21:55
flow into end that the learning like
21:57
every bit of information that's flowing
21:59
in from the sensors gets used for the
22:01
decision making. That's the only
22:02
premise. Everything else is an empirical
22:05
question. You can do different things
22:06
and see what works best and then pick
22:08
that one for your setting. Some people
22:10
prefer some output space, other people
22:12
prefer something else. But I would say
22:14
that's sort of the brands give out
22:16
things and no op in terms of like what
22:17
actually matters that doesn't matter as
22:19
much.
22:19
Thank you. Do we have any other
22:21
questions? If not, let's give a huge
22:24
round of applause to Ash. Thank you for
22:26
coming.
22:27
Thank you. Thank you for listening to my
22:28
talk.
22:28
Thank you. So we are going to continue
22:30
with our next speaker Jimmy. J waves
22:34
science department ver the research
22:36
teams to unlock new breakthrough to
22:38
enable those breakthrough to have
22:40
meaningful impact for business and to
22:42
technical and the business strategy to
22:44
ensure we stay at the forefront of
22:46
innovation. Jamie had been at the
22:47
forefront of apply research for the past
22:50
20 years before joining V J was partner
22:53
research partner director of science at
22:55
Microsoft and head of the mix reality
22:57
and I labs while at mic foundational
23:00
future for Microsoft kinetic Microsoft
23:02
line of mod sampling and input devices
23:04
and the hand and eye tracking that
23:06
enable hollow lens to interaction model
23:10
Jimmy has a PhD in computer from the
23:12
University of Cambridge and have written
23:14
multiple best paper and best demo ever
23:16
at top tier academic conferences. He was
23:18
elected a fellow of the Royal Academy of
23:21
Engineering in 2021. That's number you
23:24
gave me for this talk. Thank you.
23:25
All good.
23:26
Yeah. For me. Okay. We got the right
23:27
thing there. We got the right thing
23:28
there. We got audio. Hopefully you got a
23:31
camera. Okay. Awesome. Let's go. Good. I
23:35
think it's still morning. So, good
23:36
morning everyone. Great to be here.
23:38
Thanks for organizing this fantastic
23:40
workshop and amazing set of speakers.
23:42
So, I'm Jamie. I'm from Wave and I I
23:44
lead our research team among many other
23:46
things and I'm going to be talking a
23:49
little bit about Wave and where we are
23:50
and where we're going and a few little
23:53
sort of research snippets as well
23:54
towards the end. Well, unfortunately for
23:57
most of us, the reality of driving looks
24:00
much more like this on a day-to-day
24:01
basis. And in fact, the average North
24:03
American driver spends two full days of
24:05
their lives each year stuck in traffic.
24:08
Wouldn't it be amazing if we could get
24:09
that time back to read to play with our
24:13
kids to work? Well, seriously, there is
24:15
a huge toll we play as a society for our
24:17
use of the roads and our mobility.
24:20
Statistics from the WHO here. And the
24:23
sad fact is that the majority of those
24:26
accidents are caused by human error. Uh
24:29
these are things that machines shouldn't
24:31
fail at. And so I'm far from the first
24:34
person to have asked the question,
24:35
wouldn't it be great if cars could drive
24:38
themselves? And for these reasons, plus
24:40
the potential huge economic
24:42
possibilities of this, self-driving has
24:45
become the space race of our generation.
24:48
And we've seen just a few of the
24:50
companies in this space already this
24:52
morning talking here on the stage. But
24:54
despite progress, we still see almost
24:57
daily headlines like this appearing in
25:00
news outlets. Some of this is unfair.
25:03
Absolutely. And you know, usual media
25:06
hysteria to plug holes at things. But
25:08
equally, it's very true that autonomy is
25:12
not yet everywhere. We've seen great
25:15
examples of Whimo. They've done an
25:17
amazing job at deploying robo taxis, but
25:19
they're in a very small handful of
25:21
cities around North America at the
25:22
moment. We just saw Tesla and they've
25:25
done a a phenomenal job with
25:27
self-driving. You'll come back to that
25:28
in a second. But Tesla only sells 2% of
25:32
the world's automobiles.
25:34
How do we get this to be truly a
25:37
vehicle? And that's what we're working
25:39
on at Way. What does this mean? Well, it
25:41
means wherever you go, any city around
25:44
the world, this should just work.
25:46
Whatever vehicle you're in, it should
25:48
have autonomy built in. And ultimately,
25:50
it's not just about cars and automotive,
25:52
but it's about all of these other forms
25:55
of autonomy, robotics that could
25:57
transform how we work as a society. So,
26:00
a little history lesson, and we've heard
26:01
a little bit variance of this over the
26:03
course of the morning, but here's my
26:05
take. So, after the DARPA grand
26:07
challenges in the 2000s, people started
26:10
to wake up to the possibility that
26:13
self-driving might be a reality. And
26:15
that was further accelerated by the
26:17
advent of imageet and all of the
26:20
excitement of comnet in the 2012s and
26:22
beyond. And that's led to some of the
26:24
early investments in this space. But
26:26
they took a very reasonable approach.
26:29
You'd think as an engineer, you know,
26:31
let's throw some sensors at this. Let's
26:33
throw a bunch of computer vision
26:35
perception algorithms at the problem.
26:36
Recognize the pedestrians and the cars
26:38
and other things. And let's use maps,
26:41
right? We can we know now how to map out
26:43
cities to centimeter level detail and to
26:46
localize ourselves against those. And
26:48
then once we've got all of that
26:49
information, great. Let's then feed that
26:51
into a handdesign set of rules or more
26:54
lately a learn set of rules for planning
26:56
and prediction. And then that actual
26:59
vehicle and this can be made to work.
27:03
That is patently obvious now. We have
27:05
real deployments on the road of this
27:07
today. But it has proven really slow and
27:11
expensive to scale for all of the
27:13
reasons that I've got up here and more.
27:16
And so we've seen a pivot from at least
27:19
some companies towards an alternative
27:21
that I'll be talking about next. Now
27:23
while all of that was happening in the
27:25
world of AV in the world of AI, there
27:28
were all sorts of exciting advances.
27:31
Here's just a few and given updating
27:34
this slide because it moves too fast.
27:36
There was a huge sway of new
27:38
developments over the last decade and a
27:40
half. The foundations of modern AI,
27:42
things like the data sets and
27:43
benchmarks, self-supervised learning,
27:45
foundation models, scalable computing
27:47
architectures,