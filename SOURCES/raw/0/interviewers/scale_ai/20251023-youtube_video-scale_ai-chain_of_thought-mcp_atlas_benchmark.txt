https://www.youtube.com/watch?v=c34W8hmTxHo
Chain of Thought | Leaderboard Deep Dive - Scale's MCP Atlas Benchmark
364 views  Oct 23, 2025  Human in the Loop with Scale AI
In this episode of Chain of Thought,  @Scale_AI 's  Brad Kenstler (Head of Agent Capabilities and Environments) and Chetan Rane (Head of Product, Agents) sit down with Sami Hassaan (AI Product Manager) and Chaitanya Bandi (ML Research Scientist) to go over Scale's new benchmark - MCP Atlas. 

MCP-Atlas evaluates how well language models handle real-world tool use through the Model Context Protocol (MCP). Unlike benchmarks that test simple function calling or simulated APIs, MCP-Atlas measures performance on multi-step workflows that require discovering the right tools, calling them correctly, and synthesizing results into accurate answers.


Topics covered:
0:00 Intro
0:49 Overview on MCP
4:48 MCP vs APIs and popular MCP servers
7:26 Who's building MCP servers
11:44 How Scale evaluates MCP capabilities 
21:07 Overview on Scale's MCP Atlas benchmark
24:35 Open-sourcing MCP Atlas
26:53 Other tool-use benchmarks
29:03 Examples of MCP Atlas tasks and rubrics
35:33 Results and insights from MCP Atlas
39:27 Future of MCP and tool-use benchmarks

Check out MCP Atlas here: https://scale.com/leaderboard/mcp_atl...

---

Intro
0:01
[Music]
0:10
Welcome to Chain of Thought. Uh on this week's episode, we're going to be talking a bit about Scale's latest
0:15
research in benchmarks, specifically for MCP evaluations and MCP agents. Uh my
0:21
name is Brad Kensler. I'm the head of agent capabilities and environments research here at scale. I'm Cha Chaitana. So I've been working
0:28
on the MCP well for a while. So excited to share what we have. I'm Sami. Uh I work as a product manager
0:34
owning all things tool use. And I'm Chathan. I lead the product team for agents and environments. So we have
0:40
lots of exciting updates to share on this new benchmark. Uh you know, which is one of the first to test out uh MCP
0:47
capabilities. Uh and we'd love to dive into the research behind this, but do you mind giving us an overview of what
Overview on MCP
0:52
MCP actually is? Yeah. So MCP
0:57
uh stands for short for model context protocol. It's basically roughly it's in
1:03
the name just you know it's a way for standardizing how you provide context to models and I mean other communication
1:11
protocols exist but this is the one that is the most widely adopted in terms of standardizing it. So a brief context on
1:17
that uh LLMs are like the brains of the system. We need in order to make agents
1:23
that are reliable, that are actually able to do things uh that we do in a day-to-day basis, they need access to
1:29
information that they can retrieve externally and also be able to make changes in the state outside of their own internal memory.
1:35
Yeah. And so to do that, we need ways to connect LLMs with a lot of different tools. Now, MCP is the most widely
1:43
adopted standardization of that. uh previously we had many that you know um
1:50
maybe they weren't as adop widely adopted in terms of standardization right and this is a standard that has been
1:56
really embraced by developers so it's been uh it has gone viral the the way
2:01
and uh you know AI is all a developer play so whatever developers embrace is what's going to be uh there for a you
2:08
know for a while so MCP is something that developers really like right and like I'm curious like what
2:14
what is what came before MCP? Like if I was building an Asian application in
2:19
like 2022, right? Like how did I do that? And like how does MCP sort of
2:25
change the game for the space today? Typically back in back in 2022, the conversation
2:32
was most around rag systems. And so you'd have um which is complimentary to
2:37
MCP but it's basically having external information uh that you uh get and then
2:43
plug into your LLM. So it's more grounded but the communication between
2:49
uh different servers different external services and models that wasn't standardized. OpenAI came up with their
2:54
function calling API. That's one way one framework they provide. Other companies like land chain they have their own
3:00
systems for how to standardize communication between models and external services. Uh but MCP came out
3:06
last year and it provided a way for both ends of it the model builders to adopt it and also the the third party services
3:13
to come up with abstractions that anybody can then use. So yeah, end of 2023 to be exact. But yeah,
3:20
it's been it's been amazing since they came out uh led by Anthropic. Um and it's been it's been great after that.
3:27
Yeah. Yeah. I think I think one of the exciting things about MCP is like how
3:32
the standardization around this protocol has made it easier for uh both sides of
3:37
the ecosystem to like build something that's compatible, right? Like if you're thinking about, hey, I'm a SAS provider,
3:44
right? And I want to make my service uh readily available to um model uh MCP
3:51
clients, right? Or agents. I now have a protocol that I can design for. I can release an MCP server. Um, and so now
3:58
anyone that's actually building an agent, right, it's it's dead simple for them to just use that MCP server,
4:03
integrate it into their MCP client. Um, and so like this sort of um, decoupling,
4:08
so to speak, of like, you know, having to build uh, fitfor-purpose connectors, I think, is is what's really
4:14
turbocharging a lot of this and making it exciting. Absolutely. Yeah. Yeah. So to add to that, there's
4:19
let's say three parts, right? There's the developer and then there's all the model builders and LLMs right here on
4:25
the left and then there's the developer and all the third party services here on the right. Right? Each one of them can have its own line
4:30
and how it's integrated. So developers had to write their own adapters and so MCP makes it such that you don't have to
4:36
each one of those lines can be abstracted away and it's very simple to plug any third party service with any
4:42
model builder as long as they adopt it which recently we've seen a lot of companies adopt it. So super cool. So
MCP vs APIs and popular MCP servers
4:48
what like what specifically about MCP makes it different from like traditional APIs?
4:54
I think it's more of a standard. So um think of REST API for web development. So you know before that there used to be
5:01
a lot of protocols for web development. Rest API just made it easier for uh folks to uh educate themselves on how to
5:08
build applications and that's the standardization that's bringing out the power of MCP. So it's more of hey uh you
5:15
know is it the only way? It's not but it's a standard way so that people can now take away uh you know that mental um
5:22
you know effort to build a new one. Yeah. Right. Yeah. So I think you know that standardization definitely increases
5:28
overall adoption and uh you know MCP is obviously emerging as kind of the de facto solution now for how um you know
5:35
we think models are going to interact with different services different uh you know systems and access data um across
5:41
them. I think to maybe contextualize this for the audience, can you give us a few examples of you know MCP servers uh
5:47
that are maybe popular or that we think uh you know a lot of model developers are going to want to figure out how to
5:53
integrate with and improve their capabilities on. Yeah. So I think this really depends on
5:59
the use case that somebody targets but you can have MCP servers for your ERP
6:04
system or your CRM system or your communications channels or your productivity systems, note-taking even
6:10
your social media channels, right? Depending on the use case and so let's take an example of um notion. Um a lot
6:18
of people use notion as their note-taking platform. somebody whether it's notion or a thirdparty developer
6:23
might build an MCP server and then they'd expose abilities to uh to another
6:29
developer that would then be able to either read information from notion or even update uh things in notion for
6:36
example uh reading one of your pages around one of the reflections that you had and then updating uh with a data
6:42
with a table so notion is one example um but you know there's very a lot of interesting ways you can mix and match
6:48
these uh you might have your note-taking system have an MCP server that that connects with your Salesforce MCP and
6:55
you're able to you know do a lot of create a lot of tasks that kind of uh
7:00
and even for fun so there's a YouTube MCP server there's a WhatsApp MCP server uh you know Slack MCP server
7:07
with a YouTube MCP server yeah you know the LLM can access the transcript for example and then find uh
7:14
you know better YouTube videos for example right so it's uh you know the uh again applications have been, you know,
7:20
growing. So once the standard is there, you could build put many things together and then build many things.
7:25
Actually, one one thing I'm curious about is who's building out these MCP servers, right? Um, aside from, you know, the the training and the eval work
Who's building MCP servers
7:32
that we'll talk about, but is this being done by, you know, the third party services themselves? Are there, you
7:38
know, independent developers that are contributing to the um, you know, overall ecosystem? Yeah.
7:43
Yeah. So in in fact um so the for example each MCP requires something um
7:49
you know behind that for example let's say Wikipedia so there are you know official Wikipedia MCP servers and there
7:55
are third party folks that uh you know give access to Wikipedia as well so it's been a mixture um you know Slack has its
8:01
own MCP server um WhatsApp you know there are many official and unofficial servers
8:06
yeah so I mean a lot of them have their official ones and then a lot of third party services I mean third party uh
8:12
people just build their So like if I wanted to make a um MCB server like say
8:17
for uh AWS, right? Like is it something like I would um like
8:24
take Boto3, right? Like the Python SDK for AWS and then basically like wrap some like use some MCP framework or
8:32
something and then like create some tools that invoke the API. Exactly. So launch an instance for
8:38
example or check uh you know how your instance is doing. So you can in fact talk to it uh just like an API call but
8:44
in a in a format that's understandable by an LLM. Right. And so basically any you know
8:50
APIs or SDKs that are out there you can wrap uh an MCP around it. Um okay so I'm kind of curious I think
8:56
before we move on to talk about the benchmark like would love to drill down into like what um an MCP agent is actually doing. And
9:03
so I think Sammy, you were talking about tool use earlier on. Um, and like can you maybe just like can you guys define
9:10
for the audience like what is tool use? Like what does a tool use agent do?
9:15
Yeah. So two things that are needed for an agent to be able to do like meaningful work. One is to read
9:22
information from external sources and then the second part is to be able to make updates to any external
9:27
information. And so when we talk about to use, to use is the way that the
9:33
general umbrella under which LLMs are able to do that. Um and that's uh are
9:39
able to do that programmatically because we're not talking about the guey the graphical user interface side that's
9:44
different. But to programmatically do that tool uses a concept. Now, concretely, let's say if you're using
9:50
the OpenAI uh function calling API, um an agent would be passed in a certain
9:56
number of tool definitions based on that format. And then uh when a user asks a
10:02
question from the agent, they would the the model would then look at the prompt, look at its available tools that are
10:07
available to it. In that reasoning, it would decide which tool to call based on the description and then send an a
10:15
function invocation back to the scaffolding running the agent and then that scaffolding then runs that function
10:20
with the provided parameters, creates the output, sends it back to the LLM, LLM reasons over the output and then
10:26
continues on into its trajectory. Right? So that's that's like the concrete flow and then MCP just provides
10:31
standardization in those communications. But for agent themselves there as Sami
10:36
mentioned the access to the tools but the agent also has to figure out which tools to invoke. Right.
10:42
Right. And that's the that's the part where LLMs are good at you know thinking about reasoning LLMs and so on. So they
10:49
are able to use that reasoning power to figure out which tools to call and then how to use them.
10:54
Right. So in terms of like packaging tools, an MCP server is kind of like a box that like contains tools that the
11:01
like developer defines, right? And then when you connect an MCB server to the client, it basically exposes
11:07
those tools to the client. Yeah. And that way in terms of like context engineering for for tool use it
11:13
it solves like a practical problem where like the the individual developing the MCP client doesn't have to like uh
11:21
define all the tools in advance priority. Right. It's it's all packaged in the MCP server
11:26
and so now like that decoupling makes it easier to build the agent clients as well. Exactly. It's really interesting.
11:32
Um, so Chate, I'm curious. You were talking a bit about like, you know, one of the things that's important is
11:37
picking the right tools and things like that. So that's probably a good segue into our our latest research around um,
11:43
MCP evaluations. Um, so I'm wondering, can you tell me a bit more about um, like how scale thinks about evaluating
How Scale evaluates MCP capabilities
11:49
MCP capabilities? Right. Right. Yeah. Yeah, I think one thing that I want to firstly stress is this is a sea change in the capabilities
11:55
of LLMs. And think of it as like in human civilization, you know, people had brains, they were running around, you
12:01
know, finding animals for themselves or eating fruit. Uh but then what changed in civilization is access to tools.
12:07
Think of Bronze Age, right? So that's the kind of situation we are in right now. Um where we have LLM that are
12:14
really good at reasoning internally, but now they're being given superpowers, right? um by giving access to these
12:21
tools. So which means that now LLMs have to not just uh think about you know
12:26
using their own memory and knowledge but also extend themselves by using the right set of tools and this is the
12:34
capability that we are trying to figure out um in you know in our benchmark is to understand how good are LLMs to
12:41
actually extend their uh extend themselves by using external tools and at a high level that's what we're trying
12:47
to figure out are is a you know earlier we used to have you know let's say prints and printers training themselves
12:53
to use multiple tools. So we are trying to figure out okay are LLMs doing good at using these multiple tools. That's
12:59
what we are trying to figure out. And the approach here is to in creating a like how do we evaluate it is try to
13:06
replicate the difficulty and the realism of tasks that we were facing in the real world. And so as we'll talk about in the
13:11
benchmark, it's not just a problem set, but also adding on uh an a sandbox or an
13:17
environment that contains real data from external MCP servers um and a and a
13:22
diverse set of them uh coupled with like difficult prompts that you know then make it a we think it's more it's more
13:29
realistic of a yeah we are not the only ones thinking about this because this is such a key foundational capability for LLMs. Um but
13:36
where we differ is the diversity of uh tools that we are testing it on uh that
13:42
we are giving access to the LLMs and then checking if they are able to you know simultaneously use a notion MCP
13:49
server which is about productivity versus an entertainment MCP server is it able to understand this capture this
13:55
diversity. Yeah. And and ju like just so it's abundantly clear to the listener, uh the big news here is that we're
14:00
launching an MP benchmark, right? And so we're super excited about that and that's that's what we're talking about. Um
14:06
yeah. Um no, I mean this is all very exciting. I think uh you know when it comes to like the tasks right that are
14:12
included in this benchmark, the sort of problems that we are uh you know feeding into the model like more concretely what
14:18
are all of the different capabilities that we're testing for? So, I've heard a few things so far, including, you know, the ability to pick the right tool to
14:24
use, the ability to actually reason through multiple different outputs or to use multiple tools in conjunction. Can
14:29
you maybe just map out like what does a good task look like and what are all the different types of capabilities that we
14:36
want to be able to test models on? Great question. So, so thinking about again going back to what are we testing
14:41
for? We are testing for a model's ability to answer a certain question or
14:47
you know do a certain task by invoking multiple tools that it has access to. Right? So the first thing that we test
14:53
for um in these prompts is is it choosing the right set of tools? Right? Is it choosing the right ones? Is
14:59
it missing any needed ones? Is it ignoring some of them? So we we test for that. The second one is now once it picks a
15:06
certain tool to use, is it good at using that tool? Is it using it correctly? So this is where we talk about you know is
15:12
it um you know calling the right uh API function in a in a in the correct way
15:18
right now this is about again picking um you know calling it well but then
15:24
finally it gets back all the context which Sami was mentioning is it able to understand all the context and then you
15:30
know digest it uh understand it and reason on it and then answer the final question. So these are the three broad
15:37
capabilities that we're testing for. So it's really these endto-end tasks where you're evaluating how well can I reason
15:43
through a set of tools uh and then complete that pipeline up until it provides the right answer. Yeah, great part. So it's the end to
15:49
end. So that's why creating tasks for this is also you know we need to do that carefully. The task should be
15:55
self-contained and well you know uh uh it should be very clear what the task is
16:00
asking for. Yeah. Right. Uh so that's the key idea and then it should enable uh or it should require multiple tools to be called and
16:07
then uh you know uh digest a lot of other information and aggregate a lot of information to answer the question.
16:13
Yeah. Uh so that's the key part that we do here. Yeah. So I'm curious can you like um drill down into like what does it mean for a
16:21
model to use a tool effectively? So you kind you you spoke about these three things right like selecting which tool
16:27
to use the tool and then like digesting the output. like concretely what does it mean to use the tool effectively
16:33
right yeah this is where I think we haven't talk talked about the environment so a key difference in in
16:39
this benchmark compared to let's say other types of capabilities that you test um in other capabilities you have a
16:45
question the model does all the thinking by itself internally and then answers the question right so this is the first
16:51
time we have um you know we are evaluating model capabilities which requires an external environment this
16:57
external environment is essentially the box of tools. So um you know what it takes to finish a task is it needs to
17:05
connect well to that environment. The environment should be robust um should not fail you know because we are
17:11
evaluating LLMs we should not have a buggy environment because then you know we are not uh we are evaluating the
17:17
environment and not the MCP not the LLM itself. Uh so that's the key part that we made sure that the environment is
17:23
robust uh reproducible and then now LLM is able to use that environment to call
17:29
certain tools. By calling we mean you know it's literally making an API call or or MCP API call right uh and then
17:36
getting the answer and then digesting it. Okay. specifically there when we have in in the calling portion. So once it's got
17:43
a spec of all the tools that are available to it and the spec includes all the tools as a JSON list and then it
17:48
includes a description of what the tool is so that an LLM is able to pick the right one cuz when they when there's many it you
17:54
know we need to have that clear. Um once it picks the right tool then it's about creating that function invocation with
18:00
the right parameters. Um and then yeah that that's what it means to call it and a lot of times they either pick the
18:06
wrong tool uh or they pick an inefficient one when a task that could have been done with one tool with one call sometimes they pick another one
18:12
that has to be done five times let's say Google Google distance matching for example um and so and sometimes they might not
18:19
pass in the parameters properly so that's what we mean by calling the right tool properly so if I'm if I'm kind of going with this
18:25
analogy you had earlier about like the LLM as a bronze age caveman right so
18:30
that you know you're this caveman. Um, you've been given this like task, you know, go build a bridge or a dam or
18:37
something. I don't know what Bronze Age cave folk were doing. Um, and uh, your
18:42
environment is like this box of tools, right? Like you might have some kind of like hammer thing or like some like
18:47
knife. And so be Yeah, exactly. And so the the um model
18:52
has to figure out which tools do I want to use in this box. Um, so that's part of it, but then also once they're using
18:59
the tools, how good can they use like how effective are they with a hammer, right? Um, and then if they use the
19:04
hammer and they see that the hammer is not working or it is working, like they need to figure out what do I need next, right? And so this kind of like long
19:11
horizon thinking of using these different tools to accomplish some some tasks. Yeah, it's a multi-step process, right?
19:16
As you correctly mentioned, maybe you need to use the hammer to break the shell and the knife to, you know, cut it
19:21
down properly. So this is the type of multi-step thinking that we are evaluating uh our LLMs for
19:28
I think more importantly too you might realize oh this hammer is not breaking the shellact now what do I do right so
19:34
you need to like recover from these like failure modes exactly yeah I think just piggybacking yeah that's a great analogy and
19:40
it was chained so I s uh yeah I think just piggybacking off of this I mean one of the things you
19:45
mentioned earlier was that you know there are kind of like two functions that uh exist within this realm of like you know tool use and MCP right one uh
19:52
information retrieval where you know you are uh essentially like pinging these servers to get information back that you
19:57
can use to reason through and the other is actually performing actions or like mutations within that service. How do we
20:04
think about both of these capabilities and you know are we capturing both of these in our benchmark today? Do we
20:10
think one of these is maybe more important than the other when it comes to how people are actually using this in
20:15
in a production environment? Great question. Um I think here it talks about the environment, right? So in
20:22
order to get access to you know information um you know that's out there in fact in our environment we have a lot
20:28
of real world data and artifacts that we have collected and which we are you know populating
20:34
this environment with uh so that um you know let's say real world Slack uh you
20:39
know conversations uh there are literally you know there's an environment where we have slack conversations and then people can ask
20:44
questions on it right um so we have that but the other aspect is writing to it so we currently
20:50
don't have that in benchmark mainly because we are open sourcing it. We don't want you know people to corrupt each other's
20:55
environment. It's hard to build these hard. Exactly. But that is like once we I mean that's an important capability
21:02
that we will explore. This is just the first of of several benchmarks. Absolutely.
Overview on Scale's MCP Atlas benchmark
21:07
So can can you tell us a bit about like this environment specifically like Yeah. I mean what's what's in it,
21:13
right? Yeah. So three parts to the whole benchmark. It's the data which is the
21:19
evaluation task like 2,000 tasks that you can run on. Then 2,000 2,00.
21:24
Oh, that's actually one of one of the you know places where we differ from existing benchmarks. So we went diverse
21:30
and large number and difficulty as well. So difficult tasks as well. Cool. Yeah, we can get into the performance numbers in a bit but 2,000 tasks. Then
21:37
we've got the environment and then we've got data that we have to seed some of these environments with which CH just
21:43
talked about. I'll go into a bit of detail. So the environment basically is a docker container of MCP servers. There
21:49
are 40 plus MCP servers ranging from productivity to um project management to
21:55
social media like YouTube. And then each one of those servers has a number of tools attached to them. So in
22:00
aggregate it's about 300 plus tools. Um for those 2,000 tasks, each one of
22:06
them target different servers and different tools in different ways. Uh and so we expose those tools to uh to
22:11
each model per task. Now some of these servers are have to be stateful in
22:16
nature. For example, notion for example the local memory knowledge graph that we have right they don't have any
22:22
information within them by default. So that's where we've done the work to have real world data acquire real world data
22:28
and then populate them with this data so it resembles you know let's say real CSV files from other businesses. Um
22:35
so sorry I just want to double click on that. So what you're saying is that the the uh different like systems in this
22:40
environment uh that like need real data like we're actually we've acquired data
22:45
somewhere real world data and like put it in these environments. It's not synthetic in any way.
22:50
Some of the data for some of the service is synthetic but a lot of it is actually real world data that we've acquired from
22:56
real businesses right um and an example of this might be like you know shipping uh logs etc that a
23:03
company might have. And so uh some servers like slack for example we have done the work to generate a lot of
23:09
conversations and if see the servers with other ones for example notion air table we have a lot of real world data
23:16
with that we've populated them with and then the tasks use them um and the models are able to interact with them so
23:22
it it really reflects the realism of of the data it's not synthetically generated and and does each agent like is it given
23:30
the entire tool set and MCP servers or how does that Yeah. So for every task we we don't want
23:37
to give all the 300 plus tools for for every task. So in every task we uh we have target servers that we call. So
23:44
basically um let's say subset of them let's say 30 tools right. So among those
23:49
30 tools we are evaluating is the model picking the right tools. So that's why we don't want to you know give
23:54
everything and then you know confuse it. um we give subsets because we are trying to evaluate models um on on different
24:02
capabilities. It also a lot of models don't even support the full 300 plus for example GBD40 supports 128 as far as I know in a
24:10
single uh conversation and so uh that then combined with the
24:15
the subset that we do enable that also includes some some servers that are very relevant to the task at hand but then
24:21
others that are distractors. Uh and the point of the distractors is to be able to simulate real world scenarios where
24:27
they might have to choose the right one amongst others that might confuse it a little bit. Uh that's one of the things that makes this benchmark that we
24:33
believe more difficult. Yeah. And this is something we talked about in a previous episode on you know how to make new evals that capture real
Open-sourcing MCP Atlas
24:41
world distributions you know actually seed them with data that exists in actual business processes that look very
24:48
realistic and are not synthetically generated. So uh you know really proud of all the work that the team has done here. I think the other thing to
24:54
celebrate is that uh we are open sourcing this environment and that's not something that you hear very often from
24:59
data companies. Uh can you talk a little bit more about you know our decision to do that and how we think this is going to benefit the research community.
25:06
Yeah we really think so we are at the beginning of you know this evaluation of LLM on these capabilities. So we want
25:13
developers to uh in some customize these uh you know these tasks and the the way
25:18
they're evaluating as well. So that's why we want to make sure the environment is something that uh any let's say um
25:25
any student or you know a PhD student doing some research can quickly download and then set it up on their laptop and
25:31
then start evaluating their own small LLM. Right? So that's the key. Um and that's why we we took some effort to
25:38
figure out what are some good MCP servers that are robust to many people using them at the same time. Um and then
25:45
you know that's how we chose 40 and diverse enough that we have some people who are interested in you know building
25:50
social media tools versus people who are doing deep research uh can also use. Um
25:56
so yeah so the open sourcing serves that purpose. Yeah and to be clear the the environment is fully open source as well. So you get
26:02
the environment which contains the MCP servers and you also get you know the ability to run create trajectories uh
26:10
for all 2,000 tasks for any model of your choice that you want to evaluate let's say and then you also get the the
26:16
evaluation scripts that we use to compare those trajectories against what we think evaluate those trajectories
26:22
right and get some failure mode analysis on that. So anybody could at a lab or a researcher they could spin it up and
26:28
they could get insights onto the model that they want to evaluate. Uh and they're also able to freely mix and
26:33
match. They're able to choose what they want to modify. They can change the evaluation script if they want a bit of a different failure mode analysis. They
26:40
can extend the environment. They can add their own MCP servers. They can create their own problems if they want. So I
26:46
see it as a three-piece system and um people are able to change and extend as they want.
26:52
That's really exciting. And um you know I know we want to talk a little bit about how models are actually performing on this benchmark. What makes this you
Other tool-use benchmarks
26:58
know one of the hardest ones out there uh but just to you know give a little bit of context before that can you talk
27:04
about some of the other you know benchmarks that a researcher might use in conjunction with you know our new MCP
27:10
valuation to uh gauge you know tool use capabilities. Yeah so there are um some um other you
27:16
know benchmarks that are measuring tool calling abilities. Uh there's one from Berkeley you know it's called Berkeley
27:22
function calling leaderboard um and you know there's something called towen so there bunch of those um uh but where we
27:30
differ from a lot of these is the the scale you know for the lack of a better
27:36
word uh yeah um you the diversity uh of the types of tasks that we have and and
27:42
the difficulty so that you know we are eager to uh get to get to that part but yeah diversity the difficulty and the
27:49
number of tasks themselves. So we we we are open sourcing about 2,000 tasks. I think uh and correct me if I'm wrong,
27:54
but like you know there have been some recent MCP benchmarks come out and I think um one of the trends that I've
28:00
noticed and I've not I've seen this for a few different um agent uh benchmarks like there was a recent deep research
28:06
benchmark um that that did this. Uh there's a lot of like synthetic task generation.
28:11
Yes. Um, and I think one thing that's interesting um, and we'll talk a bit about this when we get to the results is
28:17
that like you can you can see a pretty clear uh, separation between like task complexity,
28:22
right? Uh, when you're generating tasks on the fly um, versus like having uh, actual
28:28
humans thoughtfully craft really difficult problems that stretch a model's capabilities. And so I think it's interesting because it's you know
28:34
on the one hand uh benchmarks that rely on synthetically generated tasks um are
28:40
more accessible in a way. Um but but at the same time you're uh like I think
28:45
we're seeing the ceiling of like how complex um the tasks that a model can generate uh really are. Um like I think
28:52
some of the recent ones that came out you know we're seeing pass rates at launch like in the uh mid to high 70s.
28:58
Yeah. Um, and so I think that's something that that uh we see quite different in our benchmark. But before
Examples of MCP Atlas tasks and rubrics
29:04
we get to like the benchmark results, maybe just walk through like what is an example task for the viewer so they have
29:09
a very clear understanding of like what does a task in this benchmark look like? Yeah. So it's just three simple parts.
29:17
Each pro each of the 2,00 tasks has a prompt that is written by a human, not synthetically generated. Um the prompt
29:25
in order to answer it, you need a set of tools. So those enabled tool we have a list of enabled tools that we provide to
29:30
the prompt which includes the necessary ones and then the distractor ones and then we include a um ideal trajectory uh
29:38
that is you know the ideal response to that prompt. The model would be would take the uh the prompt and the enabled
29:45
tools and then it would create a trajectory and then we would uh using our evaluation script evaluate it
29:51
against what we think the ideal trajectory is. um that's the composition of a task.
29:57
So can you maybe but as a contributor right uh yeah the example um you know not to take some example from the
30:03
benchmark but uh one of the questions that we saw was um hey I'm I'm visiting
30:08
uh Japan and there was some time when uh I went to you know Tokyo there was a
30:13
hotel close to a um you know sushi restaurant uh and there was also a dumpling restaurant right next to it.
30:20
can you find me you know um new hotels nearby and also tell tell me you know
30:26
how much it would cost for me to you know uh h a ride from the airport to that hotel. So if you see this question
30:32
it has a bunch of there's some memory aspects so where the model has to figure out where this particular hotel is and
30:40
find options for other hotels around the area. So it might have to use tools like maps for example. Um and it also has to
30:46
figure out do some calculations. Uh so figure out the distance figure out the cost of ride in Japan, a cost of taxi
30:53
for example. Uh and then calculate that. So you can see how this you know very natural type of question that you would
31:00
ask travel agent for example will be answered by LLM by g by accessing
31:06
multiple tools. Right. So so this is interesting. So you're saying that we have like a memory tool.
31:13
Okay. So that's so basically um a lot of the tasks like provide the agent with a
31:19
tool that basically gives some some memory for the that's I mean that's that's pretty comparable to how a lot of actual um
31:26
like LLM and agent applications work especially consumerf facing ones right like we have notion or you know slack
31:33
where you could say hey in a in a channel my friend was mentioning about a certain game right video game now how do
31:40
I buy similar video games that could be a But even there's those external data sources and then there's something you
31:46
know local memory stores that we have. So like this memory uh MCP server is represents knowledge in a graph
31:53
right and so like you said a lot of client facing uh applications like chat GBT they have their own memory systems that
31:59
persist across chats and so this is kind of emulates that where have you have memory stored about
32:05
you right and then the the agent is able to check retrieve information from there
32:11
populated with a Google search query and then populated into the calculator to measure distances etc. And this is a key
32:17
part that we wanted to make sure that these prompts are realistic because that's the key criticism that many
32:22
benchmarks get that hey okay so you do 80% on this benchmark but what does that mean for our real use case right
32:29
that's why we made sure that the prompts are generated by uh humans and trying to
32:34
ask regular you know usual questions not um you know not something to stump the
32:40
model for right right and uh one thing that you guys mentioned too which I think super interesting is like
32:46
this notion of distractor tools, right? So, if I'm recalling correctly, like an
32:51
example of this is something like I think one of the prompts you just mentioned which is like oh um
32:56
somebody mentioned in one of my uh messages that um there was some movie
33:02
they wanted to see or something like that, right? Um and uh where basically the prompt is is saying it's suggesting
33:07
that the agent needs to search through some messages and then the tools that are enabled are things like Slack but
33:14
also maybe something like um email um and like maybe some third like message related thing. And so the agent
33:21
basically has to figure out okay uh uh this this uh individual is talking
33:26
about some message history. I have these message related tools. I need to figure out which one's the correct one and
33:31
which one's not. Or I can just use all of them and just kind of figure out like where is actual where's the information actually stored.
33:37
Like if it if it says channel in the prompt maybe it the model would think slack. Yeah. If it if it doesn't and there's no
33:43
nothing that it can infer from then you would think maybe it should just start with one and then see how it goes right. Right.
33:48
That's what a human would do as well. And I think this is very interesting because like real world agents are equipped with you know dozens if not
33:55
maybe even hundreds of tools right and as um you know these agents take on uh larger and larger workflows in order to
34:02
get those done accurately right you just have to equip them with a lot of these capabilities and so then a fundamental
34:07
problem becomes like how well can they understand the purpose that each tool serves um you know the information that
34:13
they can retrieve from different sources and discern what the right ones are and so I think you know idea of uh you know
34:20
including distractor tools is is it's a very cool approach. Yeah. This way we can you know measure
34:26
um is it selecting the right tool right and not giving up you know just by looking at one source and then you know
34:31
if it doesn't find the answer there it shouldn't be giving it up right. So that's the aspect that we test as well.
34:36
Yeah, we talk a lot about how uh you know having um rewards and having verifiers
34:42
that are a bit more nuanced and not necessarily you know fully deterministic or you know binary zero or one um are
34:48
are good for for this sort of um you know evaluation whether it's you know looking at a bunch of claims right that
34:54
have to be true for a final response or maybe even evaluating like the process right did the agent you know call the
35:00
right tools did it figure out which ones were the distractor tools was it able to like reason through um you know the
35:06
problem properly. Yeah, there are two you're right. So, we are capturing two different types of analysis. One is is it getting the final
35:13
answer correct? It's called the final answer correctness, right? Uh the other one is the process itself. So, sometimes models get the process
35:20
right, it's calling the right tools in the right way, but maybe at the last step of aggregation it's failing. So, we
35:26
also capture okay is it at least doing these partial steps well. So, we capture those uh aspects as
35:31
exactly. Yeah. So, so what did we actually learn about like uh what are models good at
Results and insights from MCP Atlas
35:37
today out of all these capabilities? Uh where do we see you know the most room for improvement as well?
35:42
Um yeah, in a way because our tasks are difficult so I I'll talk about where they're failing, right? Sure. Yeah.
35:47
Um so I think most of them um also one thing that we realize is different
35:53
models um you know there's not much correlation between different models how they fail. So different models are
35:59
failing at different uh things for example. But most common things that come out are um picking the right tools.
36:06
So models are picking the wrong tools uh and sometimes you know trying to answer it based on their memory uh not even
36:12
trying to call tools. So this is the you know uh picking the wrong tools or missing tools what we call the failure
36:18
mode. The other failure mode is um not able to call uh you know in the right
36:24
format. So it's not picking the right parameters. So in the Japan you know hotel example maybe it will say you know
36:30
find hotels near an address and then it doesn't give the right exact address it just gives a hotel name for example and
36:37
the maps doesn't necessarily work with that so we need to be giving uh the coordinates of it and so on and so forth
36:43
right so picking the right tool but uh using it wrongly right so that's another
36:49
failure mode um and there are some failure modes on planning if it's a long horizon task some models you plan
36:56
wrongly. So they they just uh um choose the order in which they should be calling the tools you know you know
37:02
wrongly and then that makes them fail at the task. Yeah. So these are the common things. So the p the primary to summarize then
37:09
the primary ways that we see them fail the most is in tool selection and then tool construction more so than
37:16
tool output interpretation. Yeah. Which is like the reasoning which is more of the reasoning. Yep.
37:22
Yeah. I mean that that that seems to track though, right? Like I think it would be pretty surprising if if models received
37:28
a two output and couldn't parse it that well. I mean unless it was like a really oddly formatted output I suppose but
37:34
yeah so there are some models that are failing at that as well. So they ignore it because of the context limitations
37:40
and so on. Okay. Uh they can ignore they can they get a lot of output in JSON format and
37:45
whatever format the system is giving them and it can ignore some of them not put it all together. I see.
37:51
Depends on I guess how long the output is etc. But yeah, typically that doesn't feel like, you know, context management feels like
37:57
something a problem that's been worked on more so, right? Compared to tool calling.
38:02
Yeah. But it can stress a model's ability to like pick a needle out of a haststack. So it's Yeah. like a really
38:09
long output dump and you need to like find the one thing in that. Right. Right. Yeah. Yeah. And then you need to invoke it word for
38:16
word, letter by letter properly because it's a literal function invocation that the model needs to provide. So it it
38:21
makes sense why it fails there. Okay. Um, cool. Well, anything else like just key
38:29
analysis like things you want to highlight from from the uh research? Yeah, so um you know one of the key
38:36
surprises for us was the soda models are doing um this badly especially because when we looked at other benchmarks uh
38:43
you know they were getting 70% right or plus. Uh so that's been a key learning. Uh the
38:48
other learning is while we were developing these we also we assumed that the difficulty of a task would be you
38:55
know common across different models but something that we saw was the correlation was quite low. So which
39:00
means that models are each of them are getting good at different kinds of things. So that's been um you know
39:06
another learning which means that for LLM developers or model developers to improve their models they have to really
39:13
go deeper into you know where they are failing and then improve those aspects. Uh they cannot just uh you know focus on
39:19
final answer correctness and then you know um just train based on that. Yeah.
39:25
Cool. um in terms of like uh model progress for um MCP and tool use
Future of MCP and tool-use benchmarks
39:32
capabilities like you know tool use has been around for a while um and we're starting to see uh more investment on
39:40
the post- training side in um RL for tool use in general right um we're also seeing more investment in
39:46
like uh tool use data in uh pre and mid-training um and so I think uh base
39:52
models are getting a firmer understanding of like just agentic um uh
39:58
behavior and like uh uh stateful actions in general. Um and so I think that will
40:04
ultimately uh cascade downstream into um gains during post training. I also think
40:09
that um you know environments uh for training are going to get better uh and and I know that because we're going to
40:15
play a big part in that um but um so I think that's going to be a key driver. Um, but what I think is u is going to be
40:22
true is that the environments that we evaluate are going to continue to get more complex.
40:28
So like we're really excited about this benchmark, but this is just like a starting point for us, right? Like we
40:33
know that we want to continue to expand the horizon of complexities within our
40:38
benchmarks. Um, and so, you know, this captures a lot, but it it's only step one. Um, and so I'm pretty bullish
40:45
that like models are going to improve here. Um, but there's going to be another one right around the corner that
40:50
that they need to uh um hill climb on too. So, um, and I think we're going to be a we're going to continue to be a
40:56
driving force for that. Absolutely. Yeah, I agree. That makes a lot of sense. I'm also very excited about
41:03
with MCP, the thing that's most exciting is everybody anybody can create an abstraction over any API and then
41:08
contribute to this shared registry which can plug into any model. The more we do that, the each additional MCP server is
41:15
like increases the surface area for how useful of an application you can build. And I think that then means there's more
41:21
adoption on the user side. So with that cascading
41:26
um what's that word? I'm forgetting a word. It's okay. Uh use whatever word comes to mind.
41:32
Yeah, compounding. That's good. I think I think we will see some very solid
41:38
improvements and also very excited about the environment part and very looking forward to how people use it, extend it,
41:44
add more MCP servers, poke holes in it too. That's very cool as well. We love feedback. Um but super excited about this
41:51
and that's the main I would say ask our uh users to you know this is a very in
41:57
some sense a live benchmark in the sense that uh the environment is there so they can you know use parts of the AMCP.
42:04
servers there uh create their own tasks uh create their own ways to evaluate. So it's a very uh it's not a static
42:11
benchmark in that sense. Um so yeah so ask uh you know uh all all our colleagues in um companies other
42:18
companies universities uh to really download use it uh let us know what they think of it.
42:23
Yeah. Very cool. Well, uh, maybe we can end on one final question, which is, um, you
42:29
know, I think, you know, we've talked a lot about how MCP, you know, is a relatively new development. We're seeing
42:35
a lot of adoption right now. In, you know, one to two years from now, like how do you think this plays a role in
42:40
the broader agent ecosystem? Are we just going to see like, you know, thousands or like millions of MCP servers, right?
42:46
Are there any big gaps or bottlenecks that are, you know, preventing um, you know, more widespread adoption right
42:51
now? like like paint paint paint a picture of like where the world is going from here. Yeah, I think I can go and then maybe
42:57
you can add on. Um eventually like going back to the beginning of how we started this a model
43:03
needs to be really good at ingesting information and manipulating the environment outside of it for it to do
43:08
any meaningful work. And so eventually if we extend it uh and if MCP is the standard by which all communication
43:14
happens between each of these parties you know it plays a central role it becomes a critical part of that
43:20
infrastructure. Now the part that I'm excited about most is how this then plays with let's say computer use
43:26
because um you know MCP is just the standardization of tool use which is programmatic ways of calling u APIs but
43:33
then there's also other ways where we can uh manipulate information using computers browser use etc. So I think
43:40
MCP combined with other mechanisms the mechanisms that we have for direct computer use um very excited for for
43:48
that to come together and that might be you know eventually we can look into creating benchmarks there as well. Yeah. Yeah. For me I think this again
43:55
the first step towards using um static tools you can think of MCP servers as
44:00
deterministic type of tools right but then what if the other um you know entity that you are interacting with is
44:06
also another agent. So this is a natural step towards agentto agent uh communication systems right and then is
44:13
a is an LLM or an LLM based agent uh able to leverage the right agent not
44:19
just an MCP tool but the right agent I think that's where um things are going and then you know once these agents
44:26
start talking to each other uh then yeah so then it's explosion right so intelligence explosion
44:31
yeah I think the the saying used to be that like software is going to eat everything right and uh I'm pretty bullish that agents
44:38
are going to steal software's lunch and they're going to be eating everything. And so I think like in a um and really
44:43
the way they're going to do that is through MCP. So I think you know in a year two years time um you know basically our entire
44:50
digital infrastructure um is going to have integration uh entry points for agents um and that's going to be driven
44:56
by MCP. I agree. How would agents talk to each other? Like they is there is you're gonna are you going to wrap an agent
45:03
into an MCP server? Yeah. Yeah. And there are already, you know, versions of MCP for that.
45:08
Yeah, there are protocols like a to Okay, that one. That's that's very interesting.
45:14
One last thing. Um, what's next on MCP benchmarks?
45:20
I have the same answer. I'm really excited to combine um MCP we have this
45:26
benchmark. If if it needs to be more difficult, we'll continue progressing there. But what I want to do is combine
45:32
it with um other modalities and that's computer use. And so if we can create a benchmark which tests both tool use and
45:38
then computer use. Uh you know I I have a problem with my VPN right now. If it could uninstall my VPN using computer
45:45
use then pull the docs via MCP server do all of that. That would be capabilities that I would love to test.
45:51
Yeah. Yeah. For me it's uh the multi-turn continual learning aspect. So can we actually um you know check for
45:57
LLMs if they are able to do multi-turn um you know remember the interactions
46:02
they're having and then you know keep improving on that. Uh I think this and then the agent to agent I think these are the two aspects that I'm really
46:08
excited by. Cool. Well thanks guys. I think uh that wraps up uh this episode of uh chain of
46:15
thought. Uh thanks a lot. Thank you. Um for viewers if you have any uh questions please feel free to drop them
46:20
in the comments and we'll get to them next time. Thanks. [Music]