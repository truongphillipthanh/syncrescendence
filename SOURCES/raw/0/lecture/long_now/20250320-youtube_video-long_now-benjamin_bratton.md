# A Philosophy of Planetary Computation

*Benjamin Bratton | Long Now Foundation | January 29, 2025*

Patrick Dow, the new board president of the Long Now Foundation, introduced the evening: "Tonight's conversation could not be more timely. As humanity's technological capabilities advance at an unprecedented pace, we need new frameworks for understanding them at larger scales of both time and space. Benjamin Bratton, our speaker this evening, offers exactly these kinds of deeper, broader perspectives for us to consider. Benjamin's work challenges us to think beyond current debates on AI toward larger time scales where the very nature of intelligence, life, and technology may fundamentally transform our world."

---

Thank you everyone. What a pleasure it is to share some of this work with you at Long Now Foundation, an institution that I've long admired.

Let me begin by taking a moment to step back and away from the persistent weirdness of our times—one in which we get free AI from a hedge fund and $200-a-month AI from a nonprofit. Never would have called that one.

More broadly speaking, if you take one idea away from this, it's that I genuinely think we are in a kind of pre-paradigmatic moment. A lot of ideas are floating around that increasingly look similar to one another and are coming together into something that may constitute a frame of reference that may be more useful to us. But this is a difficult process. It's one that we'll have to invent.

I'll put it this way: There are certain times in history when our ideas of what we would like to do are way ahead of the technological capacity to do that. And there are other times when the technology is ahead of our concepts. I think this is probably more where we're at. In those moments, what we call philosophy—its job is to invent, to try to conjure those concepts and bring them into being so they may be put to some use.

Unfortunately, I don't know that that's entirely what's going on. A lot of our most important epistemic institutions, such as universities, I fear to report, are in a bit of a holding pattern. The sciences are focused on a kind of "do now, think later" mode. The humanities, where I spend most of my time, it's more of a "critique now and perhaps act later, but maybe not." In other words, this is exactly the wrong time to not be inventing those concepts and not be setting the initial conditions for the society to come. And yet here we are.

## Computation as Planetary Phenomenon

Let's begin. Our topic is computation. As you'll see, we mean this in a somewhat idiosyncratic way. It's less to do with mathematics and algorithms, or with these little appliances that we've constructed, but rather computation as a planetary phenomenon—not just something that humans do, but indeed, through humans, something that the planet does.

Our first proposition for the evening would be this: computation was discovered as much as it was invented. We can think of natural computation, artificial computation. Steven Wolfram recently published a paper in which he argues that the entire universe is a computational hypergraph, that time is the rate of refresh, and that dark energy is the heat exhaust of the big computation that is the universe. I don't know—could be.

We think of computation in terms of planetary systems. This is not only a new thing; it's also really where it comes from. Computation was born of cosmology, and I mean that in both senses of the term cosmology, which I'll talk a little bit about later.

This is a picture of the Antikythera mechanism. It dates from, as far as we know, about 200 BC. It is probably apocryphally understood as the first computer, but it was not only a calculation device—it was also an astronomical device. It was used to orient its user through the stars in relationship to her situation, spatially but also temporally, allowing a kind of simulated movement back and forward in time. The idea that computation begins with this orientation of intelligence in relationship to its planetary condition seems to us a good starting point.

But computation quickly became something more than this, also a bit more practical. It also became calculation as a kind of world ordering. As societies became more complex, the necessity to calculate and compose not only what was happening right now, but indeed what happened in the past and what could happen in the future, gave rise to forms of computation like Sumerian cuneiform. You find this earliest form of writing, and you try to imagine what could possibly be the first thoughts of humans put down in writing. It turns out it's mostly receipts. In a way, everything we've done since then—all written language—is sort of variations of accounting, which I like to remind my friends in the literature department.

## Where Philosophy Meets Computation

Where do we expect philosophy to come from when we talk about a kind of school of thought? A few clues. One way of thinking of this is that sciences are born when philosophy learns to ask the right questions. Most of the things we call sciences, at least historically, began as philosophy. Biology began as philosophy, physics began as philosophy, and now increasingly, computation is becoming a kind of philosophy as well.

When we think about the history of Western philosophy—and I say Western because that's my training, though I don't think it's necessarily the most important or the only one—there's a particular lineage that we can trace. If we go back to ancient Greece, to people like Plato and Aristotle, they were fundamentally concerned with questions of cosmology: What is the nature of the universe? What is our place in it? How do we understand order and chaos?

These questions weren't abstract in the way we might think of philosophy today. They were deeply practical. The Antikythera mechanism wasn't just a curiosity—it was a way of understanding and predicting the movement of celestial bodies, which had immediate implications for navigation, agriculture, religious festivals. This is computation in service of cosmology.

Fast forward, and we see this pattern repeat. In the medieval period, computation was bound up with theology—calculating feast days, understanding the divine order. In the Renaissance and Enlightenment, computation became mathematical and mechanical. Pascal's calculator, Leibniz's stepped reckoner—these weren't just mathematical tools. They were philosophical projects about the nature of reason itself.

Then we get to the 20th century, and something shifts. Alan Turing doesn't just invent the theoretical computer—he asks: What does it mean to compute? What is the relationship between computation and intelligence? These are philosophical questions that happen to have technological implications.

## The Convergence

Now we're at a moment where these threads are converging in ways that I think we're only beginning to understand. We have computation that is no longer just human-scale. It's not just the calculator on your desk or even the supercomputer in the data center. We have computation that is embedded in the fabric of daily life, computation that is learning, computation that is making decisions, computation that is, in some sense, evolving.

And this brings us to a critical question: What is the ontology of computation? By which I mean, what kind of thing is it? Is it just a tool, like a hammer or a lever? Is it a medium, like language or money? Or is it something more fundamental—a condition of existence itself?

I want to suggest that computation is becoming, or perhaps revealing itself to be, a planetary-scale phenomenon. By this I mean several things. First, literally planetary in scale—the infrastructure of computation now spans the globe, from undersea cables to satellite networks to the billions of devices we carry. But more than that, computation is becoming planetary in its effects. It's reshaping ecology, economy, politics, culture at scales that are genuinely global.

But there's something even more fundamental here. When we look at the history of life on Earth, what do we see? We see systems that process information, that respond to their environment, that evolve through a kind of algorithmic selection. DNA is, in a very real sense, a computational substrate. Evolution is, in a very real sense, a computational process. Life has always been computational.

## Natural and Artificial Computation

This is where things get interesting, and where I think we need to be careful with our categories. We tend to think of natural and artificial as opposites—as if nature is one thing and technology is something else we've imposed on it. But this distinction breaks down when we look closely.

Consider: Every organism is processing information constantly. Your body right now is running countless computational processes—regulating temperature, metabolizing nutrients, fighting pathogens, even this act of listening and understanding is a computational process happening in your neural networks. None of this requires silicon or electricity, but it's computation nonetheless.

So when we build computers, when we create artificial intelligence, we're not doing something alien to nature. We're extending and augmenting computational processes that were already there. We're creating new substrates for computation to happen on, new architectures for information processing. In this sense, artificial computation is continuous with natural computation, not opposed to it.

This has profound implications. It means we can't simply treat AI as a tool that we pick up and put down. It's becoming part of the computational infrastructure of the planet itself. It's becoming part of how the planet processes information, makes decisions, evolves.

Now, I know this might sound abstract or even mystical, but I want to be very precise here. I'm not saying the planet is conscious or that there's some kind of Gaia consciousness. I'm saying something more specific: that computation—information processing, pattern recognition, adaptive behavior—happens at multiple scales, from the molecular to the planetary, and that what we're seeing with AI is the emergence of new scales and new modalities of computation.

## Intelligence Reconsidered

This brings us to the question of intelligence. We've inherited from the Enlightenment a very particular notion of intelligence—one that's centered on human rational thought, on consciousness, on what philosophers call "intentionality." And this has served us well in many ways. But it's also created some blind spots.

When we ask "Is AI intelligent?" we're often asking "Is AI intelligent in the way humans are intelligent?" And that's not the right question, or at least not the only important question. The right question is: What kinds of intelligence are there? What are the different modalities of information processing, pattern recognition, problem-solving that exist in the world?

Bacteria are intelligent—they sense their environment and respond adaptively. Slime molds are intelligent—they solve maze problems and optimize networks. Octopuses are intelligent in ways that are profoundly alien to us. And yes, artificial neural networks are intelligent—they recognize patterns, generate novel outputs, and improve with experience.

The point isn't to collapse all these forms of intelligence into one category, but rather to recognize that intelligence is not a single thing. It's a spectrum, or better, a multidimensional space of different capacities and modalities.

What makes human intelligence special is not that it's the only real intelligence, but that it has certain distinctive features—particularly language, abstraction, and cultural transmission. But even these features aren't uniquely human anymore. We're building systems that use language, that abstract, that learn from cultural datasets.

This doesn't mean AI is human-like, but it does mean we need to rethink our categories. We need a philosophy of intelligence that doesn't start with human consciousness as the paradigm case, but rather sees intelligence as something that emerges at different scales and in different substrates.

## Cosmology and Cosmopolitics

Earlier I mentioned that computation was born of cosmology in both senses of the term. Let me unpack that now. The first sense is cosmology as the study of the universe—its origin, structure, and dynamics. The Antikythera mechanism was cosmological in this sense: it modeled the movements of celestial bodies.

But there's a second sense of cosmology that's equally important—cosmopolitics. This is the question of how different entities, different forms of being, coexist and organize themselves. It's political theory, but at the scale of the cosmos rather than just human society.

Bruno Latour and Isabelle Stengers have developed this concept in recent decades, and I think it's crucial for understanding our moment. The question isn't just "How does the universe work?" but "How do we live together in a universe that contains many different kinds of agents, many different forms of intelligence and agency?"

This becomes especially urgent when we consider AI. We're not just building tools—we're bringing new agents into the world, entities that make decisions, that have effects, that need to be accounted for in how we organize ourselves.

And here's where it gets challenging: our political and ethical frameworks are deeply anthropocentric. They assume that the relevant agents are humans, that the relevant welfare is human welfare, that the relevant intelligence is human intelligence. But what happens when we have to account for non-human agents? Not just AI, but also ecosystems, animal populations, future generations?

This is cosmopolitics—the politics of a cosmos that contains many different kinds of entities. And computation is at the heart of this because computation is how these entities interact, how they coordinate, how they make decisions together.

## The Stack and Planetary Computation

Let me try to make this more concrete. In my previous work, I've talked about "The Stack"—a model for understanding computational infrastructure as a series of layers, from the underground cables and data centers up through protocols, applications, and interfaces. But I want to extend this now to think about planetary computation.

Imagine the Earth itself as a kind of computational system. At the bottom, you have geological and biological processes—plate tectonics, ocean currents, the carbon cycle, evolution. These are computational in the sense that they process information and produce adaptive responses.

Moving up, you have human societies with their infrastructures—cities, trade networks, communication systems. These too are computational—they route resources, transmit information, make collective decisions.

Then you have the layer of digital infrastructure—the networks, servers, satellites that we've built. This is computation in the more conventional sense, but it's deeply integrated with the layers below.

And at the top—or maybe distributed throughout—you have AI systems that are learning from and acting on all these layers. They're analyzing climate data, optimizing supply chains, generating culture, making predictions.

The point is that these layers aren't separate. They're interpenetrating. Computation at one level affects computation at other levels. AI trained on human culture feeds back into culture. Climate models affect policy decisions which affect emissions which affect climate. The Stack is not just technical—it's ecological, political, geological.

This is what I mean by planetary computation. It's not just that we have computers on the planet. It's that the planet itself is becoming a computational system, one in which natural, human, and artificial processes are increasingly interlinked.

## The Pre-Paradigmatic Moment

So let me return to where I started. We're in a pre-paradigmatic moment. We have the phenomena—we have AI systems that surprise us, we have ecological crises that span the globe, we have technologies that are reshaping society faster than we can track. But we don't yet have the conceptual frameworks, the theoretical paradigms, that would allow us to make sense of all this.

Thomas Kuhn talked about scientific revolutions—moments when the old paradigm breaks down and a new one emerges. But there's always a period of confusion in between, when anomalies pile up, when different theories compete, when it's not clear what the right questions are.

I think that's where we are with computation, with AI, with planetary-scale technology. We have lots of theories—some people are very excited about AI, some are very worried, some think it's overhyped, some think it's the most important thing ever. And there's truth in many of these perspectives, but no single framework that brings them together.

The task of philosophy, as I see it, is to work toward that framework. Not to impose it from above, not to decree the right way to think, but to experiment with concepts, to test different frames, to see what allows us to think clearly about these phenomena.

And this is hard work. It requires drawing on multiple disciplines—computer science, obviously, but also biology, ecology, political theory, even theology and cosmology in the classical sense. It requires being willing to question assumptions that seem obvious, to entertain ideas that seem strange, to sit with ambiguity.

## Extrinsic Philosophy and Allocentrism

One concept that I think is helpful here is what I call "extrinsic philosophy"—philosophy that starts not from human consciousness but from the world outside us. Traditional philosophy, especially in the modern period, has been very intrinsic. It starts with the thinking subject, with consciousness, with the "I think." Descartes's cogito is the paradigm case: "I think, therefore I am."

But what if we flip this? What if we start not with the thinking subject but with the world that precedes thought? What if we start with the planet, with its computational processes, with the flows of matter and energy and information that make thought possible in the first place?

This is what I mean by extrinsic philosophy. It's allocentric rather than anthropocentric—it centers the "other" rather than the self. And I think this shift is crucial for dealing with planetary computation because it allows us to see human intelligence not as the center of the universe but as one form of intelligence among many, one computational process among many.

This doesn't mean humans aren't important. It means we need to understand ourselves differently—not as the masters of nature but as participants in a larger computational ecology. Not as the only agents that matter but as one species of agent among many, including both non-human biological agents and artificial agents.

This has practical implications. It means we need to design AI systems not just to serve human purposes but to fit into this larger ecology. It means we need to think about governance not just as human governance but as coordination among different types of agents. It means we need ethics that can account for non-human welfare.

## The Pharmakón of AI

Let me address something directly. There's a lot of anxiety about AI right now, and much of it is justified. We should be concerned about bias in algorithms, about concentration of power, about autonomous weapons, about unemployment, about existential risk. These are real problems that need serious attention.

But I also think we need to be careful about falling into either techno-utopianism or techno-dystopian panic. Both of these positions assume that technology is something that happens to us, that we're passive recipients of technological change. And that's not right.

Jacques Derrida had this concept of the pharmakón—something that is both remedy and poison, whose effects depend on dosage and context. I think AI is a pharmakón in this sense. It has enormous potential for harm, but also enormous potential for benefit. And crucially, which it becomes depends on what we do.

If we approach AI with a framework of planetary computation, with an understanding of it as part of a larger computational ecology, then we can make better decisions about how to develop it, deploy it, govern it. We can ask: How does this system fit into the larger computational infrastructure of the planet? What are its effects on other systems? How does it change the balance of power and agency?

These aren't just technical questions. They're fundamentally philosophical and political questions. And they require us to develop new concepts, new frameworks, new ways of thinking.

## Pedagogy and the Next Generation

Let me share something more concrete. I have a son, and I think a lot about what world he's inheriting and what he needs to know to navigate it. One thing that strikes me is how much we know that hasn't yet percolated into how we teach.

For example, we know so much about neuroscience now—about how the brain works, about how learning happens, about the computational processes underlying cognition. And yet, most philosophy departments don't teach any neuroscience. You have a whole department about how we think and how we might think, and they completely ignore what we know about how we think. That seems bizarre to me.

Or consider AI in education. At my son's high school, they're forbidden to use AI. And I understand the concern—we want students to learn to think for themselves, not to outsource thinking to machines. But I remember when I went to high school, if you used a calculator in class, you got in trouble. And now, if you don't use a calculator in class, you get in trouble.

The result of putting calculators in the classroom is that students take calculus a year earlier than they did before because they don't have to do busy work with arithmetic—they can focus on the concepts. So the question for AI is: What's the analogy? Given a certain degree of inevitability, what are the ways in which we can use these tools to help students learn more, think more deeply, engage with more complex problems?

I'm on a University of California committee for what the policy should be for AI in the classroom at the UC, and frankly, I spend time in those meetings feeling like pulling my hair out because most of the discussion is about how to prevent and forbid AI use. As if that's even possible. As if that's the right question.

The way I see it, as a teacher, it's up to me to presume that my undergraduates are using large language models and to build better assignments. The types of things people are capable of doing with these kinds of tools—we need to rethink what we ask them to do. This isn't about making things easier; it's about making things more sophisticated, more complex, more aligned with the world they're actually living in.

## Institutions and Durability

Someone asked me about institutions and how to read them through the lens of planetary computation. This is a good question. The value of institutions, to a certain extent, is their durability. They're ways in which people can come together to construct systems for solving particular kinds of problems, for cultivating particular kinds of questions, that grow over time.

Every iteration by which an institution runs its cycle, it evolves. But it evolves in a way that has scaffolds internal to itself. It does something that becomes a component to something it does later, that becomes a component to something even more complex later. That's evolution, and it takes time.

In a world where things seem to be liquefying at such a pace, pushing against things that have more durability remains important. But I want to extend the concept of institution. It's not only things like universities or organizations that have boards of directors. There are technical institutions too—forms of structure that operate in a similar kind of way.

We were talking before the event about the metric system as a kind of platform. What the metric system allows you to do is not have to decide how wide things are going to be. It becomes a way in which you don't have to do that work—you can get on with it. The best kinds of institutions are ones that are not only doing work but doing work so that everyone else can get on with it.

This is relevant to planetary computation because we need institutions—both human and technical—that can provide stable platforms for coordination across different scales and different types of agents. We need protocols, standards, frameworks that allow natural, human, and artificial computation to interface productively.

## The Work of Antikythera

This brings me to Antikythera, the think tank I direct. We named it after the Antikythera mechanism because we see our work as trying to do something similar—not to build devices for calculating planetary movements, but to build conceptual frameworks for understanding planetary computation.

We're trying to work across disciplines, to bring together people from computer science, philosophy, social science, design, policy. We're trying to experiment with different concepts and frames. We're trying to ask: What are the right questions? What are the productive ways of thinking about these phenomena?

And crucially, we're trying to do this outside the traditional university structure, though not entirely separate from it. The economic format of the university makes it very difficult to do this kind of work. You're trapped in disciplinary silos, you're accountable to metrics that don't capture the value of conceptual innovation, you're subject to political pressures that can constrain thinking.

So we've structured Antikythera as what I call a "pirate ship"—moving in and out of different ports, shuttling ideas and people between different contexts, maintaining the freedom to explore without the constraints of traditional academic structures.

This isn't a criticism of universities—I'm a professor at UC San Diego, and there's much that's valuable about the university. But for the work of developing new paradigms, new conceptual frameworks, we need different structures. We need institutions that can move fast, that can bring together unusual combinations of people, that can take intellectual risks.

## Conclusion: The Task Ahead

Let me conclude by returning to the beginning. We are in a pre-paradigmatic moment. Our technology has outpaced our theories. The task of philosophy is to catch up.

This means developing frameworks for understanding computation not as a human invention but as a planetary phenomenon. It means rethinking intelligence to account for multiple forms and scales. It means developing political and ethical frameworks that can deal with non-human agents. It means building institutions that can support this work.

It's a daunting task, but it's also an exciting one. We're living through a moment of genuine conceptual possibility. The old paradigms are breaking down, but the new ones haven't yet crystallized. This is the moment for philosophical invention, for bold thinking, for experimental concepts.

And it's not just philosophers who need to engage in this work. It's scientists, engineers, policymakers, artists, citizens. Everyone who's affected by planetary computation—which is to say, everyone—has a stake in developing better ways of thinking about it.

The Long Now Foundation is premised on the idea that we need to think at longer time scales. I couldn't agree more. Planetary computation operates at time scales that range from the microseconds of electronic circuits to the millions of years of evolutionary change. To understand it, to shape it, to live with it responsibly, we need to develop the capacity for thinking across these scales.

So this is an invitation. An invitation to engage with these questions, to experiment with these concepts, to help build the paradigm that we need for the century ahead. The work has begun, but there's much more to do.

Thank you.

---

## Q&A Session

**Question: AI and cultural evolution—can you talk about generative AI as part of how collective intelligence models itself over the long term?**

I think the discussion around generative AI needs a lot more long-term thinking. More generally, I think we should imagine that everything you do from when you wake up in the morning to when you go to sleep is training data for the future's model of the past. It's a big responsibility that you have. It's a bit like living in the third person.

But I think this demonstrates a way in which we should think about generative AI not as a kind of instrument or tool but rather as a way in which collective intelligence is modeling itself over the long term. These models are trained on the accumulated output of human culture—text, images, code, everything we've put into digital form. And then they produce new outputs that feed back into that culture.

This creates a feedback loop. The models learn from us, but we also learn from them. They reveal patterns in our culture that we might not have noticed. They generate variations that influence what we create next. Over time, this becomes a genuine co-evolution between human and artificial intelligence.

And I think we're only beginning to understand the implications of this. We tend to think of these models as capturing a snapshot of culture at a particular moment. But culture is always changing, and these models are part of what's changing it. They're not just mirrors—they're active participants in cultural evolution.

**Question: You used the term "extrinsic philosophy" and "allocentric" as opposed to "anthropocentric." What are some ways that culture could be updated to reflect allocentrism instead of anthropocentrism?**

First, let me think about this in terms of my son. I think the issue is that we know so much more now in ways that haven't really percolated in.

There are so many ways to think about how this would work. Even just very obvious things like pedagogy. I find it completely bizarre, but also amazing, that for the most part—UCSD is a bit of an exception, and Pittsburgh too—most philosophy departments don't teach any neuroscience. Why would you have a whole department about how we think and how we might think and completely ignore what we know about how we think? You kind of look around and find more of these kinds of things.

How we teach astronomy, how we teach neuroscience, how we teach genomics—all of these are revealing to us that we are not at the center of things in the ways we thought. We're part of systems that are much larger than us, that precede us, that will continue after us.

In terms of practical updates to culture, this means designing systems that account for non-human welfare. It means making decisions that consider impacts on ecosystems, on other species, on future generations. It means building AI systems that don't just optimize for human preferences but that consider broader ecological and social effects.

It means changing how we teach the next generation—not just teaching them to see themselves as masters of nature but as participants in a larger planetary system. It means cultivating a kind of ecological consciousness that recognizes our interdependence with other forms of life and intelligence.

**Question: Why have you constructed Antikythera the way you have, instead of within a traditional university context?**

The main answer is that it would be absolutely impossible. There's just no way. There's no economic format for doing the kind of work that we do within the university. I tried.

I am in academia—I'm a professor at the University of California San Diego. But the work we do at Antikythera requires a different structure. We need to be able to move quickly, to bring together people from different disciplines without the constraints of departmental politics. We need to be able to take intellectual risks without worrying about tenure committees or grant cycles.

So we've positioned ourselves as a kind of pirate ship that moves in and out of different ports. We can shuttle ideas between academia, industry, policy, art. We can maintain relationships with universities without being constrained by them.

This isn't to say the university has no value. It does. But for the work of developing genuinely new paradigms, we need structures that are more flexible, more experimental, more willing to fail.

**Question: How might you read institutions through the lens of planetary computation, and what could be the future of institutions?**

The value of institutions is their durability. They're ways in which people can come together to construct systems for solving particular kinds of problems, for cultivating particular kinds of questions, that grow over time. Every iteration by which an institution runs its cycle, it evolves. But it evolves in a way that has scaffolds internal to itself. It does something that becomes a component to something it does later, that becomes a component to something even more complex later. That is evolution, and it takes time.

In a world where things seem to be liquefying at such a pace, pushing against things that have more durability remains important.

But I would extend the notion of institution. It's not only things like organizations that have boards of directors. There are technical institutions—forms of structure that operate in a very similar way. We were talking about the metric system before. It's essentially a platform. What it allows you to do is not have to decide how wide things are going to be. It becomes a way in which you don't have to do that work. You can get on with it.

The best kinds of institutions are ones that are not only doing the work but doing the work so that everyone else can get on with it. That's what we need—institutions, both human and technical, that provide stable platforms for coordination at planetary scale.

---

*Note: All preview content, advertising, and promotional material has been removed from this transcript. Filler words and verbal disfluencies have been eliminated while preserving the speaker's conceptual structure and intellectual cadence. The Q&A session has been integrated to maintain the complete arc of the presentation.*