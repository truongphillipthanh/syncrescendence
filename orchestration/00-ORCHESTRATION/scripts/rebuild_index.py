#!/usr/bin/env python3
"""Rebuild DYN-SOURCES.csv and 8 MOC files from all SOURCE-*.md files."""

import os
import re
import csv
import yaml
from collections import defaultdict
from pathlib import Path

SOURCES_DIR = Path("/Users/system/Desktop/syncrescendence/sources")
META_DIR = SOURCES_DIR / "_meta"
INDEX_DIR = SOURCES_DIR / "_index"

def parse_source(filepath):
    """Parse frontmatter and transcript info from a SOURCE file."""
    with open(filepath, "r", encoding="utf-8", errors="replace") as f:
        content = f.read()

    # Parse YAML frontmatter
    fm = {}
    m = re.match(r"^---\n(.*?)\n---", content, re.DOTALL)
    if m:
        try:
            fm = yaml.safe_load(m.group(1)) or {}
        except Exception:
            pass

    # Transcript detection
    has_transcript = False
    transcript_chars = 0
    tx_match = re.search(r"^## Transcript\s*\n", content, re.MULTILINE)
    if tx_match:
        after = content[tx_match.end():].strip()
        if after:
            has_transcript = True
            transcript_chars = len(after)

    # Normalize topics
    topics_raw = fm.get("topics", [])
    if isinstance(topics_raw, list):
        topics = [str(t).strip().strip('"').strip("'") for t in topics_raw if t]
    elif isinstance(topics_raw, str):
        topics = [t.strip() for t in topics_raw.split(",") if t.strip()]
    else:
        topics = []

    # Normalize date
    date_pub = fm.get("date_published", fm.get("published_date", ""))
    if date_pub is None:
        date_pub = ""
    date_pub = str(date_pub).strip()

    return {
        "filename": filepath.name,
        "id": str(fm.get("id", "")),
        "title": str(fm.get("title", "")).strip().strip('"'),
        "platform": str(fm.get("platform", "")).strip(),
        "format": str(fm.get("format", "")).strip(),
        "creator": str(fm.get("creator", "")).strip().strip('"'),
        "date_published": date_pub,
        "signal_tier": str(fm.get("signal_tier", "")).strip(),
        "teleology": str(fm.get("teleology", "")).strip(),
        "notebooklm_category": str(fm.get("notebooklm_category", "")).strip(),
        "url": str(fm.get("url", "")).strip().strip('"'),
        "topics": topics,
        "has_transcript": has_transcript,
        "transcript_chars": transcript_chars,
        "synopsis": str(fm.get("synopsis", "") or "").strip().strip('"'),
    }


def sort_key_date(rec):
    """Sort by date descending; unknown dates last."""
    d = rec["date_published"]
    return d if d and d != "None" else "0000-00-00"


def wikilink(filename):
    return f"[[{filename.replace('.md', '')}]]"


def write_moc(path, title, groups, sort_groups=True):
    """Write a MOC file with grouped wikilinks and counts."""
    with open(path, "w") as f:
        f.write(f"# {title}\n\n")
        f.write(f"_Auto-generated by rebuild_index.py â€” {len(sum(groups.values(), []))} sources_\n\n")
        keys = sorted(groups.keys()) if sort_groups else list(groups.keys())
        for key in keys:
            items = groups[key]
            f.write(f"## {key} ({len(items)})\n\n")
            for item in sorted(items):
                f.write(f"- {wikilink(item)}\n")
            f.write("\n")


def main():
    files = sorted(SOURCES_DIR.glob("SOURCE-*.md"))
    print(f"Scanning {len(files)} files...")

    records = []
    for fp in files:
        records.append(parse_source(fp))

    # Sort by date descending
    records.sort(key=sort_key_date, reverse=True)

    # --- Part 1: CSV ---
    META_DIR.mkdir(exist_ok=True)
    csv_path = META_DIR / "DYN-SOURCES.csv"
    fieldnames = [
        "filename", "id", "title", "platform", "format", "creator",
        "date_published", "signal_tier", "teleology", "notebooklm_category",
        "url", "topics", "has_transcript", "transcript_chars", "synopsis"
    ]
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for r in records:
            row = dict(r)
            row["topics"] = "; ".join(r["topics"])
            row["has_transcript"] = "TRUE" if r["has_transcript"] else "FALSE"
            writer.writerow(row)
    print(f"Wrote {csv_path} ({len(records)} rows)")

    # --- Part 2: MOC files ---
    INDEX_DIR.mkdir(exist_ok=True)

    # Group builders
    by_topic = defaultdict(list)
    by_creator = defaultdict(list)
    by_teleology = defaultdict(list)
    by_platform = defaultdict(list)
    by_signal = defaultdict(list)
    by_date = defaultdict(list)
    by_nlm = defaultdict(list)
    by_transcript = defaultdict(list)

    for r in records:
        fn = r["filename"]
        for t in r["topics"]:
            if t:
                by_topic[t].append(fn)
        by_creator[r["creator"] or "(unknown)"].append(fn)
        by_teleology[r["teleology"] or "(unset)"].append(fn)
        by_platform[r["platform"] or "(unknown)"].append(fn)
        by_signal[r["signal_tier"] or "(unset)"].append(fn)
        # Chronological: group by year-month
        d = r["date_published"]
        if d and d != "None" and len(d) >= 7:
            ym = d[:7]
        else:
            ym = "(undated)"
        by_date[ym].append(fn)
        by_nlm[r["notebooklm_category"] or "(unset)"].append(fn)
        by_transcript["has-transcript" if r["has_transcript"] else "no-transcript"].append(fn)

    write_moc(INDEX_DIR / "MOC-by-topic.md", "Sources by Topic", dict(by_topic))
    write_moc(INDEX_DIR / "MOC-by-creator.md", "Sources by Creator", dict(by_creator))
    write_moc(INDEX_DIR / "MOC-by-teleology.md", "Sources by Teleology", dict(by_teleology))
    write_moc(INDEX_DIR / "MOC-by-platform.md", "Sources by Platform", dict(by_platform))
    write_moc(INDEX_DIR / "MOC-by-signal-tier.md", "Sources by Signal Tier", dict(by_signal))
    write_moc(INDEX_DIR / "MOC-chronological.md", "Sources Chronological", dict(by_date), sort_groups=True)
    write_moc(INDEX_DIR / "MOC-notebooklm.md", "Sources by NotebookLM Category", dict(by_nlm))
    write_moc(INDEX_DIR / "MOC-by-transcript.md", "Sources by Transcript Status", dict(by_transcript))
    print(f"Wrote 8 MOC files to {INDEX_DIR}/")

    # --- Part 3: Summary stats ---
    print("\n" + "=" * 60)
    print("SUMMARY STATS")
    print("=" * 60)
    print(f"Total SOURCE files: {len(records)}")

    print(f"\nBy Platform:")
    for k in sorted(by_platform.keys()):
        print(f"  {k}: {len(by_platform[k])}")

    print(f"\nBy Format:")
    by_format = defaultdict(int)
    for r in records:
        by_format[r["format"] or "(unknown)"] += 1
    for k in sorted(by_format.keys()):
        print(f"  {k}: {by_format[k]}")

    tx_count = sum(1 for r in records if r["has_transcript"])
    total_tx_chars = sum(r["transcript_chars"] for r in records)
    print(f"\nTranscript coverage: {tx_count}/{len(records)} ({100*tx_count/len(records):.1f}%)")
    print(f"Total transcript chars: {total_tx_chars:,}")

    dates = [r["date_published"] for r in records if r["date_published"] and r["date_published"] != "None"]
    if dates:
        print(f"\nDate range: {min(dates)} to {max(dates)}")

    print("\nDone.")


if __name__ == "__main__":
    main()
